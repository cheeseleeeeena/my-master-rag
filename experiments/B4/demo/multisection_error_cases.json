{
    "notitle": {
        "b584739622d0c53830e60430b13fd3ae6ff43669": {
            "question_text": "What are the evaluation metrics and criteria used to evaluate the model performance?",
            "from_paper": "1911.10742",
            "gold": [
                "Experiments ::: Automatic Evaluation Metrics",
                "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.",
                "Response-Intent Prediction (RIP) $\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\%$ accuracy and the semantic slot predictor achieves $77\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).",
                "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.",
                "Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.",
                "Fluency Fluency is used to explore different models' language generation quality.",
                "Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.",
                "Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.",
                "Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.",
                "Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score."
            ],
            "gold_section": [
                "Experiments ::: Automatic Evaluation Metrics",
                "Experiments ::: Human Evaluation Metrics"
            ],
            "predicted": [
                "Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.",
                "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.",
                "Table TABREF19 presents the main experiment results on AntiScam dataset, for both automatic evaluation metrics and human evaluation metrics. The experiment results on PersuasionForGood are shown in Table TABREF23. We observe that MISSA outperforms two baseline models (TransferTransfo and hybrid model) on almost all the metrics on both datasets. For further analysis, examples of real dialogs from the human evaluation are presented in Table TABREF21."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Experiments ::: Automatic Evaluation Metrics",
                "Experiments ::: Human Evaluation Metrics"
            ]
        },
        "bc8526d4805e2554adb2e9c01736d3f3a3b19895": {
            "question_text": "What baselines did they compare with?",
            "from_paper": "1604.02038",
            "gold": [
                "The following baselines were used in our experiments:",
                "LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.",
                "Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.",
                "HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.",
                "GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own."
            ],
            "gold_section": [
                "Quantitative Results"
            ],
            "predicted": [
                "The following baselines were used in our experiments:",
                "We base our experiments on two benchmark datasets:",
                "We have conducted experiments to compare SLRTM with several strong topic model baselines on two tasks: generative model evaluation (i.e. test set perplexity) and document classification. The results on several benchmark datasets quantitatively demonstrate SLRTM's advantages in modeling documents. We further provide some qualitative results on topic2sentence, the generated sentences for different topics clearly demonstrate the power of SLRTM in topic-sensitive short text conversations."
            ],
            "predicted_section": [
                "Quantitative Results",
                "Introduction"
            ]
        },
        "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd": {
            "question_text": "Which NER dataset do they use?",
            "from_paper": "1911.04474",
            "gold": [
                "We evaluate our model in two English NER datasets and four Chinese NER datasets.",
                "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.",
                "(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.",
                "(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.",
                "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.",
                "(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.",
                "(6) Resume NER was annotated by BIBREF33."
            ],
            "gold_section": [
                "Experiment ::: Data"
            ],
            "predicted": [
                "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.",
                "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.",
                "We first present our results in the four Chinese NER datasets. Since Chinese NER is directly based on the characters, it is more straightforward to show the abilities of different models without considering the influence of word representation."
            ],
            "predicted_section": [
                "Experiment ::: Data",
                "Experiment ::: Results on Chinese NER Datasets"
            ]
        },
        "371433bd3fb5042bacec4dfad3cfff66147c14f0": {
            "question_text": "How do data-driven models usually respond to abuse?",
            "from_paper": "1909.04387",
            "gold": [
                "4 Data-driven approaches:",
                "Cleverbot BIBREF12;",
                "NeuralConvo BIBREF13, a re-implementation of BIBREF14;",
                "an implementation of BIBREF15's Information Retrieval approach;",
                "a vanilla Seq2Seq model trained on clean Reddit data BIBREF1.",
                "Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the \u201cdeflection\u201d strategy, such as \u201cWhy do you ask?\u201d. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the \u201cclean\u201d seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users."
            ],
            "gold_section": [
                "Data Collection",
                "Results ::: Systems"
            ],
            "predicted": [
                "Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3) All system were rated significantly higher than our negative adult-only baselines - except two data-driven systems, one of which is a Seq2Seq model trained on \u201cclean\" data where all utterances containing abusive words were removed BIBREF1. This leads us to believe that data-driven response generation need more effective control mechanisms BIBREF30.",
                "Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them.",
                "This paper presents the first user study on perceived appropriateness of system responses after verbal abuse. We put strategies used by state-of-the-art systems to the test in a large-scale, crowd-sourced evaluation. The full annotated corpus contains 2441 system replies, categorised into 14 response types, which were evaluated by 472 raters - resulting in 7.7 ratings per reply."
            ],
            "predicted_section": [
                "Conclusion",
                "Introduction"
            ]
        },
        "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162": {
            "question_text": "Was the automatic annotation evaluated?",
            "from_paper": "2003.13016",
            "gold": [
                "The dataset includes two different versions of annotations, one with a set of 19 fine-grained semantic classes and another one with a set of 7 coarse-grained classes (Table ). There are 53,632 annotated entities in total, the majority of which (74.34 %) are legal entities, the others are person, location and organization (25.66 %). Overall, the most frequent entities are law GS (34.53 %) and court decision RS (23.46 %). The other legal classes (ordinance VO, European legal norm EUN, regulation VS, contract VT, and legal literature LIT) are much less frequent (1\u20136 % each). Even less frequent (less than 1 %) are lawyer AN, street STR, landscape LDS, and brand MRK.",
                "The dataset was thoroughly evaluated, see leitner2019fine for more details. As state of the art models, Conditional Random Fields (CRFs) and bidirectional Long-Short Term Memory Networks (BiLSTMs) were tested with the two variants of annotation. For CRFs, these are: CRF-F (with features), CRF-FG (with features and gazetteers), CRF-FGL (with features, gazetteers and lookup). For BiLSTM, we used models with pre-trained word embeddings BIBREF22: BiLSTM-CRF BIBREF23, BiLSTM-CRF+ with character embeddings from BiLSTM BIBREF24, and BiLSTM-CNN-CRF with character embeddings from CNN BIBREF25. To evaluate the performance we used stratified 10-fold cross-validation. As expected, BiLSTMs perform best (see Table ). The F$_1$ score for the fine-grained classification reaches 95.46 and 95.95 for the coarse-grained one. CRFs reach up to 93.23 F$_1$ for the fine-grained classes and 93.22 F$_1$ for the coarse-grained ones. Both models perform best for judge, court and law."
            ],
            "gold_section": [
                "Evaluation",
                "Description of the Dataset ::: Annotation of Named Entities"
            ],
            "predicted": [
                "The dataset was originally annotated by the first author. To evaluate and potentially improve the quality of the annotations, part of the dataset was annotated by a second linguist (using the annotation guidelines specifically prepared for its construction). We selected a small part that could be annotated in approx. two weeks. For the sentence extraction we paid special attention to the anonymised mentions of person, location or organization entities, because these are usually explained at their first mention. The resulting sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a token basis. All class labels were taken into account in accordance with the IOB2 scheme BIBREF18. The inter-annotator agreement is 0.89, i. e., there is mostly very good agreement between the two annotators. Differences were in the identification of court decision and legal literature. Some unusual references of court decision (consisting only of decision type, court, date, file number) were not annotated such as `Urteil des Landgerichts Darmstadt vom 16. April 2014 \u2013 7 S 8/13 \u2013'. Apart from missing legal literature annotations, author names and law designations were annotated according to their categories (i. e., `Schoch, in: Schoch/Schneider/Bier, VwGO \u00a7 123 Rn. 35', `Bekanntmachung des BMG gem\u00e4\u00df \u00a7\u00a7 295 und 301 SGB V zur Anwendung des OPS vom 21.10.2010').",
                "The second annotator had difficulties annotating the class law, not all instances were identified (`\u00a7 272 Abs. 1a und 1b HGB', `\u00a7 3c Abs. 2 Satz 1 EStG'), others only partially (`\u00a7 716 in Verbindung mit' in `\u00a7 716 in Verbindung mit \u00a7\u00a7 321 , 711 ZPO'). Some titles of contract were not recognised and annotated (`BAT', `TV-L', `TV\u00dc-L\u00e4nder' etc.).",
                "This evaluation has revealed deficiencies in the annotation guidelines, especially regarding court decision and legal literature as well as non-entities. It would also be helpful for the identification and classification to list well-known sources of law, court decision, legal literature etc."
            ],
            "predicted_section": [
                "Description of the Dataset ::: Annotation of Named Entities"
            ]
        },
        "780c7993d446cd63907bb38992a60bbac9cb42b1": {
            "question_text": "What language are the captions in?",
            "from_paper": "1909.09070",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "All the captions show a strong semantic correspondence with their associated figures. Figure FIGREF29 shows the activation heatmaps for two sample captions, calculated on the embeddings layer of the language subnetwork. The upper one corresponds to the fourth column left-right and third figure top-down in figure FIGREF28. Its caption reads: \"The Aliev-Panfilov model with $\\alpha =0.01$...The phase portrait depicts trajectories for distinct initial values $\\varphi _0$ and $r_0$...\". Below, (first column, fourth figure in figure FIGREF28): \"Relative protein levels of ubiquitin-protein conjugates in M. quadriceps...A representative immunoblot specific to ubiquitin...\". Consistently with our analysis, activation focuses on the most relevant tokens for each text feature: \"Aliev-Panfilov model\" and \"immunoblot\", respectively.",
                "The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like.",
                "We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie charts, are used to showcase data in any field from health to engineering; figures and natural images appear indistinctly, etc. Also, note that we only rely on the actual figure, not the text fragment where it is mentioned in the paper."
            ],
            "predicted_section": [
                "Qualitative Analysis",
                "Results and Discussion ::: Caption and Figure Classification",
                "Figure-Caption Correspondence"
            ]
        },
        "c58ef13abe5fa91a761362ca962d7290312c74e4": {
            "question_text": "What aspects are considered?",
            "from_paper": "1908.11049",
            "gold": [
                "Treating hate speech classification as a binary task may not be enough to inspect the motivation and the behavior of the users promoting it and, how people would react to it. For instance, the hateful tweets presented in Figure FIGREF5 show toxicity directed towards different targets, with or without using slurs, and generating several types of reactions. We believe that, in order to balance between truth and subjectivity, there are at least five important aspects in hate speech analysis. Hence, our annotations indicate (a) whether the text is direct or indirect; (b) if it is offensive, disrespectful, hateful, fearful out of ignorance, abusive, or normal; (c) the attribute based on which it discriminates against an individual or a group of people; (d) the name of this group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments. To the best of our knowledge there are no other hate speech datasets that attempt to capture fear out of ignorance in hateful tweets or examine how people react to hate speech. We claim that our multi-aspect annotation schema would provide a valuable insight into several linguistic and cultural differences and bias in hate speech."
            ],
            "gold_section": [
                "Introduction"
            ],
            "predicted": [
                "We report and discuss the results of five classification tasks: (1) the directness of the speech, (2) the hostility type of the tweet, (3) the discriminating target attribute, (4) the target group, and (5) the annotator's sentiment.",
                "In this section, we present our data collection methodology and annotation process.",
                "We report both the micro and macro-F1 scores of the different classification tasks in Tables TABREF27 and TABREF28. Majority refers to labeling based on the majority label, LR to logistic regression, STSL to single task single language models, STML to single task multilingual models, and MTML to multitask multilingual models."
            ],
            "predicted_section": [
                "Dataset",
                "Experiments",
                "Experiments ::: Results and Analysis"
            ]
        },
        "5ed02ae6c534cd49d405489990f0e4ba0330ff1b": {
            "question_text": "Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?",
            "from_paper": "2004.04124",
            "gold": [
                "The overall pipeline of LadaBERT (Lightweight Adaptation of BERT) is illustrated in Figure FIGREF8. As shown in the figure, the pre-trained BERT model (e.g., BERT-Base) is served as the teacher as well as the initial status of the student model. Then, the student model is compressed towards smaller parameter size through a hybrid model compression framework in an iterative manner until the target compression ratio is reached. Concretely, in each iteration, the parameter size of student model is first reduced by $1-\\Delta $ based on weight pruning and matrix factorization, and then the parameters are fine-tuned by the loss function of knowledge distillation. The motivation behind is that matrix factorization and weight pruning are complementary with each other. Matrix factorization calculates the optimal approximation under a certain rank, while weight pruning introduces additional sparsity to the decomposed matrices. Moreover, weight pruning and matrix factorization generates better initial and intermediate status of the student model, which improve the efficiency and effectiveness of knowledge distillation. In the following subsections, we will introduce the algorithms in detail.",
                "The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation."
            ],
            "gold_section": [
                "Lightweight Adaptation of BERT ::: Overview",
                "Experiments ::: Performance Comparison"
            ],
            "predicted": [
                "With model size of $2.5\\times $ reduction, LadaBERT-1 performs significantly better than BERT-PKD, boosting the performance by relative 8.9, 8.1, 6.1, 3.8 and 5.8 percentages on MNLI-m, MNLI-mm, SST-2, QQP and QNLI datasets respectively. Recall that BERT-PKD initializes the student model by selecting 3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have huge impact on the model performance, which is hard to be recovered by knowledge distillation. On the other hand, LadaBERT generates the student model by iterative pruning on the pre-trained teacher. In this way, the original knowledge in the teacher model can be preserved to the largest extent, and the benefit of which is complementary to knowledge distillation.",
                "To further demonstrate the efficiency of LadaBERT, we visualize the learning curves on MNLI-m and QQP datasets in Figure FIGREF42 and FIGREF42, where LadaBERT-3 is compared to the strongest baseline, TinyBERT, under $7.5 \\times $ compression ratio. As shown in the figures, LadaBERT-3 achieves good performances much faster and results in a better convergence point. After training $2 \\times 10^4$ steps (batches) on MNLI-m dataset, the performance of LadaBERT-3 is already comparable to TinyBERT after convergence (approximately $2 \\times 10^5$ steps), achieving nearly $10 \\times $ acceleration. And on QQP dataset, both performance improvement and training speed acceleration is very significant. This clearly shows the superiority of combining matrix factorization, weight pruning and knowledge distillation in a reinforce manner. Instead, TinyBERT is based on pure knowledge distillation, so the learning speed is much slower.",
                "We conduct extensive experiments on five public datasets of natural language understanding. As an example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is illustrated in Figure FIGREF1. We can see that LadaBERT outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and LadaBERT-3 outperforms TinyBERT under $7.5\\times $ compression ratio while the training speed is accelerated by an order of magnitude."
            ],
            "predicted_section": [
                "Experiments ::: Learning curve comparison",
                "Experiments ::: Performance Comparison",
                "Introduction"
            ]
        },
        "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea": {
            "question_text": "What are the parts of the \"multimodal\" resources?",
            "from_paper": "1912.02866",
            "gold": [
                "From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing."
            ],
            "gold_section": [
                "Introduction"
            ],
            "predicted": [
                "This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4",
                "Unlike many other areas, the study of diagrammatic representations is particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14.",
                "Understanding and making inferences about the structure of diagrams and other forms of multimodal discourse may be broadly conceptualised as multimodal discourse parsing. Recent examples of work in this area include alikhanietal2019 and ottoetal2019, who model discourse relations between natural language and photographic images, drawing on linguistic theories of coherence and text\u2013image relations, respectively. In most cases, however, predicting a single discourse relation covers only a part of the discourse structure. sachanetal2019 note that there is a need for comprehensive theories and models of multimodal communication, as they can be used to rethink tasks that have been previously considered only from the perspective of natural language processing."
            ],
            "predicted_section": [
                "Introduction",
                "Data"
            ]
        },
        "a57e266c936e438aeeab5e8d20d9edd1c15a32ee": {
            "question_text": "Are annotators familiar with the science topics annotated?",
            "from_paper": "1912.02866",
            "gold": [
                "The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.",
                "AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:"
            ],
            "gold_section": [
                "Data ::: Crowd-sourced Annotations from AI2D",
                "Data ::: Expert Annotations from AI2D-RST"
            ],
            "predicted": [
                "The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema.",
                "This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer.",
                "Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24."
            ],
            "predicted_section": [
                "Discussion",
                "Introduction"
            ]
        },
        "088d42ecb1e15515f6a97a0da2fed81b61d61a23": {
            "question_text": "Is this more effective for low-resource than high-resource languages?",
            "from_paper": "1909.00437",
            "gold": [
                "We use BLEU score BIBREF15 to evaluate the quality of our translation model(s). Our mNMT model performs worse than the bilingual baseline on high resource language pairs but improves upon it on low resource language pairs. The average drop in BLEU score on 204 language pairs as compared to bilingual baselines is just 0.25 BLEU. This is impressive considering we are comparing one multilingual model to 204 different bilingual models. Table TABREF14 compares the BLEU scores achieved by mNMT to that of the bilingual baselines on 10 representative language pairs. These scores are obtained on an internal evaluation set which contains around 5k examples per language pair."
            ],
            "gold_section": [
                "Massively Multilingual Neural Machine Translation Model ::: Pre-training ::: Model quality"
            ],
            "predicted": [
                "We train a massively multilingual NMT system using parallel data from 103 languages and exploit representations extracted from the encoder for cross-lingual transfer on various classification and sequence tagging tasks spanning over 50 languages. We find that the positive language transfer visible in improved translation quality for low resource languages is also reflected in the cross-lingual transferability of the extracted representations. The gains observed on various tasks over mBERT suggest that the translation objective is competitive with specialized approaches to learn cross-lingual embeddings.",
                "We find that there is a trade off between the number of languages in the multilingual model and efficiency of the learned representations due to the limited capacity. Scaling up the model to include more languages without diminishing transfer learning capability is a direction for future work. Finally, one could also consider integrating mBERT's objective with the translation objective to pre-train the mNMT system.",
                "Another setting of importance is the in-language training where instead of training one model for each language, we concatenate all the data and train one model jointly on all languages. We perform this experiment on the POS tagging dataset with 48 languages and report results in Table TABREF35. We observe that MMTE performance is on par with mBERT. We also find that the 48 language average improves by 0.2 points as compared to the one model per language setting in Table TABREF27."
            ],
            "predicted_section": [
                "Conclusion and Future Work",
                "Analysis ::: One Model for all Languages"
            ]
        },
        "cfdd583d01abaca923f5c466bb20e1d4b8c749ff": {
            "question_text": "what context aware models were experimented?",
            "from_paper": "1810.02268",
            "gold": [
                "This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.",
                "baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.",
                "concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .",
                "s-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.",
                "s-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.",
                "s-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .",
                "concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.",
                "concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .",
                "BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work."
            ],
            "gold_section": [
                "Recurrent Models",
                "Context-Aware NMT Models",
                "Transformer Models"
            ],
            "predicted": [
                "This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.",
                "We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size context models BIBREF7 , BIBREF10 is interesting future work.",
                "The following models are taken, or slightly adapted, from BIBREF9 . For this reason, we give only a very short description of them here and the reader is referred to their work for details."
            ],
            "predicted_section": [
                "Recurrent Models",
                "Conclusions",
                "Context-Aware NMT Models"
            ]
        },
        "91e361e85c6d3884694f3c747d61bfcef171bab0": {
            "question_text": "How do they obtain the entity linking results in their model?",
            "from_paper": "1909.12079",
            "gold": [
                "Given a piece of text and the span of an entity mention in this text, fine-grained entity typing (FET) is the task of assigning fine-grained type labels to the mention BIBREF0. The assigned labels should be context dependent BIBREF1. For example, in the sentence \u201cTrump threatens to pull US out of World Trade Organization,\u201d the mention \u201cTrump\u201d should be labeled as /person and /person/politician, although Donald Trump also had other occupations such as businessman, TV personality, etc.",
                "Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to make typing decisions because if a mention is correctly linked to its target entity, we can directly obtain the type information about this entity in the knowledge base (KB). For example, in the sentence \u201cThere were some great discussions on a variety of issues facing Federal Way,\u201d the mention \u201cFederal Way\u201d may be incorrectly labeled as a company by some FET models. Such a mistake can be avoided after linking it to the city Federal Way, Washington. For cases that require the understanding of the context, using entity linking results is also beneficial. In the aforementioned example where \u201cTrump\u201d is the mention, obtaining all the types of Donald Trump in the knowledge base (e.g., politician, businessman, TV personality, etc.) is still informative for inferring the correct type (i.e., politician) that fits the context, since they narrows the possible labels down.",
                "In this paper, we use a simple EL algorithm that directly links the mention to the entity with the greatest commonness score. Commonness BIBREF17, BIBREF18 is calculated base on the anchor links in Wikipedia. It estimates the probability of an entity given only the mention string. In our FET approach, the commonness score is also used as the confidence on the linking result (i.e., the $\\mathbf {g}$ used in the prediction part of Subsection SECREF5). Within a same document, we also use the same heuristic used in BIBREF19 to find coreferences of generic mentions of persons (e.g., \u201cMatt\u201d) to more specific mentions (e.g., \u201cMatt Damon\u201d)."
            ],
            "gold_section": [
                "Method ::: Entity Linking Algorithm",
                "Introduction"
            ],
            "predicted": [
                "The benefit of using entity linking in our approach can be verified by comparing Ours (Full) and Ours (NoEL). The performance on both datasets decreases if the entity linking part is removed. Especially on FIGER (GOLD), the strict accuracy drops from 75.5 to 69.8. Using entity linking improves less on BBN. We think this is because of three reasons: 1) BBN has a much smaller tag set than FIGER (GOLD); 2) BBN does not allow a mention to be annotated with multiple type paths (e.g., labeling a mention with both /building and /location is not allowed), thus the task is easier; 3) By making the model deep, the performance on BBN is already improved a lot, which makes further improvement harder.",
                "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$. Then the score of the mention $m$ having the type $t_i\\in T$ is calculated as the dot product of $\\mathbf {u}_m$ and $\\mathbf {t}_i$:",
                "Thus, the use of extra information to help with the classification process becomes very important. In this paper, we improve FET with entity linking (EL). EL is helpful for a model to make typing decisions because if a mention is correctly linked to its target entity, we can directly obtain the type information about this entity in the knowledge base (KB). For example, in the sentence \u201cThere were some great discussions on a variety of issues facing Federal Way,\u201d the mention \u201cFederal Way\u201d may be incorrectly labeled as a company by some FET models. Such a mistake can be avoided after linking it to the city Federal Way, Washington. For cases that require the understanding of the context, using entity linking results is also beneficial. In the aforementioned example where \u201cTrump\u201d is the mention, obtaining all the types of Donald Trump in the knowledge base (e.g., politician, businessman, TV personality, etc.) is still informative for inferring the correct type (i.e., politician) that fits the context, since they narrows the possible labels down."
            ],
            "predicted_section": [
                "Experiments ::: Results",
                "Introduction",
                "Method ::: Fine-grained Entity Typing Model ::: Prediction"
            ]
        },
        "6295951fda0cfa2eb4259d544b00bc7dade7c01e": {
            "question_text": "Which model architecture do they use?",
            "from_paper": "1909.12079",
            "gold": [
                "Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention.",
                "To obtain the context representation, we first use a special token $w_m$ to represent the mention (the token \u201c[Mention]\u201d in Figure FIGREF4). Then, the word sequence of the sentence becomes $w_1,...,w_{p_l-1},w_m,w_{p_l+1},...,w_n$. Their corresponding word embeddings are fed into two layers of BiLSTMs. Let $\\mathbf {h}_m^1$ and $\\mathbf {h}_m^2$ be the output of the first and the second layer of BiLSTMs for $w_m$, respectively. We use $\\mathbf {f}_c=\\mathbf {h}_m^1+\\mathbf {h}_m^2$ as the context representation vector.",
                "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$. Then the score of the mention $m$ having the type $t_i\\in T$ is calculated as the dot product of $\\mathbf {u}_m$ and $\\mathbf {t}_i$:"
            ],
            "gold_section": [
                "Method ::: Fine-grained Entity Typing Model ::: Input",
                "Method ::: Fine-grained Entity Typing Model ::: Context Representation",
                "Method ::: Fine-grained Entity Typing Model ::: Prediction"
            ],
            "predicted": [
                "We use Ours (Full) to represent our full model, and also compare with five variants of our own approach: Ours (DirectTrain) is trained without adding random person types while obtaining the KB type representation, and $\\lambda _P$ is set to 1; Ours (NoEL) does not use entity linking, i.e., the KB type representation and the entity linking confidence score are removed, and the model is trained in DirectTrain style; Ours (NonDeep) uses one BiLSTM layer and replaces the MLP with a dense layer; Ours (NonDeep NoEL) is the NoEL version of Ours (NonDeep); Ours (LocAttEL) uses the entity linking approach proposed in BIBREF19 instead of our own commonness based approach. Ours (Full), Ours (DirectTrain), and Ours (NonDeep) all use our own commonness based entity linking approach.",
                "We propose a deep neural fine-grained entity typing model that utilizes type information from KB obtained through entity linking.",
                "We compare with the following existing approaches: AFET BIBREF3, AAA BIBREF16, NFETC BIBREF9, and CLSC BIBREF21."
            ],
            "predicted_section": [
                "Experiments ::: Compared Methods",
                "Introduction"
            ]
        },
        "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa": {
            "question_text": "How does labeling scheme look like?",
            "from_paper": "2003.11687",
            "gold": [
                "SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.",
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme",
                "abb: represents abbreviations such as TRL representing Technology Readiness Level.",
                "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.",
                "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.",
                "opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.",
                "seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.",
                "event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.",
                "org: represents an organization such as `NASA', `aerospace industry', etc.",
                "art: represents names of artifacts or instruments such as `AS1300'",
                "cardinal: represents numerical values such as `1', `100', 'one' etc.",
                "loc: represents location-like entities such as component facilities or centralized facility.",
                "mea: represents measures, features, or behaviors such as cost, risk, or feasibility."
            ],
            "gold_section": [
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme",
                "CONCEPT RECOGNITION"
            ],
            "predicted": [
                "In the hand-labelled dataset, each word gets a label. The idea is to perform multi-class classification using BERT's pre-trained cased language model. We use pytorch transformers and hugging face as per the tutorial by BIBREF17 which uses $BertForTokenClassification$. The text is embedded as tokens and masks with a maximum token length. This embedded tokens are provided as the input to the pre-trained BERT model for a full fine-tuning. The model gives an F1-score of $0.89$ for the concept recognition task. An 80-20 data split is used for training and evaluation. Detailed performance of the CR is shown in Table 2 and 3. Additionally, we also implemented CR using spaCy BIBREF18 which also produced similar results.",
                "SE concepts are less ambiguous as compared to generic natural language text. A word usually means one concept. For example, the word `system' usually means the same when referring to a `complex system', `system structure', or `management system' in the SE domain. In generic text, the meaning of terms like `evaluation', `requirement', or `analysis' may contextually differ. We would like domain specific phrases such as `system evaluation', `performance requirement', or `system analysis' to be single entities. Based on the operational and system concepts described in BIBREF0, we carefully construct a set of concept-labels for the SE handbook which is shown in the next section.",
                "art: represents names of artifacts or instruments such as `AS1300'"
            ],
            "predicted_section": [
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme",
                "CONCEPT RECOGNITION ::: Fine tuning with BERT",
                "CONCEPT RECOGNITION"
            ]
        },
        "31e6062ba45d8956791e1b86bad7efcb6d1b191a": {
            "question_text": "What word embeddings are used?",
            "from_paper": "1703.10152",
            "gold": [
                "The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 ."
            ],
            "gold_section": [
                "Models"
            ],
            "predicted": [
                "The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 .",
                "The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories.",
                "In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results."
            ],
            "predicted_section": [
                "Training strategy",
                "Discussion",
                "Classification and evaluation"
            ]
        },
        "c0355afc7871bf2e12260592873ffdb5c0c4c919": {
            "question_text": "What is their baseline?",
            "from_paper": "1909.10481",
            "gold": [
                "We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:",
                "CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.",
                "Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.",
                "Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM."
            ],
            "gold_section": [
                "Experiments ::: Question Generation ::: English-English Question Generation"
            ],
            "predicted": [
                "We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG.",
                "We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:",
                "In the zero-shot English-Chinese question generation experiments, we use Xlm and Pipeline (Xlm) as our baselines. Pipeline (Xlm) is a pipeline method that uses En-En-QG with Xlm to generate questions, and then translates the results to Chinese. Because there is no annotations for En-Zh-QG, we perform human evaluation studies for this setting. Table TABREF19 shows the human evaluation results, where our model surpasses all the baselines especially in terms of relatedness and correctness."
            ],
            "predicted_section": [
                "Experiments ::: Question Generation ::: English-Chinese Question Generation",
                "Experiments ::: Question Generation ::: English-English Question Generation"
            ]
        },
        "47d54a6dd50cab8dab64bfa1f9a1947a8190080c": {
            "question_text": "what datasets where used?",
            "from_paper": "1805.07882",
            "gold": [
                "Tasks & Datasets",
                "We evaluate our model on three tasks:",
                "Table TABREF30 shows the statistic of the three datasets. Because of not dealing with name entities and multi-word idioms, the vocabulary size of SICK is quite small compared to the others."
            ],
            "gold_section": [
                "Tasks & Datasets"
            ],
            "predicted": [
                "Table TABREF30 shows the statistic of the three datasets. Because of not dealing with name entities and multi-word idioms, the vocabulary size of SICK is quite small compared to the others.",
                "We study five pre-trained word embeddings for our model:",
                "This work was done while Nguyen Tien Huy was an intern at Toshiba Research Center."
            ],
            "predicted_section": [
                "Pre-trained word embeddings",
                "Acknowledgments",
                "Tasks & Datasets"
            ]
        },
        "67cb001f8ca122ea859724804b41529fea5faeef": {
            "question_text": "what are the state of the art methods they compare with?",
            "from_paper": "1805.07882",
            "gold": [
                "We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.",
                "We evaluate our M-MaxLSTM-CNN model on three tasks: STS, textual entailment recognition, paraphrase identification. The advantages of M-MaxLSTM-CNN are: i) simple but efficient for combining various pre-trained word embeddings with different dimensions; ii) using Multi-level comparison shows better performances compared to using only sentence-sentence comparison; iii) does not require hand-crafted features (e.g., alignment features, Ngram overlaps, syntactic features, dependency features) compared to the state-of-the-art ECNU BIBREF6 on STS Benchmark dataset."
            ],
            "gold_section": [
                "Evaluation of exploiting multiple pre-trained word embeddings",
                "Introduction"
            ],
            "predicted": [
                "We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.",
                "Besides existing methods, we also compare our model with several sentence modeling approaches using multiple pre-trained word embeddings:",
                "At SemEval-2017 STS task, hybrid approaches obtain strong performances. BIBREF24 train a linear regression model with WordNet, alignment features and the word embedding word2vec. BIBREF6 develop an ensemble model with multiple boosting techniques (i.e., Random Forest, Gradient Boosting, and XGBoost). This model incorporates traditional features (i.e., n-gram overlaps, syntactic features, alignment features, bag-of-words) and sentence modeling methods (i.e., Averaging Word Vectors, Projecting Averaging Word Vectors, LSTM)."
            ],
            "predicted_section": [
                "Overall evaluation",
                "Related work"
            ]
        },
        "18fbfb1f88c5487f739aceffd23210a7d4057145": {
            "question_text": "what models did they compare with?",
            "from_paper": "1907.05338",
            "gold": [
                "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.",
                "In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .",
                "The DenseNet structure contains four independent blocks and each block has four CNNs connected by residual. We initialize word embedding in the word representation layer with BERT. We initialize each character as a 768-dimension vector. In the experiment of training DenseNet,we concat the output vector of DenseNet with [CLS] for prediction.",
                "We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score.",
                "Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again."
            ],
            "gold_section": [
                "Experiment A: Sequence Labeling",
                "Experiment B: Text Classification",
                "Experiment C: Semantic Similarity Tasks"
            ],
            "predicted": [
                "Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again.",
                "We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score.",
                "We use \u201cQuora-Question-Pair\u201d dataset 1. This is a commonly used dataset containing 400k question pairs, annotated manually to be semantically equivalent or not. Due to its high quality, it is a standard dataset to test the success of various semantic similarity tasks. Various models which are tested on this data set are proposed, including but not limited to BIBREF35 , BIBREF36 , BIBREF37 , BIBREF38 ."
            ],
            "predicted_section": [
                "Experiment B: Text Classification",
                "Experiment C: Semantic Similarity Tasks"
            ]
        },
        "9c1f70affc87024b4280f0876839309b8dddd579": {
            "question_text": "How did they annotate the corpus?",
            "from_paper": "2003.08437",
            "gold": [
                "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese."
            ],
            "gold_section": [
                "Corpus Annotation ::: Reliability of Annotation"
            ],
            "predicted": [
                "In this paper, we presented the first corpus annotated with adposition supersenses in Mandarin Chinese. The corpus is a valuable resource for examining similarities and differences between adpositions in different languages with parallel corpora and can further support automatic disambiguation of adpositions in Chinese. We intend to annotate additional genres\u2014including native (non-translated) Chinese and learner corpora\u2014in order to more fully capture the semantic behavior of adpositions in Chinese as compared to other languages.",
                "This paper describes the first corpus with broad-coverage annotation of adpositions in Chinese. For this corpus we have adapted schneider-etal-2018-comprehensive Semantic Network of Adposition and Case Supersenses annotation scheme (SNACS; see sec:snacs) to Chinese. Though other languages were taken into consideration in designing SNACS, no serious annotation effort has been undertaken to confirm empirically that it generalizes to other languages. After developing new guidelines for syntactic phenomena in Chinese (subsec:adpositioncriteria), we apply the SNACS supersenses to a translation of The Little Prince (3 2 3), finding the supersenses to be robust and achieving high inter-annotator agreement (sec:corpus-annotation). We analyze the distribution of adpositions and supersenses in the corpus, and compare to adposition behavior in a separate English corpus (see sec:corpus-analysis). We also examine the predictions of a part-of-speech tagger in relation to our criteria for annotation targets (sec:adpositionidentification). The annotated corpus and the Chinese guidelines for SNACS will be made freely available online.",
                "To date, most wide-coverage semantic annotation of prepositions has been dictionary-based, taking a word sense disambiguation perspective BIBREF16, BIBREF17, BIBREF18. BIBREF19 proposed a supersense-based (unlexicalized) semantic annotation scheme which would be applied to all tokens of prepositions in English text. We adopt a revised version of the approach, known as SNACS (see sec:snacs). Previous SNACS annotation efforts have been mostly focused on English\u2014particularly STREUSLE BIBREF20, BIBREF0, the semantically annotated corpus of reviews from the English Web Treebank BIBREF21. We present the first adaptation of SNACS for Chinese by annotating an entire Chinese translation of The Little Prince."
            ],
            "predicted_section": [
                "Conclusion",
                "Related Work",
                "Introduction"
            ]
        },
        "f8264609a44f059b74168995ffee150182a0c14f": {
            "question_text": "What models are explored in this paper?",
            "from_paper": "2003.04978",
            "gold": [
                "Once the representations of text are pre-trained from previous unsupervised learning, the representations are then fed into 5 different models to perform supervised learning on the downstream task. In this case, the downstream task is a binary classification of the fake news as either real or fake. A k-fold prediction error is obtained from each of the 5 models, and since we have 3 different pre-training models, we have a total of 15 models to compare.",
                "Methods ::: Fine-tuning ::: Artificial Neural Network (ANN)",
                "We trained simple Artificial Neural Networks which contains an input layer, particular number of output layers (specified by a hyperparameter) in which each hidden layer contains the same number of neurons and the same activation function, and an output layer with just one node for the classification (real or fake) which uses sigmoid as an activation function. We chose sigmoid as the output layer activation and the binary_crossentropy as the loss since it is a binary classification problem and the use of softmax normalizes the results which is not needed for this problem and since we use only one output node to return the activation, we applied sigmoid for the output layer activation. We performed Grid Search strategy to find the best hyper-parameters such as activations, optimizers, number of hidden layers and number of hidden neurons. We had used Keras Sequential model and we used Dense Layers which contains connections to every hidden node in the next layer.",
                "Due to the limitation of computing resource, the grid search for Neural Networks is divided into three sequential steps. Instead of performing grid search on all the hyperparameters all at once, we chose to do grid search for the activations for the hidden layers, optimizers and the number of hidden layers and hidden neurons (done together). We coupled the number of hidden layers and the number of neurons since we believed that each of these hyperparameters interact with each other in improving the model training. We also did a K-fold Split for 3 splits at each step and picked the best hyperparameters which renders the highest accuracy.",
                "Methods ::: Fine-tuning ::: Long Short Term Memory networks (LSTMs)",
                "Long Short Term Memory networks (LSTMs) is a special recurrent neural network (RNN) introduced by Hochreiter & Schmidhuber (1997)$^{8}$.",
                "(Christopher Olah. \u201cUnderstanding LSTM Networks.\u201d)",
                "The chain-like nature of an RNN allows information to be passed from the beginning all the way to the end. The prediction at time step $t$ depends on all previous predictions at time step $t\u2019 < t$. However, when a typical RNN is used in a larger context (i.e. a relatively large time steps), the RNN suffers from the issue of vanishing gradient descent $^{9}$. LSTMs, a special kind of RNN, can solve this long-term dependency problem.",
                "Each cell in a typical LSTMs network contains 3 gates (i.e., forget gate, input gate, and output gate) to decide whether or not information should be maintained in the cell state $C_t$.",
                "For CountVectorizer and TfidfVectorizer, each sample of text is converted into a 1-d feature vector of size 10000. As a result, the number of time steps (i.e. the maximum amount of word vectors for each sample) for these two can only be set to 1, as the pre-trained representations are done at the sample\u2019s level. By contrast, the number of time steps for Word2Vec can either be 1, if we simply take an average of the word embeddings, or the length of the sentence, where each word has an embedding and thus the pre-trained representations are done at the word\u2019s level. We choose the approach with 1 timestep in our model because it requires less computation power. Meanwhile, we also do the length of the sentence, and 200 time steps are chosen as 200 is close to the mean amount of words in each sample and it is a fairly common choice in practice. However, since we do not have enough computation power to fine-tune (grid search) our model, we leave it out for our model and include it only in the final section.",
                "In the LSTM layer, a dropout rate of 0.2, a common choice in practice$^{10}$ , is used to prevent overfitting. Grid search is performed in order to pick decent values of hyperparameters, including the number of hidden units in the LSTM layer, the number of hidden layers, the activation functions and the number of nodes in the hidden layer, and the optimizer. Relatively small numbers of hidden layers (i.e., {0, 1, 2}) and nodes (i.e., {200, 400, 600}) are selected as the basis for grid search, because this is a simple binary classification task and too many of them would cause overfitting.",
                "Due to the limitation of computing resource, the grid search for LSTMs is divided into four sequential steps. Instead of performing grid search on all the hyperparameters all at once, the grid search is first done on the number of hidden layers and all other hyperparameters are randomly selected from the subset. Then, the grid search is done on the number of nodes in the hidden layer(s), using the best number of hidden layer found in step 1. The grid search completes when all four steps are finished. In each step we used K-fold cross validation with $K = 3$.",
                "Methods ::: Fine-tuning ::: Random Forest",
                "A random forest is an ensemble classifier that estimates based on the combination of different decision trees. So random forest will fit a number of decision tree classifiers on various subsamples of the dataset. A random best subsets are built by each tree in the forest. In the end, it gives the best subset of features among all the random subsets of features.",
                "In our project, 3 random forest algorithms have been applied with models count vectorizer, tfidf and word-to-vector. Random forest algorithm requires 4 hyperparameters to tune, such as the number of trees in the forest (i.e., {200, 400, 800}); the maximum depth of the tree (i.e., {1,5,9}); the minimum number of samples required to be at a lead node (i.e., {2, 4}); The minimum number of samples at each leaf node has the effect of smoothing the model, especially during regression; the minimum number of samples required to be at a leaf node (i.e., {5, 10}). All parameters are applied to grid search and in the end, the best set of parameters can be determined as we used K-fold cross validation with $K = 3$.",
                "Methods ::: Fine-tuning ::: Logistic Regression",
                "Logistic regression is a statistical machine learning algorithm that classifies the data by considering outcome variables on extreme ends and this algorithm is providing a discriminatory line between classes. Compared to another simple model, linear regression, which requires hard threshold in classification, logistic regression can overcome threshold values for a large dataset. Logistic regression produces a logistic curve, which is limited to values between 0 to 1, by adding sigmoid function in the end.",
                "In regards to our project, three logistic regressions have been applied with models CountVectorizer, TF-IDF and Word2Vec. We did grid search on the solvers, including newton-cg, sag, lbfgs and liblinear. Grid search is also performed on the inverse of regularization parameter with values being {0, 4, 10}. Best parameter sets can be determined as we used K-fold cross validation with $K = 3$.",
                "Methods ::: Fine-tuning ::: Support Vector Machine (SVM)",
                "SVM is a supervised machine learning algorithm in which a hyperplane is created in order to separate and categorize features. The optimal hyperplane is usually calculated by creating support vectors on both sides of the hyperplane in which each vector must maximize the distance between each other. In other words, the larger the distance between each vector around the hyperplane, the more accurate the decision boundary will be between the categories of features.",
                "In regards to our project, we fit 3 support vector machines on CountVectorizer, TfidfVectorizer, and WordToVectorizer. An SVM requires specific parameters such as a kernel type, $C$, maximum iterations, etc. In our case, we needed to determine the optimal $C$ as well as the optimal kernel for each fit. We used K-fold cross validation with $K = 3$. A grid search of kernel types and $C$ was performed in order to give us the most accurate svm model. The parameters we used for each kernel were linear and rbf while the values we used for $C$ were 0.25 ,0.5, and 0.75. Once the grid search was completed for these hyperparameters, the model was evaluated with the most optimal hyperparameters using cross validation of 3 splits."
            ],
            "gold_section": [
                "Methods ::: Fine-tuning",
                "Methods ::: Fine-tuning ::: Support Vector Machine (SVM)",
                "Methods ::: Fine-tuning ::: Long Short Term Memory networks (LSTMs)",
                "Methods ::: Fine-tuning ::: Artificial Neural Network (ANN)",
                "Methods ::: Fine-tuning ::: Logistic Regression",
                "Methods ::: Fine-tuning ::: Random Forest"
            ],
            "predicted": [
                "http://www.bioinf.jku.at/publications/older/2604.pdf",
                "The model is evaluated using a 3-fold of cross validation. Out of the fifteen models, CountVectorizer with LSTMs performs the best. Word2Vec performs the worst among the three pre-training algorithms. Random forest performs the worst among the five fine-tuning algorithms.",
                "Once the representations of text are pre-trained from previous unsupervised learning, the representations are then fed into 5 different models to perform supervised learning on the downstream task. In this case, the downstream task is a binary classification of the fake news as either real or fake. A k-fold prediction error is obtained from each of the 5 models, and since we have 3 different pre-training models, we have a total of 15 models to compare."
            ],
            "predicted_section": [
                "Results",
                "Methods ::: Fine-tuning",
                "References"
            ]
        },
        "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a": {
            "question_text": "How does the proposed training framework mitigate the bias pattern?",
            "from_paper": "1909.04242",
            "gold": [
                "When the annotation artifacts of the training set cannot be generalized to the testing set, which should be more common in the real-world, predicting by artifacts may hurt models' performance. Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.",
                "Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability."
            ],
            "gold_section": [
                "Experimental Results ::: Debiasing Results ::: Benefits of Debiasing"
            ],
            "predicted": [
                "Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic relationships between given sentence pairs. However, it is shown that models are unintentionally utilizing the annotation artifacts BIBREF4, BIBREF2. If the evaluation is conducted under a similar distribution as the training data, e.g., with the given testing set, models will enjoy additional advantages, making the evaluation results over-estimated. On the other hand, if the bias pattern cannot be generalized to the real-world, it may introduce noise to models, thus hurting the generalization ability.",
                "We make a few assumptions about an artifact-balanced distribution and how the biased datasets are generated from it, and demonstrate that we can train models fitting the artifact-balanced distribution using only the biased datasets.",
                "BIBREF4 further investigate models' robustness to the bias pattern using swapping operations. BIBREF6 demonstrate that the annotation artifacts widely exist among NLI datasets. They show that hypothesis-only-model, which refers to models trained and predict only with hypotheses, outperforms always predicting the majority-class in six of ten NLI datasets."
            ],
            "predicted_section": [
                "Detailed Assumptions and Proof of Theorem @!START@UID1@!END@",
                "Related Work",
                "Introduction"
            ]
        },
        "6cad6f074b0486210ffa4982c8d1632f5aa91d91": {
            "question_text": "How does the model proposed extend ENAMEX?",
            "from_paper": "1912.10162",
            "gold": [
                "In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC). A percentage of Greek Wikipedia was parsed and used for training in spaCy. The results from the training are presented in SECREF13."
            ],
            "gold_section": [
                "Creating a state of the art Named Entity Recognizer using spaCy ::: Usage of Wikipedia dataset for training"
            ],
            "predicted": [
                "What can be concluded is that the model did not have a flexibility in OOV words. Of course, this can also be an advantage, meaning that the model recognized the mismatch of a wrong word with its class.",
                "SpaCy uses a deep learning formula for implementing NLP models, summarised as \u201cembed, encode, attend, predict\u201d. In spaCy's approach text is inserted in the model in the form of unique numerical values (ID) for every input that can represent a token of a corpus or a class of the NLP task (part of speech tag, named entity class). At the embedding stage, features such as the prefix, the suffix, the shape and the lowercase form of a word are used for the extraction of hashed values that reflect word similarities.",
                "Both sources had good results in non entity tokens, which affected the F1 score. Moreover, the model did not perform well for facilities, as polyglot's Greek recognizer does not support that class and FAC entities cover a small amount of the list."
            ],
            "predicted_section": [
                "Creating a state of the art Named Entity Recognizer using spaCy ::: Evaluation and comparison of results",
                "Creating a Greek POS Tagger using spaCy ::: Evaluation and comparison of results",
                "SpaCy's deep learning model for POS tagging and Named Entity Recognition"
            ]
        },
        "74396ead9f88a9efc7626240ce128582ab69ef2b": {
            "question_text": "by how much did their approach outperform previous work?",
            "from_paper": "1806.03369",
            "gold": [
                "The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features."
            ],
            "gold_section": [
                "Results"
            ],
            "predicted": [
                "Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best.",
                "Regarding the general features developed for this work, the polarity- and subjectivity-based features performed well, while performance using only PMI features was lower. PMI scores in particular may have been negatively impacted by common Twitter characteristics, such as the trend to join keywords together in hashtags, and the use of acronyms that are unconventional in other domains. These issues could be addressed to some extent in the future via word segmentation tools, spell-checkers, and acronym expansion.",
                "Overall, the system cut the error rate from .256 to .220, representing a 14% relative reduction in error over prior best results on the Amazon dataset. Our results testing on Twitter are not directly comparable to others, since prior work's datasets could not be released; however, our results ( INLINEFORM0 =0.583) are in line with those reported previously ( BIBREF4 RiloffSarcasm: INLINEFORM1 =0.51; BIBREF13 davidov-tsur-rappoport:2010:CONLL: INLINEFORM2 =0.545). Additionally, our Twitter data did not contain many indicators shown to be discriminative in the past (leading our general features to be better predictors of sarcasm even when training/testing entirely within the domain), and our focus in developing features was on general performance rather than performance on Twitter specifically."
            ],
            "predicted_section": [
                "Evaluation",
                "Discussion"
            ]
        },
        "de4e949c6917ff6933f5fa2a3062ba703aba014c": {
            "question_text": "What are two use cases that demonstrate capability of created system?",
            "from_paper": "1909.08250",
            "gold": [
                "We describe our method of generating natural language in two applications. The first application is to generate a natural language description for workflow created by the system built in the Phylotastic project described in BIBREF2. Instead of requiring that the ontologies are annotated using Attempto, we use natural language sentences to annotate the ontologies. To test the feasibility of the approach, we also conduct another use case with the second ontology, that is entirely different from the ontologies used in the Phylotastic project. The ontology is about people and includes descriptions for certain class.",
                "The second application targets the challenge of creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference BIBREF7. We create an intermediate representation that can be used to translate the original article in English to another language. In this use case, we translate the intermediate representation back to English and measure how the translated version stacks up again the original one. We assess the generation quality automatically with BLEU-3 and ROUGE-L (F measure). BLEU BIBREF16 and ROUGE BIBREF17 algorithms are chosen to evaluate our generator since the central idea of both metrixes is \u201cthe closer a machine translation is to a professional human translation, the better it is\u201d, thus, they are well-aligned with our use cases' purpose. In short, the higher BLUE and ROUGE score are, the more similar the hypothesis text and the reference text is. In our use case, the hypothesis for BLEU and ROUGE is the generated English content from the intermediate representation, and the reference text is the original text from Wikipedia."
            ],
            "gold_section": [
                "Experiments"
            ],
            "predicted": [
                "GF has been used in a variety of applications, such as query-answering systems, voice communication, language learning, text analysis and translation, natural language generation BIBREF8, BIBREF9, automatic translation.",
                "In the first type of applications, the system can work with annotated ontologies to translate a set of atoms\u2014representing the answer to a query to the ontology\u2014to a set of sentences. To do so, the system extracts the annotations related to the atoms in the answer and creates a GF program that is then used to generate natural language description of the given set of atoms. In the second type of applications, the system receives a paragraph of text and generates an intermediate representation\u2014as a GF program\u2014for the paragraph, which can be used for different purpose such as cross-translation, addressing a need identified in BIBREF7 .",
                "Natural language generation (NLG) has been one of the key topics of research in natural language processing, which was highlighted by the huge body of work on NLG surveyed in BIBREF0, BIBREF1. With the advances of several devices capable of understanding spoken language and conducting conversation with human (e.g., Google Home, Amazon Echo) and the shrinking gap created by the digital devices, it is not difficult to foresee that the market and application areas of NLG systems will continue to grow, especially in applications whose users are non-experts. In such application, a user often asks for certain information and waits for the answer and a NLG module would return the answer in spoken language instead of text such as in question-answering systems or recommendation systems. The NLG system in these two applications uses templates to generate the answers in natural language for the users. A more advanced NLG system in this direction is described in BIBREF2, which works with ontologies annotated using the Attempto language and can generate a natural language description for workflows created by the systems built in the Phylotastic project. The applications targeted by these systems are significantly different from NLG systems, whose main purpose is to generate high-quality natural language description of objects or reports, such as those reported in the recent AAAI conference BIBREF3, BIBREF4, BIBREF5."
            ],
            "predicted_section": [
                "Conclusions and Future Work",
                "Introduction",
                "Background: Grammatical Framework"
            ]
        },
        "ab37ae82e38f64d3fa95782f2c791488f26cd43f": {
            "question_text": "Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?",
            "from_paper": "1612.03762",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "At this stage, we have a set of MedDRA terms which \u201ccovers\u201d the narrative description. We further select a subset INLINEFORM0 of INLINEFORM1 with a second heuristic, the maximal-set-of-voters criterium.",
                "Table TABREF58 shows the results of this first performance test. We group narrative descriptions by increasing length (in terms of characters). We note that reported results are computed considering terms at PT level. By moving to PT level, instead of using the LLT level, we group together terms that represent the same medical concept (i.e., the same adverse reaction). In this way, we do not consider an error when MagiCoder and the human expert use two different LLTs for representing the same adverse event. The use of the LLT level for reporting purpose and the PT level for analysis purpose is suggested also by MedDRA BIBREF5 . With common PT we mean the percentage of preferred terms retrieved by human reviewers that have been recognized also by MagiCoder. Reported performances are summarized also in FIGREF59 . Note that, false positive and false negative errors are required to be as small as possible, while common PT, recall, and precision have to be as large as possible.",
                "From an abstract point of view, we try to recognize, in the narrative description, single words belonging to LLTs, which do not necessarily occupy consecutive positions in the text. This way, we try to \u201creconstruct\u201d MedDRA terms, taking into account the fact that in a description the reporter can permute or omit words. As we will show, MagiCoder has not to deal with computationally expensive tasks, such as taking into account subroutines for permutations and combinations of words (as, for example, in BIBREF19 )."
            ],
            "predicted_section": [
                "MagiCoder: overview",
                "Experiment about MagiCoder performances"
            ]
        },
        "71413505d7d6579e2a453a1f09f4efd20197ab4b": {
            "question_text": "How is the system constructed to be linear in the size of the narrative input and the terminology?",
            "from_paper": "1612.03762",
            "gold": [
                "INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description. Moreover, it keeps track of the position where the INLINEFORM5 -th word occurs in INLINEFORM6 ."
            ],
            "gold_section": [],
            "predicted": [
                "Let INLINEFORM0 be the input size (the length, in terms of words, of the narrative description). Let INLINEFORM1 be the cardinality of the dictionary (i.e., the number of terms). Moreover, let INLINEFORM2 be the number of distinct words occurring in the dictionary and let INLINEFORM3 be the length of the longest term in the dictionary. For MedDRA, we have about 75K terms ( INLINEFORM4 ) and 17K unique words ( INLINEFORM5 ). Notice that, reasonably, INLINEFORM6 is a small constant for any dictionary; in particular, for MedDRA we have INLINEFORM7 . We assume that all update operations on auxiliary data structures require constant time INLINEFORM8 .",
                "At this stage, we have a set of MedDRA terms which \u201ccovers\u201d the narrative description. We further select a subset INLINEFORM0 of INLINEFORM1 with a second heuristic, the maximal-set-of-voters criterium.",
                "Thus, we decided to design and develop an ad hoc algorithm for the problem we are facing, namely that of deriving MedDRA terms from narrative text and mapping segments of text in effective LLTs. This task has to be done in a very feasible time (we want that each interaction user/MagiCoder requires less than a second) and the solution offered to the expert has to be readable and useful. Therefore, we decided to ignore the structure of the narrative description and address the issue in a simpler way. Main features of MagiCoder can be summarized as follows:"
            ],
            "predicted_section": [
                "MagiCoder: overview",
                "MagiCoder complexity analysis",
                "MagiCoder: an NLP software for ADR automatic encoding"
            ]
        },
        "c2ce25878a17760c79031a426b6f38931cd854b2": {
            "question_text": "What is the source of the training/testing data?",
            "from_paper": "2003.11528",
            "gold": [
                "Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.",
                "We implement the GPT-2 model based on the transformers library BIBREF8. The model configuration is 8 attention heads per layer, 8 layers, 512 embedding dimensions, and 1024 feed-forward layer dimensions. We employ the OpenAIAdam optimizer and train the model with 400,000 steps in total on 4 NVIDIA 1080Ti GPUs. The characters with frequency less than 3 in CCPC1.0 are treated as UNK and a vocabulary with 11259 tokens (characters) is finally built up."
            ],
            "gold_section": [
                "Experiment ::: Experiment Setup",
                "Introduction"
            ],
            "predicted": [
                "After pre-processing, all the formatted poem samples will be sent to the poetry generation model for training, as illustrated in Figure 3.",
                "State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Tsinghua University, Beijing, China"
            ],
            "predicted_section": [
                "Model ::: Pre-processing",
                " :::  ::: "
            ]
        },
        "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d": {
            "question_text": "what is the previous work they are comparing to?",
            "from_paper": "1801.03615",
            "gold": [
                "Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.",
                "Besides, we compared our system with a fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github."
            ],
            "gold_section": [
                "Baselines"
            ],
            "predicted": [
                "We thank the anonymous reviewers for their detailed and constructed comments. Yue Zhang and Min Zhang are the corresponding authors. The research work is supported by the National Natural Science Foundation of China (61525205, 61432013, 61373095). Thanks for Xiaoqing Li, Heng Yu and Zhdanova Liubov for their useful discussion. ",
                "We use BLEU BIBREF26 as our evaluation metric. The performance of different systems are shown in Table TABREF34 and TABREF35 . On both the news and e-commerce domains, our system performs better than baseline systems.",
                "On news domain, the average improvement of our method is 1.75 and 0.97 BLEU score when implemented on RNN-based NMT, compared with subword BIBREF3 method and fully character-based BIBREF4 method, respectively. When implemented on Transformer BIBREF15 , average improvement is 1.47 BLEU compared with subword method. On the e-commerce domain, which use 50M sentences as training corpus, the average improvement of our method is 0.68 BLEU compared with the subword method."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Acknowledgments"
            ]
        },
        "4e2b12cfc530a4682b06f8f5243bc9f64bd41135": {
            "question_text": "How is quality of the word vectors measured?",
            "from_paper": "1910.09362",
            "gold": [
                "To show the advantages of our noise distribution, we conduct experiments on three evaluation tasks. While the word analogy task BIBREF12 is our focus for testing the linear relationships between word vectors, we also evaluate the learned word vectors on the word similarity task BIBREF0 and the synonym selection task BIBREF3.",
                "The task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\rho _p$ as the metric, the higher of which the better the word vectors are. The expression of $\\rho _p$ is",
                "where $\\phi $ and $\\hat{\\phi }$ are random variables for the word similarity scores by human judgment and the cosine distances between word vectors, respectively. Benchmark datasets for this task include RG BIBREF31, MC BIBREF32, WS BIBREF33, MEN BIBREF34, and RW BIBREF35."
            ],
            "gold_section": [
                "Experiments ::: Task 1: Word Similarity Task ::: Task Description",
                "Experiments"
            ],
            "predicted": [
                "where $v_{w}$ and $v_{w}^{\\prime }$ are the vectors of the \u201cinput\u201d and \u201coutput\u201d words, and $|V|$ is the size of vocabulary.",
                "The task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\rho _p$ as the metric, the higher of which the better the word vectors are. The expression of $\\rho _p$ is",
                "The recent decade has witnessed the great success achieved by word representation in natural language processing (NLP). It proves to be an integral part of most other NLP tasks, in which words have to be vectorized before input to the models. High quality word vectors have boosted the performance of many tasks, such as named entity recognition BIBREF0, BIBREF1, sentence completion BIBREF2, BIBREF3, part-of-speech tagging BIBREF4, BIBREF5, sentiment analysis BIBREF6, BIBREF7, and machine translation BIBREF8, BIBREF9. In a conventional way, word vectors are obtained from word-context co-occurrence matrices by either cascading the row and column vectors BIBREF10 or applying singular value decomposition (SVD) BIBREF11. However, these approaches are limited by their sub-optimal linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which utilizes two shallow neural networks, i.e., skip-gram and continuous bag-of-words, to learn word vectors from large corpora. The model is also capable of capturing interesting linear relationships between word vectors."
            ],
            "predicted_section": [
                "Experiments ::: Task 1: Word Similarity Task ::: Task Description",
                "Word2Vec ::: Architectures",
                "Introduction"
            ]
        },
        "ccec4f8deff651858f44553f8daa5a19e8ed8d3b": {
            "question_text": "What are the datasets used in the paper?",
            "from_paper": "1909.05190",
            "gold": [
                "One challenge for incorporating intents into event embeddings is that we should have a large-scale labeled dataset, which annotated the event and its actor's intents. Recently, BIBREF6 P18-1043 and BIBREF7 sap2018atomic released such valuable commonsense knowledge dataset (ATOMIC), which consists of 25,000 event phrases covering a diverse range of daily-life events and situations. For example, given an event \u201cPersonX drinks coffee in the morning\u201d, the dataset labels PersonX's likely intent is \u201cPersonX wants to stay awake\u201d.",
                "We use the New York Times Gigaword Corpus (LDC2007T07) for pre-training event embeddings. Event triples are extracted based on the Open Information Extraction technology BIBREF15. We initialize the word embedding layer with 100 dimensional pre-trained GloVe vectors BIBREF8, and fine-tune initialized word vectors during our model training. We use Adagrad BIBREF16 for optimizing the parameters with initial learning rate 0.001 and batch size 128.",
                "We first follow BIBREF5 (BIBREF5) evaluating our proposed approach on the hard similarity task. The goal of this task is that similar events should be close to each other in the same vector space, while dissimilar events should be far away with each other. To this end, BIBREF5 (BIBREF5) created two types of event pairs, one with events that should be close to each other but have very little lexical overlap (e.g., police catch robber / authorities apprehend suspect), and another with events that should be farther apart but have high overlap (e.g., police catch robber / police catch disease).",
                "The labeled dataset contains 230 event pairs (115 pairs each of similar and dissimilar types). Three different annotators were asked to give the similarity/dissimilarity rankings, of which only those the annotators agreed upon completely were kept. For each event representation learning method, we obtain the cosine similarity score of the pairs, and report the fraction of cases where the similar pair receives a higher cosine value than the dissimilar pair (we use Accuracy $\\in [0,1]$ denoting it). To evaluate the robustness of our approach, we extend this dataset to 1,000 event pairs (similar and dissimilar events each account for 50%), and we will release this dataset to the public.",
                "Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings."
            ],
            "gold_section": [
                "Experiments ::: Event Similarity Evaluation ::: Hard Similarity Task",
                "Commonsense Knowledge Enhanced Event Representations ::: Intent Embedding",
                "Experiments ::: Script Event Prediction",
                "Commonsense Knowledge Enhanced Event Representations ::: Joint Event, Intent and Sentiment Embedding"
            ],
            "predicted": [
                "We use the New York Times Gigaword Corpus (LDC2007T07) for pre-training event embeddings. Event triples are extracted based on the Open Information Extraction technology BIBREF15. We initialize the word embedding layer with 100 dimensional pre-trained GloVe vectors BIBREF8, and fine-tune initialized word vectors during our model training. We use Adagrad BIBREF16 for optimizing the parameters with initial learning rate 0.001 and batch size 128.",
                "The labeled dataset contains 230 event pairs (115 pairs each of similar and dissimilar types). Three different annotators were asked to give the similarity/dissimilarity rankings, of which only those the annotators agreed upon completely were kept. For each event representation learning method, we obtain the cosine similarity score of the pairs, and report the fraction of cases where the similar pair receives a higher cosine value than the dissimilar pair (we use Accuracy $\\in [0,1]$ denoting it). To evaluate the robustness of our approach, we extend this dataset to 1,000 event pairs (similar and dissimilar events each account for 50%), and we will release this dataset to the public.",
                "Except for the hard similarity task, we also evaluate our approach on the transitive sentence similarity dataset BIBREF19, which contains 108 pairs of transitive sentences: short phrases containing a single subject, object and verb (e.g., agent sell property). It also has another dataset which consists of 200 sentence pairs. In this dataset, the sentences to be compared are constructed using the same subject and object and semantically correlated verbs, such as `spell\u2019 and `write\u2019; for example, `pupils write letters\u2019 is compared with `pupils spell letters\u2019. As this dataset is not suitable for our task, we only evaluate our approach and baselines on 108 sentence pairs."
            ],
            "predicted_section": [
                "Experiments ::: Event Similarity Evaluation ::: Hard Similarity Task",
                "Experiments ::: Event Similarity Evaluation ::: Transitive Sentence Similarity",
                "Commonsense Knowledge Enhanced Event Representations ::: Joint Event, Intent and Sentiment Embedding"
            ]
        },
        "649e77ac2ecce42ab2efa821882675b5a0c993cb": {
            "question_text": "What languages do they apply the model to?",
            "from_paper": "1606.02601",
            "gold": [
                "Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy BIBREF8 , BIBREF9 . Ballesteros et al. ballesteros train a character-level model for parsing. Zhang et al. zhang do away with words completely, and train a convolutional neural network to do text classification directly from characters.",
                "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.",
                "What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators.",
                "The Char2Vec model",
                "We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that incorporating morphological knowledge helps structure the embedding space in such a way that affixation corresponds to a regular shift in the embedding space. We test both hypotheses directly in \u00a7 \"Capturing semantic similarity\" and \u00a7 \"Capturing syntactic and semantic regularity\" respectively.",
                "The starting point for our model is the skip-gram with negative sampling (SGNS) objective of Mikolov et al. word2vec2. For a vocabulary $V$ of size $|V|$ and embedding size $N$ , SGNS learns two embedding tables $W, C \\in \\mathbb {R}^{N \\times |V|}$ , the target and context vectors. Every time a word $w$ is seen in the corpus with a context word $c$ , the tables are updated to maximize",
                "$$\\log \\sigma (w \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-w \\cdot \\tilde{c}_i)]$$ (Eq. 7)",
                "where $P(w)$ is a noise distribution from which we draw $k$ negative samples. In the end, the target vector for a word $w$ should have high inner product with context vectors for words with which it is typically seen, and low inner products with context vectors for words it is not typically seen with. Figure 1 illustrates this for a particular example. In Mikolov et al. word2vec2, the noise distribution $P(w)$ is proportional to the unigram probability of a word raised to the 3/4th power BIBREF11 .",
                "Our innovation is to replace $W$ with a trainable function $f$ that accepts a sequence of characters and returns a vector of length $N$ (i.e. $f: A^{<\\omega } \\rightarrow \\mathbb {R}^N$ , where $A$ is the alphabet we are considering and $A^{<\\omega }$ denotes the finite length strings over the alphabet $A$ ). We still keep the table of context embeddings $C$ , and our model objective is still to minimize",
                "$$\\log \\sigma (f(w) \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-f(w) \\cdot \\tilde{c}_i)]$$ (Eq. 8)",
                "where we now treat $w$ as a sequence of characters. After training, $f$ can be used to produce an embedding for any sequence of characters, even if it was not previously seen in training.",
                "The process of calculating $f$ on a word is illustrated in Figure 2 . We first pad the word with beginning and end of word tokens, and then pass the characters of the word into a character lookup table. As the link between characters and morphemes is non-compositional and requires essentially memorizing a sequence of characters, we use LSTMs BIBREF21 to encode the letters in the word, as they have been shown to capture non-local and non-linear dependencies. We run a forward and a backward LSTM over the character embeddings. The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol. This is necessary to align the resulting embeddings, so that the LSTM hidden states taken together correspond to a partition of the word into two without overlap.",
                "The LSTMs output two sequences of vectors $h_0^{f}, \\dots , h_n^f$ and $h_n^{b}, \\dots , h_0^b$ . We then concatenate the resulting vectors, and pass them through a shared feed-forward layer to obtain a final sequence of vectors $h_i$ . Each vector corresponds to two half-words: one half read by the forward LSTM, and the other by the backward LSTM.",
                "We then learn an attention model over these hidden states: given a hidden state $h_i$ , we calculate a weight $\\alpha _i = a(h_i)$ such that $\\sum \\alpha _i = 1$ , and then calculate the resulting vector for the word $w$ as $f(w) = \\sum \\alpha _i h_i$ . Following Bahdanau et al. bahdanau, we calculate $a$ as",
                "$$a(h_i) = \\frac{\\exp (v^{T} \\tanh (Wh_i))}{\\sum _j \\exp (v^{T} \\tanh (Wh_j))}$$ (Eq. 10)",
                "i.e. a softmax over the hidden states.",
                "Capturing morphology via attention",
                "Previous work on bidirectional LSTM character-level models used both LSTMs to read the entire word BIBREF8 , BIBREF22 . This can lead to redundancy, as both LSTMs are used to capture the full word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves. This means one of the LSTMs can specialize on word prefixes and roots, while the other memorizes possible suffixes. In addition, when dealing with an unknown word, it can be split into known and unknown components. The model can then use the semantic knowledge it has learnt for a known component to predict a representation for the unknown word as a whole.",
                "We hypothesize that the natural place to split words is on morpheme boundaries, as morphemes are the smallest unit of language which carry semantic meaning. We test the splitting capabilities of our model in \u00a7 \"Morphological awareness\" .",
                "Experiments",
                "We evaluate our model on three tasks: morphological analysis (\u00a7 \"Morphological awareness\" ), semantic similarity (\u00a7 \"Capturing semantic similarity\" ), and analogy retrieval (\u00a7 \"Capturing syntactic and semantic regularity\" ). We trained all of the models once, and then use the same trained model for all three tasks \u2013 we do not perform hyperparameter tuning to optimize performance on each task.",
                "We trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006 cleaned-up dump of Wikipedia. We only trained on words which appeared more than 5 times in our corpus. We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and Theano BIBREF25 , BIBREF26 . We initialized the context lookup table using word2vec, and kept it fixed during training. In all character-level models, the character embeddings have dimension $d_C = 64$ , while the forward and backward LSTMs have dimension $d_{LSTM} = 256$ . The concatenation of both therefore has dimensionality $d = 512$ . The concatenated LSTM hidden states are then compressed down to $d_{word} = 256$ by a feed-forward layer.",
                "As baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the character-level model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and backward states, passed through a feedforward layer. We refer to this model as C2V-NO-ATT. We also constructed count-based vectors using SVD on PPMI-weighted co-occurence counts, with a window size of 3. We kept the top 256 principal components in the SVD decomposition, to obtain embeddings with the same size as our other models.",
                "To evaluate our model, we evaluate its use as a morphological analyzer (\u00a7 \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (\u00a7 \"Capturing semantic similarity\" ), and examine the structure of the embedding space (\u00a7 \"Capturing syntactic and semantic regularity\" ).",
                "The main innovation of our Char2Vec model compared to existing recurrent character-level models is the capability to split words and model each half independently. Here we test whether our model segmentations correspond to gold-standard morphological analyses.",
                "We obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project BIBREF27 . We then converted these into surface-level segmentations using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three.",
                "Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above non-morpheme boundaries.",
                "We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model.",
                "We report results of a random baseline as a point of comparison, which randomly places morpheme boundaries inside the word. We also report the results of the Porter stemmer, where we place a morpheme boundary at the end of the stem, then randomly thereafter.",
                "Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP.",
                "As the test set is dominated by words with simple morphology, we also extracted all the morphologically rich words with 3 or more morphemes, and created a separate evaluation on this subsection. We report the results in Table 1 .",
                "As the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes.",
                "We show some model analyses against the gold standard in Table 2 .",
                "Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 .",
                "We use the WordSim353 dataset BIBREF29 , the test split of the MEN dataset BIBREF30 , and the Rare Word (RW) dataset BIBREF31 . The word pairs in the WordSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset.",
                "We report results for in-corpus word pairs in Table 3 , and for all word pairs for those models able to predict vectors for unseen words in Table 4 .",
                "Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process.",
                "We also present some word nearest neighbours for our Char2Vec model in Table 5 , both on the whole vocabulary and then filtering the nearest neighbours to only include words which appear 100 times or more in our corpus. This corresponds to keeping the top 10k words, which is common among language models BIBREF8 , BIBREF9 . We note that nearest neighbour predictions include words that are orthographically distant but semantically similar, showing that our model has the capability to learn to compose characters into word meanings.",
                "We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can `memorize' the mapping between the orthography and word semantics.",
                "Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space.",
                "To do this, we use the Google analogy dataset BIBREF3 . This consists of 19544 questions of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (\u201cParis is to France as Berlin is to X) and currency-country relationships. Example syntactic questions are adjective-adverb relationships (\u201camazing is to amazingly as apparent is to X\u201d) and opposites formed by prefixing a negation particle (\u201cacceptable is to unacceptable as aware is to X\u201d). This results in 5537 semantic analogies and 10411 syntactic analogies.",
                "We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies",
                "$$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$ (Eq. 28)",
                "where $a,\\, b,\\, c$ are the word vectors for the words A, B and C respectively.",
                "We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.",
                "Discussion",
                "We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.",
                "This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.",
                "Conclusion",
                "In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space.",
                "We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features.",
                "Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better."
            ],
            "gold_section": [
                "Discussion",
                "Capturing morphology via attention",
                "Capturing syntactic and semantic regularity",
                "Experiments",
                "Introduction",
                "Morphological awareness",
                "The Char2Vec model",
                "Conclusion",
                "Capturing semantic similarity",
                "Character-level models"
            ],
            "predicted": [
                "To evaluate our model, we evaluate its use as a morphological analyzer (\u00a7 \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (\u00a7 \"Capturing semantic similarity\" ), and examine the structure of the embedding space (\u00a7 \"Capturing syntactic and semantic regularity\" ).",
                "This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.",
                "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks."
            ],
            "predicted_section": [
                "Discussion",
                "Character-level models",
                "Introduction"
            ]
        },
        "f85520bbc594918968d7d9f33d11639055458344": {
            "question_text": "What are the deep learning architectures used?",
            "from_paper": "1909.11232",
            "gold": [
                "Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal). Unlike traditional image classification or object detection models where neural networks learn hierarchical spatial features from data, sign recognition requires capture of temporal body motion.",
                "RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated. The forget gate processes it to remove unnecessary information, and outputs ${f_t}$ which gets multiplied with the previously stored memory ${C_{t-1}}$ and produces a refined memory for the current time.",
                "Given a sample skeletal data of $R^{T \\times J \\times 3}$, where $T$ denotes time axis, $J$ is the number of body joints and the last dimension is the 3D coordinates of each joint. We flatten every dimension except time and at each time step we can feed a vector of size $R^{3 \\times J}$ as input. However, we have empirically verified that learning a sequential pattern for each coordinate axis independently and combining them later shows stronger classification performance. Based on this, we trained three different 2 layer LSTMs for data from x, y, and z coordinates separately; and concatenate their final embedding to produce softmax output. In this setting, each separate LSTM receives data as $R^{T \\times J}$ and final embedding size is $R^{3\\times S}$ where $S$ is the state size of LSTM cell. Figure FIGREF15 (a) shows the architecture where as a sample arrives, just before entering into main network, data along separate axis is split and entered into three different LSTM networks. The model concatenates the final state from each of the separate LSTM networks; followed by feeding this into the softmax layer for classification. This approach is referred by Axis Independent Architecture (AI-LSTM). Implementation details such as values of T and J are provided in the `Experiments' section.",
                "AI-LSTM, described in last section, works by modeling temporal dynamics of body joints' data over time. However, there can be spatial interactions with joints at a specific time step. It fails to capture any such interaction among joints in a given time. To incorporate spatial relationship among joints, we propose a simple novel data augmentation technique for skeletal data. We do this by origin transfer. For each frame in a gesture sample, we use each wrist joints as origin and transform all other joints' data by subtracting that origin from them. In this way spatial information is added to the input. We refer this model with spatial data augmentation as Spatial AI-LSTM. This augmentation technique is depicted in Figure FIGREF21. A sample data of form $R^{T \\times 6 \\times 3}$ results in a representation of $R^{T \\times 5 \\times 3}$ after subtracting left wrist joint (origin transfer). After this augmentation process, each sample is a $R^{20 \\times 16 \\times 3}$ matrix. Hence, each separate LSTM networks in our Spatial AI-LSTM network receives an input of $R^{20 \\times 16}$.",
                "We hypothesize that, some signs that have mostly similar skeletal motion pattern could be distinguishable using hand shape information. We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back\u2013propagation, both networks are trained on their own score. The combined network acts like a model ensemble and some sign classes which are confused by RNN network alone might have an improved recognition accuracy with this approach.",
                "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train."
            ],
            "gold_section": [
                "Our Approach ::: Recurrent Neural Networks (RNN)",
                "Our Approach",
                "Our Approach ::: Combined Network",
                "Experiments ::: Training Details",
                "Our Approach ::: Axis Independent LSTM",
                "Our Approach ::: Spatial AI-LSTM"
            ],
            "predicted": [
                "Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal). Unlike traditional image classification or object detection models where neural networks learn hierarchical spatial features from data, sign recognition requires capture of temporal body motion.",
                "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train.",
                "Table TABREF28 shows the comparative results among our proposed architectures and baselines. Overall, we use data from 12 subjects for our experiments which sum up to 13107 sign gesture samples in total. To evaluate model performance on a specific subject (test subject), we adopt cross subject evaluation criteria. Suppose, X is the test subject. We train our networks with all sign samples except those are from subject X. We use subject X's data as test split to evaluate the performance of the networks. Table TABREF28 shows the average test accuracy for all 12 subjects. We can see that 3D CNN network alone performs worse than simpler baselines. But when coupled with AI-LSTM as Max CNN-LSTM, it shows an increase in recognition accuracy by 2% from AI-LSTM alone. This is because some of the signs are confused by the AI-LSTM network because of similar skeletal motion pattern. Incorporating spatial relationship among joints leads to a significant accuracy gain. The Spatial AI-LSTM is trained only on skeletal data but outperforms the combined network by 6%."
            ],
            "predicted_section": [
                "Experiments ::: Training Details",
                "Experiments ::: Experimental Results",
                "Our Approach"
            ]
        },
        "f7b91b99279833f9f489635eb8f77c6d13136098": {
            "question_text": "Which sentence compression technique works best?",
            "from_paper": "1912.11980",
            "gold": [
                "To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.",
                "According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:"
            ],
            "gold_section": [
                "Experiments ::: Main Results ::: Machine Translation",
                "Experiments ::: Main Results ::: Sentence Compression"
            ],
            "predicted": [
                "Generally, sentence compression is a typical sequence generation task which aims to maximize the absorption and long-term retention of large amounts of data over a relatively short sequence for text understanding BIBREF5, BIBREF6. To distinguish the importance of words in the sentence and, more importantly, to dig out the most salient part in the sentence representation, we utilize the sentence compression method to explicitly distill the key knowledge that can retain the key meaning of the sentence, termed explicit sentence compression (ESC) in this paper. Depending on whether or not the sentence compression is trained using human annotated data, the proposed method can be implemented in three ways: supervised ESC, unsupervised ESC, and semi-supervised ESC.",
                "Different from these work, our proposed sentence compression model does not rely on any known linguistics motivated (such as syntax) skeleton simplification, but directly trains a computation motivated sentence compression model to learn to compress sentences and re-paraphrase them directly in seq2seq model. Though with a pure computation source, our sentence compression model can surprisingly generate more grammatically correct and refined sentences, and the words in the compressed sentence do not have to be the same as the original sentence. In the meantime, our sentence compression model can stably give source backbone representation exempt from unstable performance of a syntactic parser which is essential for syntactic skeleton simplification. Our sentence compression model can perform unsupervised training on large-scale data sets, and then use the supervised data for finetune, which is more promising from the results.",
                "To demonstrate the effectiveness of sentence compression, we compared the compressed sentences ($\\gamma = 0.6$) generated in the Transformer translation system (BBFNMT) under different settings: AllText, F8W, RandSample (random sampling), supervised ESC, Unsupervised ESC and semi-supervised ESC. Table TABREF39 shows the results on newstest2014 for the EN-DE translation task."
            ],
            "predicted_section": [
                "Explicit Sentence Compression",
                "Related Work",
                "Experiments ::: Ablation Study ::: Evaluating Sentence Compression"
            ]
        },
        "99e514acc0109b7efa4e3860ce1e8c455f5bb790": {
            "question_text": "Do they compare performance against state of the art systems?",
            "from_paper": "1912.11980",
            "gold": [
                "The proposed NMT model was evaluated on the WMT14 English-to-German (EN-DE) and English-to-French (EN-FR) tasks, which are both standard large-scale corpora for NMT evaluation. For the EN-DE translation task, 4.43 M bilingual sentence pairs from the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7. The newstest2013 and newstest2014 datasets were used as the dev set and test set, respectively. For the EN-FR translation task, 36 M bilingual sentence pairs from the WMT14 dataset were were used as training data. Newstest12 and newstest13 were combined for validation and the newstest14 was the test set, following the setting of BIBREF29. The BPE algorithm BIBREF23 was also adopted, and the joint vocabulary size was set at 40 K. For the hyper-parameters of our Transformer (base/large) models, we followed the settings used in BIBREF0's work.",
                "In addition, we also reported the state-of-the-art results in recent literatures, including modelling local dependencies (Localness) BIBREF30, fusing multiple-layer representations in SANs (Context-Aware) BIBREF31, and fusing all global context representations in SANs (global-deep context) BIBREF32. MultiBLEU was used to evaluate the translation task.",
                "According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:"
            ],
            "gold_section": [
                "Experiments ::: Main Results ::: Machine Translation",
                "Experiments ::: Setup ::: Machine Translation"
            ],
            "predicted": [
                "4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines.",
                "To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.",
                "A major challenge in supervised sentence compression is the scarce high quality human annotated parallel data. In practice, due to the lack of parallel annotated data, the supervised sentence compression model cannot be trained or the annotated data domain is different, resulting in the sentence compression model trained on the in-domain performing poorly on the out-of-domain."
            ],
            "predicted_section": [
                "Experiments ::: Main Results ::: Machine Translation",
                "Explicit Sentence Compression ::: Unsupervised ESC",
                "Experiments ::: Main Results ::: Sentence Compression"
            ]
        },
        "ac87dd34d28c3edd9419fa0145f3d38c87d696aa": {
            "question_text": "What is the dataset that is used to train the embeddings?",
            "from_paper": "1807.08089",
            "gold": [
                "We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features."
            ],
            "gold_section": [
                "Dataset"
            ],
            "predicted": [
                "In Stage 2, the two encoders INLINEFORM0 and INLINEFORM1 were both 2-hidden-layer fully-connected feedforward networks with size 256. The size of embedding vectors was set to be 128. The context window size was 5, and the negative sampling number was 5.",
                "For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5.",
                "Assume we have the audio embeddings for a set of spoken words INLINEFORM0 INLINEFORM1 , where INLINEFORM2 is the embedding obtained for a spoken word INLINEFORM3 and INLINEFORM4 is the total number of distinct spoken words in the audio corpus. On the other hand, assume we have the text embeddings INLINEFORM5 INLINEFORM6 , where INLINEFORM7 is the embedding of the INLINEFORM8 -th text word for the INLINEFORM9 distinct text words. Although the distributions of INLINEFORM10 and INLINEFORM11 in their respective spaces are not parallel, that is, a specific dimension in the space for INLINEFORM12 does not necessarily correspond to a specific dimension in the space for INLINEFORM13 , there should exist some consistent relationship between the two distributions. For example, the relationships among the words {France, Paris, Germany} learned from context should be consistent in some way, regardless of whether they are in text or spoken form. So we try to learn a mapping relation between the two spaces. It will be clear below such a mapping relation can be used to evaluate the phonetic and semantic information carried by the audio embeddings."
            ],
            "predicted_section": [
                "Model Implementation",
                "Parallelizing Audio and Text Embeddings for Evaluation Purposes"
            ]
        },
        "50bcbb730aa74637503c227f022a10f57d43f1f7": {
            "question_text": "what is the baseline model",
            "from_paper": "1703.05320",
            "gold": [
                "We ran SVM-Rank with different combinations of features listed in Table 2 , but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability."
            ],
            "gold_section": [
                "Information Retrieval"
            ],
            "predicted": [
                "where: $S_0$ is the highest relevant score. In other words, the score ratio of a relevant article and the most relevant article should not be lower than 85% (choosing the value 0.85 for this threshold is simply heuristic based). This is to prevent a relevant article to have a very low score as opposed to the most relevant article.",
                "where: $f(x)=w^Tx$ is a linear scoring function, $(x_u,x_v)$ is a pairwise and $\\xi _{u,v}^{(i)}$ is the loss. The document pairwise in our model is a pair of a query and an article.",
                "For information retrieval task, 20% of query-article pairs are used for evaluating our model while the rest is for training. As we only consider single-paragraph articles in the training phase, if a multiple-paragraph article is relevant, all of its generated single-paragraph articles will be marked as relevant. In addition, the label for each query-article pair is set either 1 (relevant) or 0 (irrelevant). In our experiment, instead of selecting top $k$ retrieved articles as relevant articles, we consider a retrieved article $A_i$ as a relevant article if its score $S_i$ satisfies Equation ( 26 ): "
            ],
            "predicted_section": [
                "Legal Information Retrieval",
                "Information Retrieval"
            ]
        },
        "8246d1eee1482555d075127ac84f2e1d0781a446": {
            "question_text": "what datasets were used?",
            "from_paper": "1805.11598",
            "gold": [
                "We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates."
            ],
            "gold_section": [
                "Data"
            ],
            "predicted": [
                "We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.",
                "We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table TABREF3 .",
                "Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other."
            ],
            "predicted_section": [
                "Monolingual Baseline",
                "Data"
            ]
        },
        "1ec0be667a6594eb2e07c50258b120e693e040a8": {
            "question_text": "what is the monolingual baseline?",
            "from_paper": "1805.11598",
            "gold": [
                "We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency."
            ],
            "gold_section": [
                "Monolingual Baseline"
            ],
            "predicted": [
                "In the first polyglot variant, we consider multilingual sharing between each language and English by using pretrained multilingual embeddings. This polyglot model is trained on the union of annotations in the two languages. We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch.",
                "We present our results in Table TABREF11 . We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) show the most improvement, while large-dataset languages such as Czech or Chinese see little or no improvement (Figure FIGREF10 ).",
                "The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning BIBREF9 . We produced multilingual embeddings from the monolingual embeddings using the method of ammar2016massively: for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space BIBREF10 ."
            ],
            "predicted_section": [
                "Experiments",
                "Simple Polyglot Sharing"
            ]
        },
        "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c": {
            "question_text": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?",
            "from_paper": "1707.06519",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of INLINEFORM0 with different levels of accessibility to the low-resource target language along with two baseline models, INLINEFORM1 and INLINEFORM2 trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. In Table TABREF20 , we took different partitions of the target language training sets to fine tune the INLINEFORM3 pretrained by the source languages. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning.",
                "To evaluate the quality of language transfer, we trained the Audio Word2Vec model by INLINEFORM0 from the source language, English, and applied it on different target languages, French (FRE), German (GER), Czech (CZE), and Spanish (ESP). We computed the average cosine similarity of the vector representations for each pair of the audio segments in the retrieval database of the target languages (20K segments for each language), and compare it with the phoneme sequence edit distance (PSED). The average and variance (the length of the black line on each bar) of the cosine similarity for groups of pairs clustered by the phoneme sequence edit distances (PSED) between the two words are shown in Fig. FIGREF14 . For comparison, we also provide the results obtained from the English retrieval database (250K segments), where the segments were not seen by the model in training procedure.",
                "From Table TABREF20 , INLINEFORM0 trained by source language generally outperforms the INLINEFORM1 trained by the limited amount of target language (\" INLINEFORM2 No Transfer\"), proving that with enough audio segments, INLINEFORM3 can identify and encode universal phonetic structure. Comparing with NE, INLINEFORM4 surpasses INLINEFORM5 in German and French even without fine-tuning, whereas in Czech, INLINEFORM6 also achieves better score than INLINEFORM7 with fine-tuning. However, in Spanish, INLINEFORM8 achieved a MAP score of 0.13 with fine-tuning, slightly lower than 0.17 obtained by INLINEFORM9 . Back to Fig. FIGREF14 , the gap between phoneme sequence edit distances 2 and 3 in Spanish is smaller than other languages. Also, as discussed earlier in Section 6.2, the variance in Spanish is also bigger. The smaller gap and bigger variance together indicate that the model is weaker on Spanish at identifying audio segments of different words and thus affects the MAP performance in Spanish."
            ],
            "predicted_section": [
                "Language Transferring on STD",
                "Analysis of Language Transfer"
            ]
        },
        "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14": {
            "question_text": "Which languages do they explore?",
            "from_paper": "1911.12848",
            "gold": [
                "Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.",
                "Code-mixing is mixing two or more languages while communicating in person or over the web. Code-mixing is basically observed in the multilingual speakers. Code-mixed languages are a challenge to the sentiment analysis problem. A classic example of the code-mix language is Hinglish which is combination of English and Hindi words present in a sentence. Hinglish is widely used language in India to communicate over the web. For e.g. movie review in Hinglish is \u201cyeh movie kitni best hai.. Awesome.\u201d In this sentence movie, best and awesome are English words but the remaining words are Hindi words, so the language identification becomes the first step in code mix languages followed by the SA which indirectly increases the overhead for the researchers and becomes time consuming process.",
                "Pandey et al. BIBREF12 defined a framework to carry out the SA task on the Hindi movie reviews. BIBREF12 observed that the lower accuracy was obtained by using SWN as a classification technique and hence suggested using synset replacement algorithm along with the SWN. Synset replacement algorithms groups the synonymous words having same concepts together. It helped in increasing the accuracy of the system because if the word was not present in the Hindi SWN then it found the closest word and assigned the score of that word BIBREF12. In the study, Bhargava et al. BIBREF13 completed the SA task on the FIRE 2015 dataset. The dataset consisted of code-mixed sentences in English along with 4 Indian languages (Hindi, Bengali, Tamil, Telugu). The architecture consisted of 2 main steps Language Identification and Sentiment Classification. Punctuations, hashtags were identified and handled by the CMU Ark tagger. Machine learning techniques like logistic regression and SVM were used for language identification. SWN\u2019s of each language were used for sentiment classification. The results of the implemented system were compared with the previous language translation technique and 8% better precision was observed BIBREF13."
            ],
            "gold_section": [
                "Introduction ::: Indigenous Languages",
                "Sentiment Analysis Levels ::: Sentence Level",
                "Introduction ::: Code Mix Languages"
            ],
            "predicted": [
                "According to the authors Medagoda et al. BIBREF0 there has being a continuous research going on in the English language but the research carried out in the indigenous languages is less. Also, the researches in indigenous languages follow the techniques used for the English language but this has one disadvantage which is, techniques have properties which are specific to a language. Hence It is really important to understand and analyze Indigenous language data because it can give meaningful insights to the companies. For example, India and China have world's largest population and are rich in diverse languages, analysing these indigenous language will be useful to companies because they have large share of users in India and China. In the current study, the types of languages i.e. indigenous languages and code mix languages are discussed prior to the approaches, methodologies used by the researchers and challenges faced by them.",
                "With the increasing use of the web there is a lot of User Generated Content (UGC) available on different websites. Lot of research is carried out for the English language. Work done for the indigenous languages is less as compared to the English language. By studying different papers on SA, it can be found out that researchers have started working on the indigenous languages. Data for the indigenous languages is available across the web but is mainly collected from social media platforms like Twitter, Facebook and YouTube.",
                "Majority of the research carried out for indigenous languages is performed using Machine Learning algorithms except the research carried out by the authors in BIBREF12, BIBREF24, BIBREF26, BIBREF25. Deep learning algorithms have time and again proved to be much better than the traditional machine learning techniques."
            ],
            "predicted_section": [
                "Datasets",
                "Discussions and Analysis",
                "Introduction"
            ]
        },
        "5d790459b05c5a3e6f1e698824444e55fc11890c": {
            "question_text": "What are two baseline methods?",
            "from_paper": "1911.01770",
            "gold": [
                "Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.",
                "The proposed model architecture is based on a multi-path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to BIBREF19. In Figure FIGREF4, the overall structure is presented. For the instruction encoder, we utilized a self-attention mechanism BIBREF20, which learns which words of the instructions are relevant with a certain ingredient. In order to encode the ingredients, a bidirectional RNN is used, since ingredients are an unordered list of words. All RNNs in the ingredients path were implemented with Long Short-Term Memory (LSTM) cells BIBREF21. We fixed the ingredient representation to have a length of 600, independent of the amount of ingredients. Lastly, the outputs of the self-attention-instruction encoder with ingredient attention and the output of the bidirectional LSTM ingredient-encoder are concatenated and mapped to the joint embedding space. The image analysis path is composed of a ResNet-50 model BIBREF22, pretrained on the ImageNet Dataset BIBREF23, with a custom top layer for mapping the image features to the joint embedding space. All word embeddings are pretrained with the word2vec algorithm BIBREF24 and fine tuned during the joint embedding learning phase. We chose 512-dimensional word embedding for our model with self-attention, whereas BIBREF19 and BIBREF17 chose a vector length of 300. In the following sections, more details about the aforementioned paths are presented.",
                "The emergence of multi-modal databases has led to novel approaches for meal image analysis. The fusion of visual features learned from images by deep Convolution Neural Networks (CNN) and textual features lead to outstanding results in food recognition applications. An early approach for recipe retrieval was based on jointly learning to predict food category and its ingredients using deep CNN BIBREF16. In a following step, the predicted ingredients are matched against a large corpus of recipes. More recent approach is proposed by BIBREF15 and is based on jointly learning recipe-text and image representations in a shared latent space. Recurrent Neural Networks (RNN) and CNN are mainly used to map text and image into the shared space. To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. BIBREF17 proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. BIBREF15, they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup. To address the challenge of encoding long sequences (like recipe instructions), BIBREF15 chose to represent single instructions as sentence embedding using the skip-thought technique BIBREF18. These encoded instruction sentences are referred to as skip-instructions and their embedding is not fine tuned when learning the image-text joint embedding."
            ],
            "gold_section": [
                "Materials and Methods ::: Model Architecture",
                "Experimental Setup and Results",
                "Introduction"
            ],
            "predicted": [
                "Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure FIGREF16 we present a few typical results on the intended recipe retrieval task.",
                "where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$.",
                "We have trained our model using cosine similarity loss with margin as in BIBREF19 and with the triplet loss proposed by BIBREF17. Both objective functions and the semantic regularization by BIBREF19 aim at maximizing intra-class correlation and minimizing inter-class correlation."
            ],
            "predicted_section": [
                "Materials and Methods ::: Loss function",
                "Experimental Setup and Results"
            ]
        },
        "1f8044487af39244d723582b8a68f94750eed2cc": {
            "question_text": "What unsupervised approach was used to deduce the thematic information?",
            "from_paper": "1910.01340",
            "gold": [
                "Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.",
                "Previous works BIBREF7 have investigated IRA campaign efforts on Facebook, and they found that IRA pages have posted more than $\\sim $80K posts focused on division issues in US. Later on, the work in BIBREF2 has analyzed Facebook advertised posts by IRA and they specified the main themes that these advertisements discussed. Given the results of the previous works, we applied a topic modeling technique on our dataset to extract its main themes. We aim to detect IRA trolls by identifying their suspicious ideological changes across a set of themes.",
                "Given our dataset, we applied Latent Dirichlet Allocation (LDA) topic modeling algorithm BIBREF8 on the tweets after a prepossessing step where we maintained only nouns and proper nouns. In addition, we removed special characters (except HASH \"#\" sign for the hashtags) and lowercase the final tweet. To ensure the quality of the themes, we removed the hashtags we used in the collecting process where they may bias the modeling algorithm. We tested multiple number of themes and we chose seven of them. We manually observed the content of these themes to label them. The extracted themes are: Police shootings, Islam and War, Supporting Trump, Black People, Civil Rights, Attacking Hillary, and Crimes. In some themes, like Supporting Trump and Attacking Hillary, we found contradicted opinions, in favor and against the main themes, but we chose the final stance based on the most representative hashtags and words in each of them (see Figure FIGREF11). Also, the themes Police Shooting and Crimes are similar, but we found that some words such as: police, officers, cops, shooting, gun, shot, etc. are the most discriminative between these two themes. In addition, we found that the Crimes theme focuses more on raping crimes against children and women. Our resulted themes are generally consistent with the ones obtained from the Facebook advertised posts in BIBREF2, and this emphasizes that IRA efforts organized in a similar manner in both social media platforms."
            ],
            "gold_section": [
                "Textual Representation ::: Thematic Information",
                "Introduction"
            ],
            "predicted": [
                "Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets.",
                "Why do the thematic information help? The Flip-Flop behavior. As an example, let's considering the fear and joy emotions in Figure FIGREF34. We can notice that all the themes that used to nudge the division issues have a decreasing dashed line, where others such as Supporting Trump theme has an extremely increasing dashed line. Therefore, we manually analyzed the tweets of some IRA accounts and we found this observation clear, as an example from user $x$:",
                "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section)."
            ],
            "predicted_section": [
                "Textual Representation ::: Thematic Information",
                "Experiments and Analysis ::: Results",
                "Experiments and Analysis ::: Analysis"
            ]
        },
        "595fe416a100bc7247444f25b11baca6e08d9291": {
            "question_text": "What profile features are used?",
            "from_paper": "1910.01340",
            "gold": [
                "Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.",
                "As Twitter declared, although the IRA campaign was originated in Russia, it has been found that IRA trolls concealed their identity by tweeting in English. Furthermore, for any possibility of unmasking their identity, the majority of IRA trolls changed their location to other countries and the language of the Twitter interface they use. Thus, we propose the following features to identify these users using only their tweets text:",
                "Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.",
                "Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length."
            ],
            "gold_section": [
                "Textual Representation ::: Profiling IRA Accounts",
                "Introduction"
            ],
            "predicted": [
                "For the theme-based features, we use the following features that we believe that they change based on the themes:",
                "IRA dataset provided by Twitter contains less information about the accounts details, and they limited to: profile description, account creation date, number of followers and followees, location, and account language. Therefore, as another baseline we use the number of followers and followees to assess their identification ability (we will mention them as Network Features in the rest of the paper).",
                "Based on our thematic information, we model the users textual features w.r.t. each of these themes. In other words, we model a set of textual features independently for each of the former themes to capture the emotional, stance, and others changes in the users tweets."
            ],
            "predicted_section": [
                "Experiments and Analysis ::: Baselines",
                "Textual Representation ::: Thematic Information"
            ]
        },
        "3a25f82512d56d9e1ffba72f977f515ae3ba3cca": {
            "question_text": "Does programme plans gathering and open sourcing some large dataset for Icelandic language?",
            "from_paper": "2003.09244",
            "gold": [
                "As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.",
                "We will update the IGC with new data from more sources and continue collecting data from rights holders who have given their permission for using their material. A new version will be released each year during the five-year programme.",
                "Treebanks. The largest of the syntactically parsed treebanks that exist is the Icelandic Parsed Historical Corpus (IcePaHC; Wallenberg et al., 2011; R\u00f6gnvaldsson et al., 2011, 2012), which contains one million words from the 12th to the 21st century. The scheme used for the syntactic annotation is based on the Penn Parsed Corpora of Historical English BIBREF24, BIBREF25. On the other hand, no Universal Dependencies (UD)-treebanks are available for Icelandic. Within the programme, a UD-treebank will by built, based on IcePaHC, and extended with new material.",
                "Morphological database. The Database of Icelandic Morphology (DIM; Bjarnad\u00f3ttir et al., 2019) contains inflectional paradigms of about 287,000 lemmas. A part of the database, DMII-Core, only includes data in a prescriptive context and is suited for language learners, creating teaching material and other prescriptive uses. It consists of the inflection of approx. 50,000 words. We will extend it by reviewing ambiguous inflection forms. We will define format for data publication as the core will be available for use by a third party. For the sake of simplifying the process of adding material to the database and its maintenance, we will take advantage of the lexicon acquisition tool described in Section SECREF16 and adapt it for DIM."
            ],
            "gold_section": [
                "Core Projects ::: Language Resources"
            ],
            "predicted": [
                ". The text-to-speech project will produce language resources that enable voice building for Icelandic.",
                "The Icelandic Government decided in 2017 to fund a five-year programme for Icelandic LT, based on a report written by a group of LT experts BIBREF0. After more than two years of preparation, a consortium consisting of universities, institutions, associations, and private companies started the work on the programme on the 1st of October 2019. The goal of the programme is to ensure that Icelandic can be made available in LT applications, and thus will be usable in all areas of communication. Furthermore, that access to information and other language-based communication and interaction in Icelandic will be accessible to all, e.g. via speech synthesis or speech-to-text systems.",
                "Parallel data. Icelandic's rich morphology and relatively free word order is likely to demand large amount of training data in order to develop MT systems that produce adequate and fluent translations. The ParIce corpus currently consists of only 3.5 million sentence pairs which is rather small in relation to parallel corpora in general. The goal of this phase is to create an aligned and filtered parallel corpus of translated documents from the European Economic Area (EEA) domain (e.g. regulations and directives). As of 2017, around 7,000 documents were available in Icelandic with corresponding documents in English. The aim is to pair all accessible documents in the course of the project."
            ],
            "predicted_section": [
                "Core Projects ::: Speech Synthesis (TTS)",
                "Core Projects ::: Machine Translation",
                "Introduction"
            ]
        },
        "b59f3a58939f7ac007d3263a459c56ebefc4b49a": {
            "question_text": "What concrete software is planned to be developed by the end of the programme?",
            "from_paper": "2003.09244",
            "gold": [
                "Previously, two tokenisers have been built for Icelandic, one is a part of IceNLP and the other a part of Greynir. As Greynir is still in active development, it will be used as a base for the LT project's development. In order to be able to test the tokenisers' accuracy, a test set that takes different tokeniser settings into account will be developed.",
                "Software development. The software development tasks of the spell and grammar checking project will build a working open source correction system whose development is informed by the analysis of the data sets created within the project. The spell and grammar checker will be based on the foundation for processing Icelandic text provided by the Greynir system.",
                "Software implementation and research. The research areas are chosen so to enhance the language resource development for Icelandic. A punctuation system for Icelandic will be analysed and implemented. Compound words are common in Icelandic and the language also has a relatively rich inflection structure so it is important to address those features for language modeling. Pronunciation analysis, speaker diarization and speech analysis will also be addressed especially for Icelandic, and acoustic modelling for children and teenagers receive attention in the project."
            ],
            "gold_section": [
                "Core Projects ::: Spell and Grammar Checking",
                "Core Projects ::: Automatic Speech Recognition (ASR)",
                "Core Projects ::: NLP Tools"
            ],
            "predicted": [
                "The focus of the programme will be on the development of text and speech-based language resources, on the development of core natural language processing (NLP) tools like tokenisers, taggers and parsers, and finally, to publish open-source software in the areas of speech recognition, speech synthesis, machine translation, and spell and grammar checking. All deliverables of the programme will be published under open licenses, to encourage use of resources and software in commercial products.",
                "After studying somewhat similar national programmes in other European countries, we have defined the most important factors that in our opinion will help lead to the success of the programme: First, we have defined core projects that comprise the most important language resources and software tools necessary for various LT applications. Second, all deliverables will be published under as open licenses as possible and all resources and software will be easily accessible. The deliverables will be packaged and published for use in commercial applications, where applicable. Third, from the beginning of the programme, we encourage innovation projects from academia and industry through a competitive R&D fund, and fourth, constant communication with users and industry through conferences, events and direct interaction will be maintained, with the aim of putting deliverables to use in products as soon as possible. The cooperation between academia and industry is also reflected in the consortium of universities, institutions, associations, and private companies, performing the R&D work for all core projects.",
                "The plan was to facilitate the development of tools and linguistic resources. Examples of tools are named entity recognisers, word-sense disambiguation, tools for computing semantic similarity and text classification, automatic summarisation and MT. Examples of linguistic resources to be developed in the programme are parallel corpora, lists of proper nouns, terminology lists and dictionaries."
            ],
            "predicted_section": [
                "Conclusion",
                "Other European LT Programmes ::: Spain",
                "Introduction"
            ]
        },
        "477da8d997ff87400c6aad19dcc74f8998bc89c3": {
            "question_text": "How are the results evaluated?",
            "from_paper": "1805.11850",
            "gold": [
                "Here, we describe the experimental method used to validate the effectiveness of the NJM. We compare the proposed method with two other methods of generating funny captions: 1) human generated captions, which are highly ranked on Bokete (indicated by \u201cHuman\" in Table TABREF10 ), and 2) Japanese image caption generation using CNN+LSTM pre-trained by STAIR caption BIBREF7 . Based on the captions provided by MS COCO, the STAIR caption is translated from English to Japanese (indicated by \u201cSTAIR caption\u201d in Table TABREF10 ). We use a questionnaire as the evaluation method. We selected a total of 30 themes from the Bokete Ogiri website that included \u201cpeople\u201d, \u201ctwo or more people\u201d, \u201canimals\u201d, \u201clandscape\u201d, \u201cinorganics\u201d, and \u201cillustrations\u201d. The questionnaire asks respondents to rank the captions provided by humans, the NJM, and STAIR caption in order of \u201cfunniness\u201d. The questionnaire does not reveal the origins of the captions."
            ],
            "gold_section": [
                "Experimental contents"
            ],
            "predicted": [
                "We conducted evaluations to confirm the effectiveness of the proposed method. We describe the experimental method in Section SECREF11 , and the experimental results are presented in Section SECREF12 .",
                "The Bokete Ogiri website uses the number of stars to evaluate the degree of funniness of a caption. The user evaluates the \u201cfunniness\u201d of a posted caption and assigns one to three stars to the caption. Therefore, funnier captions tend to be assigned a lot of stars. We focus on the number of stars in order to propose an effective training method, in which the Funny Score enables us to evaluate the funniness of a caption. Based on the results of our pre-experiment, a Funny Score of 100 stars is treated as a threshold. In other words, the Funny Score outputs a loss value INLINEFORM0 when #star is less than 100. In contrast, the Funny Score returns INLINEFORM1 when #star is over 100. The loss value INLINEFORM2 is calculated with the LSTM as an average of each mini-batch.",
                "In the experimental section, we compare the proposed method based on Funny Score and BoketeDB pre-trained parameters with a baseline provided by MS COCO Pre-trained CNN+LSTM. We also compare the results of the NJM with funny captions provided by humans. In an evaluation by humans, the results provided by the proposed method were ranked lower than those provided by humans (22.59% vs. 67.99%) but were ranked higher than the baseline (9.41%). Finally, we show the generated funny captions for several images."
            ],
            "predicted_section": [
                "Experiment",
                "Funny Score",
                "Introduction"
            ]
        },
        "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5": {
            "question_text": "Which of their proposed domain adaptation methods proves best overall?",
            "from_paper": "1710.06923",
            "gold": [
                "We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair and fitness functions, namely Equation (), would probably yield much better performance accuracies. However, experimental results confirm that the proposed Evo-Devo mechanism is an approach that is able to adapt INLINEFORM0 to get closer to INLINEFORM1 . We present a snapshot of the experiments with Google ASR (Ga) and calculate accuracy with respect to the user spoken question as shown in Table TABREF16 .",
                "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.",
                "The combination of features INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 namely, (bag of consonants, bag of vowels, left context, number of words, right context) gave the best results with INLINEFORM5 % improvement in accuracy in classification over 10-fold validation."
            ],
            "gold_section": [
                "Machine Learning experiments",
                " Evo-Devo based experiments"
            ],
            "predicted": [
                "We present the results of these two adaptation and gauge the usefulness of each mechanism. The rest of the paper is organized as follows, in Section SECREF2 we briefly describe the work done in this area which motivates our contribution. The main contribution of our work is captured in Section SECREF3 and we show the performance of our approach through experiments in Section SECREF4 . We conclude in Section SECREF5 .",
                "Table TABREF16 clearly demonstrates the promise of the evo-devo mechanism for adaptation/repair. In our experiments we observed that the adaptation/repair of sub-parts in ASR-output ( INLINEFORM0 ) that most probably referred to domain terms occurred well and were easily repaired, thus contributing to increase in accuracy. For non-domain-specific linguistic terms the method requires one to build very good linguistic repair rules, without which the method could lead to a decrease in accuracy. One may need to fine-tune the repair, match and fitness functions for linguistic terms. However, we find the abstraction of evo-devo mechanism is very apt to use.",
                "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors."
            ],
            "predicted_section": [
                "Machine Learning experiments",
                " Evo-Devo based experiments",
                "Introduction"
            ]
        },
        "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86": {
            "question_text": "Which intrisic measures do they use do evaluate obtained representations?",
            "from_paper": "1811.04791",
            "gold": [
                "The results above were presented as part of an earlier conference version of this paper BIBREF3 . Here, we expand upon that work in several ways. First, we include new results on the corpora and evaluation measures used in the zrsc, to allow more direct comparisons with other work. In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures. This permits the two measures themselves to be better compared. Finally, we provide both a qualitative analysis of the differences between the different features we extract, and a quantitative evaluation on the downstream target-language task of unsupervised full-coverage speech segmentation and clustering using the system of BIBREF4 . This is the first time that multilingual features are used in such a system, which performs a complete segmentation of input speech into hypothesized words. As in our intrinsic evaluations, we find that the multilingual bnfs consistently outperform the best unsupervised cae features, which in turn outperform or do similarly to MFCCs.",
                "In this work we use the cae in our experiments on unsupervised representation learning, since it performed well in the 2015 ZRSC, achieved some of the best-reported results on the same-different task (which we also consider), and has readily available code. As noted above, the cae attempts to normalize out non-linguistic factors such as speaker, channel, gender, etc., by using top-down information from pairs of similar speech segments. Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase. Each pair is then aligned at the frame level using dtw, and pairs of aligned frames are presented as the input INLINEFORM0 and target output INLINEFORM1 of a dnn. After training, a middle layer INLINEFORM2 is used as the learned feature representation."
            ],
            "gold_section": [
                "Background and Motivation",
                "Introduction"
            ],
            "predicted": [
                "We use the GlobalPhone corpus of speech read from news articles BIBREF20 . We chose 6 languages from different language families as zero-resource languages on which we evaluate the new feature representations. That means our models do not have any access to the transcriptions of the training data, although transcriptions still need to be available to run the evaluation. The selected languages and dataset sizes are shown in Table TABREF8 . Each GlobalPhone language has recordings from around 100 speakers, with 80% of these in the training sets and no speaker overlap between training, development, and test sets.",
                "We use several metrics to compare the resulting segmented word tokens to ground truth forced alignments of the data. By mapping every discovered word token to the ground truth word with which it overlaps most, average cluster purity can be calculated as the total proportion of correctly mapped tokens in all clusters. More than one cluster may be mapped to the same ground truth word type. In a similar way, we can calculate unsupervised word error rate (WER), which uses the same cluster-to-word mapping but also takes insertions and deletions into account. Here we consider two ways to perform the cluster mapping: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or one-to-one, where at most one cluster is mapped to a ground truth word type (accomplished in a greedy fashion). We also compute the gender and speaker purity of the clusters, where we want to see clusters that are as diverse as possible on these measures, i.e., low purity. To explicitly evaluate how accurate the model performs segmentation, we compare the proposed word boundary positions to those from forced alignments of the data (falling within a single true phoneme from the boundary). We calculate boundary precision and recall, and report the resulting word boundary F-scores. We also calculate word token F-score, which requires that both boundaries from a ground truth word token be correctly predicted.",
                "Our experiments therefore aim to evaluate on a wider range of target languages, and to explore the effects of both the amount of labeled data, and the number of languages from which it is obtained."
            ],
            "predicted_section": [
                "Experimental Setup",
                "Background and Motivation",
                "Experimental Setup and Evaluation"
            ]
        },
        "101d7a355e8bf6d1860917876ee0b9971eae7a2f": {
            "question_text": "Do they report results only for English data?",
            "from_paper": "1611.04887",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "This test essentially captures the importance of \u201cnatural word order\u201d. We found that LDA was invariant to the reordering of the words in the tweet for most of the tasks. This result is not surprising as LDA considers each word in the tweet independently. CNN, LSTM and BLSTM rely on the word order significantly to perform well for most of the prediction tasks.",
                "This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.",
                "In this section we perform an extensive evaluation of all the models in an attempt to find the significance of different representation models. Essentially we study every model (with optimal settings reported in the corresponding paper) with respect to the following three perspectives."
            ],
            "predicted_section": [
                "Sensitivity of Property Prediction Task to Word Order",
                "Conclusion",
                "Experiments"
            ]
        },
        "4288621e960ffbfce59ef1c740d30baac1588b9b": {
            "question_text": "What conclusions do the authors draw from their experiments?",
            "from_paper": "1611.04887",
            "gold": [
                "This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order."
            ],
            "gold_section": [
                "Conclusion"
            ],
            "predicted": [
                "The paper is organized as follows. Sections 2 and 3 discuss the set of proposed elementary property prediction tasks and the models considered for this study respectively. Section 4 and 5 presents the experiment setup and result analysis respectively. We conclude the work with a brief summary in Section 5.",
                "In this section we list down the set of models considered in the study.",
                "In this section we perform an extensive evaluation of all the models in an attempt to find the significance of different representation models. Essentially we study every model (with optimal settings reported in the corresponding paper) with respect to the following three perspectives."
            ],
            "predicted_section": [
                "Experiments",
                "Representation Models",
                "Introduction"
            ]
        },
        "042800c3336ed5f4826203616a39747c61382ba6": {
            "question_text": "Which commonsense knowledge base are they using?",
            "from_paper": "1709.05453",
            "gold": [
                "In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion."
            ],
            "gold_section": [
                "ConceptNet"
            ],
            "predicted": [
                "Several commonsense knowledge bases have been constructed during the past decade, such as ConceptNet BIBREF17 and SenticNet BIBREF18 . The aim of commonsense knowledge representation and reasoning is to give a foundation of real-world knowledge to a variety of AI applications, e.g., sentiment analysis BIBREF19 , handwriting recognition BIBREF20 , e-health BIBREF21 , aspect extraction BIBREF22 , and many more. Typically, a commonsense knowledge base can be seen as a semantic network where concepts are nodes in the graph and relations are edges (Figure 2 ). Each $<concept1, relation, concept2 >$ triple is termed an assertion.",
                "In the context of artificial intelligence (AI), commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate BIBREF3 , BIBREF4 , BIBREF5 . Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods.",
                "In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \\in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \\in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., \u201cdog\u201d and \u201cbook\u201d) or a multi-word expression (e.g., \u201ctake_a_stand\u201d and \u201cgo_shopping\u201d). We build a dictionary $C$2 out of $C$3 where every concept $C$4 is a key and a list of all assertions in $C$5 concerning $C$6 , i.e., $C$7 or $C$8 , is the value. Our goal is to retrieve commonsense knowledge about every concept covered in the message."
            ],
            "predicted_section": [
                "Commonsense Knowledge Retrieval",
                "Commonsense Knowledge",
                "Introduction"
            ]
        },
        "74fcb741d29892918903702dbb145fef372d1de3": {
            "question_text": "What is the model trained?",
            "from_paper": "1909.02322",
            "gold": [
                "We propose an alternative to the Extract first, Abstract later (EA) approach which eliminates the need for an extractive model and enables the use of all input documents when generating the summary. Figure FIGREF5 illustrates our Condense-Abstract (CA) framework. In lieu of an integrated encoder-decoder, we generate summaries using two separate models. The Condense model returns document encodings for $N$ input documents, while the Abstract model uses these encodings to create an abstractive summary. This two-step approach has at least three advantages for multi-document summarization. Firstly, optimization is easier since parameters for the encoder and decoder weights are learned separately. Secondly, CA-based models are more space-efficient, since $N$ documents in the cluster are not treated as one very large instance but as $N$ separate instances when training the Condense model. Finally, it is possible to generate customized summaries targeting specific aspects of the input since the Abstract model operates over the encodings of all available documents.",
                "Let $\\mathcal {D}$ denote a cluster of $N$ documents about a specific target (e.g., a movie or product). For each document $X=\\lbrace w_1,w_2,...,w_M\\rbrace \\in \\mathcal {D}$, the Condense model learns an encoding $d$, and word-level encodings $h_1, h_2, ..., h_M$. We use a BiLSTM autoencoder as the Condense model. Specifically, we employ a Bidirectional Long Short Term Memory (BiLSTM) encoder BIBREF31:",
                "The decoder generates summaries conditioned on the reduced document encoding $d^{\\prime }$ and reduced word-level encodings $h^{\\prime }_1,h^{\\prime }_2,...,h^{\\prime }_V$. We use a simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32. We set the first hidden state $s_0$ to $d^{\\prime }$, and run an LSTM to calculate the current hidden state using the previous hidden state $s_{t-1}$ and word $y^{\\prime }_{t-1}$ at time step $t$:"
            ],
            "gold_section": [
                "Condense-Abstract Framework ::: The Condense Model",
                "Condense-Abstract Framework ::: The Abstract Model ::: Decoder",
                "Condense-Abstract Framework"
            ],
            "predicted": [
                "We use two objective functions to train the Abstract model. Firstly, we use a maximum likelihood loss to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y=\\lbrace y_1,y_2,...,y_L\\rbrace $ provided at training time:",
                "The auto-encoder is trained with a maximum likelihood loss:",
                "For all experiments, our model used word embeddings with 128 dimensions, pretrained on the training data using GloVe BIBREF34. We set the dimensions of all hidden vectors to 256, the batch size to 8, and the beam search size to 5. We applied dropout BIBREF35 at a rate of 0.5. The model was trained using the Adam optimizer BIBREF36 and $l_2$ constraint BIBREF37 of 2. We performed early stopping based on model performance on the development set. Our model is implemented in PyTorch."
            ],
            "predicted_section": [
                "Condense-Abstract Framework ::: The Condense Model",
                "Experimental Setup ::: Training Configuration",
                "Condense-Abstract Framework ::: The Abstract Model ::: Training"
            ]
        },
        "6a20a3220c4edad758b912e2d3e5b99b0b295d96": {
            "question_text": "How exactly do they weigh between different statistical models?",
            "from_paper": "1805.04579",
            "gold": [
                "After generating summary from a particular model, our aim is to compute summaries through overlap of different models. Let us have INLINEFORM0 summaries from INLINEFORM1 different models. For INLINEFORM2 summarization model, let the INLINEFORM3 sentences contained be:-",
                "Given a document INLINEFORM0 we tokenize it into sentences as < INLINEFORM1 >.",
                "Now for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.",
                "Here, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3"
            ],
            "gold_section": [
                "Prepossessing",
                "Single Document Summarization"
            ],
            "predicted": [
                "For each model, assign the weights using INLINEFORM0 ",
                "Here, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3 ",
                "As we discussed earlier, summarization models are field selective. Some models tend to perform remarkably better than others in certain fields. So, instead of assigning uniform weights to all models we can go by the following approach."
            ],
            "predicted_section": [
                "Single Document Summarization",
                "Domain-Specific Single Document Summarization",
                "Multi-Document/Domain-Specific Summarization"
            ]
        },
        "a1a0365bf6968cbdfd1072cf3923c26250bc955c": {
            "question_text": "What type of neural models are used?",
            "from_paper": "1803.08419",
            "gold": [
                "Sequence to Sequence approaches for dialogue modelling",
                "The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.",
                "Language Model based approaches for dialogue modelling",
                "Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses."
            ],
            "gold_section": [
                "Sequence to Sequence approaches for dialogue modelling",
                "Language Model based approaches for dialogue modelling"
            ],
            "predicted": [
                "After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.",
                "Although these neural models are really powerful, so much so that they power most of the commercially available smart assistants and conversational agents. However these agents lack a sense of context and a grounding in common sense that their human interlocutors possess. This is especially evident when interacting with a commercial conversation agent, when more often that not the agent has to fall back to canned responses or resort to displaying Internet search results in response to an input utterance. One of the main goals of the research community, over the last year or so, has been to overcome this fundamental problem with conversation agents. A lot of different approaches have been proposed ranging from using knowledge graphs BIBREF20 to augment the agent's knowledge to using latest advancements in the field of online learning BIBREF21 . In this section we discuss some of these approaches.",
                "For each utterance, they calculated features like n-grams of the words and their POS tags, dialog act and task/subtask label. Then they put those features in the binary MaxEnt classifier. For this, their model was able to achieve an error rate of 25.1% for the dialog act classification which was better than the best performing models at that time. Although, according to the modern standards, the results are not that great but the approach they suggested (of using data to build machine learning models) forms the basis of the techniques that are currently used in this area."
            ],
            "predicted_section": [
                "Knowledge augmented models",
                "Reinforcement Learning based models",
                "Machine Learning Methods"
            ]
        },
        "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550": {
            "question_text": "What was the proposed use of conversational agents in pioneering work?",
            "from_paper": "1803.08419",
            "gold": [
                "Early Techniques",
                "Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times)."
            ],
            "gold_section": [
                "Early Techniques"
            ],
            "predicted": [
                "Next came the era of using machine learning methods in the area of conversation agents which totally revolutionized this field.",
                "Starting with pattern matching programs like ELIZA developed at MIT in 1964 to the current commercial conversational agents and personal assistants (Siri, Allo, Alexa, Cortana et al) that all of us carry in our pockets, conversational agents have come a long way. In this paper we look at this incredible journey. We start by looking at early rule-based methods which consisted of hand engineered features, most of which were domain specific. However, in our view, the advent of neural networks that were capable of capturing long term dependencies in text and the creation of the sequence to sequence learning model BIBREF1 that was capable of handling utterances of varying length is what truly revolutionized the field. Since the sequence to sequence model was first used to build a neural conversational agent BIBREF2 in 2016 the field has exploded. With a multitude of new approaches being proposed in the last two years which significantly impact the quality of these conversational agents, we skew our paper towards the post 2016 era. Indeed one of the key features of this paper is that it surveys the exciting new developments in the domain of conversational agents.",
                "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years. The results have been impressive enough to find their way into commercial applications such that these agents have become truly ubiquitous. We attempt to present a broad view of these advancements with a focus on the main challenges encountered by the conversational agents and how these new approaches are trying to mitigate them."
            ],
            "predicted_section": [
                "Conclusion",
                "Introduction",
                "Machine Learning Methods"
            ]
        },
        "e8fa4303b36a47a5c87f862458442941bbdff7d9": {
            "question_text": "What machine learning techniques are used in the model architecture?",
            "from_paper": "1910.03355",
            "gold": [
                "SMT systems were trained with Moses BIBREF29, following the standard procedure: we estimated a 5-gram language model\u2014smoothed with the improved KneserNey method\u2014using SRILM BIBREF30, and optimized the weights of the log-linear model with MERT BIBREF31.",
                "We built our NMT systems using NMT-Keras BIBREF32. We used long short-term memory units BIBREF33, with all model dimensions set to 512. We trained the system using Adam BIBREF34 with a fixed learning rate of $0.0002$ and a batch size of 60. We applied label smoothing of $0.1$ BIBREF35. At inference time, we used beam search with a beam size of 6. We applied joint byte pair encoding to all corpora BIBREF36, using $32,000$ merge operations.",
                "Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras."
            ],
            "gold_section": [
                "Experiments ::: MT Systems"
            ],
            "predicted": [
                "This neural network usually follows an encoder-decoder architecture, featuring recurrent networks BIBREF23, BIBREF24, convolutional networks BIBREF25 or attention mechanisms BIBREF26. Model parameters are jointly estimated on large parallel corpora, using stochastic gradient descent BIBREF27, BIBREF28. At decoding time, the system obtains the most likely translation using a beam search method.",
                "Statistical IMT systems were implemented following the procedure of word graph exploration and generation of a best suffix for a given prefix described by BIBREF5. Neural IMT systems were built using the interactive branch of NMT-Keras.",
                "For years, the prevailing approach to compute this expression have been phrase-based models BIBREF19. These models rely on a log-linear combination of different models BIBREF20: namely, phrase-based alignment models, reordering models and language models; among others BIBREF21, BIBREF22. However, more recently, this approach has shifted into neural models (see se:NMT)."
            ],
            "predicted_section": [
                "Interactive Machine Translation",
                "Experiments ::: MT Systems",
                "Interactive Machine Translation ::: Neural Machine Translation"
            ]
        },
        "58a3cfbbf209174fcffe44ce99840c758b448364": {
            "question_text": "what are the recent models they compare with?",
            "from_paper": "1707.05589",
            "gold": [
                "Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .",
                "Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model.",
                "In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task."
            ],
            "gold_section": [
                "Enwik8",
                "Penn Treebank",
                "Wikitext-2"
            ],
            "predicted": [
                "In summary, the three recurrent cell architectures are closely matched on all three datasets, with minuscule differences on Enwik8 where regularisation matters the least. These results support the claims of BIBREF21 , that capacities of various cells are very similar and their apparent differences result from trainability and regularisation. While comparing three similar architectures cannot prove this point, the inclusion of NAS certainly gives it more credence. This way we have two of the best human designed and one machine optimised cell that was the top performer among thousands of candidates.",
                "Shallow LSTMs do especially well here. Deeper models have gradually degrading perplexity, with RHNs lagging all of them by a significant margin. NAS is not quite up there with the LSTM suggesting its architecture might have overfitted to Penn Treebank, but data for deeper variants would be necessary to draw this conclusion.",
                "In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task."
            ],
            "predicted_section": [
                "Enwik8",
                "Wikitext-2",
                "Analysis"
            ]
        },
        "636ac549cf4917c5922cd09a655abf278924c930": {
            "question_text": "how was the experiment evaluated?",
            "from_paper": "1910.03943",
            "gold": [
                "A robust metric for evaluating a set of hotel embeddings (or, more generally, any set of items displayed to a user in response to an information need) is its ability to predict a user's next click/selection. In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions."
            ],
            "gold_section": [
                "Experimental Results ::: Quantitative Analysis ::: Hits@k for hotel context prediction"
            ],
            "predicted": [
                "In this section, we present several experiments to evaluate the performance of the trained hotel2vec embeddings. Before diving into the details of the experiments, we first describe the dataset and model parameters.",
                "We show significant gains over previous work based on click-embedding in several experimental studies.",
                "Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1."
            ],
            "predicted_section": [
                "Experimental Results",
                "Experimental Results ::: Experimental Framework",
                "Introduction"
            ]
        },
        "de0154affd86c608c457bf83d888bbd1f879df93": {
            "question_text": "What were the results of their experiment?",
            "from_paper": "1911.12722",
            "gold": [
                "To provide an idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results.",
                "We train a Bidirectional LSTM with a CRF inference layer, which has shown to be competitive for several other sequence labeling tasks BIBREF26, BIBREF27, BIBREF28. We use the IOB2 label encoding for sources, targets, and polar expressions, including the polarity of the latter, giving us nine tags in total. This naturally leads to a lossy representation of the original data, as the relations, nested annotations, and polar intensity are ignored.",
                "Table TABREF37 shows the results of the proportional and binary Overlap measures for precision, recall, and $\\text{F}_1$. The baseline model achieves modest results when compared to datasets that do not involve multiple domains BIBREF11, BIBREF10, with .41, .31, and .31 Proportional $\\text{F}_1$ on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary $\\text{F}_1$). However, this is still better than previous results on cross-domain datasets BIBREF33. The domain variation between documents leads to a lower overlap between Holders, Targets, and Polar Expressions seen in training and those at test time (56%, 28%, and 50%, respectively). We argue, however, that this is a more realistic situation regarding available data, and that it is important to move away from simplifications where training and test data are taken from the same distribution."
            ],
            "gold_section": [
                "Experiments ::: Results",
                "Experiments ::: Experimental Setup",
                "Experiments"
            ],
            "predicted": [
                "In this work, we describe the annotation of a fine-grained sentiment dataset for Norwegian, NoReC$_\\text{\\textit {fine}}$, the first such dataset available in Norwegian. The underlying texts are taken from the Norwegian Review Corpus (NoReC) BIBREF0 \u2013 a corpus of professionally authored reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants, etc. In Mae:Bar:Ovr:2019, a subset of the documents, dubbed NoReC$_\\text{\\textit {eval}}$, were annotated at the sentence-level, indicating whether or not a sentence contains an evaluation or not. These prior annotations did not include negative or positive polarity, however, as this can be mixed at the sentence-level. In this work, the previous annotation effort has been considerably extended to include the span of polar expressions and the corresponding targets and holders of the opinion. We also indicate the intensity of the positive or negative polarity on a three-point scale, along with a number of other attributes of the expressions. In addition to discussing annotation principles and examples, we also present the first experimental results on the dataset.",
                "The annotation was performed by several student assistants with a background in linguistics and with Norwegian as their native language. 100 documents containing 2065 sentences were annotated doubly and disagreements were resolved before moving on. The remaining documents were annotated by one annotator. The doubly annotated documents were adjudicated by a third annotator different from the two first annotators. In the single annotation phase, all annotators were given the possibility to discuss difficult choices in joint annotator meetings, but were encouraged to take independent decisions based on the guidelines if possible. Annotation was performed using the web-based annotation tool Brat BIBREF22.",
                "To provide an idea of the difficulty of the task, here we report some preliminary experimental results for the new dataset, intended as benchmarks for further experiments. Casting the problem as a sequence labeling task, we train a model to jointly predict holders, targets and polar expressions. Below, we first describe the evaluation metrics and the experimental setup, before finally discussing the results."
            ],
            "predicted_section": [
                "Annotations ::: Annotation Procedure",
                "Experiments",
                "Introduction"
            ]
        },
        "2df3cd12937591481e85cf78c96a24190ad69e50": {
            "question_text": "What are existing baseline models on these benchmark datasets?",
            "from_paper": "2004.02214",
            "gold": [
                "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:",
                "Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:",
                "To examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:",
                "Model proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:",
                "Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):",
                "Model proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):",
                "For this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):",
                "Given the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected."
            ],
            "gold_section": [
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):",
                "Experiments ::: Model Comparison",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:"
            ],
            "predicted": [
                "To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines.",
                "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.",
                "Both human and automatic evaluation results on the three benchmark datasets are shown in Table TABREF25, TABREF26 and TABREF27. For each dataset, we present results on individual styles as well as the overall results. We observe that the proposed model achieves the top performance results on most of the metrics. It generates responses with both intense style and high response quality. In addition, we also measure the diversity of the generated responses with two automatic metrics: Distinct-1 and Distinct-2 BIBREF16. The results show that the proposed model achieves the closest performance to that of the RRe approach whose responses are all written by human. On the ranking metric which jointly evaluates the content quality and the style expression, the proposed model outperforms other approaches by a substantial margin."
            ],
            "predicted_section": [
                "Experiments ::: Model Comparison",
                "Experiments ::: Main Results",
                "Introduction"
            ]
        },
        "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb": {
            "question_text": "What sources of less sensitive data are available?",
            "from_paper": "1703.10090",
            "gold": [
                "paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.",
                "Next, the work on surrogate data has recently seen a surge in activity. Increasingly more health-related texts are produced in social media BIBREF40 , and patient-generated data are available online. Admittedly, these may not resemble the clinical discourse, yet they bear to the same individuals whose health is documented in the clinical reports. Indeed, linking individuals' health information from online resources to their health records to improve documentation is an active line of research BIBREF41 . Although it is generally easier to obtain access to social media data, the use of social media still requires similar ethical considerations as in the clinical domain. See for example the influential study on emotional contagion in Facebook posts by Kramer et al. KramerEtAl2014, which has been criticized for not properly gaining prior consent from the users who were involved in the study BIBREF42 .",
                "Another way of reducing sensitivity of data and improving chances for IRB approval is to work on derived data. Data that can not be used to reconstruct the original text (and when sanitized, can not directly re-identify the individual) include text fragments, various statistics and trained models. Working on randomized subsets of clinical notes may also improve the chances of obtaining the data. When we only have access to trained models from disparate sources, we can refine them through ensembling and creation of silver standard corpora, cf. Rebholz-Schuhmann et al. RebholzSchuhmannEtAl2011.",
                "Finally, clinical NLP is also possible on veterinary texts. Records of companion animals are perhaps less likely to involve legal issues, while still amounting to a large pool of data. As an example, around 40M clinical documents from different veterinary clinics in UK and Australia are stored centrally in the VetCompass repository. First NLP steps in this direction were described in the invited talk at the Clinical NLP 2016 workshop BIBREF43 ."
            ],
            "gold_section": [
                "Protecting the individual"
            ],
            "predicted": [
                "paragraph4 0.9ex plus1ex minus.2ex-1em Secure access Since withholding data from researchers would be a dubious way of ensuring confidentiality BIBREF21 , the research has long been active on secure access and storage of sensitive clinical data, and the balance between the degree of privacy loss and the degree of utility. This is a broad topic that is outside the scope of this article. The interested reader can find the relevant information in Dwork and Pottenger DworkAndPottenger2013, Malin et al. MalinEtAL2013 and Rindfleisch Rindfleisch1997.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Are there less sensitive data? One criterion which may have influence on data accessibility is whether the data is about living subjects or not. The HIPAA privacy rule under certain conditions allows disclosure of personal health information of deceased persons, without the need to seek IRB agreement and without the need for sanitization BIBREF39 . It is not entirely clear though how often this possibility has been used in clinical NLP research or broader.",
                "Yet another possibility is open consent, in which individuals make their data publicly available. Initiatives like Personal Genome Project may have an exemplary role, however, they can only provide limited data and they represent a biased population sample BIBREF33 ."
            ],
            "predicted_section": [
                "Protecting the individual"
            ]
        },
        "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a": {
            "question_text": "Other than privacy, what are the other major ethical challenges in clinical data?",
            "from_paper": "1703.10090",
            "gold": [
                "Unlocking knowledge from free text in the health domain has a tremendous societal value. However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. The question is therefore what the most important biases are and how to overcome them, not only out of ethical but also legal responsibility. Related to the question of bias is so-called algorithm transparency BIBREF44 , BIBREF45 , as this right to explanation requires that influences of bias in training data are charted. In addition to sampling bias, which we introduced in section 2, we discuss in this section further sources of bias. Unlike sampling bias, which is a corpus-level bias, these biases here are already present in documents, and therefore hard to account for by introducing larger corpora.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Data quality Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made BIBREF47 . If we fail to detect a medical concept during automated processing, this can not necessarily be a sign of negative evidence. Work on identifying and imputing missing values holds promise for reducing incompleteness, see Lipton et al. LiptonEtAl2016 for an example in sequential modeling applied to diagnosis classification.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Reporting bias Clinical texts may include bias coming from both patient's and clinician's reporting. Clinicians apply their subjective judgments to what is important during the encounter with patients. In other words, there is separation between, on the one side, what is observed by the clinician and communicated by the patient, and on the other, what is noted down. Cases of more serious illness may be more accurately documented as a result of clinician's bias (increased attention) and patient's recall bias. On the other hand, the cases of stigmatized diseases may include suppressed information. In the case of traffic injuries, documentation may even be distorted to avoid legal consequences BIBREF48 .",
                "We need to be aware that clinical notes may reflect health disparities. These can originate from prejudices held by healthcare practitioners which may impact patients' perceptions; they can also originate from communication difficulties in the case of ethnic differences BIBREF49 . Finally, societal norms can play a role. Brady et al. BradyEtAl2016 find that obesity is often not documented equally well for both sexes in weight-addressing clinics. Young males are less likely to be recognized as obese, possibly due to societal norms seeing them as \u201cstocky\" as opposed to obese. Unless we are aware of such bias, we may draw premature conclusions about the impact of our results.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Dual use We have already mentioned linking personal health information from online texts to clinical records as a motivation for exploring surrogate data sources. However, this and many other applications also have potential to be applied in both beneficial and harmful ways. It is easy to imagine how sensitive information from clinical notes can be revealed about an individual who is present in social media with a known identity. More general examples of dual use are when the NLP tools are used to analyze clinical notes with a goal of determining individuals' insurability and employability."
            ],
            "gold_section": [
                "Social impact and biases"
            ],
            "predicted": [
                "The ethics discussion is gaining momentum in general NLP BIBREF8 . We aim in this paper to gather the ethical challenges that are especially relevant for clinical NLP, and to stimulate discussion about those in the broader NLP community. Although enhancing privacy through restricted data access has been the norm, we do not only discuss the right to privacy, but also draw attention to the social impact and biases emanating from clinical notes and their processing. The challenges we describe here are in large part not unique to clinical NLP, and are applicable to general data science as well.",
                "In this paper, we reviewed some challenges that we believe are central to the work in clinical NLP. Difficult access to data due to privacy concerns has been an obstacle to progress in the field. We have discussed how the protection of privacy through sanitization measures and the requirement for informed consent may affect the work in this domain. Perhaps, it is time to rethink the right to privacy in health in the light of recent work in ethics of big data, especially its uneasy relationship to the right to science, i.e. being able to benefit from science and participate in it BIBREF51 , BIBREF52 . We also touched upon possible sources of bias that can have an effect on the application of NLP in the health domain, and which can ultimately lead to unfair or harmful treatment.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Promotion of knowledge and application of best-of-class approaches to health data is seen as one of the ethical duties of researchers BIBREF23 , BIBREF37 . But for this to be put in practice, ways need to be guaranteed (e.g. with government help) to provide researchers with access to the relevant data. Researchers can also go to the data rather than have the data sent to them. It is an open question though whether medical institutions\u2014especially those with less developed research departments\u2014can provide the infrastructure (e.g. enough CPU and GPU power) needed in statistical NLP. Also, granting access to one healthcare organization at a time does not satisfy interoperability (cross-organizational data sharing and research), which can reduce bias by allowing for more complete input data. Interoperability is crucial for epidemiology and rare disease research, where data from one institution can not yield sufficient statistical power BIBREF13 ."
            ],
            "predicted_section": [
                "Protecting the individual",
                "Conclusion",
                "Introduction"
            ]
        },
        "8126c6b8a0cab3e22661d3d71d96aa57360da65c": {
            "question_text": "what evaluation metrics were used?",
            "from_paper": "1905.10039",
            "gold": [
                "To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely",
                "[leftmargin=*]",
                "EM INLINEFORM0 : evaluates the overall accuracy of the generated outline based on exact matching. That is, if both the predicted section boundaries and the generated section headings in a document exactly match with the ground-truth, we treat the document as a positive sample. Otherwise the document is a negative sample.",
                "EM INLINEFORM0 : evaluates the accuracy of the section boundary prediction based on exact matching. Namely, if the predicted section boundaries in a document exactly match with the ground-truth, we treat the document as a positive sample. Otherwise the document is a negative sample.",
                "Rouge INLINEFORM0 evaluates the similarities between generated headings and referenced headings only for the correctly predicted sections. Specifically, we employ Rouge-1 BIBREF39 to measure the uni-gram recall on the reference headings."
            ],
            "gold_section": [
                "Hierarchical Decoder",
                "Evaluation Metrics"
            ],
            "predicted": [
                "To measure the quality of outline generated by our model and the baselines, we employ three automatic metrics, namely",
                "In order to study and evaluate the OG task, we build a new benchmark dataset WIKIOG. We take Wikipedia articles as our source articles since (1) Wikipedia is publicly available and easy to collect; (2) Most multi-paragraph Wikipedia articles contain outlines as an overview of the article, which are constructed by professional human editors. Specifically, we collect English Wikipedia articles under three categories, i.e., \u201ccelebrity\u201d, \u201ccities\u201d and \u201cmusic\u201d. We only make use of the first-level headings as our ground-truth, and leave the deeper-level headings (e.g., second-level headings) generation for the future study. Articles with no headings or more than ten first-level headings are removed, leaving us roughly INLINEFORM0 million articles in total. Table TABREF9 shows the overall statistics of our WIKIOG benchmark dataset.",
                "For evaluation, we compare with several state-of-the-art methods to verify the effectiveness of our model. Empirical results demonstrate that outline generation for capturing the inherent content structure is feasible and our proposed method can outperform all the baselines significantly. We also provide detailed analysis on the proposed model, and conduct case studies to provide better understanding on the learned content structure."
            ],
            "predicted_section": [
                "Evaluation Metrics",
                "Benchmark Construction",
                "Introduction"
            ]
        },
        "1a419468d255d40ae82ed7777618072a48f0091b": {
            "question_text": "How to extract affect attributes from the sentence?",
            "from_paper": "1704.06851",
            "gold": [
                "Motivated by these advances in neural language modeling and affective analysis of text, in this paper we propose a model for representation and generation of emotional text, which we call the Affect-LM. Our model is trained on conversational speech corpora, common in language modeling for speech recognition applications BIBREF10 . Figure 1 provides an overview of our Affect-LM and its ability to generate emotionally colored conversational text in a number of affect categories with varying affect strengths. While these parameters can be manually tuned to generate conversational text, the affect category can also be automatically inferred from preceding context words. Specifically for model training, the affect category is derived from features generated using keyword spotting from a dictionary of emotional words, such as the LIWC (Linguistic Inquiry and Word Count) tool BIBREF11 . Our primary research questions in this paper are:",
                "Our proposed model learns a generative model of the next word $w_t$ conditioned not only on the previous words $w_1,w_2,...,w_{t-1}$ but also on the affect category $\\mathbf {e_{t-1}}$ which is additional information about emotional content. During model training, the affect category is inferred from the context data itself. Thus we define a suitable feature extractor which can utilize an affective lexicon to infer emotion in the context. For our experiments, we have utilized the Linguistic Inquiry and Word Count (LIWC) text analysis program for feature extraction through keyword spotting. Introduced by BIBREF11 pennebaker2001linguistic, LIWC is based on a dictionary, where each word is assigned to a predefined LIWC category. The categories are chosen based on their association with social, affective, and cognitive processes. For example, the dictionary word worry is assigned to LIWC category anxiety. In our work, we have utilized all word categories of LIWC corresponding to affective processes: positive emotion, angry, sad, anxious, and negative emotion. Thus the descriptor $\\mathbf {e_{t-1}}$ has five features with each feature denoting presence or absence of a specific emotion, which is obtained by binary thresholding of the features extracted from LIWC. For example, the affective representation of the sentence i will fight in the war is $\\mathbf {e_{t-1}}=$ {\u201csad\":0, \u201cangry\":1, \u201canxiety\":0, \u201cnegative emotion\":1, \u201cpositive emotion\":0}."
            ],
            "gold_section": [
                "Descriptors for Affect Category Information",
                "Introduction"
            ],
            "predicted": [
                "Q1:Can Affect-LM be used to generate affective sentences for a target emotion with varying degrees of affect strength through a customizable model parameter?",
                "The parameter $\\beta $ defined in Equation 7 , which we call the affect strength defines the influence of the affect category information (frequency of emotionally colored words) on the overall prediction of the target word $w_t$ given its context. We can consider the formulation as an energy based model (EBM), where the additional energy term captures the degree of correlation between the predicted word and the affective input BIBREF13 .",
                "Affect is a term that subsumes emotion and longer term constructs such as mood and personality and refers to the experience of feeling or emotion BIBREF0 . BIBREF1 picard1997affective provides a detailed discussion of the importance of affect analysis in human communication and interaction. Within this context the analysis of human affect from text is an important topic in natural language understanding, examples of which include sentiment analysis from Twitter BIBREF2 , affect analysis from poetry BIBREF3 and studies of correlation between function words and social/psychological processes BIBREF4 . People exchange verbal messages which not only contain syntactic information, but also information conveying their mental and emotional states. Examples include the use of emotionally colored words (such as furious and joy) and swear words. The automated processing of affect in human verbal communication is of great importance to understanding spoken language systems, particularly for emerging applications such as dialogue systems and conversational agents."
            ],
            "predicted_section": [
                "Proposed Model: Affect-LM",
                "Introduction"
            ]
        },
        "8b1af67e3905244653b4cf66ba0acec8d6bff81f": {
            "question_text": "How were the ngram models used to generate predictions on the data?",
            "from_paper": "1704.08390",
            "gold": [
                "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0",
                "After training the N-gram language models, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. Table 1 shows an example of two scored tweets from hashtag file Bad_Job_In_5_Words.tsv based on the tweet data trigram language model. Note that KenLM reports the log of the probability of the N-grams rather than the actual probabilities so the value closer to 0 (-19) has the higher probability and is associated with the tweet judged to be funnier."
            ],
            "gold_section": [
                "Tweet Scoring"
            ],
            "predicted": [
                "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0 ",
                "Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.",
                "We relied on bigram and trigram language models because tweets are short and concise, and often only consist of just a few words."
            ],
            "predicted_section": [
                "Language Model Training",
                "Discussion and Future Work",
                "Background"
            ]
        },
        "a3a867f7b3557c168d05c517c468ff6c7337bff9": {
            "question_text": "What dataset did they use?",
            "from_paper": "1701.03578",
            "gold": [
                "To resolve these limitations, we propose fast transfer learning schemes. It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are proposed for achieving the result. In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model. With this approach, the final model can mimic target user's language style with proper syntax.",
                "In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users' dialogues.",
                "We also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT'14 corpus for 10 days, is used. English bible data is added and employed in training for another 4 hours using proposed transfer learning schemes."
            ],
            "gold_section": [
                "Literary-Style to Spoken-Style Sentence Completion",
                "Introduction"
            ],
            "predicted": [
                "To check the influence of training data size (number of sentences) in personalized language model, we trained the general language model (trained with \u201cFriends\" corpus, message-reply prediction model) with different sizes of personal (\u201cchandler\" and \u201crachel\") dataset. The proposed scheme_2 method was used for this test. Table 4 shows evaluation results of the trained models. Dataset '0' means the model is not trained with personal dataset. The perplexity shows lower value as we use more dataset in training, and it outperforms \u201cfriends 5-gram\u201d model from the 2,000 dataset cases.",
                "We mainly conduct two types of experiments. The first one is a sentence completion experiment, and the other one is a message-reply prediction experiment. In the former case, we train a general language model with literary-style data and apply a proposed transfer learning scheme with spoken-style data to achieve a personalized language model. With this setting, the difference between general and personalized language models can be measured in a quantitative and a qualitative manner. For the latter case, we use dialogue-style data such as drama scripts to train a general language model. From the drama scripts, some characters' data are taken apart and are used to train the personalized language model. With this setting, the output of the personalized model is compared to the original dialogue of the same character.",
                "The characteristics of a user speech can mainly be distinguished by the word dictionary. Thus, this metric tries to measure the differences of the word dictionary among the comparing set. Table 1 shows the quantitative measure results from the dialogue set of the main characters in drama data from \u201cFriends,\" a famous American television sitcom. In the figures, \u201ccharacter_1\" to \u201ccharacter_6\" are the main characters of the drama (Chandler, Joey, Monica, Phoebe, Rachel, and Ross, respectively). The dialogues were measured against one another by using the cross entropy metric. As shown in the table, the lower cross entropy value among the same character's dialogue was calculated, and the higher value was calculated among the different character's dialogues as expected. This result demonstrates that the cross entropy metric can be used to measure the similarities among the members of the set."
            ],
            "predicted_section": [
                "Measures",
                "General-Style to Personal-Style Message-Reply Prediction",
                "Experiments"
            ]
        },
        "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c": {
            "question_text": "What is the improvement in performance compared to the linguistic gold standard?",
            "from_paper": "2003.03131",
            "gold": [
                "Table contains the error analysis for English, Finnish and North S\u00e1mi. For English and North S\u00e1mi, EM+Prune results in less under-segmentation but worse over-segmentation. For Finnish these results are reversed. However, the suffixes are often better modeled, as shown by lower under-segmentation on SUF-SUF (all languages) and STM-SUF (English and North S\u00e1mi)."
            ],
            "gold_section": [
                "Results"
            ],
            "predicted": [
                "Figure shows the Precision\u2013Recall curves for the primary systems, for all four languages. While increasing the Morfessor cost, forced splitting improves BPR. Tables to show test set Boundary Precision, Recall and F$_{1}$-score (BPR) results at the optimal tuning point (selected using a development set) for each model, for English, Finnish, Turkish and North S\u00e1mi, respectively. The default Morfessor EM+Prune configuration (\u201csoft\u201d EM, full prior, forcesplit) significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi, for which there is no significant difference between the methods.",
                "a better training algorithm for Morfessor Baseline, with reduction of search error during training, and improved segmentation quality for English, Finnish and Turkish;",
                "The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score."
            ],
            "predicted_section": [
                "Results",
                "Experimental Setup ::: Evaluation",
                "Introduction"
            ]
        },
        "252599e53f52b3375b26d4e8e8b66322a42d2563": {
            "question_text": "Which data augmentation techniques do they use?",
            "from_paper": "1709.06365",
            "gold": [
                "To sample INLINEFORM0 , we first marginalise out INLINEFORM1 in the right part of Eq. ( SECREF4 ) with the Dirichlet multinomial conjugacy: +rCl+x* d=1D (d,)(d, + md,)Gamma ratio 1 k=1K (d,k + md,k)(d,k)Gamma ratio 2 where INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 is the gamma function. Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 . Given a set of INLINEFORM8 for all the documents, Gamma ratio 1 can be approximated by the product of INLINEFORM9 , i.e., INLINEFORM10 .",
                "Gamma ratio 2 in Eq. ( SECREF17 ) is the Pochhammer symbol for a rising factorial, which can be augmented with an auxiliary variable INLINEFORM0 BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 as follows: +rCl+x* (d,k + md,k)(d,k)Gamma ratio 2 = td,k=0md,k Smd,ktd,k d,ktd,k where INLINEFORM1 indicates an unsigned Stirling number of the first kind. Gamma ratio 2 is a normalising constant for the probability of the number of tables in the Chinese Restaurant Process (CRP) BIBREF28 , INLINEFORM2 can be sampled by a CRP with INLINEFORM3 as the concentration and INLINEFORM4 as the number of customers: +rCl+x* td,k = i=1md,k Bern(d,kd,k+i) where INLINEFORM5 samples from the Bernoulli distribution. The complexity of sampling INLINEFORM6 by Eq. ( SECREF17 ) is INLINEFORM7 . For large INLINEFORM8 , as the standard deviation of INLINEFORM9 is INLINEFORM10 BIBREF28 , one can sample INLINEFORM11 in a small window around the current value in complexity INLINEFORM12 .",
                "Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity."
            ],
            "gold_section": [
                "Sampling \u03bb l,k \\lambda _{l,k}:",
                "Inference"
            ],
            "predicted": [
                "In the experiments, three regular text datasets and three short text datasets were used:",
                "With the rapid growth of the internet, huge amounts of text data are generated in social networks, online shopping and news websites, etc. These data create demand for powerful and efficient text analysis techniques. Probabilistic topic models such as Latent Dirichlet Allocation (LDA) BIBREF0 are popular approaches for this task, by discovering latent topics from text collections. Many conventional topic models discover topics purely based on the word-occurrences, ignoring the meta information (a.k.a., side information) associated with the content. In contrast, when we humans read text it is natural to leverage meta information to improve our comprehension, which includes categories, authors, timestamps, the semantic meanings of the words, etc. Therefore, topic models capable of using meta information should yield improved modelling accuracy and topic quality.",
                "We conduct extensive experiments with several real datasets including regular and short texts in various domains. The experimental results demonstrate that MetaLDA achieves improved performance in terms of perplexity, topic coherence, and running time."
            ],
            "predicted_section": [
                "Datasets",
                "Introduction"
            ]
        },
        "f1f7a040545c9501215d3391e267c7874f9a6004": {
            "question_text": "what dataset was used?",
            "from_paper": "1612.09535",
            "gold": [
                "The program was developed in R BIBREF16 and makes use of some specific text mining packages. We have implemented our method using the following R packages: tm BIBREF17 , cwhmisc BIBREF18 , memoise BIBREF19 , openNLP BIBREF20 , Hmisc BIBREF21 . The OpenNLP POS Tagger uses a probability model to predict the correct POS tag and, for Portuguese language, it was trained on CoNLL_X bosque data.",
                "In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'."
            ],
            "gold_section": [
                "Implementation",
                "Comparing PAMPO with other NER tools"
            ],
            "predicted": [
                "The authors would like to thank SAPO Labs (http://labs.sapo.pt) for providing the data set of news from Lusa agency. The authors would also like to thank grant #2014/08996-0 and grant #2013/14757-6, S\u00e3o Paulo Research Foundation (FAPESP). This work is partially funded by FCT/MEC through PIDDAC and ERDF/ON2 within project NORTE-07-0124-FEDER-000059 and through the COMPETE Programme (operational programme for competitiveness) and by National Funds through the FCT - Funda\u00e7\u00e3o para a Ci\u00eancia e a Tecnologia (Portuguese Foundation for Science and Technology) within project FCOMP-01-0124-FEDER-037281.",
                "Table TABREF27 shows the most frequent `candidate entities' from the whole book, as extracted by Algorithm 1 and which of those candidate entities were considered as actual `named entities' by Algorithm 2.",
                "Figure FIGREF42 presents scatter plots of INLINEFORM0 vs INLINEFORM1 for the four extractors, PAMPO, AlchemyAPI, Rembrandt and Zemanta for the `Sports news' and `News' corpora, first four panels and four bottom panels, respectively. It is noteworthy that almost all the 881 points of the `Sports news' for PAMPO extractor are in the upper right corner of the scatter plot, as well as almost all the 227 points of the `News'. The other tools present a more dispersed solution quality."
            ],
            "predicted_section": [
                "Analysis of results",
                "Evaluation",
                "Acknowledgements"
            ]
        },
        "2ca3ca39d59f448e30be6798514709be7e3c62d8": {
            "question_text": "Which datasets did they use to train the model?",
            "from_paper": "1603.01547",
            "gold": [
                "The first two datasets BIBREF1 were constructed from a large number of news articles from the CNN and Daily Mail websites. The main body of each article forms a context, while the cloze-style question is formed from one of short highlight sentences, appearing at the top of each article page. Specifically, the question is created by replacing a named entity from the summary sentence (e.g. \u201cProducer X will not press charges against Jeremy Clarkson, his lawyer says.\u201d).",
                "The third dataset, the Children's Book Test (CBT) BIBREF3 , is built from books that are freely available thanks to Project Gutenberg. Each context document is formed by 20 consecutive sentences taken from a children's book story. Due to the lack of summary, the cloze-style question is then constructed from the subsequent (21st) sentence."
            ],
            "gold_section": [
                "Datasets"
            ],
            "predicted": [
                "Performance of our models on the CNN and Daily Mail datasets is summarized in Table TABREF27 , Table TABREF28 shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set new state-of-the-art results on all evaluated datasets.",
                "We did not perform any text pre-processing since the original datasets were already tokenized.",
                "Furthermore the named entities in the whole dataset were replaced by anonymous tokens which were further shuffled for each example so that the model cannot build up any world knowledge about the entities and hence has to genuinely rely on the context document to search for an answer to the question."
            ],
            "predicted_section": [
                "Datasets",
                "Training Details",
                "Results"
            ]
        },
        "20e2b517fddb0350f5099c39b16c2ca66186d09b": {
            "question_text": "What baseline do they compare against?",
            "from_paper": "1603.01547",
            "gold": [
                "Several recent deep neural network architectures BIBREF1 , BIBREF3 , BIBREF7 , BIBREF12 were applied to the task of text comprehension. The last two architectures were developed independently at the same time as our work. All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.",
                "Attentive and Impatient Readers were proposed in BIBREF1 . The simpler Attentive Reader is very similar to our architecture. It also uses bidirectional document and query encoders to compute an attention in a similar way we do. The more complex Impatient Reader computes attention over the document after reading every word of the query. However, empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets.",
                "Chen et al. 2016",
                "A model presented in BIBREF7 is inspired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that is INLINEFORM0 . The document embedding INLINEFORM1 is computed using a weighted sum as in the Attentive Reader, INLINEFORM2 . In the end INLINEFORM3 , where INLINEFORM4 is a new embedding function.",
                "Memory Networks",
                "MenNN BIBREF13 were applied to the task of text comprehension in BIBREF3 .",
                "Dynamic Entity Representation",
                "The Dynamic Entity Representation model BIBREF12 has a complex architecture also based on the weighted attention mechanism and max-pooling over contextual embeddings of vectors for each named entity.",
                "One can also see how the task complexity varies with the type of the omitted word (named entity, common noun, verb, preposition). BIBREF3 have shown that while standard LSTM language models have human level performance on predicting verbs and prepositions, they lack behind on named entities and common nouns. In this article we therefore focus only on predicting the first two word types."
            ],
            "gold_section": [
                "Attentive and Impatient Readers",
                "Dynamic Entity Representation",
                "Datasets",
                "Chen et al. 2016",
                "Related Work",
                "Memory Networks"
            ],
            "predicted": [
                "We will now briefly summarize important features of the datasets.",
                "In Section SECREF6 we analysed how the test accuracy depends on how frequent the correct answer is compared to other answer candidates for the news datasets. The plots for the Children's Book Test looks very similar, however we are adding it here for completeness.",
                "Performance of our models on the CNN and Daily Mail datasets is summarized in Table TABREF27 , Table TABREF28 shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set new state-of-the-art results on all evaluated datasets."
            ],
            "predicted_section": [
                "Datasets",
                "Dependence of accuracy on the frequency of the correct answer",
                "Results"
            ]
        },
        "058b6e3fdbb607fa7dbfc688628b3e13e130c35a": {
            "question_text": "Does the paper list other heuristic biases in the LSTMs?",
            "from_paper": "1912.00239",
            "gold": [
                "Interestingly, even though the case orders preferred by the LSTM correlate with those of humans, there are also subtle differences: we also find that models tend to prefer argument orders that start with dative to those that start with accusative, when the opposite is true for human grammaticality scores. The origin of such differences is unclear. Understanding it more fully would require to obtain distributional statistics on the order of such phrases in the original corpus.",
                "To check for the existence of such effect, we categorized the nouns in all of our sentences as animate and inanimate, and computed the human and machine scores of our grammatical sentences as a function of the association between case and animacy. Table TABREF22 shows that indeed, both humans and machines are biased by animacy-case associations: all share a preference for animate for nominative (subject) and dative (indirect object). By contrast, negative AUC values for accusative indicate that direct objects are preferred as inanimate."
            ],
            "gold_section": [
                "Results ::: Argument Order Preferences",
                "Results ::: Animacy Preferences"
            ],
            "predicted": [
                "We set up a well controlled grammaticality test for the processing of argument structure in neural language models and in humans. The results show that LSTMs are better than chance in detecting an abnormal argument structure, despite the fact that the arguments could occur in any position, due to the generally free word order of phrases in German relative clauses. The average performance of models, though, is far from 100% correct and lower than humans, and the error patterns differ markedly. Contrary to humans, neural language models are overly sensitive to frequency distribution of phrase types. For instance, they assign a higher probability to sentences containing multiple nominative phrases than a correct sentence with only one nominative phrase. This frequency bias directly reflects the frequency of nominative, accusative and dative in the language, as the same bias is found in unigram and bigram models. Similar to the conclusion reached by BIBREF21 in their investigation of the error patterns made by RNNs and humans on syntactic agreement, we find that the syntactic representations of humans and LSTMs differ in some respects.",
                "Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact. For humans, the wider spread in results can be attributed partially to the fact that 55 random permutations out of the 144 permutations were annotated for each template. Therefore, it might have been easier to distinguish grammatical sentences from ungrammatical ones for some templates than others.",
                "We evaluate three LMs on our dataset, the two-layer LSTM of BIBREF8 trained on German Wikipedia text, as well as n-gram baselines using the same corpus. We ask proficient German speakers to annotate our sentences for grammaticality, providing a human comparison. Since some of these sentences are rather implausible because of the permutations, we also collect human meaningfulness scores. We find that our dataset is challenging for both LMs and humans and that LMs lag behind human performance."
            ],
            "predicted_section": [
                "Conclusions",
                "Results ::: Main Classification Task",
                "Introduction"
            ]
        },
        "bdae851d4cf1d05506cf3e8359786031ac4f756f": {
            "question_text": "What models have been evaluated?",
            "from_paper": "1911.12237",
            "gold": [
                "We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):",
                "Pointer generator network BIBREF4. In the case of Pointer Generator, we use a default configuration, changing only the minimum length of the generated summary from 35 (used in news) to 15 (used in dialogues).",
                "Transformer BIBREF16. The model is trained using OpenNMT library. We use the same parameters for training both on news and on dialogues, changing only the minimum length of the generated summary \u2013 35 for news and 15 for dialogues.",
                "Fast Abs RL BIBREF5. It is trained using its default parameters. For dialogues, we change the convolutional word-level sentence encoder (used in extractor part) to only use kernel with size equal 3 instead of 3-5 range. It is caused by the fact that some of utterances are very short and the default setting is unable to handle that.",
                "Fast Abs RL Enhanced. The additional variant of the Fast Abs RL model with slightly changed utterances i.e. to each utterance, at the end, after artificial separator, we add names of all other interlocutors. The reason for that is that Fast Abs RL requires text to be split into sentences (as it selects sentences and then paraphrase each of them). For dialogues, we divide text into utterances (which is a natural unit in conversations), so sometimes, a single utterance may contain more than one sentence. Taking into account how this model works, it may happen that it selects an utterance of a single person (each utterance starts with the name of the author of the utterance) and has no information about other interlocutors (if names of other interlocutors do not appear in selected utterances), so it may have no chance to use the right people's names in generated summaries.",
                "LightConv and DynamicConv BIBREF17. The implementation is available in fairseq BIBREF18. We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation BIBREF19; (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small BIBREF20."
            ],
            "gold_section": [
                "Experimental setup ::: Models"
            ],
            "predicted": [
                "The results for the news summarization task are shown in Table TABREF25 and for the dialogue summarization \u2013 in Table TABREF26. In both domains, the best models' ROUGE-1 exceeds 39, ROUGE-2 \u2013 17 and ROUGE-L \u2013 36. Note that the strong baseline for news (Lead-3) is outperformed in all three metrics only by one model. In the case of dialogues, all tested models perform better than the baseline (LONGEST-3).",
                "Table TABREF34 and TABREF35 show a few selected dialogues, together with summaries produced by the best tested models:",
                "We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):"
            ],
            "predicted_section": [
                "Experimental setup ::: Models",
                "Results",
                "Difficulties in dialogue summarization"
            ]
        },
        "07d98dfa88944abd12acd45e98fb7d3719986aeb": {
            "question_text": "Are all generated examples semantics-preserving perturbations to the original text?",
            "from_paper": "1909.07873",
            "gold": [
                "Alzantot et al. BIBREF20 proposed a black-box targeted attack using a population-based optimization via genetic algorithm BIBREF21. The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier. While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy. Our attacks can be a test of how robust the text classification models are to word and character-level perturbations.",
                "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "The primary purpose of pretraining AEG is to enable our hybrid encoder-decoder to encode both character and word information from the input example and produce both word and character-level transformations in the form of paraphrases or misspellings. Though the pretraining helps us mitigate the cold-start issue, it does not guarantee that these perturbed texts will fool the target model. There are large number of valid perturbations that can be applied due to multiple ways of arranging text units to produce paraphrases or different misspellings. Thus, minimizing $J_{mle}$ is not sufficient to generate adversarial examples.",
                "The reward $r(\\hat{y})$ for the sequence generated is a weighted sum of different constraints required for generating adversarial examples. Since our model operates at word and character levels, we therefore compute three rewards: adversarial reward, semantic similarity and lexical similarity reward. The reward should be high when: (a) the generated sequence causes the target model to produce a low classification prediction probability for its ground truth category, (b) semantic similarity is preserved and (c) the changes made to the original text are minimal.",
                "Inspired by the work of Li et al. BIBREF37, we train a deep matching model that can represent the degree of match between two texts. We use character based biLSTM models with attention BIBREF38 to handle word and character level perturbations. The matching model will help us compute the the semantic similarity $R_S$ between the text generated and the original input text.",
                "Since our model functions at both character and word level, we compute the lexical similarity. The purpose of this reward is to keep the changes as minimal as possible to just fool the target classifier. Motivated by the recent work of Moon et al. BIBREF39, we pretrain a deep neural network to compute approximate Levenshtein distance $R_{L}$ composed of character based bi-LSTM model. We replicate that model by generating a large number of text with perturbations in the form of insertions, deletions or replacements. We also include words which are prominent nicknames, abbreviations or inconsistent notations to have more lexical similarity. This is generally not possible using direct Levenshtein distance computation. Once trained, it can produce a purely lexical embedding of the text without semantic allusion. This can be used to compute the lexical similarity between the generated text $y$ and the original input text $x$ for our purpose.",
                "Table TABREF29 summarizes the data and models used in our experiments. We compare our proposed model with the following black-box non-targeted attacks:",
                "Random: We randomly select a word in the text and introduce some perturbation to that word in the form of a character replacement or synonymous word replacement. No specific strategy to identify importance of words.",
                "NMT-BT: We generate paraphrases of the sentences of the text using a back-translation approach BIBREF23. We used pretrained English$\\leftrightarrow $German translation models to obtain back-translations of input examples.",
                "DeepWordBug BIBREF24: A scoring function is used to determine the important tokens to change. The tokens are then modified to evade a target model.",
                "No-RL: We use our pretrained model without the reinforcement learning objective.",
                "Given different settings of the adversary, there are other works that have designed attacks in \u201cgray-box\u201d settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of \u201cgray-box\u201d attacks are quite different in each of these approaches. In this paper, we focus on \u201cblack-box\u201d setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization. The adversarial sequences generated are able to effectively fool the classifiers without changing the semantics of the text. Our contributions are as follows:"
            ],
            "gold_section": [
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "Training ::: Training with Reinforcement learning ::: Rewards ::: Lexical Similarity",
                "Training ::: Training with Reinforcement learning ::: Rewards ::: Semantic Similarity",
                "Introduction",
                "Experiments ::: Setup",
                "Related Work",
                "Training ::: Training with Reinforcement learning ::: Rewards",
                "Proposed Attack Strategy"
            ],
            "predicted": [
                "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.",
                "We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.",
                "Most of the prior work has focused on image classification models where adversarial examples are obtained by introducing imperceptible changes to pixel values through optimization techniques BIBREF4, BIBREF5. However, generating natural language adversarial examples can be challenging mainly due to the discrete nature of text samples. Continuous data like image or speech is much more tolerant to perturbations compared to text BIBREF6. In textual domain, even a small perturbation is clearly perceptible and can completely change the semantics of the text. Another challenge for generating adversarial examples relates to identifying salient areas of the text where a perturbation can be applied successfully to fool the target classifier. In addition to fooling the target classifier, the adversary is designed with different constraints depending on the task and its motivations BIBREF7. In our work, we focus on constraining our adversary to craft examples with semantic preservation and minimum perturbations to the input text."
            ],
            "predicted_section": [
                "Experiments ::: Human Evaluation",
                "Proposed Attack Strategy",
                "Introduction"
            ]
        },
        "7f90e9390ad58b22b362a57330fff1c7c2da7985": {
            "question_text": "Do they use already trained model on some task in their reinforcement learning approach?",
            "from_paper": "1909.07873",
            "gold": [
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "The primary purpose of pretraining AEG is to enable our hybrid encoder-decoder to encode both character and word information from the input example and produce both word and character-level transformations in the form of paraphrases or misspellings. Though the pretraining helps us mitigate the cold-start issue, it does not guarantee that these perturbed texts will fool the target model. There are large number of valid perturbations that can be applied due to multiple ways of arranging text units to produce paraphrases or different misspellings. Thus, minimizing $J_{mle}$ is not sufficient to generate adversarial examples.",
                "Training ::: Training with Reinforcement learning",
                "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm."
            ],
            "gold_section": [
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "Training ::: Training with Reinforcement learning"
            ],
            "predicted": [
                "No-RL: We use our pretrained model without the reinforcement learning objective.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "We adopt a self-critical sequence training technique to train our model to generate examples that can fool or increase the probability of misclassification in text classifiers."
            ],
            "predicted_section": [
                "Experiments ::: Setup",
                "Proposed Attack Strategy",
                "Introduction"
            ]
        },
        "3e3e45094f952704f1f679701470c3dbd845999e": {
            "question_text": "How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?",
            "from_paper": "1909.07873",
            "gold": [
                "Proposed Attack Strategy",
                "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "Training ::: Training with Reinforcement learning",
                "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.",
                "Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)",
                "In SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{<j},h)$ and (b) $\\hat{y}$ obtained by greedily decoding ($argmax$ predictions) from the distribution $p(\\hat{y}_j|\\hat{y}_{<j},h)$ Next, rewards $r(y^{\\prime }_j),r(\\hat{y}_j)$ are computed for both the sequences using a reward function $r(\\cdot )$, explained in Section SECREF26. We train the model by minimizing:",
                "Here $r(\\hat{y})$ can be viewed as the baseline reward. This approach, therefore, explores different sequences that produce higher reward compared to the current best policy."
            ],
            "gold_section": [
                "Training ::: Training with Reinforcement learning",
                "Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)",
                "Proposed Attack Strategy"
            ],
            "predicted": [
                "We propose a black-box non-targeted attack strategy by combining ideas of substitute network and adversarial example generation. We formulate it as a reinforcement learning task.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself. Depending on the purpose of the adversary, adversarial attacks can be categorized as (a) targeted attack and (b) non-targeted attack. In a targeted attack, the output category of a generated example is intentionally controlled to a specific target category with limited change in semantic information. While a non-targeted attack doesn't care about the category of misclassified results."
            ],
            "predicted_section": [
                "Proposed Attack Strategy",
                "Introduction"
            ]
        },
        "3fd8eab282569b1c18b82f20d579b335ae70e79f": {
            "question_text": "What languages do they experiment with?",
            "from_paper": "1906.01502",
            "gold": [
                "We perform ner experiments on two datasets: the publicly available CoNLL-2002 and -2003 sets, containing Dutch, Spanish, English, and German BIBREF5 , BIBREF6 ; and an in-house dataset with 16 languages, using the same CoNLL categories. Table TABREF4 shows M-Bert zero-shot performance on all language pairs in the CoNLL data."
            ],
            "gold_section": [
                "Named entity recognition experiments"
            ],
            "predicted": [
                "It is our hope that these kinds of probing experiments will help steer researchers toward the most promising lines of inquiry by encouraging them to focus on the places where current contextualized word representation approaches fall short.",
                "In this paper, we empirically investigate the degree to which these representations generalize across languages. We explore this question using Multilingual BERT (henceforth, M-Bert), released by BIBREF0 as a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. M-Bert is particularly well suited to this probing study because it enables a very straightforward approach to zero-shot cross-lingual model transfer: we fine-tune the model using task-specific supervised training data from one language, and evaluate that task in a different language, thus allowing us to observe the ways in which the model generalizes information across languages.",
                "Code-switching (CS)\u2014the mixing of multiple languages within a single utterance\u2014and transliteration\u2014writing that is not in the language's standard script\u2014present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target."
            ],
            "predicted_section": [
                "Code switching and transliteration",
                "Conclusion",
                "Introduction"
            ]
        },
        "8e9561541f2e928eb239860c2455a254b5aceaeb": {
            "question_text": "What language pairs are affected?",
            "from_paper": "1906.01502",
            "gold": [
                "M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.",
                "Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.",
                "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.",
                "Generalizing across typological features ",
                "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.",
                "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order."
            ],
            "gold_section": [
                "Generalizing across typological features ",
                "Generalization across scripts",
                "Introduction"
            ],
            "predicted": [
                "In this paper, we empirically investigate the degree to which these representations generalize across languages. We explore this question using Multilingual BERT (henceforth, M-Bert), released by BIBREF0 as a single language model pre-trained on the concatenation of monolingual Wikipedia corpora from 104 languages. M-Bert is particularly well suited to this probing study because it enables a very straightforward approach to zero-shot cross-lingual model transfer: we fine-tune the model using task-specific supervised training data from one language, and evaluate that task in a different language, thus allowing us to observe the ways in which the model generalizes information across languages.",
                "In this section, we study the structure of M-Bert's feature space. If it is multilingual, then the transformation mapping between the same sentence in 2 languages should not depend on the sentence itself, just on the language pair.",
                "Code-switching (CS)\u2014the mixing of multiple languages within a single utterance\u2014and transliteration\u2014writing that is not in the language's standard script\u2014present unique test cases for M-Bert, which is pre-trained on monolingual, standard-script corpora. Generalizing to code-switching is similar to other cross-lingual transfer scenarios, but would benefit to an even larger degree from a shared multilingual representation. Likewise, generalizing to transliterated text is similar to other cross-script transfer experiments, but has the additional caveat that M-Bert was not pre-trained on text that looks like the target."
            ],
            "predicted_section": [
                "Multilingual characterization of the feature space ",
                "Code switching and transliteration",
                "Introduction"
            ]
        },
        "860257956b83099cccf1359e5d960289d7d50265": {
            "question_text": "Which neural network architecture do they use?",
            "from_paper": "1702.02367",
            "gold": [
                "The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 . From now on, we denote the contextual representation for the word INLINEFORM5 by INLINEFORM6 and the contextual representation for the word INLINEFORM7 in the document INLINEFORM8 by INLINEFORM9 . Differently from BIBREF3 , we build a unique representation for the whole set of documents INLINEFORM10 related to the query INLINEFORM11 by stacking each contextual representation INLINEFORM12 obtaining a matrix INLINEFORM13 , where INLINEFORM14 .",
                "This phase uncovers a possible inference chain which models meaningful relationships between the query and the set of related documents. The inference chain is obtained by performing, for each inference step INLINEFORM0 , the attention mechanisms given by the query attentive read and the document attentive read keeping a state of the inference process given by an additional recurrent neural network with GRU units. In this way, the network is able to progressively refine the attention weights focusing on the most relevant tokens of the query and the documents which are exploited by the prediction neural network to select the correct answers among the candidate ones."
            ],
            "gold_section": [
                "Inference phase",
                "Encoding phase"
            ],
            "predicted": [
                "The neural network weights are supposed to learn latent features which encode relationships between the most relevant words for the given query to predict the correct answers. The outer sigmoid activation function is used to treat the problem as a multi-label classification problem, so that each candidate answer is independent and not mutually exclusive. In this way the neural network generates a score which represents the probability that the candidate answer is correct. Moreover, differently from BIBREF3 , the candidate answer INLINEFORM0 can be any word, even those which not belong to the documents related to the query.",
                "According to the experimental evaluations conducted on the above-mentioned datasets, high-level performance can be obtained exploiting complex attention mechanisms which are able to focus on relevant evidences in the processed content. One of the earlier approaches used to solve these tasks is given by the general Memory Network BIBREF21 , BIBREF22 framework which is one of the first neural network models able to access external memories to extract relevant information through an attention mechanism and to use them to provide the correct answer. A deep Recurrent Neural Network with Long Short-Term Memory units is presented in BIBREF18 , which solves CNN/Daily Mail datasets by designing two different attention mechanisms called Impatient Reader and Attentive Reader. Another way to incorporate attention in neural network models is proposed in BIBREF23 which defines a pointer-sum loss whose aim is to maximize the attention weights which lead to the correct answer.",
                "This work is supported by the IBM Faculty Award \"Deep Learning to boost Cognitive Question Answering\". The Titan X GPU used for this research was donated by the NVIDIA Corporation."
            ],
            "predicted_section": [
                "Prediction phase",
                "Acknowledgments",
                "Related work"
            ]
        },
        "d126d5d6b7cfaacd58494f1879547be9e91d1364": {
            "question_text": "What similarity metrics have been tried?",
            "from_paper": "2004.04478",
            "gold": [
                "In table TABREF6, we present the n-gram percent match among the domain data used in our experiments. We observe that the n-gram match from among this corpora is relatively low and simple corpus similarity measures which use orthographic techniques cannot be used to obtain domain similarity. Hence, we propose the use of the metrics detailed below to perform our experiments.",
                "We use a total of 11 metrics over two scenarios: the first that uses labelled data, while the second that uses unlabelled data.",
                "We explain all our metrics in detail later in this section. These 11 metrics can also be classified into two categories:",
                "Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings.",
                "Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap",
                "All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\chi ^2$ value greater than or equal to 1. The $\\chi ^2$ value is calculated as follows:",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)",
                "KL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity",
                "This metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change",
                "Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec",
                "We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity BIBREF17. It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity distinguishes nearly parallel vectors much better, we use it instead of Cosine Similarity. We obtain a similarity value by averaging over all common adjectives. For the final similarity value of this metric, we use Jaccard Similarity Coefficient here as well:",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec",
                "Doc2Vec represents each sentence by a dense vector which is trained to predict words in the sentence, given the model. It tries to overcome the weaknesses of the bag-of-words model. Similar to Word2Vec, we train Doc2Vec models on each domain to extract sentence vectors. We train the models over 100 epochs for 100 dimensions, where the learning rate is 0.025. Since we can no longer leverage adjectives for sentiment, we use SentiWordnet for assigning sentiment scores (ranging from -1 to +1 where -1 denotes a negative sentiment, and +1 denotes a positive sentiment) to reviews (as detailed above) and select reviews which have a score above a certain threshold. We have empirically arrived at $\\pm 0.01$ as the threshold value. Any review with a score outside this window is selected. We also restrict the length of reviews to a maximum of 100 words to reduce sparsity.",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe",
                "Both Word2Vec and GloVe learn vector representations of words from their co-occurrence information. However, GloVe is different in the sense that it is a count-based model. In this metric, we use GloVe embeddings for adjectives shared by domain-pairs. We train GloVe models for each domain over 50 epochs, for 50 dimensions with a learning rate of 0.05. For computing similarity of a domain-pair, we follow the same procedure as described under the Word2Vec metric. The final similarity value is obtained using equation (DISPLAY_FORM29).",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText",
                "We train monolingual word embeddings-based models for each domain using the FastText library. We train these models with 100 dimensions and 0.1 as the learning rate. The size of the context window is limited to 5 since FastText also uses sub-word information. Our model takes into account character n-grams from 3 to 6 characters, and we train our model over 5 epochs. We use the default loss function (softmax) for training.",
                "We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\pm 0.01$.",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo",
                "We use the pre-trained deep contextualized word representation model provided by the ELMo library. Unlike Word2Vec, GloVe, and FastText, ELMo gives multiple embeddings for a word based on different contexts it appears in the corpus."
            ],
            "gold_section": [
                "Similarity Metrics",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change"
            ],
            "predicted": [
                "Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.",
                "In table TABREF6, we present the n-gram percent match among the domain data used in our experiments. We observe that the n-gram match from among this corpora is relatively low and simple corpus similarity measures which use orthographic techniques cannot be used to obtain domain similarity. Hence, we propose the use of the metrics detailed below to perform our experiments.",
                "Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings."
            ],
            "predicted_section": [
                "Similarity Metrics"
            ]
        },
        "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654": {
            "question_text": "What high-resource language pair is the parent model trained on?",
            "from_paper": "1604.02201",
            "gold": [
                "We start by a brief description of our NMT model in Section \"NMT Background\" . Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" .",
                "The transfer learning approach we use is simple and effective. We first train an NMT model on a dataset where there is a large amount of bilingual data (e.g., French-English), which we call the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a dataset with very little bilingual data (e.g., Uzbek-English), which we call the child model. This means that the low-data NMT model will not start with random weights, but with the weights from the parent model."
            ],
            "gold_section": [
                "Transfer Learning",
                "Introduction"
            ],
            "predicted": [
                "For all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu. The target language is always English. Table 1 shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 Bleu on the development set. Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems without using our transfer method.",
                "The results of this experiment are shown in Table 6 . We get a 4.3 Bleu improvement with an unrelated parent (i.e. French-parent and Uzbek-child), but we get a 6.7 Bleu improvement with a `closely related' parent (i.e. French-parent and French'-child). We conclude that the choice of parent model can have a strong impact on transfer models, and choosing better parents for our low-resource languages (if data for such parents can be obtained) could improve the final results.",
                "Our experimental results are shown in Table 5 , where we use French and German as parent languages. If we just train a model with no transfer on a small Spanish-English training set we get a Bleu score of 16.4. When using our transfer method using French and German as parent languages, we get Bleu scores of 31.0 and 29.8 respectively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language."
            ],
            "predicted_section": [
                "Different Parent Languages",
                "Effects of having Similar Parent Language",
                "Experiments"
            ]
        },
        "d0dc6729b689561370b6700b892c9de8871bb44d": {
            "question_text": "How did they constrain training using the parameters?",
            "from_paper": "1604.02201",
            "gold": [
                "In this paper, we give a method for substantially improving NMT results on these languages. Neural models learn representations of their input that are often useful across tasks. Our key idea is to first train a high-resource language pair, then use the resulting trained network (the parent model) to initialize and constrain training for our low-resource language pair (the child model). We find that we can optimize our results by fixing certain parameters of the parent model, letting the rest be fine-tuned by the child model. We report NMT improvements from transfer learning of 5.6 Bleu on average, and we provide an analysis of why the method works. The final NMT system approaches strong SBMT baselines in in all four language pairs, and exceeds SBMT performance in one of them. Furthermore, we show that NMT is an exceptional re-scorer of `traditional' MT output; even NMT that on its own is worse than SBMT is consistently able to improve upon SBMT system output when incorporated as a re-scoring model.",
                "A justification for this approach is that in scenarios where we have limited training data, we need a strong prior distribution over models. The parent model trained on a large amount of bilingual data can be considered an anchor point, the peak of our prior distribution in model space. When we train the child model initialized with the parent model, we fix parameters likely to be useful across tasks so that they will not be changed during child-model training. In the French-English to Uzbek-English example, as a result of the initialization, the English word embeddings from the parent model are copied, but the Uzbek words are initially mapped to random French embeddings. The English embeddings should be kept but the Uzbek embeddings should be modified during training of the child model. Freezing certain portions of the parent model and fine tuning others can be considered a hard approximation to a tight prior or strong regularization applied to some of the parameters. We also experiment with ordinary L2 regularization, but find it does not significantly improve over the parameter freezing described above."
            ],
            "gold_section": [
                "Transfer Learning",
                "Introduction"
            ],
            "predicted": [
                "For our NMT system, we use development sets for Hausa, Turkish, Uzbek, and Urdu to tune the learning rate, parameter initialization range, dropout rate and hidden state size for all the experiments. For training we use a minibatch size of 128, hidden state size of 1000, a target vocabulary size of 15K and a source vocabulary size of 30K. The child models are trained with a dropout probability of 0.5 as in dropout. The common parent model is trained with a dropout probability of 0.2. The learning rate used for both child and parents is 0.5 with a decay rate of 0.9 when the development perplexity does not improve. The child models are all trained for 100 epochs. We re-scale the gradient when the gradient norm is greater than 5. The initial parameter range is $[$ -0.08, +0.08 $]$ .",
                "The fact that the two models start from and converge to very different points, yet have similar training set performances, indicates that our architecture and training algorithm are able to reach a good minimum of the training objective regardless of the initialization. However, the training objective seems to have a large basin of models with similar performance and not all of them generalize well to the development set. The transfer model, starting with and staying close to a point known to perform well on a related task, is guided to a final point in the weight space that generalizes to the development set much better.",
                "In all the above experiments, only the target input and output embeddings are fixed during training. In this section we analyze what happens when different parts of the model are fixed, in order to see what yields optimal performance. Figure 2 shows a diagram of the components of a sequence-to-sequence model. Table 7 shows how we begin to allow more components of the child NMT model to be trained and see the effect on performance in the model. We see that the optimal setting for transferring from French-English to Uzbek-English in terms of Bleu performance is to allow all of the components of the child model to be trained except for the input and output target embeddings."
            ],
            "predicted_section": [
                "Learning Curve",
                "Experiments",
                "Ablation Analysis"
            ]
        },
        "72c04eb3fc323c720f7f8da75c70f09a35abf3e6": {
            "question_text": "How much were the gains they obtained?",
            "from_paper": "1806.06571",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "This work has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement no. 645452 (QT21), the grant GAUK 8502/2016, and SVV project number 260 333.",
                "Vector representations of words learned using neural networks (NN) have proven helpful in many algorithms for image annotation BIBREF0 or BIBREF1 , language modeling BIBREF2 , BIBREF3 and BIBREF4 or other natural language processing (NLP) tasks BIBREF5 or BIBREF6 .",
                "past-tense: Remove ing and add ed at the end of the verb."
            ],
            "predicted_section": [
                "Rule-Based Baseline Approach",
                "Acknowledgment",
                "Introduction"
            ]
        },
        "ab8b0e6912a7ca22cf39afdac5531371cda66514": {
            "question_text": "By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s ",
            "from_paper": "1912.03010",
            "gold": [
                "The experiment results are listed in Table TABREF21, showing a similar trend as the results in Librispeech dataset. Semantic mask is complementary to specagumentation, which enables better S2S language modeling training in an E2E model, resulting in a relative 4.5$\\%$ gain. The experiment proves the effectiveness of semantic mask on a different and smaller dataset.",
                "As far as we know, our model is the best E2E ASR system on the Librispeech testset, which achieves a comparable result with wav2letter Transformer on test-clean dataset and a better result on test-other dataset, even though our model (75M parameters) is much smaller than the wav2letter Transformer (210M parameters). The reason might be that our semantic masking is more suitable on a noisy setting, because the input features are not reliable and the model has to predict the next token relying on previous ones and the whole context of the input. Our model is built upon the code base of ESPnet, and achieves relative $10\\%$ gains due to the better architecture and masking strategy. Comparing with hybrid methods, our model obtains a similar performance on the test-clean set, but is still worse than the best hybrid model on the test-other dataset."
            ],
            "gold_section": [
                "EXPERIMENT ::: Librispeech 960h",
                "EXPERIMENT ::: TedLium2"
            ],
            "predicted": [
                "We represent input signals as a sequence of 80-dim log-Mel filter bank with 3-dim pitch features BIBREF17. SentencePiece is employed as the tokenizer, and the vocabulary size is 5000. The hyper-parameters in Transformer and SpecAugment follow BIBREF6 for a fair comparison. We use Adam algorithm to update the model, and the warmup step is 25000. The learning rate decreases proportionally to the inverse square root of the step number after the 25000-th step. We train our model 100 epochs on 4 P40 GPUs, which approximately costs 5 days to coverage. We also apply speed perturbation by changing the audio speed to 0.9, 1.0 and 1.1. Following BIBREF6, we average the last 5 checkpoints as the final model. Unlike BIBREF14 and BIBREF15, we use the same checkpoint for test-clean and test-other dataset.",
                "End-to-end (E2E) acoustic models, particularly with the attention-based encoder-decoder framework BIBREF0, have achieved a competitive recognition accuracy in a wide range of speech datasets BIBREF1. This model directly learns the mapping from the input acoustic signals to the output transcriptions without decomposing the problems into several different modules such as lexicon modeling, acoustic modeling and language modeling as in the conventional hybrid architecture. While this kind of E2E approach significantly simplifies the speech recognition pipeline, the weakness is that it is difficult to tune the strength of each component. One particular problem from our observations is that the attention based E2E model tends to make grammatical errors, which indicates that the language modeling power of the model is weak, possibly due to the small amount of training data, or the mismatch between the training and evaluation data. However, due to the jointly model approach in the attention model, it is unclear how to improve the strength of the language modeling power, i.e., attributing more weights to the previous output tokens in the decoder, or to improve the strength of the acoustic modeling power, i.e., attributing more weights to the context vector from the encoder.",
                "In principle, our approach is applicable to the attention-based E2E framework with any type of neural network encoder. To constrain our research scope, we focus on the transformer architecture BIBREF5, which is originally proposed for neural machine translation. Recently, it has been shown that the transformer model can achieve competitive or even higher recognition accuracy compared with the recurrent neural network (RNN) based E2E model for speech recognition BIBREF6. Compared with RNNs, the transformer model can capture the long-term correlations with a computational complexity of $O(1)$, instead of using many steps of back-propagation through time (BPTT) as in RNNs. We evaluate our transformer model with semantic masking on Librispeech and TedLium datasets. We show that semantic masking can achieve significant word error rate reduction (WER) on top of SpecAugment, and we report the lowest WERs on the test sets of the Librispeech corpus with an E2E model."
            ],
            "predicted_section": [
                "EXPERIMENT ::: Librispeech 960h",
                "Introduction"
            ]
        },
        "230f127e83ac62dd65fccf6b1a4960cf0f7316c7": {
            "question_text": "How are experiments designed to measure impact on performance by different choices?",
            "from_paper": "2004.02401",
            "gold": [
                "The learning rate boundary of the CLR is selected by the range test (shown in Figure FIGREF7). The base and maximal learning rates adopted in this study are presented in Table TABREF13. Shrink strategy is applied when examining the effects of CLR in training NMT. The optimizers (Adam and SGD) are assigned with two options: 1) without shrink (as \u201cnshrink\"); 2) with shrink at a rate of 0.5 (\u201cyshrink\"), which means the maximal learning rate for each cycle is reduced at a decay rate of 0.5."
            ],
            "gold_section": [
                "Experiments ::: Experiment Settings"
            ],
            "predicted": [
                "The contributions of this study are to:",
                "While the aforementioned works have helped to contribute our understanding of the nature of the various optimizers, their learning rates and batch size effects, they are mainly focused on computer vision (CV) related deep learning networks and datasets. In contrast, the rich body of works in Neural Machine Translation (NMT) and other Natural Language Processing (NLP) related tasks have been largely left untouched. Recall that CV deep learning networks and NMT deep learning networks are very different. For instance, the convolutional network that forms the basis of many successful CV deep learning networks is translation invariant, e.g., in a face recognition network, the convolutional filters produce the same response even when the same face is shifted or translated. In contrast, Recurrent Neural Networks (RNN) BIBREF12, BIBREF13 and transformer-based deep learning networks BIBREF14, BIBREF15 for NMT are specifically looking patterns in sequences. There is no guarantee that the results from the CV based studies can be carried across to NMT. There is also a lack of awareness in the NMT community when it comes to optimizers and other related issues such as learning rate policy and batch size. It is often assumed that using the mainstream optimizer (Adam) with the default settings is good enough. As our study shows, there is significant room for improvement.",
                "Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;"
            ],
            "predicted_section": [
                "Introduction ::: The Contributions",
                "Introduction"
            ]
        },
        "75c221920bee14a6153bd5f4c1179591b2f48d59": {
            "question_text": "What impact on performance is shown for different choices of optimizers and learning rate policies?",
            "from_paper": "2004.02401",
            "gold": [
                "Applying CLR has positive impacts on NMT training for both Adam and SGD. When applied to SGD, CLR exempts the needs for a big initial learning rate as it enables the optimizer to explore the local minima better. Shrinking on CLR for SGD is not desirable as a higher learning rate is required (Figure FIGREF16). It is noted that applying CLR to Adam produces consistent improvements regardless of shrink options (Figure FIGREF15). Furthermore, it can be observed that the effects of applying CLR to Adam are more significant than those of SGD, as shown in Figure FIGREF17. Similar results are obtained from our experiments on \u201cIWSLT2017-de-en\" and \u201cIWSLT2014-fr-en\" corpora (Figures FIGREF30 and FIGREF31 in Appendix SECREF7). The corresponding BLEU scores are presented in Table TABREF18, in which the above-mentioned effects of CLR on Adam can also be established. The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18)."
            ],
            "gold_section": [
                "Experiments ::: Effects of Applying CLR to NMT Training"
            ],
            "predicted": [
                "Raise awareness of how a judicial choice of optimizer with a good learning rate policy can help improve performance;",
                "While the aforementioned works have helped to contribute our understanding of the nature of the various optimizers, their learning rates and batch size effects, they are mainly focused on computer vision (CV) related deep learning networks and datasets. In contrast, the rich body of works in Neural Machine Translation (NMT) and other Natural Language Processing (NLP) related tasks have been largely left untouched. Recall that CV deep learning networks and NMT deep learning networks are very different. For instance, the convolutional network that forms the basis of many successful CV deep learning networks is translation invariant, e.g., in a face recognition network, the convolutional filters produce the same response even when the same face is shifted or translated. In contrast, Recurrent Neural Networks (RNN) BIBREF12, BIBREF13 and transformer-based deep learning networks BIBREF14, BIBREF15 for NMT are specifically looking patterns in sequences. There is no guarantee that the results from the CV based studies can be carried across to NMT. There is also a lack of awareness in the NMT community when it comes to optimizers and other related issues such as learning rate policy and batch size. It is often assumed that using the mainstream optimizer (Adam) with the default settings is good enough. As our study shows, there is significant room for improvement.",
                "There has been many interests in deep learning optimizer research recently BIBREF0, BIBREF1, BIBREF2, BIBREF3. These works attempt to answer the question: what is the best step size to use in each step of the gradient descent? With the first order gradient descent being the de facto standard in deep learning optimization, the question of the optimal step size or learning rate in each step of the gradient descent arises naturally. The difficulty in choosing a good learning rate can be better understood by considering the two extremes: 1) when the learning rate is too small, training takes a long time; 2) while overly large learning rate causes training to diverge instead of converging to a satisfactory solution."
            ],
            "predicted_section": [
                "Introduction ::: The Contributions",
                "Introduction"
            ]
        },
        "cfc73e0c82cf1630b923681c450a541a964688b9": {
            "question_text": "How do they operationalize socioeconomic status from twitter user data?",
            "from_paper": "1804.01155",
            "gold": [
                "To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location. Further we limited our users to ones located throughout the French territory thus not considering others tweeting from places outside the country. This selection method provided us with $110,369$ geolocated users who are either detected as French speakers or assigned to be such by Twitter and all associated to specific 'home' GPS coordinates in France. To verify the spatial distribution of the selected population, we further assessed the correlations between the true population distributions (obtained from census data BIBREF28 ) at different administrative level and the geolocated user distribution aggregated correspondingly. More precisely, we computed the $R^2$ coefficient of variation between the inferred and official population distributions (a) at the level of 22 regions. Correlations at this level induced a high coefficient of $R^2\\simeq 0.89$ ( $p<10^{-2}$ ); (b) At the arrondissement level with 322 administrative units and coefficient $R^2\\simeq 0.87$ ( $p<10^{-2}$ ); and (c) at the canton level with 4055 units with a coefficient $R\\simeq 0.16$ ( $p<10^{-2}$ ). Note that the relatively small coefficient at this level is due to the interplay of the sparsity of the inferred data and the fine grained spatial resolution of cantons. All in all, we can conclude that our sample is highly representative in terms of spatial population distribution, which at the same time validate our selection method despite the potential inherent biases induced by the method taking the most frequented GPS coordinates as the user's home location.",
                "The second dataset we used was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This data corpus BIBREF29 contains a set of sociodemographic aggregated indicators, estimated from the 2010 tax return in France, for each 4 hectare ( $200m \\times 200m$ ) square patch across the whole French territory. Using these indicators, one can estimate the distribution of the average socioeconomic status (SES) of people with high spatial resolution. In this study, we concentrated on three indicators for each patch $i$ , which we took to be good proxies of the socioeconomic status of the people living within them. These were the $S^i_\\mathrm {inc}$ average yearly income per capita (in euros), the $S^i_{\\mathrm {own}}$ fraction of owners (not renters) of real estate, and the $S^i_\\mathrm {den}$ density of population defined respectively as",
                "To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators."
            ],
            "gold_section": [
                "Combined dataset: individual socioeconomic features",
                "Twitter dataset: sociolinguistic features"
            ],
            "predicted": [
                "Many studies have overcome this limitation by taking advantage of the geolocation feature allowing Twitter users to include in their posts the location from which they were tweeted. Based on this metadata, studies have been able to assign home location to geolocated users with varying degrees of accuracy BIBREF15 . Subsequent work has also been devoted to assigning to each user some indicator that might characterize their socioeconomic status based on their estimated home location. These indicators are generally extracted from other datasets used to complete the Twitter one, namely census data BIBREF16 , BIBREF12 , BIBREF17 or real estate online services as Zillow.com BIBREF18 . Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 . Despite the relative success of these methods, their common limitation is to provide observations and predictions based on a carefully hand-picked small set of users, letting alone the problem of socioeconomic status inference on larger and more heterogeneous populations. Our work stands out from this well-established line of research by expanding the definition of socioeconomic status to include several demographic features as well as by pinpointing potential home location to individual users with an unprecedented accuracy. Identifying socioeconomic status and the network effects of homophily BIBREF20 is an open question BIBREF21 . However, recent results already showed that status homophily, i.e. the tendency of people of similar socioeconomic status are better connected among themselves, induce structural correlations which are pivotal to understand the stratified structure of society BIBREF22 . While we verify the presence of status homophily in the Twitter social network, we detect further sociolinguistic correlations between language, location, socioeconomic status, and time, which may inform novel methods to infer socioeconomic status for a broader set of people using common information available on Twitter.",
                "To do so, first we took the geolocated Twitter users in France and partitioned them into nine socioeconomic classes using their inferred income $S_\\mathrm {inc}^u$ . Partitioning was done first by sorting users by their $S^u_\\mathrm {inc}$ income to calculate their $C(S^u_\\mathrm {inc})$ cumulative income distribution function. We defined socioeconomic classes by segmenting $C(S^u_\\mathrm {inc})$ such that the sum of income is the same for each classes (for an illustration of our method see Fig. 6 a in the Appendix). We constructed a social network by considering mutual mention links between these users (as introduced in Section \"Data Description\" ). Taking the assigned socioeconomic classes of connected individuals, we confirmed the effects of status homophily in the Twitter mention network by computing the connection matrix of socioeconomic groups normalized by the equivalent matrix of corresponding configuration model networks, which conserved all network properties except structural correlations (as explained in the Appendix). The diagonal component in Fig. 6 matrix indicated that users of similar socioeconomic classes were better connected, while people from classes far apart were less connected than one would expect by chance from the reference model with users connected randomly.",
                "Data collected from Twitter provides a large variety of information about several users including their tweets, which disclose their interests, vocabulary, and linguistic patterns; their direct mentions from which their social interactions can be inferred; and the sequence of their locations, which can be used to infer their representative location. However, no information is directly available regarding their socioeconomic status, which can be pivotal to understand the dynamics and structure of their personal linguistic patterns."
            ],
            "predicted_section": [
                "Combined dataset: individual socioeconomic features",
                "Network variation",
                "Related Work"
            ]
        },
        "98b97d24f31e9c535997e9b6cb126eb99fc72a90": {
            "question_text": "What empirical evaluation was used?",
            "from_paper": "1910.01160",
            "gold": [
                "We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.",
                "First, we consider the semantic representation with BERT. Our experiments included multiple pre-trained models of BERT with different sizes and cases sensitivity, among which the large uncased model, bert_uncased_L-24_H-1024_A-16, gave the best results. We use the recommended settings of hyper-parameters in BERT's Github repository and use the fake news and satire data to fine-tune the model. Furthermore, we tested separate models based on the headline and body text of a story, and in combination. Results are shown in Table TABREF6. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance.",
                "With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."
            ],
            "gold_section": [
                "Evaluation ::: Insights on Linguistic Nuances",
                "Evaluation ::: Classification Between Fake News and Satire"
            ],
            "predicted": [
                "We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.",
                "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression. Those components that are also significant in the step-wise model appear in bold.",
                "To investigate the predictive power of the linguistic cues, we use those Coh-Metrix indices that were significant in both the logistic and step-wise backward elimination regression models, and train a classifier on fake news and satire articles. We tested a few classification models, including Naive Bayes, Support Vector Machine (SVM), logistic regression, and gradient boosting - among which the SVM classifier gave the best results."
            ],
            "predicted_section": [
                "Evaluation ::: Classification Between Fake News and Satire",
                "Method ::: Linguistic Analysis with Coh-Metrix"
            ]
        },
        "998fa38634000f2d7b52d16518b9e18e898ce933": {
            "question_text": "Does the SESAME dataset include discontiguous entities?",
            "from_paper": "1908.05758",
            "gold": [
                "The next step consists of detecting mentions to entities in the raw text. To do this, we tag character segments that exactly match one of the known names of an entity. For instance, we can tag two different entities in the following text:"
            ],
            "gold_section": [
                "Preprocessing ::: Identifying entity mentions in text"
            ],
            "predicted": [
                "SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45.",
                "Additionally, the baseline was developed on a balanced re-sample of SESAME with a total of 1,216,976 sentences. The model also receives additional categorical features for each word, signalizing whether it: (1) starts with a capital letter, (2) has capitalized letters only, (3) has lowercase letters only, (4) contains digits, (5) has mostly digits ($>$ 50%) and (6) has digits only.",
                "(1) Entities which are not contained in a single sentence:"
            ],
            "predicted_section": [
                "Preprocessing ::: Tokenization of words and sentences",
                "Preprocessing ::: SESAME ::: Tokens",
                "Baseline ::: Baseline results"
            ]
        },
        "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3": {
            "question_text": "How many layers of recurrent neural networks do they use for encoding the global context?",
            "from_paper": "1809.00129",
            "gold": [
                "CEQE consists of three major components: (1) embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network.",
                "RNN-based Encoding",
                "After we obtain the representation of the source-target word pair by the convolution layer, we follow a similar architecture as BIBREF6 to refine the representation of the word pairs using feed-forward and recurrent networks.",
                "Two feed-forward layers of size 400 with rectified linear units (ReLU; BIBREF9 );",
                "One bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 .",
                "Two feed-forward layers of hidden size 200 with rectified linear units;",
                "One BiGRU layer with hidden size 100 using the same configuration of the previous BiGRU layer;",
                "Two feed-forward layers of size 100 and 50 respectively with ReLU activation."
            ],
            "gold_section": [
                "Model",
                "RNN-based Encoding"
            ],
            "predicted": [
                "In this paper, we propose a deep neural architecture for word-level QE. Our framework leverages a one-dimensional convolution on the concatenated word embeddings of target and its aligned source words to extract salient local feature maps. In additions, bidirectional RNNs are applied to capture temporal dependencies for better sequence prediction. We conduct thorough experiments on four language pairs in the WMT2018 shared task. The proposed framework achieves highly competitive results, outperforms all other participants on English-Czech and English-Latvian word-level, and is second place on English-German, and German-English language pairs.",
                "CEQE consists of three major components: (1) embedding layers for words and part-of-speech (POS) tags in both languages, (2) convolution encoding of the local context for each target word, and (3) encoding the global context by the recurrent neural network.",
                "One bi-directional gated recurrent unit (BiGRU; BIBREF10 ) layer with hidden size 200, where the forward and backward hidden states are concatenated and further normalized by layer normalization BIBREF11 ."
            ],
            "predicted_section": [
                "Model",
                "Conclusion",
                "RNN-based Encoding"
            ]
        },
        "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6": {
            "question_text": "What is the Semantic Web?",
            "from_paper": "1911.01248",
            "gold": [
                "NLG is the process of automatically generating coherent NL text from non-linguistic data BIBREF0. Recently, the field has seen an increased interest in the development of NLG systems focusing on verbalizing resources from SW data BIBREF1. The SW aims to make information available on the Web easier to process for machines and humans. However, the languages underlying this vision, i.e., RDF, SPARQL and OWL, are rather difficult to understand for non-expert users. For example, while the meaning of the OWL class expression Class: Professor SubClassOf: worksAt SOME University is obvious to every SW expert, this expression (\u201cEvery professor works at a university\u201d) is rather difficult to fathom for lay persons.",
                "OWL BIBREF15 is the de-facto standard for machine processable and interoperable ontologies on the SW. In its second version, OWL is equivalent to the description logic $\\mathcal {SROIQ}(D)$. Such expressiveness has a higher computational cost but allows the development of interesting applications such as automated reasoning BIBREF16. OWL 2 ontologies consist of the following three different syntactic categories:",
                "RDF BIBREF18 uses a graph-based data model for representing knowledge. Statements in RDF are expressed as so-called triples of the form (subject, predicate, object). RDF subjects and predicates are IRI and objects are either IRI or literals. RDF literals always have a datatype that defines its possible values. A predicate denotes a property and can also be seen as a binary relation taking subject and object as arguments. For example, the following triple expresses that Albert Einstein was born in Ulm:"
            ],
            "gold_section": [
                "Background ::: RDF",
                "Background ::: OWL",
                "Introduction"
            ],
            "predicted": [
                "In this paper, we present an open-source holistic NLG framework for the SW, named LD2NL, which facilitates the verbalization of the three key languages of the SW, i.e., RDF, OWL, and SPARQL into NL. Our framework is based on a bottom-up paradigm for verbalizing SW data. Additionally, LD2NL builds upon SPARQL2NL as it is open-source and the paradigm it follows can be reused and ported to RDF and OWL. Thus, LD2NL is capable of generating either a single sentence or a summary of a given resource, rule, or query. To validate our framework, we evaluated LD2NL using experts 66 in NLP and SW as well as 20 non-experts who were lay users or non-users of SW. The results suggest that LD2NL generates texts which can be easily understood by humans. The version of LD2NL used in this paper, all experimental results will be publicly available.",
                "NLG is the process of automatically generating coherent NL text from non-linguistic data BIBREF0. Recently, the field has seen an increased interest in the development of NLG systems focusing on verbalizing resources from SW data BIBREF1. The SW aims to make information available on the Web easier to process for machines and humans. However, the languages underlying this vision, i.e., RDF, SPARQL and OWL, are rather difficult to understand for non-expert users. For example, while the meaning of the OWL class expression Class: Professor SubClassOf: worksAt SOME University is obvious to every SW expert, this expression (\u201cEvery professor works at a university\u201d) is rather difficult to fathom for lay persons.",
                "The goal of LD2NL is to provide an integrated system which generates a complete and correct NL representation for the most common used SW modeling languages RDF and OWL, and SPARQL. In terms of the standard model of NL generation proposed by Reiter & Dale BIBREF19, our steps mainly play the role of the micro-planner, with focus on aggregation, lexicalization, referring expressions and linguistic realization. In the following, we present our approach to formalizing NL sentences for each of the supported languages."
            ],
            "predicted_section": [
                "LD2NL Framework",
                "Introduction"
            ]
        },
        "1128a600a813116cba9a2cf99d8568ae340f327a": {
            "question_text": "What datasets do they use in the experiment?",
            "from_paper": "1802.08969",
            "gold": [
                "For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .",
                "The remaining two datasets are two sub-datasets about movie reviews.",
                "IMDB The movie reviews with labels of subjective or objective BIBREF28 .",
                "MR The movie reviews with two classes BIBREF29 .",
                "For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 ."
            ],
            "gold_section": [
                "Exp-I: Multi-task Learning of text classification",
                "Exp-II: Multi-task Learning of Sequence Tagging"
            ],
            "predicted": [
                "We first conduct our experiment on classification tasks.",
                "The remaining two datasets are two sub-datasets about movie reviews.",
                "For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 ."
            ],
            "predicted_section": [
                "Exp-I: Multi-task Learning of text classification"
            ]
        },
        "d64fa192a7e9918c6a22d819abad581af0644c7d": {
            "question_text": "What new tasks do they use to show the transferring ability of the shared meta-knowledge?",
            "from_paper": "1802.08969",
            "gold": [
                "To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. The parameters of transferred Meta-LSTM, $\\theta ^{(s)}_m$ in Eq.( 33 ), are fixed and cannot be updated on the new task.",
                "We demonstrate the effectiveness of our architectures on two kinds of NLP tasks: text classification and sequence tagging. Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.",
                "Table 5 shows the accuracies or F1 scores on the sequence tagging datasets of our models, compared to some state-of-the-art results. As shown, our proposed Meta-LSTM performs better than our competitor models whether it is single or multi-task learning."
            ],
            "gold_section": [
                "Exp-I: Multi-task Learning of text classification",
                "Exp-II: Multi-task Learning of Sequence Tagging",
                "Introduction"
            ],
            "predicted": [
                "The meta network can be considered as off-the-shelf knowledge and then be used for unseen new tasks.",
                "In this paper, we introduce a novel knowledge sharing scheme for multi-task learning. The difference from the previous models is the mechanisms of sharing information among several tasks. We design a meta network to store the knowledge shared by several related tasks. With the help of the meta network, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. Experimental results show that our model can improve the performances of several related tasks by exploring common features and outperforms the representational sharing scheme. The knowledge captured by the meta network can be transferred across other new tasks.",
                "In this paper, we take a very different multi-task architecture from meta-learning perspective BIBREF25 . One goal of meta-learning is to find efficient mechanisms to transfer knowledge across domains or tasks BIBREF26 ."
            ],
            "predicted_section": [
                "Conclusion and Future Work",
                "Exp-I: Multi-task Learning of text classification",
                "Meta Multi-Task Learning"
            ]
        },
        "3d1ad8a4aaa2653d0095bafba74738bd20795acf": {
            "question_text": "what dataset were used?",
            "from_paper": "1909.07158",
            "gold": [
                "Experimental Setting",
                "We first present the data sets used for the evaluation of the proposed approach, followed by the experimental scenario. The results are presented in Section SECREF5.",
                "Experimental Setting ::: Hate Speech Data Sets",
                "We use three data sets related to the hate speech.",
                "Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval",
                "data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).",
                "Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic",
                "data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.",
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets."
            ],
            "gold_section": [
                "Experimental Setting",
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "Experimental Setting ::: Hate Speech Data Sets",
                "Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic",
                "Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval"
            ],
            "predicted": [
                "We use three data sets related to the hate speech.",
                "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.",
                "We first present the data sets used for the evaluation of the proposed approach, followed by the experimental scenario. The results are presented in Section SECREF5."
            ],
            "predicted_section": [
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "Experimental Setting ::: Hate Speech Data Sets",
                "Experimental Setting"
            ]
        },
        "344238de7208902f7b3a46819cc6d83cc37448a0": {
            "question_text": "Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?",
            "from_paper": "1908.06024",
            "gold": [
                "Razavi et al. razavi were the first to adopt lexicon-based abuse detection. They constructed an insulting and abusing language dictionary of words and phrases, where each entry had an associated weight indicating its abusive impact. They utilized semantic rules and features derived from the lexicon to build a three-level Naive Bayes classification system and apply it to a dataset of INLINEFORM0 messages ( INLINEFORM1 flame and the rest okay) extracted from the Usenet newsgroup and the Natural Semantic Module company's employee conversation thread ( INLINEFORM2 accuracy). Njagi et al. gitari also employed such a lexicon-based approach and, more recently, Wiegand et al. wiegand proposed an automated framework for generating such lexicons. While methods based on lexicons performed well on explicit abuse, the researchers noted their limitations on implicit abuse.",
                "Bag-of-words (bow) features have been integral to several works on abuse detection. Sood et al. sood2012 showed that an svm trained on word bi-gram features outperformed a word-list baseline utilizing a Levenshtein distance-based heuristic for detecting profanity. Their best classifier (combination of SVMs and word-lists) yielded an F INLINEFORM0 of INLINEFORM1 . Warner and Hirschberg warner employed a template-based strategy alongside Brown clustering to extract surface-level bow features from a dataset of paragraphs annotated for antisemitism, and achieved an F INLINEFORM2 of INLINEFORM3 using svms. Their approach is unique in that they framed the task as a word-sense disambiguation problem, i.e., whether a term carried an anti-semitic sense or not. Other examples of bow-based methods are those of Dinakar et al. dinakar2011modeling, Burnap and Williams burnap and Van Hee et al. vanhee who use word n-grams in conjunction with other features, such as typed-dependency relations or scores based on sentiment lexicons, to train svms ( INLINEFORM4 F INLINEFORM5 on the data-bully dataset). Recenlty, Salminen et al. salminen2018anatomy showed that a linear SVM using tf\u2013idf weighted n-grams achieves the best performance (average F INLINEFORM6 of INLINEFORM7 ) on classification of hateful comments (from a YouTube channel and Facebook page of an online news organization) as one of 29 different hate categories (e.g., accusation, promoting violence, humiliation, etc.).",
                "Several researchers have directly incorporated features and identity traits of users in order to model the likeliness of abusive behavior from users with certain traits, a process known as user profiling. Dadvar et al. davdar included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, while Gal\u00e1n-Garc\u00eda et al. galan2016supervised utilized the time of publication, geo-position and language in the profile of Twitter users. Waseem and Hovy waseemhovy exploited gender of Twitter users alongside character n-gram counts to improve detection of sexism and racism in tweets from data-twitter-wh (F INLINEFORM0 increased from INLINEFORM1 to INLINEFORM2 ). Using the same setup, Unsv\u00e5g and Gamb\u00e4ck unsvaag2018effects showed that the inclusion of social network-based (i.e., number of followers and friends) and activity-based (i.e., number of status updates and favorites) information of users alongside their gender further enhances performance ( INLINEFORM3 gain in F INLINEFORM4 ).",
                "Building on the work of Djuric et al., Nobata et al. nobata evaluated the performance of a large range of features on the Yahoo! datasets (data-yahoo-*) using a regression model: (1) word and character n-grams; (2) linguistic features, e.g., number of polite/hate words and punctuation count; (3) syntactic features, e.g., parent and grandparent of node in a dependency tree; (4) distributional-semantic features, e.g., paragraph2vec comment representations. Although the best results were achieved with all features combined (F INLINEFORM0 INLINEFORM1 on data-yahoo-fin-a, INLINEFORM2 on data-yahoo-news-a), character n-grams on their own contributed significantly more than other features due to their robustness to noise (i.e., obfuscations, misspellings, unseen words). Experimenting with the data-yahoo-fin-dj dataset, Mehdad and Tetreault mehdad investigated whether character-level features are more indicative of abuse than word-level ones. Their results demonstrated the superiority of character-level features, showing that svm classifiers trained on Bayesian log-ratio vectors of average counts of character n-grams outperform the more intricate approach of Nobata et al. nobata in terms of AUC ( INLINEFORM3 vs. INLINEFORM4 ) as well as other rnn-based character and word-level models.",
                "Samghabadi et al. W17-3010 utilized a similar set of features as Nobata et al. and augmented it with hand-engineered ones such as polarity scores derived from SentiWordNet, measures based on the LIWC program, and features based on emoticons. They then applied their method to three different datasets: data-wiki-att, a Kaggle dataset annotated for insult, and a dataset of questions and answers (each labeled as invective or neutral) that they created by crawling ask.fm. Distributional-semantic features combined with the aforementioned features constituted an effective feature space for the task ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 F INLINEFORM3 on data-wiki-att, Kaggle, ask.fm respectively). In line with the findings of Nobata et al. and Mehdad and Tetreault, character n-grams performed well on these datasets too."
            ],
            "gold_section": [
                "Feature engineering based approaches",
                "Neural network based approaches"
            ],
            "predicted": [
                "With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. Munro munro2011 studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. Pew Research Center, in its latest report on online harassment BIBREF0 , revealed that INLINEFORM0 of adults in the United States have experienced abusive behavior online, of which INLINEFORM1 have faced severe forms of harassment, e.g., that of sexual nature. The report goes on to say that harassment need not be experienced first-hand to have an impact: INLINEFORM2 of American Internet users admitted that they stopped using an online service after witnessing abusive and unruly behavior of their fellow users. These statistics stress the need for automated abuse detection and moderation systems. Therefore, in the recent years, a new research effort on abuse detection has sprung up in the field of NLP.",
                "Online abuse stands as a significant challenge before society. Its nature and characteristics constantly evolve, making it a complex phenomenon to study and model. Automated abuse detection methods have seen a lot of development in recent years: from simple rule-based methods aimed at identifying directed, explicit abuse to sophisticated methods that can capture rich semantic information and even aspects of user behavior. By comprehensively reviewing the investigated methods to date, our survey aims to provide a platform for future research, facilitating progress in this important area. While we see an array of challenges that lie ahead, e.g., modeling extra-propositional aspects of language, user behavior and wider conversation, we believe that recent progress in the areas of semantics, dialogue modeling and social media analysis put the research community in a strong position to address them. Summaries of public datasets In table TABREF4 , we summarize the datasets described in this paper that are publicly available and provide links to them. A discussion of metrics The performance results we have reported highlight that, throughout work on abuse detection, different researchers have utilized different evaluation metrics for their experiments \u2013 from area under the receiver operating characteristic curve (auroc) BIBREF79 , BIBREF48 to micro and macro F INLINEFORM0 BIBREF28 \u2013 regardless of the properties of their datasets. This makes the presented techniques more difficult to compare. In addition, as abuse is a relatively infrequent phenomenon, the datasets are typically skewed towards non-abusive samples BIBREF6 . Metrics such as auroc may, therefore, be unsuitable since they may mask poor performance on the abusive samples as a side-effect of the large number of non-abusive samples BIBREF52 . Macro-averaged precision, recall, and F INLINEFORM1 , as well as precision, recall, and F INLINEFORM2 on specifically the abusive classes, may provide a more informative evaluation strategy; the primary advantage being that macro-averaged metrics provide a sense of effectiveness on the minority classes BIBREF73 . Additionally, area under the precision-recall curve (auprc) might be a better alternative to auroc in imbalanced scenarios BIBREF46 . ",
                "In this paper, we survey the methods that have been developed for automated detection of online abuse, analyzing their strengths and weaknesses. We first describe the datasets that exist for abuse. Then we review the various detection methods that have been investigated by the NLP community. Finally, we conclude with the main trends that emerge, highlight the challenges that remain, outline possible solutions, and propose guidelines for ethics and explainability. To the best of our knowledge, this is the first comprehensive survey in this area. We differ from previous surveys BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 in the following respects: 1) we discuss the categorizations of abuse based on coarse-grained vs. fine-grained taxonomies; 2) we present a detailed overview of datasets annotated for abuse; 3) we provide an extensive review of the existing abuse detection methods, including ones based on neural networks (omitted by previous surveys); 4) we discuss the key outstanding challenges in this area; and 5) we cover aspects of ethics and explainability."
            ],
            "predicted_section": [
                "Conclusions",
                "Introduction"
            ]
        },
        "22225ba18a6efe74b1315cc08405011d5431498e": {
            "question_text": "Do they use external financial knowledge in their approach?",
            "from_paper": "1705.00571",
            "gold": [
                "Domain specific terminology is expected to play a key part in this task, as reporters, investors and analysts in the financial domain will use a specific set of terminology when discussing financial performance. Potentially, this may also vary across different financial domains and industry sectors. Therefore, we took an exploratory approach and investigated how various features and learning algorithms perform differently, specifically SVR and BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper we are releasing our source code and the finance specific BLSTM word embedding model.",
                "The training data published by the organisers for this track was a set of headline sentences from financial news articles where each sentence was tagged with the company name (which we treat as the aspect) and the polarity of the sentence with respect to the company. There is the possibility that the same sentence occurs more than once if there is more than one company mentioned. The polarity was a real value between -1 (negative sentiment) and 1 (positive sentiment).",
                "We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language."
            ],
            "gold_section": [
                "Data",
                "Introduction"
            ],
            "predicted": [
                "Domain specific terminology is expected to play a key part in this task, as reporters, investors and analysts in the financial domain will use a specific set of terminology when discussing financial performance. Potentially, this may also vary across different financial domains and industry sectors. Therefore, we took an exploratory approach and investigated how various features and learning algorithms perform differently, specifically SVR and BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper we are releasing our source code and the finance specific BLSTM word embedding model.",
                "We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship.",
                "There is a growing amount of research being carried out related to sentiment analysis within the financial domain. This work ranges from domain-specific lexicons BIBREF2 and lexicon creation BIBREF3 to stock market prediction models BIBREF4 , BIBREF5 . BIBREF4 used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. BIBREF5 showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 BIBREF1 . The winning system BIBREF6 used many different linguistic features and an ensemble model, and the runner up BIBREF7 used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. BIBREF8 created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation. This system outperformed all others on certain languages on the SemEval-2016 task 5 dataset BIBREF1 and on other languages performed close to the best systems. BIBREF9 also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect."
            ],
            "predicted_section": [
                "Acknowledgements",
                "Related Work",
                "Introduction"
            ]
        },
        "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55": {
            "question_text": "what four learning strategies are investigated?",
            "from_paper": "1908.11664",
            "gold": [
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@",
                "This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.",
                "We achieve this by pre-training our basic model $Model^{I}_{Base}$ with BERT BIBREF28, which is one of the most successful learning frameworks. Then we investigate if BERT can provide domain information and bring the model good domain adaptability. To avoid introducing new structures, we use the feature-based BERT with its parameters fixed.",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@",
                "The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@",
                "In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35."
            ],
            "gold_section": [
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@"
            ],
            "predicted": [
                "We investigate the effectiveness of the above four strategies under three evaluation settings: in-domain, out-of-domain and cross-dataset. These settings make it possible to explicitly evaluate models both on the quality of domain-aware text representation and on their adaptation ability to derive reasonable representations in unfamiliar domains.",
                "In this paper, we explore publication in the context of the domain and investigate the domain shift problem in summarization. When verified its existence, we propose to build a multi-domain testbed for summarization that requires both training and measuring performance on a set of domains. Under these new settings, we propose four learning schemes to give a preliminary explore in characteristics of different learning strategies when dealing with multi-domain summarization tasks.",
                "Methodologically, we employ four types of models with their characteristics under different settings. The first model is inspired by the joint training strategy, and the second one builds the connection between large-scale pre-trained models and multi-domain learning. The third model directly constructs a domain-aware model by introducing domain type information explicitly. Lastly, we additionally explore the effectiveness of meta-learning methods to get better generalization. By analyzing their performance under in-domain, out-of-domain, and cross-dataset, we provide a preliminary guideline in Section SECREF31 for future research in multi-domain learning of summarization tasks."
            ],
            "predicted_section": [
                "Experiment",
                "Conclusion",
                "Introduction"
            ]
        },
        "654306d26ca1d9e77f4cdbeb92b3802aa9961da1": {
            "question_text": "By how much did the new model outperform multilingual BERT?",
            "from_paper": "1912.07076",
            "gold": [
                "Table TABREF25 summarizes the results for POS tagging. We find that neither M-BERT model improves on the previous state of the art for any of the three resources, with results ranging 0.1-0.8% points below the best previously published results. By contrast, both language-specific models outperform the previous state of the art, with absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points. While these improvements over the already very high reference results are modest in absolute terms, the relative reductions in errors are notable: in particular, the FinBERT cased error rate on FTB is less than half of the best CoNLL'18 result BIBREF22. We also note that the uncased models are surprisingly competitive with their cased equivalents for a task where capitalization has long been an important feature: for example, FinBERT uncased performance is within approx. 0.1% points of FinBERT cased for all corpora.",
                "Table TABREF41 shows LAS results for predicted and gold segmentation. While Udify initialized with M-BERT fails to outperform our strongest baseline BIBREF22, Udify initialized with FinBERT achieves notably higher performance on all three treebanks, establishing new state-of-the-art parsing results for Finnish with a large margin. Depending on the treebank, Udify with cased FinBERT LAS results are 2.3\u20133.6% points above the previous state of the art, decreasing errors by 24%\u201331% relatively.",
                "Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest."
            ],
            "gold_section": [
                "Evaluation ::: Text classification ::: Results",
                "Evaluation ::: Dependency Parsing ::: Results",
                "Evaluation ::: Part of Speech Tagging ::: Results"
            ],
            "predicted": [
                "Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest.",
                "The BERT model of devlin2018bert has been particularly influential, establishing state-of-the-art results for English for a range of NLU tasks and NER when it was released. For most languages, the only currently available BERT model is the multilingual model (M-BERT) trained on pooled data from 104 languages. While M-BERT has been shown to have a remarkable ability to generalize across languages BIBREF3, several studies have also demonstrated that monolingual BERT models, where available, can notably outperform M-BERT. Such results include the evaluation of the recently released French BERT model BIBREF4, the preliminary results accompanying the release of a German BERT model, and the evaluation of ronnqvist-etal-2019-multilingual comparing M-BERT with English and German monolingual models.",
                "In this paper, we study the application of language-specific and multilingual BERT models to Finnish NLP. We introduce a new Finnish BERT model trained from scratch and perform a comprehensive evaluation comparing its performance to M-BERT on established datasets for POS tagging, NER, and dependency parsing as well as a range of diagnostic text classification tasks. The results show that 1) on most tasks the multilingual model does not represent an advance over previous state of the art, indicating that multilingual models may fail to deliver on the promise of deep transfer learning for lower-resourced languages, and 2) the custom Finnish BERT model systematically outperforms the multilingual as well as all previously proposed methods on all benchmark tasks, showing that language-specific deep transfer learning models can provide comparable advances to those reported for much higher-resourced languages."
            ],
            "predicted_section": [
                "Evaluation ::: Text classification ::: Results",
                "Introduction"
            ]
        },
        "5a7d1ae6796e09299522ebda7bfcfad312d6d128": {
            "question_text": "What previous proposed methods did they explore?",
            "from_paper": "1912.07076",
            "gold": [
                "The current transfer learning methods have evolved from word embedding techniques, such as word2vec BIBREF5, GLoVe BIBREF6 and fastText BIBREF7, to take into account the textual context of words. Crucially, incorporating the context avoids the obvious limitations stemming from the one-vector-per-unique-word assumption inherent to the previous word embedding methods. The current successful wave of work proposing and applying different contextualized word embeddings was launched with ELMo BIBREF0, a context embedding method based on bidirectional LSTM networks. Another notable example is the ULMFit model BIBREF8, which specifically focuses on techniques for domain adaptation of LSTM-based language models. Following the introduction of the attention-based (as opposed to recurrent) Transformer architecture BIBREF9, BERT was proposed by BIBREF2, demonstrating superior performance on a broad array of tasks. The BERT model has been further refined in a number of follow-up studies BIBREF10, BIBREF11 and, presently, BERT and related models form the de facto standard approach to embedding text segments as well as individual words in context."
            ],
            "gold_section": [
                "Related Work"
            ],
            "predicted": [
                "Unless stated otherwise, all experiments follow the basic setup used in the experiments of devlin2018bert, selecting the learning rate, batch size and the number of epochs used for fine-tuning separately for each model and dataset combination using a grid search with evaluation on the development data. Other model and optimizer parameters were kept at the BERT defaults. Excepting for the parsing experiments, we repeat each experiment 5-10 times and report result mean and standard deviation.",
                "The FinBERT models and all of the tools and resources introduced in this paper are available under open licenses from https://turkunlp.org/finbert.",
                "Two primary sources were used to create pretraining data from unrestricted crawls. First, we compiled documents from the dedicated internet crawl of the Finnish internet of luotolahti2015towards run between 2014 and 2016 using the SpiderLing crawler BIBREF16. Second, we selected texts from the Common Crawl project by running a a map-reduce language detection job on the plain text material from Common Crawl. These sources were supplemented with plain text extracted from the Finnish Wikipedia using the mwlib library. Following initial compilation, this text collection was analyzed for using the Onion deduplication tool. Duplicate documents were removed, and remaining documents grouped by their level of duplication."
            ],
            "predicted_section": [
                "Evaluation",
                "Pretraining ::: Pretraining Data ::: Internet crawl",
                "Conclusions"
            ]
        },
        "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8": {
            "question_text": "What is the state-of-the-art approach?",
            "from_paper": "1911.11161",
            "gold": [
                "We first compare the performance of our approach with the baseline results obtained from Rashkin et al. BIBREF3 that uses a full transformer architecture BIBREF26, consisting of an encoder and decoder. Table TABREF9 provides a comparison of our approach with to the baseline approach. In Table TABREF9, we refer our \u201cOur Model Fine-Tuned\u201d as the baseline fine-tuned GPT-2 model trained on the dialogue and \u201cOur-model Emo-prepend\u201d as the GPT-2 model that is fine-tuned on the dialogues but also conditioned on the emotion displayed in the conversation. We find that fine-tuning the GPT-2 language model using a transfer learning approach helps us achieve a lower perplexity and a higher BLEU scores. The results from our approach are consistent with the empirical study conducted by Edunov et al BIBREF27 that demonstrate the effectiveness of the using pre-trained model diminishes when added to the decoder network in an seq2seq approach. We also perform a comparison between our two models on the metrics of length, diversity, readability and coherence. We find that our baseline model produces less diverse responses compared to when the model is conditioned on emotion. We find that the our emo-prepend model also higher a slightly higher readability score that our baseline model."
            ],
            "gold_section": [
                "Results ::: Automated Metrics"
            ],
            "predicted": [
                "Our work advances the field of conversational agents by applying the transfer learning approach towards generating emotionally relevant responses that is grounded on emotion and situational context. We find that our fine-tuning based approach outperforms the current state of the art approach on the automated metrics of the BLEU and perplexity. We also show that transfer learning approach helps produce well crafted responses on smaller dialogue corpus.",
                "The area of dialogue systems has been studied extensively in both open-domain BIBREF28 and goal-oriented BIBREF29 situations. Extant approaches towards building dialogue systems has been done predominantly through the seq2seq framework BIBREF0. However, prior research has shown that these systems are prone to producing dull and generic responses that causes engagement with the human to be affected BIBREF0, BIBREF2. Researchers have tackled this problem of dull and generic responses through different optimization function such as MMI BIBREF30 and through reinforcement learning approachesBIBREF31. Alternative approaches towards generating more engaging responses is by grounding them in personality of the speakers that enables in creating more personalized and consistent responses BIBREF1, BIBREF32, BIBREF13.",
                "To assess the quality of generations, we conducted a MTurk human evaluation. We recruited a total of 15 participants and each participant was asked to evaluate 25 randomly sampled outputs from the test set on three metrics:"
            ],
            "predicted_section": [
                "Results ::: Qualitative Evaluation",
                "Related Work",
                "Introduction"
            ]
        },
        "ca595151735444b5b30a003ee7f3a7eb36917208": {
            "question_text": "What type of features are extracted with this language?",
            "from_paper": "2002.03056",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Next, let us consider scenarios when features are specified as elements of a language. Let us refer to this language as NLP Feature Specification Language (nlpFSpL) such that a program in nlpFSpL would specify which features should be used by the underlying ML based solution to achieve goals of the target application. Given a corpus of unstructured natural language text data and a specifications in the nlpFSpL, an interpreter can be implemented as feature extraction system (FExSys) to automatically generate feature matrix which can be directly used by underlying ML technique.",
                "As the process of defining features is manual, prior experience and expertize of the designer affects which features to extract and how to extract these features from input text. Current practice lacks standardization and automation in feature definition process, provides partial automation in extraction process, and does not enable automated reuse of features across related application.",
                "In this paper, we have presented high level overview of a feature specification language for ML based TA applications and an approach to enable reuse of feature specifications across semantically related applications. Currently, there is no generic method or approach, which can be applied during TA applications' design process to define and extract features for any arbitrary application in an automated or semi-automated manner primarily because there is no standard way to specify wide range of features which can be extracted and used. We considered different classes of features including linguistic, semantic, and statistical for various levels of analysis including words, phrases, sentences, paragraphs, documents, and corpus. As a next step, we presented an approach for building a recommendation system for enabling automated reuse of features for new application scenarios which improves its underlying similarity model based upon user feedback."
            ],
            "predicted_section": [
                "Life Cycle View",
                "Conclusion"
            ]
        },
        "330fe3815f74037a9be93a4c16610c736a2a27b3": {
            "question_text": "How big are OSA and PD corporas used for testing?",
            "from_paper": "2003.00864",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Four corpora were used in our experiments: three to determine the presence or absence of PD and OSA, which include a European Portuguese PD corpus (PPD), a European Portuguese OSA corpus (POSA) and a Spanish PD corpus (SPD); one task-agnostic European Portuguese corpus to train the i-vector and x-vector extractors. For each of the disease-related datasets, we compared three distinct data representations: knowledge-based features, i-vectors and x-vectors. All disease classifications were performed with an SVM classifier. Further details on the corpora, data representations and classification method follow bellow.",
                "This section contains the results obtained for all three tasks: PD detection with the PPD corpus, OSA detection with the PSD corpus and PD detection with the SPD corpus. Results are reported in terms of average Precision, Recall and F1 Score. The values highlighted in Tables TABREF19, TABREF21 and TABREF23 represent the best results, both at the speaker and segment levels.",
                "Considering the limited size of the corpora, fewer than 3h each, we chose to use leave-one-speaker-out cross validation as an alternative to partitioning the corpora into train, development and test sets. This was done to add significance to our results."
            ],
            "predicted_section": [
                "Experimental Setup",
                "Results",
                "Experimental Setup ::: Model training and parameters"
            ]
        },
        "0dfe43985dea45d93ae2504cccca15ae1e207ccf": {
            "question_text": "What methods are used to build two other Viatnamese datsets?",
            "from_paper": "2002.00175",
            "gold": [
                "We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set."
            ],
            "gold_section": [
                "Experiments ::: Experiment Settings ::: Dataset preparation"
            ],
            "predicted": [
                "Secondly, we introduce our annotation tool for dataset construction, which is also published to help annotators conveniently create captions.",
                "In this section, we describes procedures of building our sportball Vietnamese dataset, called UIT-ViIC.",
                "The structure of the paper is organized as follows. Related documents and studies are presented in Section SECREF2. UIT-ViIC dataset creation is described in Section SECREF3. Section SECREF4 describes the methods we implement. The experimental results and analysis are presented in Section SECREF5. Conclusion and future work are deduced in Section SECREF6."
            ],
            "predicted_section": [
                "Dataset Creation ::: Annotation Process",
                "Introduction"
            ]
        },
        "b0e894536857cb249bd75188c3ca5a04e49ff0b6": {
            "question_text": "How do attention, recurrent and convolutional networks differ on the language classes they accept?",
            "from_paper": "1906.01615",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Attention is a popular enhancement to sequence-to-sequence (seq2seq) neural networks BIBREF9 , BIBREF10 , BIBREF11 . Attention allows a network to recall specific encoder states while trying to produce output. In the context of machine translation, this mechanism models the alignment between words in the source and target languages. More recent work has found that \u201cattention is all you need\u201d BIBREF12 , BIBREF13 . In other words, networks with only attention and no recurrent connections perform at the state of the art on many tasks.",
                "Now, we analyze the effect of adding attention to an acceptor network. Because we are concerned with language acceptance instead of transduction, we consider a simplified seq2seq attention model where the output sequence has length 1:",
                "This paper follows BIBREF1 by analyzing the expressiveness of neural network acceptors under asymptotic conditions. We formalize asymptotic language acceptance, as well as an associated notion of network memory. We use this theory to derive computation upper bounds and automata-theoretic characterizations for several different kinds of recurrent neural networks section:rnns, as well as other architectural variants like attention section:attention and convolutional networks (CNNs) section:cnns. This leads to a fairly complete automata-theoretic characterization of sequential neural networks."
            ],
            "predicted_section": [
                "Attention",
                "Introduction"
            ]
        },
        "94c22f72665dfac3e6e72e40f2ffbc8c99bf849c": {
            "question_text": "What type of languages do they test LSTMs on?",
            "from_paper": "1906.01615",
            "gold": [
                "BIBREF1 show how the LSTM can simulate a simplified variant of the counter machine. Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.",
                "Another important formal language task for assessing network memory is string reversal. Reversing requires remembering a INLINEFORM0 prefix of characters, which implies INLINEFORM1 state complexity.",
                "We frame reversing as a seq2seq transduction task, and compare the performance of an LSTM encoder-decoder architecture to the same architecture augmented with attention. We also report the results of BIBREF22 for a stack neural network (StackNN), another architecture with INLINEFORM0 state complexity (thm:stackstatecomplexity).",
                "Counting",
                "The goal of this experiment is to evaluate which architectures have memory beyond finite state. We train a language model on INLINEFORM0 with INLINEFORM1 and test it on longer strings INLINEFORM2 . Predicting the INLINEFORM3 character correctly while maintaining good overall accuracy requires INLINEFORM4 states. The results reported in fig:countingresults demonstrate that all recurrent models, with only two hidden units, find a solution to this task that generalizes at least over this range of string lengths.",
                "Counting with Noise",
                "In order to abstract away from asymptotically unstable representations, our next experiment investigates how adding noise to an RNN's activations impacts its ability to count. For the SRN and GRU, noise is added to INLINEFORM0 before computing INLINEFORM1 , and for the LSTM, noise is added to INLINEFORM2 . In either case, the noise is sampled from the distribution INLINEFORM3 .",
                "Reversing"
            ],
            "gold_section": [
                "Counting with Noise",
                "Reversing",
                "Counting"
            ],
            "predicted": [
                " BIBREF1 show how the LSTM can simulate a simplified variant of the counter machine. Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.",
                "thm:lstmupperbound constitutes a very tight upper bound on the expressiveness of LSTM computation. Asymptotically, LSTMs are not powerful enough to model even the deterministic context-free language INLINEFORM0 .",
                "An LSTM is a recurrent network with a complex gating mechanism that determines how information from one time step is passed to the next. Originally, this gating mechanism was designed to remedy the vanishing gradient problem in SRNs, or, equivalently, to make it easier for the network to remember long-term dependencies BIBREF5 . Due to strong empirical performance on many language tasks, LSTMs have become a canonical model for NLP."
            ],
            "predicted_section": [
                "Long Short-Term Memory Networks"
            ]
        },
        "ce8d8de78a21a3ba280b658ac898f73d0b52bf1b": {
            "question_text": "What is possible future improvement for proposed method/s?",
            "from_paper": "1910.10487",
            "gold": [
                "In future work, the memory module could be applied to other domains such as summary generation. While memory modules are able to capture neural vectors of information, they may not easily capture specific words for later use. A possible future approach might combine memory module architectures with pointer softmax networks BIBREF16 to allow memory models to store information about which words from previous utterances of the conversation to use in future responses."
            ],
            "gold_section": [
                "Conclusion"
            ],
            "predicted": [
                "vinyals2015neural train a sequence-to-sequence LSTM-based dialogue model on messages from an IT help-desk chat service, as well as the OpenSubtitles corpus, which contains subtitles from popular movies. This model was able to answer philosophical questions and performed well with common sense reasoning. Similarly, serban2016building train a hierarchical LSTM architecture (HRED) on the MovieTriples dataset, which contains examples of the form (utterance #1, utterance #2, utterance #3). However, this dataset is small and does not have conversations of larger length. They show that using a context recurrent neural network (RNN) to read representations at the utterance-level allows for a more top-down perspective on the dialogue history. Finally, serban2017hierarchical build a dialogue system which injects diversity into output responses (VHRED) through the use of a latent variable for variational inference BIBREF3. They argue that the injection of information from the latent variables during inference increases response coherence without degrading response quality. They train the full system on the Twitter Dialogue corpus, which contains generic multi-turn conversations from public Twitter accounts. They also train on the Ubuntu Dialogue Corpus, a collection of multi-turn vocabulary-rich conversations extracted from Ubuntu chat logs. du2018variational adapt from the VHRED architecture by increasing the influence of the latent variables on the output utterance. In this work, a backwards RNN carries information from future timesteps to present ones, such that a backward state contains a summary of all future utterances the model is required to generate. The authors constrain this backward state at each time step to be a latent variable, and minimize the KL loss to restrict information flow. At inference, all backward state latent variables are sampled from and decoded to the output response. The authors interpret the sampling of the latent variables as a \"plan\" of what to generate next.",
                "Other NTM variants have also been proposed recently. DBLP:journals/corr/ZhangYZ15 propose structured memory architectures for NTMs, and argue they could alleviate overfitting and increase predictive accuracy. DBLP:journals/nature/GravesWRHDGCGRA16 propose a memory access mechanism on top of NTM, which they call the Differentiable Neural Computer (DNC). DNC can store the transitions between memory locations it accesses, and thus can model some structured data. DBLP:journals/corr/GulcehreCCB16 proposed a Dynamic Neural Turing Machine (D-NTM) model, which allows more addressing mechanisms, such as multi-step addressing. DBLP:journals/corr/GulcehreCB17 further simplified the algorithm, so a single trainable matrix is used to get locations for read and write. Both models separate the address section from the content section of memory.",
                "In recent years, there have been proposals to use memory neural networks to capture long-term information. A memory module is defined as an external component of the neural network system, and it is theoretically unlimited in capacity. weston2014memory propose a sequence prediction method using a memory with content-based addressing. In their implementation for the bAbI task BIBREF9 for example, their model encodes and sequentially saves words from text in memory slots. When a question about the text is asked, the model uses content-based addressing to retrieve memories relevant to the question, in order to generate answers. They use the k-best memory slots, where k is a relative small number (1 or 2 in their paper). sukhbaatar2015end propose an end-to-end neural network model, which uses content-based addressing to access multiple memory layers. This model has been implemented in a relatively simple goal-oriented dialogue system (restaurant booking) and has decent performance BIBREF10."
            ],
            "predicted_section": [
                "Recent Work"
            ]
        },
        "2b32cf05c5e736f764ceecc08477e20ab9f2f5d7": {
            "question_text": "Does the paper report F1-scores with and without post-processing for the second task?",
            "from_paper": "1908.06493",
            "gold": [
                "Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.",
                "As described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.",
                "For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling.",
                "Table TABREF28 shows the comparison of the different examined approaches in subtask B in the preliminary phase. Both implementations, Hsklearn and our own produced very similar results, so for the sake of reproducibility, we chose to continue with Hsklearn. We can see here, in contrary to the subtask A, that -0.25 achieved for one configuration better results, indicating that -0.2 could be overfitted on subtask A and a value diverging from that could also perform better. The extended approach means that an extra feature extraction module was added (having 3 instead of only 2) with n-gram 1-2 and stopwords removal. The LCA approach yielded here a worse score in the normalized but almost comparable in the non-normalized. However, the simple threshold approach performed better and therefore more promising."
            ],
            "gold_section": [
                "Data and Methodology ::: System Definition ::: Post-processing: Threshold",
                "Experiments ::: Preliminary Experiments on Development Set"
            ],
            "predicted": [
                "In Fig. FIGREF26, a graph showing the dependency between the threshold set and the micro F-1 score achieved in the development set is depicted. The curve fitted was $a*x^2+b*x+c$ which has the maximum at approx. -0.2. We chose -0.25 in the expectation that the test set would not be exactly as the development set and based on our previous experience with other multi-label datasets (such as the RCv1-v2) which have an optimal threshold at -0.3. Also as we will see, the results proved us right achieving the best recall, yet not surpassing the precision score. This is a crucial aspect of the F-1 measure, as it is the harmonic mean it will push stronger and not linearly the result towards the lower end, so if decreasing the threshold, increases the recall linearly and decreases also the precision linearly, balancing both will consequently yield a better F-1 score.",
                "Using the ensemble feature model produced the best results without post-processing. The simple use of a low threshold yielded also astonishingly good results. This indicates that the SVM's score production was very good, yet the threshold 0 was too cautious.",
                "In Table TABREF30, the best results by team regarding micro F-1 are shown. Our approach reached second place. The difference between the first four places were mostly 0.005 between each, showing that only a minimal change could lead to a place switching. Also depicted are not null improvements results, i.e. in a following post-processing, starting from the predictions, the highest score label is predicted for each sample, even though the score was too low. It is worth-noting that the all but our approaches had much higher precision compared to the achieved recall."
            ],
            "predicted_section": [
                "Experiments ::: Subtask A",
                "Experiments ::: Preliminary Experiments on Development Set"
            ]
        },
        "14fdc8087f2a62baea9d50c4aa3a3f8310b38d17": {
            "question_text": "What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?",
            "from_paper": "1909.12208",
            "gold": [
                "Experiments were performed using the CHiME-5 data. Distant microphone recordings (U data) during training and/or testing were processed using the speech enhancement methods depicted in Table TABREF6. Speech was either left unprocessed, enhanced using a weighted delay-and-sum beamformer (BFIt) BIBREF21 with or without dereverberation (WPE), or processed using the guided source separation (GSS) approach described in Section SECREF3. In Table TABREF6, the strength of the enhancement increases from top to bottom, i.e., GSS6 signals are much cleaner than the unprocessed ones.",
                "An extensive set of experiments was performed to measure the WER impact of enhancement on the CHiME-5 training and test data. We test enhancement methods of varying strengths, as described in Section SECREF5, and the results are depicted in Table TABREF12. In all cases, the (unprocessed) worn dataset was also included for AM training since it was found to improve performance (supporting therefore the argument that data variability helps ASR robustness).",
                "In Table TABREF12, in each row the recognition accuracy improves monotonically from left to right, i.e., as the enhancement strategy on the test data becomes stronger. Reading the table in each column from top to bottom, one observes that accuracy improves with increasing power of the enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data. Compared with unprocessed training and test data (None-None), GSS6-GSS6 yields roughly 35% (24%) relative WER reduction on the DEV (EVAL) set, and 12% (11%) relative WER reduction when compared with the None-GSS6 scenario. Comparing the amount of training data used to train the acoustic models, we observe that it decreases drastically from no enhancement to the GSS6 enhancement.",
                "In this paper we performed an extensive experimental evaluation on the acoustically very challenging CHiME-5 dinner party data showing that: (i) cleaning up training data can lead to substantial word error rate reduction, and (ii) enhancement in training is advisable as long as enhancement in test is at least as strong as in training. This approach stands in contrast and delivers larger accuracy gains at a fraction of training data than the common data simulation strategy found in the literature. Using a CNN-TDNNF acoustic model topology along with GSS enhancement refined with time annotations from ASR, discriminative training and RNN LM rescoring, we achieved a new single-system state-of-the-art result on CHiME-5, which is 41.6% (43.2%) on the development (evaluation) set, which is a 8% relative improvement of the word error rate over a comparable system reported so far."
            ],
            "gold_section": [
                "Experiments ::: General configuration",
                "Conclusions",
                "Experiments ::: Enhancement effectiveness for ASR training and test"
            ],
            "predicted": [
                "An extensive set of experiments was performed to measure the WER impact of enhancement on the CHiME-5 training and test data. We test enhancement methods of varying strengths, as described in Section SECREF5, and the results are depicted in Table TABREF12. In all cases, the (unprocessed) worn dataset was also included for AM training since it was found to improve performance (supporting therefore the argument that data variability helps ASR robustness).",
                "Based on the distributions in Fig. FIGREF19, the test data was split. Two cases were considered: (a) same enhancement for training and test data (matched case, Table TABREF20), and (b) unprocessed training data and enhanced test data (mismatched case, Table TABREF21). As expected, the WER increases monotonically as the amount of overlap increases in both scenarios, and the recognition accuracy improves as the enhancement method becomes stronger.",
                "However, there has been a long debate whether it is advisable to apply speech enhancement on data used for ASR training, because it is generally agreed upon that the recognizer should be exposed to as much acoustic variability as possible during training, as long as this variability matches the test scenario BIBREF1, BIBREF2, BIBREF3. Multi-channel speech enhancement, such as acoustic BF or source separation, would not only reduce the acoustic variability, it would also result in a reduction of the amount of training data by a factor of $M$, where $M$ is the number of microphones BIBREF4. Previous studies have shown the benefit of training an ASR on matching enhanced speech BIBREF5, BIBREF6 or on jointly training the enhancement and the acoustic model BIBREF7. Alternatively, the training data is often artificially increased by adding even more degraded speech to it. For instance, Ko et al. BIBREF8 found that adding simulated reverberated speech improves accuracy significantly on several large vocabulary tasks. Similarly, Manohar et al. BIBREF9 improved the WER of the baseline CHiME-5 system by relative 5.5% by augmenting the training data with approx. 160hrs of simulated reverberated speech. However, not only can the generation of new training data be costly and time consuming, the training process itself is also prolonged if the amount of data is increased."
            ],
            "predicted_section": [
                "Experiments ::: Enhancement effectiveness for ASR training and test",
                "Introduction",
                "Discussion ::: Analysis of speaker overlap effect on WER accuracy"
            ]
        },
        "fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b": {
            "question_text": "What is the difference in size compare to the previous model?",
            "from_paper": "1805.09821",
            "gold": [
                "Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);",
                "Most works in the literature use only 1 000 examples to train the document classifier. To invest the impact of more training data, we also provide training corpora of 2 000, 5 000 and 10 000 documents. The development corpus for each language is composed of 1 000 and the test set of 4 000 documents respectively. All have uniform class distributions. An important aspect of this work is to provide a framework to study and evaluate cross-lingual document classification for many language pairs. In that spirit, we will name this corpus \u201cMultilingual Document Classification Corpus\u201d, abbreviated as MLDoc. The full Reuters Corpus Volume 2 has a special license and we can not distribute it ourselves. Instead, we provide tools to extract all the subsets of MLDoc at https://github.com/facebookresearch/MLDoc.",
                "We have defined a new evaluation framework for cross-lingual document classification in eight languages. This corpus largely extends previous corpora which were also based on the Reuters Corpus Volume 2, but mainly considered the transfer between English and German. We also provide detailed baseline results using two competitive approaches (multilingual word and sentence embeddings, respectively), for cross-lingual document classification between all eight languages. This new evaluation framework is freely available at https://github.com/facebookresearch/MLDoc."
            ],
            "gold_section": [
                "Conclusion",
                "Multilingual document classification"
            ],
            "predicted": [
                "In this section, we provide comparative results on our new Multilingual Document Classification Corpus. Since the initial work by BIBREF0 many alternative approaches to cross-lingual document classification have been developed. We will encourage the respective authors to evaluate their systems on MLDoc. We believe that a large variety of transfer language pairs will give valuable insights on the performance of the various approaches.",
                "A subset of the English and German sections of RCV2 was defined by BIBREF0 to evaluate cross-lingual document classification. This subset was used in several follow-up works and many comparative results are available for the transfer between German and English. BIBREF1 extended the use of RCV2 for cross-lingual document classification to the French and Spanish language (transfer from and to English). An analysis of these evaluation corpora has shown that the class prior distributions vary significantly between the classes (see Table 2 ). For German and English, more than 80% of the examples in the test set belong to the classes GCAT and MCAT and at most 2% to the class CCAT. These class prior distributions are very different for French and Spanish: the class CCAT is quite frequent with 21% and 15% of the French and Spanish test set respectively. One may of course argue that variability in the class prior distribution is typical for real-world problems, but this shifts the focus from a high quality cross-lingual transfer to \u201ctricks\u201d for how to best handle the class imbalance. Indeed, in previous research the transfer between English and German achieves accuracies higher than 90%, while the performance is below 80% for EN/FR or even 70% EN/ES. We have seen experimental evidence that these important differences are likely to be caused by the discrepancy in the class priors of the test sets.",
                "Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);"
            ],
            "predicted_section": [
                "Baseline results",
                "Cross-lingual document classification",
                "Multilingual document classification"
            ]
        },
        "c035a011b737b0a10deeafc3abe6a282b389d48b": {
            "question_text": "What are the components of the classifier?",
            "from_paper": "1707.07212",
            "gold": [
                "We model the conditional distribution over a tweet's veridicality toward a candidate INLINEFORM0 winning a contest against a set of opponents, INLINEFORM1 , using a log-linear model: INLINEFORM2",
                "where INLINEFORM0 is the veridicality (positive, negative or neutral).",
                "To extract features INLINEFORM0 , we first preprocessed tweets retrieved for a specific event to identify named entities, using BIBREF20 's Twitter NER system. Candidate ( INLINEFORM1 ) and opponent entities were identified in the tweet as follows:",
                "We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."
            ],
            "gold_section": [
                "Veridicality Classifier",
                "Features"
            ],
            "predicted": [
                "We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword.",
                "- target ( INLINEFORM0 ). A target is a named entity that matches a contender name from our queries.",
                "Distance to keyword. We also compute the distance of target and opponent entities to the keyword."
            ],
            "predicted_section": [
                "Features",
                "Veridicality Classifier"
            ]
        },
        "4367617c0b8c9f33051016e8d4fbb44831c54d0f": {
            "question_text": "What is the state-of-the-art model?",
            "from_paper": "1902.10246",
            "gold": [
                "Most supervised approaches focus on extracting features from words in the context. Early approaches mostly depend on hand-crafted features. For example, IMS by BIBREF2 uses POS tags, surrounding words and collections of local words as features. These approaches are later improved by combining with word embedding features BIBREF0 , which better represents the words' semantic information in a real-value space. However, these methods neglect the valuable positional information between the words in the sequence BIBREF3 . The bi-directional Long-Short-Term-Memory (LSTM) approach by BIBREF3 provides one way to leverage the order of words. Recently, BIBREF4 improved the performance by pre-training a LSTM language model with a large unlabelled corpus, and using this model to generate sense vectors for further WSD predictions. However, LSTM significantly increases the computational complexity during the training process.",
                "Table TABREF6 presents the micro F1 scores from different models. Note that we use a corpus with 0.8 billion words and vocabulary of 100,000 words when training the language model, comparing with BIBREF4 using 100 billion words and vocabulary of 1,000,000 words. The context abstraction using the language model is the most crucial step. The sizes of the training corpus and vocabulary significantly affect the performance of this process, and consequently the final WSD results. However, BIBREF4 did not publish the 100 billion words corpus used for training their LSTM language model."
            ],
            "gold_section": [
                "Results",
                "Introduction"
            ],
            "predicted": [
                "To evaluate the performance of our proposed model, we implemented our model using Tensorflow BIBREF11 and conducted experiments on standard SemEval data that are labelled by senses from WordNet 3.0 BIBREF12 . We built the classifier using SemCor BIBREF13 as training corpus, and evaluated on Senseval2 BIBREF14 , and SemEval-2013 Task 12 BIBREF15 .",
                "Recently, BIBREF9 reimplemented the LSTM-based WSD classifier. The authors trained the language model with a smaller corpus Gigaword BIBREF16 of 2 billion words and vocabulary of 1 million words, and reported the performance. Their published code also enabled us to train an LSTM model with the same data used in training our FOFE model, and compare the performances at the equivalent conditions.",
                "The fact that human languages consist of variable-length sequence of words requires NLP models to be able to consume variable-length data. RNN/LSTM addresses this issue by recurrent connections, but such recurrence consequently increases the computational complexity. On the contrary, feed forward neural network (FFNN) has been widely adopted in many artificial intelligence problems due to its powerful modelling ability and fast computation, but is also limited by its requirement of fixed-size input. FOFE aims at encoding variable-length sequence of words into a fixed-size representation, which subsequently can be fed into an FFNN."
            ],
            "predicted_section": [
                "Experiment",
                "Results",
                "Fixed-size Ordinally Forgetting Encoding"
            ]
        },
        "aa287673534fc05d8126c8e3486ca28821827034": {
            "question_text": "What language are the tweets in?",
            "from_paper": "1607.00167",
            "gold": [
                "Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. \u201cCristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.\u201cRonaldo\", \u201cCR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords.",
                "A set of portuguese and english stopwords are removed - these contain very common and not meaningful words such as \u201cthe\" or \u201ca\";",
                "A word-level sentiment analysis was made, using Sentilex-PT BIBREF7 - a sentiment lexicon for the portuguese language, which can be used to determine the sentiment polarity of each word, i.e. a value of -1 for negative words, 0 for neutral words and 1 for positive words. A visualization system was created that displays the most mentioned words for each entity/day and their respective polarity using correspondingly colored and sized circles, which are called SentiBubbles."
            ],
            "gold_section": [
                "Tweets Pre-processing",
                "Sentiment Analysis",
                "Tweets Collection"
            ],
            "predicted": [
                "Before actually analyzing the text in the tweets, we apply the following operations:",
                "If any tweet has less than 40 characters it is discarded. These tweets are considered too small to have any meaningful content;",
                "Figure 1 depicts an overview of the data mining process pipeline applied in this work. To collect and process raw Twitter data, we use an online reputation monitoring platform BIBREF5 which can be used by researchers interested in tracking entities on the web. It collects tweets from a pre-defined sample of users and applies named entity disambiguation BIBREF6 . In this particular scenario, we use tweets from January 2014 to December 2015. In order to extract tweets related to an entity, two main characteristics must be defined: its canonical name, that should clearly identify it (e.g. \u201cCristiano Ronaldo\") and a set of keywords that most likely refer to that particular entity when mentioned in a sentence (e.g.\u201cRonaldo\", \u201cCR7\"). Entity related data is provided from a knowledge base of Portuguese entities. These can then be used to retrieve tweets from that entity, by selecting the ones that contain one or more of these keywords."
            ],
            "predicted_section": [
                "Tweets Pre-processing",
                "Tweets Collection"
            ]
        },
        "415014a5bcd83df52c9307ad16fab1f03d80f705": {
            "question_text": "What syntactic and semantic features are proposed?",
            "from_paper": "1605.05156",
            "gold": [
                "Semantic Features",
                "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.",
                "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.",
                "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.",
                "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.",
                "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.",
                "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.",
                "Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.",
                "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.",
                "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "gold_section": [
                "Semantic Features",
                "Syntactic Features"
            ],
            "predicted": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Next, we wanted to measure the contributions of our semantic and syntactic features. To do so, we trained two versions of our Twitter-wide logistic regression classifier, one using only semantic features and the other using syntactic features. As shown in Table TABREF11 , the semantic and syntactic classifiers' performance was fairly similar, both being on average significantly worse than the combined classifier. The combined classifier outperformed the semantic and syntactic classifiers on all other categories, which strongly suggests that both feature categories contribute to the classification of speech acts.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "predicted_section": [
                "Supervised Speech Act Classifier",
                "Features",
                "Syntactic Features"
            ]
        },
        "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c": {
            "question_text": "what are the proposed semantic features?",
            "from_paper": "1605.05156",
            "gold": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.",
                "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.",
                "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.",
                "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.",
                "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question."
            ],
            "gold_section": [
                "Semantic Features",
                "Features"
            ],
            "predicted": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Next, we wanted to measure the contributions of our semantic and syntactic features. To do so, we trained two versions of our Twitter-wide logistic regression classifier, one using only semantic features and the other using syntactic features. As shown in Table TABREF11 , the semantic and syntactic classifiers' performance was fairly similar, both being on average significantly worse than the combined classifier. The combined classifier outperformed the semantic and syntactic classifiers on all other categories, which strongly suggests that both feature categories contribute to the classification of speech acts.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "predicted_section": [
                "Supervised Speech Act Classifier",
                "Features",
                "Syntactic Features"
            ]
        },
        "95bbd91badbfe979899cca6655afc945ea8a6926": {
            "question_text": "what syntactic features are proposed?",
            "from_paper": "1605.05156",
            "gold": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Syntactic Features",
                "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.",
                "Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.",
                "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.",
                "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "gold_section": [
                "Features",
                "Syntactic Features"
            ],
            "predicted": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.",
                "Next, we wanted to measure the contributions of our semantic and syntactic features. To do so, we trained two versions of our Twitter-wide logistic regression classifier, one using only semantic features and the other using syntactic features. As shown in Table TABREF11 , the semantic and syntactic classifiers' performance was fairly similar, both being on average significantly worse than the combined classifier. The combined classifier outperformed the semantic and syntactic classifiers on all other categories, which strongly suggests that both feature categories contribute to the classification of speech acts."
            ],
            "predicted_section": [
                "Supervised Speech Act Classifier",
                "Features",
                "Syntactic Features"
            ]
        },
        "e14e3e0944ec3290d1985e9a3da82a7df17575cd": {
            "question_text": "Which dataset do they evaluate on?",
            "from_paper": "1806.07042",
            "gold": [
                "Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results."
            ],
            "gold_section": [
                "Introduction"
            ],
            "predicted": [
                "In terms of ensemble models and our editing model, the validation set and the test set are the same with datasets prepared for retrieval and generation models. Besides, for each context in the validation and test sets, we select its prototypes with the method described in Section \u201cPrototype Selector\". We follow Song et al. song2016two to construct a training data set for ensemble models, and construct a training data set with the method described in Section \u201cPrototype Selector\" for our editing models. We can obtain 42,690,275 INLINEFORM0 quadruples with the proposed data preparing method. For a fair comparison, we randomly sample 19,623,374 instances for the training of our method and the ensemble method respectively. To facilitate further research, related resources of the paper can be found at https://github.com/MarkWuNLP/ResponseEdit.",
                "We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU. Following BIBREF10 , we evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct-1 and Distinct-2. In this paper, we define a new metric, originality, that is defined as the ratio of generated responses that do not appear in the training set. Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines. We ask the same three native speakers to do a side-by-side comparison BIBREF15 on the 1,000 contexts. Given a context and two responses generated by different models, we ask annotators to decide which response is better (Ties are permitted).",
                "Regarding human side-by-side evaluation, we can find that Edit-Default and Edit-N-rerank are slightly better than Retrieval-default and Retrieval-rerank (The winning examples are more than the losing examples), indicating that the post-editing is able to improve the response quality. Ed-Default is worse than Ens-Default, but Ed-N-Rerank is better than Ens-Rerank. This is mainly because the editing model regards the prototype response as the source language, so it is highly depends on the quality of prototype response."
            ],
            "predicted_section": [
                "Experiment setting",
                "Evaluation Results",
                "Evaluation Metrics"
            ]
        },
        "a0197894ee94b01766fa2051f50f84e16b5c9370": {
            "question_text": "Do they reason why greedy decoding works better then beam search?",
            "from_paper": "1606.07947",
            "gold": [
                "We run experiments to compress a large state-of-the-art INLINEFORM0 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a INLINEFORM1 LSTM that roughly matches the performance of the full system. We see similar results compressing a INLINEFORM2 model down to INLINEFORM3 on a smaller data set. Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time. As a result we are able to perform greedy decoding on the INLINEFORM4 model 10 times faster than beam search on the INLINEFORM5 model with comparable performance. Our student models can even be run efficiently on a standard smartphone. Finally, we apply weight pruning on top of the student network to obtain a model that has INLINEFORM6 fewer parameters than the original teacher model. We have released all the code for the models described in this paper.",
                "Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but fine-tuned towards Seq-Inter data (Baseline INLINEFORM0 Seq-Inter). In fact, greedy decoding with this fine-tuned model has similar performance ( INLINEFORM1 ) as beam search with the original model ( INLINEFORM2 ), allowing for faster decoding even with an identically-sized model."
            ],
            "gold_section": [
                "Results and Discussion",
                "Introduction"
            ],
            "predicted": [
                "Run-time complexity for beam search grows linearly with beam size. Therefore, the fact that sequence-level knowledge distillation allows for greedy decoding is significant, with practical implications for running NMT systems across various devices. To test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the average number of source words translated per second (Table 2). We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone. We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU ( INLINEFORM0 vs INLINEFORM1 words/sec), with similar performance.",
                "Finally, although past work has shown that models with lower perplexity generally tend to have higher BLEU, our results indicate that this is not necessarily the case. The perplexity of the baseline INLINEFORM0 English INLINEFORM1 German model is INLINEFORM2 while the perplexity of the corresponding Seq-KD model is INLINEFORM3 , despite the fact that Seq-KD model does significantly better for both greedy ( INLINEFORM4 BLEU) and beam search ( INLINEFORM5 BLEU) decoding.",
                "We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher's mode) instead of `wasting' parameters on trying to model the entire space of translations. Our results suggest that this is indeed the case: the probability mass that Seq-KD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: INLINEFORM0 ). For example, on English INLINEFORM1 German the (approximate) INLINEFORM2 for the INLINEFORM3 Seq-KD model (on average) accounts for INLINEFORM4 of the total probability mass, while the corresponding number is INLINEFORM5 for the baseline. This also explains the success of greedy decoding for Seq-KD models\u2014since we are only modeling around the teacher's mode, the student's distribution is more peaked and therefore the INLINEFORM6 is much easier to find. Seq-Inter offers a compromise between the two, with the greedily-decoded sequence accounting for INLINEFORM7 of the distribution."
            ],
            "predicted_section": [
                "Decoding Speed",
                "Results and Discussion"
            ]
        },
        "cbb4eba59434d596749408be5b923efda7560890": {
            "question_text": "What baselines is the neural relation extractor compared to?",
            "from_paper": "1603.00957",
            "gold": [
                "Table 1 summarizes the results on the test data along with the results from the literature. We can see that joint EL and RE performs better than the default pipelined approach, and outperforms most semantic parsing based models, except BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing. In addition, inference on unstructured data helps the default model. The joint EL and RE combined with inference on unstructured data further improves the default pipelined model by 9.2% (from 44.1% to 53.3%), and achieves a new state-of-the-art result beating the previous reported best result of yih-EtAl:2015:ACL-IJCNLP (with one-tailed t-test significance of $p < 0.05$ ).",
                "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction."
            ],
            "gold_section": [
                "Relation Extraction",
                "Results and Discussion"
            ],
            "predicted": [
                "Our work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs BIBREF46 , BIBREF47 , BIBREF48 , we work with sentence level relation extraction for question answering. krishnamurthy2012weakly and fader2014open adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is yao-jacana-freebase-acl2014 and yao-scratch-qa-naacl2015 who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models.",
                "This method involves inference on Freebase only. First the entity linking (EL) system is run to predict the topic entity. Then we run the relation extraction (RE) system and select the best relation that can occur with the topic entity. We choose this entity-relation pair to predict the answer.",
                "We first evaluate the EL component using the gold entity annotations on the development set. As shown in Table 2 , for 79.8% questions, our entity linker can correctly find the gold standard topic entities. The joint inference improves this result to 83.2%, a 3.4% improvement. Next we use the surrogate gold relations to evaluate the performance of the RE component on the development set. As shown in Table 2 , the relation prediction accuracy increases by 9.4% (from 45.9% to 55.3%) when using the joint inference."
            ],
            "predicted_section": [
                "Results and Discussion",
                "Related Work"
            ]
        },
        "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b": {
            "question_text": "What is the previous state-of-the-art?",
            "from_paper": "1603.00957",
            "gold": [
                "The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .",
                "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.",
                "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction."
            ],
            "gold_section": [
                "Relation Extraction",
                "Introduction"
            ],
            "predicted": [
                "We would like to thank Weiwei Sun, Liwei Chen, and the anonymous reviewers for their helpful feedback. This work is supported by National High Technology R&D Program of China (Grant No. 2015AA015403, 2014AA015102), Natural Science Foundation of China (Grant No. 61202233, 61272344, 61370055) and the joint project with IBM Research. For any correspondence, please contact Yansong Feng.",
                "Over time, the QA task has evolved into two main streams \u2013 QA on unstructured data, and QA on structured data. TREC QA evaluations BIBREF26 were a major boost to unstructured QA leading to richer datasets and sophisticated methods BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . While initial progress on structured QA started with small toy domains like GeoQuery BIBREF34 , recent focus has shifted to large scale structured KBs like Freebase, DBPedia BIBREF35 , BIBREF36 , BIBREF3 , BIBREF4 , BIBREF37 , and on noisy KBs BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 . An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly BIBREF43 , BIBREF44 , BIBREF45 . QALD tasks and linked data initiatives are contributing to this trend.",
                "Our model combines the best of both worlds by inferring over structured and unstructured data. Though earlier methods exploited unstructured data for KB-QA BIBREF40 , BIBREF3 , BIBREF7 , BIBREF6 , BIBREF16 , these methods do not rely on unstructured data at test time. Our work is closely related to joshi:2014 who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to sun2015open who does question answering on unstructured data but enrich it with Freebase, a reversal of our pipeline. Other line of very recent related work include Yahya:2016:RQE:2835776.2835795 and savenkovknowledge."
            ],
            "predicted_section": [
                "Acknowledgments",
                "Related Work"
            ]
        },
        "23cbf6ab365c1eb760b565d8ba51fb3f06257d62": {
            "question_text": "What are the baseline models?",
            "from_paper": "1910.02677",
            "gold": [
                "Table TABREF24 compares our best model to state-of-the-art methods:",
                "In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia BIBREF11, BIBREF12.",
                "Phrase-Based MT system with candidate reranking. Dissimilar candidates are favored based on their Levenshtein distance to the source.",
                "BIBREF33",
                "Deep semantics sentence representation fed to a monolingual MT system.",
                "Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by choosing relevant parameters; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-the-box Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI BIBREF9 on the WikiLarge benchmark BIBREF10, a +1.42 gain over previous scores, without requiring any external resource or modified training objective.",
                "Syntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI.",
                "Seq2Seq trained with reinforcement learning, combined with a lexical simplification model.",
                "Lately, SS has mostly been tackled using Seq2Seq MT models BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.",
                "Seq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks.",
                "Standard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI.",
                "BIBREF35",
                "Seq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI.",
                "Seq2Seq integrating the simple PPDB simplification database BIBREF36 as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words."
            ],
            "gold_section": [
                "Related Work ::: Sentence Simplification",
                "Introduction",
                "Experiments ::: Overall Performance"
            ],
            "predicted": [
                "ACCESS scores best on SARI (41.87), a significant improvement over previous state of the art (40.45), and third to best FKGL (7.22). The second and third models in terms of SARI, DMASS+DCSS (40.45) and SBMT+PPDB+SARI (39.96), both use the external resource Simple PPDB BIBREF36 that was extracted from 1000 times more data than what we used for training. Our FKGL is also better (lower) than these methods. The Hybrid model scores best on FKGL (4.56) i.e. they generated the simplest (and shortest) sentences, but it was done at the expense of SARI (31.40).",
                "We select the model with the best SARI on the validation set and report its scores on the test set. This model only uses three parameters out of four: NbChars$_{0.95}$, LevSim$_{0.75}$ and WordRank$_{0.75}$ (optimal target ratios are in subscript).",
                "Table TABREF24 compares our best model to state-of-the-art methods:"
            ],
            "predicted_section": [
                "Experiments ::: Overall Performance"
            ]
        },
        "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e": {
            "question_text": "What datasets are used?",
            "from_paper": "2002.04374",
            "gold": [
                "The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.",
                "Speech recordings of 88 PD patients and 88 HC speakers from Germany are considered BIBREF17. The participants performed four speech task: the rapid repetition of /pa-ta-ka/, 5 sentences, one text with 81 words, and a monologue.",
                "A total of 100 native Czech speakers (50 PD, 50 HC) were considered BIBREF18. The speech tasks performed by the participants include the rapid repetition of the syllables /pa-ta-ka/, a read text with 80 words, and a monologue."
            ],
            "gold_section": [
                "Materials and methods ::: Data ::: Spanish",
                "Materials and methods ::: Data ::: German",
                "Materials and methods ::: Data ::: Czech"
            ],
            "predicted": [
                "Speech recordings of patients in three different languages are considered: Spanish, German, and Czech. All of the recordings were captured in noise controlled conditions. The speech signals were down-sampled to 16 kHz. The patients in the three datasets were evaluated by a neurologist expert according to the third section of the movement disorder society, unified Parkinson's disease rating scale (MDS-UPDRS-III) BIBREF16. Table TABREF5 summarizes the information about the patients and healthy speakers.",
                "Table TABREF13 shows the results obtained for the baseline and the CNNs trained for each language individually. Similar accuracies are obtained between the baseline and the CNN model for Spanish language, which also exhibit the highest accuracy among the three languages. Note that the highest accuracy for German language was obtained with the baseline model. Conversely, for Czech language the CNN produces the highest accuracy. Note also that for the three languages, the results are unbalanced towards one of the two classes according to the specificity and sensitivity values. The difference in the results obtained among the three languages can be explained considering the information provided in Table TABREF5. For the patients in the Spanish language, the average MDS-UPDRS-III score is higher compared with the German and Czech patients, i.e, there are patients with higher disease severity in the Spanish data compared to German and Czech patients.",
                "Time frequency representations based on the short-time Fourier transform (STFT) are used as input to a CNN, which extract the most suitable features to discriminate between PD patients and HC subjects. The STFT with 256 frequency bins is computed for each segmented transition, for a window length of 16 ms and a step-size of 4 ms, forming 41 time frames per transition. The obtained spectrogram is transformed into the Mel-scale using 80 filters, forming an spectrogram with a size of 80$\\times $41, which is used to train the CNNs. The architecture of the implemented CNN is summarized in Table TABREF9. It consists of four convolutional and max-pooling layers, dropout to regularize the weights, and two fully connected layers followed by the output layer to make the final decision using a softmax activation function. The number of feature maps on each convolutional layer is twice the previous one in order to get more detailed representations of the input space in the deeper layers. The CNN is trained using the the cross-entropy as the loss function, using an Adam optimizer BIBREF19."
            ],
            "predicted_section": [
                "Experiments and results ::: Baseline and individual CNN models",
                "Materials and methods ::: Data",
                "Materials and methods ::: CNN model"
            ]
        },
        "ed15a593d64a5ba58f63c021ae9fd8f50051a667": {
            "question_text": "Is this model trained in unsuperized manner?",
            "from_paper": "2001.05540",
            "gold": [
                "The shifted alphabetic sequence task should be trivial to solve for a powerful sequence to sequence model implemented with Transformers. The next translation task we teach the model is Caesar's cipher. This is an old encryption method, in which each letter in the source sequence is replaced by a letter some fixed number of positions down the alphabet. The sequences do not need to be in alphabetic order, meaning the diversity of input sequences will be much larger than with the previous task. We again sample a $\\text{min}_n <= n < \\text{max}_n$, where $\\text{min}_n = 3$ and $\\text{max}_n = 25$ this time. We shift each letter in the source sequence by $\\text{max}_n = 25$. If the sampled $n$ is 5, we randomly sample 5 letters from the alphabet and shift each letter in the target to the left by one character we get the following example:",
                "Source $ h\\ k\\ b\\ e\\ t $",
                "Target $ g\\ j\\ a\\ d\\ s $",
                "The first task we train the insertion-deletion model on is shifting alphabetic sequences. For generation of data we sample a sequence length $\\text{min}_n <= n < \\text{max}_n$ from a uniform distribution where $\\text{min}_n = 3$ and $\\text{max}_n = 10$. We then uniformly sample the starting token and finish the alphabetic sequence until it has length $n$. For a sampled $n = 5$ and starting letter $\\text{c}$, shifting each letter by $\\text{max}_n$ to ensure the source and target have no overlapping sequence, here is one example sequence:",
                "Source $ c\\ d\\ e\\ f\\ g $",
                "Target $ m\\ n\\ o\\ p\\ q $"
            ],
            "gold_section": [
                "Experiments ::: Learning shifted alphabetic sequences",
                "Experiments ::: Learning Caesar's Cipher"
            ],
            "predicted": [
                "We generate 1000 of examples for training, and evaluate on 100 held-out examples. Table TABREF10 reports our BLEU. We train our models for 200k steps, batch size of 32 and perform no model selection. We see our Insertion-Deletion Transformer model outperforms the Insertion Transformer significantly on this task. One randomly chosen example of the interaction between the insertion and the deletion model during a decoding step is shown in Table TABREF9.",
                "We generate 100k examples to train on, and evaluate on 1000 held-out examples. We train our models for 200k steps, batch size of 32 and perform no model selection. The table below shows that the deletion model again increases the BLEU score over just the insertion model, by around 2 BLEU points.",
                "Since the signal for the deletion model is dependent on the insertion model's state, it is possible that the deletion model does not receive a learning signal during training. This happens when either the insertion model is too good and never inserts a wrong token, or when the insertion model does not insert anything at all. To mitigate this problem we propose an adversarial sampling method. To ensure that the deletion model always has a signal, with some probability $p_{\\text{adv}}$ we mask the ground-truth tokens in the target for the insertion model during training. This has the effect that when selecting the token to insert in the input sequence, before passing it to the deletion model, the insertion model selects the incorrect token it is most confident about. Therefore, the deletion model always has a signal and trains for a situation that it will most likely also encounter during inference."
            ],
            "predicted_section": [
                "Method ::: Learning",
                "Experiments ::: Learning shifted alphabetic sequences",
                "Experiments ::: Learning Caesar's Cipher"
            ]
        },
        "6c4e1a1ccc0c5c48115864a6928385c248f4d8ad": {
            "question_text": "What are the solutions proposed for the seq2seq shortcomings?",
            "from_paper": "1612.02695",
            "gold": [
                "We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts. A term depending on the transcript length was proposed for both CTC BIBREF3 and seq2seq BIBREF11 networks, but its usage was reported to be difficult because beam search was looping over parts of the recording and additional constraints were needed BIBREF11 . To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than INLINEFORM0 : DISPLAYFORM0",
                "The coverage criterion prevents looping over the utterance because once the cumulative attention bypasses the threshold INLINEFORM0 a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost. In our implementation, the coverage is recomputed at each beam search iteration using all attention weights produced up to this step.",
                "Label Smoothing Prevents Overconfidence",
                "A elegant solution to model overconfidence was problem proposed for the Inception image recognition architecture BIBREF15 . For the purpose of computing the training cost the ground-truth label distribution is smoothed, with some fraction of the probability mass assigned to classes other than the correct one. This in turn prevents the model from learning to concentrate all probability mass on a single token. Additionally, the model receives more training signal because the error function cannot easily saturate."
            ],
            "gold_section": [
                "Label Smoothing Prevents Overconfidence",
                "Solutions to Partial Transcripts Problem"
            ],
            "predicted": [
                "Understanding and preventing limitations specific to seq2seq models is crucial for their successful development. Discriminative training allows seq2seq models to focus on the most informative features. However, it also increases the risk of overfitting to those few distinguishing characteristics. We have observed that seq2seq models often yield very sharp predictions, and only a few hypotheses need to be considered to find the most likely transcription of a given utterance. However, high confidence reduces the diversity of transcripts obtained using beam search.",
                "Deep learning BIBREF0 has led to many breakthroughs including speech and image recognition BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . A subfamily of deep models, the Sequence-to-Sequence (seq2seq) neural networks have proved to be very successful on complex transduction tasks, such as machine translation BIBREF7 , BIBREF8 , BIBREF9 , speech recognition BIBREF10 , BIBREF11 , BIBREF12 , and lip-reading BIBREF13 . Seq2seq networks can typically be decomposed into modules that implement stages of a data processing pipeline: an encoding module that transforms its inputs into a hidden representation, a decoding (spelling) module which emits target sequences and an attention module that computes a soft alignment between the hidden representation and the targets. Training directly maximizes the probability of observing desired outputs conditioned on the inputs. This discriminative training mode is fundamentally different from the generative \"noisy channel\" formulation used to build classical state-of-the art speech recognition systems. As such, it has benefits and limitations that are different from classical ASR systems.",
                "Our seq2seq networks are locally normalized, i.e. the speller produces a probability distribution at every step. Alternatively normalization can be performed globally on whole transcripts. In discriminative training of classical ASR systems normalization is performed over lattices BIBREF30 . In the case of recurrent networks lattices are replaced by beam search results. Global normalization has yielded important benefits on many NLP tasks including parsing and translation BIBREF31 , BIBREF32 . Global normalization is expensive, because each training step requires running beam search inference. It remains to be established whether globally normalized models can be approximated by cheaper to train locally normalized models with proper regularization such as label smoothing."
            ],
            "predicted_section": [
                "Related Work",
                "Introduction"
            ]
        },
        "523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd": {
            "question_text": "What experiments do they perform?",
            "from_paper": "2002.04745",
            "gold": [
                "Experiments ::: Experiment Settings ::: Machine Translation",
                "We conduct our experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. For the IWSLT14 De-En task, we use the same model configuration as in Section 3. For the WMT14 En-De task, we use the Transformer base setting. More details can be found in the supplementary material.",
                "For training the Pre-LN Transformer, we remove the learning rate warm-up stage. On the IWSLT14 De-En task, we set the initial learning rate to be $5e^{-4}$ and decay the learning rate at the 8-th epoch by 0.1. On the WMT14 En-De task, we run two experiments in which the initial learning rates are set to be $7e^{-4}/1.5e^{-3}$ respectively. Both learning rates are decayed at the 6-th epoch followed by the inverse square root learning rate scheduler.",
                "Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)",
                "We follow BIBREF8 to use English Wikipedia corpus and BookCorpus for pre-training. As the dataset BookCorpus BIBREF40 is no longer freely distributed. We follow the suggestions from BIBREF8 to crawl and collect BookCorpus on our own. The concatenation of two datasets contains roughly 3.4B words in total, which is comparable with the data corpus used in BIBREF8. We randomly split documents into one training set and one validation set. The training-validation ratio for pre-training is 199:1.",
                "We use base model configuration in our experiments. Similar to the translation task, we train the Pre-LN BERT without the warm-up stage and compare it with the Post-LN BERT. We follow the same hyper-parameter configuration in BIBREF8 to train the Post-LN BERT using 10k warm-up steps with $\\text{lr}_{max}=1e^{-4}$. For the Pre-LN BERT, we use linear learning rate decay starting from $3e^{-4}$ without the warm-up stage. We have tried to use a larger learning rate (such as $3e^{-4}$) for the Post-LN BERT but found the optimization diverged."
            ],
            "gold_section": [
                "Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)",
                "Experiments ::: Experiment Settings ::: Machine Translation"
            ],
            "predicted": [
                "As our theory is derived based on several simplifications of the problem, we conduct experiments to study whether our theoretical insights are consistent with what we observe in real scenarios. The general model and training configuration exactly follow Section 3.2. The experiments are repeated ten times using different random seeds.",
                "Our contributions are summarized as follows:",
                "We use the validation set for evaluation. To fine-tune the models, following BIBREF8, BIBREF39, we search the optimization hyper-parameters in a search space including different batch sizes (16/32), learning rates ($1e^{-5}$ - $1e^{-4}$) and number of epochs (3-8). We find that the validation accuracy are sensitive to random seeds, so we repeat fine-tuning on each task for 6 times using different random seeds and compute the 95% confidence interval of validation accuracy."
            ],
            "predicted_section": [
                "Optimization for the Transformer ::: Empirical verification of the theory and discussion",
                "Experimental Settings ::: GLUE Dataset ::: Fine-tuning on GLUE tasks",
                "Introduction"
            ]
        },
        "34b434825f0ca3225dc8914f9da865d2b4674f08": {
            "question_text": "Does the baseline use any contextual information?",
            "from_paper": "1912.08084",
            "gold": [
                "The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.",
                "First, there is a random baseline, followed by an SVM classifier based on a bag-of-words representation with TF.IDF weights learned on the training data. Then come three versions of the ClaimBuster system: CB-Platform uses scores from the online demo, which we accessed on December 20, 2016, and SVM$_{CBfeat}$ and FNN$_{CBfeat}$ are our re-implementations, trained on our dataset."
            ],
            "gold_section": [
                "Experiments and Evaluation ::: Evaluation Results",
                "Related Work"
            ],
            "predicted": [
                "Then we see the group of contextual features Metadata with MAP=.256, and P@50=.370, followed by two sentence-level features: length and named entities, with MAP of .254 and .236, and P@50 of .340 and .280, respectively.",
                "At the bottom of the table we find position, a general contextual feature with MAP of .212 and P@50 of .230, followed by discourse and topics.",
                "The feature groups in this subsection contain a mixture of sentence- and of contextual-level features. For example, if we use a discourse parser to parse the target sentence only, any features we extract from the parse would be sentence-level. However, if we parse an entire segment, we would also have contextual features."
            ],
            "predicted_section": [
                "Experiments and Evaluation ::: Individual Feature Types",
                "Modeling Check-Worthiness ::: Mixed Features"
            ]
        },
        "61a2599acfbd3d75de58e97ecdba2d9cf0978324": {
            "question_text": "What is the strong rivaling system?",
            "from_paper": "1912.08084",
            "gold": [
                "The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement."
            ],
            "gold_section": [
                "Related Work"
            ],
            "predicted": [
                "State-of-the-art results: We achieve state-of-the-art results, outperforming a strong rivaling system by a margin, while also demonstrating that this improvement is due primarily to our modeling of the context.",
                "Metadata (8 C features): Check-worthy claims often contain mutual accusations between the opponents, as the following example shows (from the 2nd presidential debate):",
                "Similarity to known positive/negative examples (kNN) (2+1 S+C features): We used three more features inspired by $k$-nearest neighbor (kNN) classification. The first one (sentence-level) uses the maximum over the training sentences of the number of matching words between the testing and the training sentence, which is further multiplied by $-1$ if the latter was not check-worthy. We also used another version of the feature, where we multiplied it by 0 if the speakers were different (contextual). A third version took as a training set all claims checked by PolitiFact (excluding the target sentence)."
            ],
            "predicted_section": [
                "Modeling Check-Worthiness ::: Contextual Features",
                "Modeling Check-Worthiness ::: Mixed Features",
                "Introduction"
            ]
        },
        "c27b885b1e38542244f52056abf288b2389b9fc6": {
            "question_text": "How do they determine demographics on an image?",
            "from_paper": "1905.01347",
            "gold": [
                "In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias.",
                "Face Detection",
                "The FaceBoxes network BIBREF15 is employed for face detection, consisting of a lightweight CNN that incorporates novel Rapidly Digested and Multiple Scale Convolutional Layers for speed and accuracy, respectively. This model was trained on the WIDER FACE dataset BIBREF16 and achieves average precision of 95.50% on the Face Detection Data Set and Benchmark (FDDB) BIBREF17 . On a subset of 1,000 images from FDDB hand-annotated by the author for apparent age and gender, the model achieves a relative fair performance across intersectional groups, as show in Table TABREF1 .",
                "The task of apparent age annotation arises as ground-truth ages of individuals in images are not possible to obtain in the domain of web-scraped datasets. In this work, we follow Merler et al. BIBREF18 and employ the Deep EXpectation (DEX) model of apparent age BIBREF19 , which is pre-trained on the IMDB-WIKI dataset of 500k faces with real ages and fine-tuned on the APPA-REAL training and validation sets of 3.6k faces with apparent ages, crowdsourced from an average of 38 votes per image BIBREF20 . As show in Table TABREF2 , the model achieves a mean average error of 5.22 years on the APPA-REAL test set, but exhibits worse performance on younger and older age groups.",
                "We recognize that a binary representation of gender does not adequately capture the complexities of gender or represent transgender identities. In this work, we express gender as a continuous value between 0 and 1. When thresholding at 0.5, we use the sex labels of `male' and `female' to define gender classes, as training datasets and evaluation benchmarks use this binary label system. We again follow Merler et al. BIBREF18 and employ a DEX model to annotate the gender of an individual. When tested on APPA-REAL, with enhanced annotations provided by BIBREF21 , the model achieves an accuracy of 91.00%, however its errors are not evenly distributed, as shown in Table TABREF3 . The model errs more on younger and older age groups and on those with a female gender label."
            ],
            "gold_section": [
                "Gender Annotation",
                "Methodology",
                "Apparent Age Annotation",
                "Face Detection"
            ],
            "predicted": [
                "In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias.",
                "Through the introduction of a preliminary pipeline for automated demographic annotations, this work hopes to provide insight into the ImageNet dataset, a tool that is commonly abstracted away by the computer vision community. In the future, we will continue this work to create fair models for automated demographic annotations, with emphasis on the gender annotation model. We aim to incorporate additional measures of diversity into the pipeline, such as Fitzpatrick skin type and other craniofacial measurements. When annotation models are evaluated as fair, we plan to continue this audit on all 14.2M images of ImageNet and other large image datasets. With accurate coverage of the demographic attributes of ImageNet, we will be able to investigate the downstream impact of under- and over-represented groups in the features learned in pretrained CNNs and how bias represented in these features may propagate in transfer learning to new applications.",
                "First, candidate images for each synset were sourced from commercial image search engines, including Google, Yahoo!, Microsoft's Live Search, Picsearch and Flickr BIBREF10 . Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size. They state that a diverse synset will result in a blurrier average image and smaller file, representative of diversity in appearance, position, viewpoint and background. This method, however, cannot quantify diversity with respect to demographic characteristics such as age, gender, and skin type."
            ],
            "predicted_section": [
                "Diversity Considerations in ImageNet",
                "Methodology",
                "Conclusion"
            ]
        },
        "eafea4a24d103fdecf8f347c7d84daff6ef828a3": {
            "question_text": "Which training dataset do they use?",
            "from_paper": "1611.01116",
            "gold": [
                "To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.",
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents."
            ],
            "gold_section": [
                "Experiments",
                "Introduction"
            ],
            "predicted": [
                "We use AdaGrad BIBREF17 for training and inference in all experiments reported in this work. During training we employ dropout BIBREF18 in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by BIBREF9 . Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.",
                "The 20 Newsgroups dataset comes with reference train/test sets. In case of RCV1 we used half of the documents for training and the other half for evaluation. In case of English Wikipedia we held out for testing randomly selected 10% of the documents. We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes. We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious. In this case we adopted the relevancy measure used by BIBREF3 . That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows BIBREF3 , enabling comparison with semantic hashing codes. To assess the relevancy of articles in English Wikipedia we can employ categories assigned to them. However, unlike in RCV1, Wikipedia categories can have multiple parent categories and cyclic dependencies. Therefore, for this dataset we adopted a simplified relevancy measure: two articles are relevant if they share at least one category. We also removed from the test set categories with less than 20 documents as well as documents that were left with no categories. Overall, the relevancy is measured over more than INLINEFORM0 categories, making English Wikipedia harder than the other two benchmarks.",
                "Binary codes have also been applied to cross-modal retrieval where text is one of the modalities. Specifically, BIBREF4 incorporated tag information that often accompany text documents, while BIBREF5 employed siamese neural networks to learn single binary representation for text and image data."
            ],
            "predicted_section": [
                "Experiments",
                "Introduction"
            ]
        },
        "e099a37db801718ab341ac9a380a146c7452fd21": {
            "question_text": "Do they analyze the produced binary codes?",
            "from_paper": "1611.01116",
            "gold": [
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.",
                "Visualization of Binary PV codes",
                "For an additional comparison with semantic hashing, we used t-distributed Stochastic Neighbor Embedding BIBREF23 to construct two-dimensional visualizations of codes learned by Binary PV-DBOW with bigrams. We used the same subsets of newsgroups and RCV1 topics that were used by BIBREF3 . Codes learned by Binary PV-DBOW (Figure FIGREF20 ) appear slightly more clustered."
            ],
            "gold_section": [
                "Visualization of Binary PV codes",
                "Introduction"
            ],
            "predicted": [
                "Binary codes have also been applied to cross-modal retrieval where text is one of the modalities. Specifically, BIBREF4 incorporated tag information that often accompany text documents, while BIBREF5 employed siamese neural networks to learn single binary representation for text and image data.",
                "In this article we presented simple neural networks that learn short binary codes for text documents. Our networks extend Paragraph Vector by introducing a sigmoid nonlinearity before the softmax that predicts words in documents. Binary codes inferred with the proposed networks achieve higher retrieval precision than semantic hashing codes on two popular information retrieval benchmarks. They also retain a lot of their precision when trained on an unrelated text corpus. Finally, we presented a network that simultaneously learns short binary codes and longer, real-valued representations.",
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents."
            ],
            "predicted_section": [
                "Conclusion",
                "Introduction"
            ]
        },
        "5f25b57a1765682331e90a46c592a4cea9e3a336": {
            "question_text": "Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?",
            "from_paper": "1912.04979",
            "gold": [
                "Our vision processing module (see Fig. FIGREF1) locates and identifies all persons in a room for each frame captured by the camera. The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable.",
                "To handle this variability, we integrate information across time using face tracking as implied by our formulation of $P(h | r, V)$, which requires face identification to be performed only at a tracklet level. Our face tracking uses face detection and low-level tracking to maintain a set of tracklets, where each tracklet is defined as a sequence of faces in time that belong to the same person. We use a method similar to that in BIBREF50 with several adaptions to our specific setting, such as exploiting the stationarity of the camera for detecting motion, performing the low-level tracking by color based mean-shift instead of gray-level based normalized correlation, tuning the algorithm to minimize the risk of tracklet mergers (which in our context are destructive), etc. Also, the faces in each tracklet are augmented with attributes such as face position, dimensions, head pose, and face feature vectors. The tracklet set defines $\\mathcal {R}$ of equation (DISPLAY_FORM7).",
                "Face identification calculates person ID posterior probabilities for each tracklet. Guest IDs (e.g., 'Speaker1') are produced online, each representing a unique person in the meeting who is not on the invitee list. We utilize a discriminative face embedding which converts face images into fixed-dimensional feature vectors, or 128-dimensional vectors obtained as output layer activations of a convolutional neural network. For the face embedding and detection components, we use the algorithms from Microsoft Cognitive Services Face API BIBREF51, BIBREF52. Face identification of a tracklet is performed by comparing the set of face features extracted from its face instances, to the set of features from a gallery of each person's faces. For invited people, the galleries are taken from their enrollment videos, while for guests, the gallery pictures are accumulated online from the meeting video. We next describe our set-to-set similarity measure designed to perform this comparison.",
                "Our set-to-set similarity is designed to utilize information from multiple frames while remaining robust to head pose, lighting conditions, blur and other misleading factors. We follow the matched background similarity (MBGS) approach of BIBREF53 and make crucial adaptations to it that increase accuracy significantly for our problem. As with MBGS, we train a discriminative classifier for each identity $h$ in $\\mathcal {H}$. The gallery of $h$ is used as positive examples, while a separate fixed background set $B$ is used as negative examples. This approach has two important benefits. First, it allows us to train a classifier adapted to a specific person. Second, the use of a background set $B$ lets us account for misleading sources of variation e.g. if a blurry or poorly lit face from $B$ is similar to one of the positive examples, the classifier's decision boundary can be chosen accordingly. During meeting initialization, an support vector machine (SVM) classifier is trained to distinguish between the positive and negative sets for each invitee. At test time, we are given a tracklet $T=\\big \\lbrace \\mathbf {t}_1,...,\\mathbf {t}_N\\big \\rbrace $ represented as a set of face feature vectors $\\mathbf {t}_i\\in {\\mathbb {R}^d}$, and we classify each member $\\mathbf {t}_i$ with the classifier of each identity $h$ and obtain a set of classification confidences $\\big \\lbrace s\\big (T\\big )_{i,h}\\big \\rbrace $. Hereinafter, we omit argument $T$ for brevity. We now aggregate the scores of each identity to obtain the final identity scores $s_h=\\text{stat}\\big (\\big \\lbrace s_{i,h}\\big \\rbrace _{i=1}^N\\big )$ where $\\text{stat}(\\cdot )$ represents aggregation by e.g. taking the mean confidence. When $s=\\max _{h} s_h$ is smaller than a threshold, a new guest identity is added to $\\mathcal {H}$, where the classifier for this person is trained by using $T$ as positive examples. $\\lbrace s_h\\rbrace _{h \\in \\mathcal {H}}$ is converted to a set of posterior probabilities $\\lbrace P(h | r, V)\\rbrace _{h \\in \\mathcal {H}}$ with a trained regression model.",
                "The SSL generative model, $p(A_s | r; M)$, is defined by using a complex angular central Gaussian model (CACGM) BIBREF45. The SSL generative model can be written as follows:",
                "Speaker Diarization ::: Sound source localization",
                "$A$ and $V$ are the audio and video signals, respectively. $M$ is the set of the TF masks of the current CSS channel within the input segment. The speaker ID inventory, $\\mathcal {H}$, consists of the invited speaker names (e.g., `Alice' or `Bob') and anonymous `guest' IDs produced by the vision module (e.g., `Speaker1' or `Speaker2'). In what follows, we propose a model for combining face tracking, face identification, speaker identification, SSL, and the TF masks generated by the preceding CSS module to calculate the speaker ID posterior probability of equation (DISPLAY_FORM5). The integration of these complementary cues would make speaker attribution robust to real world challenges, including speech overlaps, speaker co-location, and the presence of guest speakers.",
                "First, by treating the face position trajectory of the speaking person as a latent variable, the speaker ID posterior probability can be represented as",
                "where $\\mathcal {R}$ includes all face position trajectories detected by the face tracking module within the input period. We call a face position trajectory a tracklet. The joint posterior probability on the right hand side (RHS) can be factorized as",
                "The RHS first term, or the tracklet-conditioned speaker ID posterior, can be further decomposed as",
                "The RHS first term, calculating the speaker ID posterior given the video signal and the tracklet calls for a face identification model because the video signal and the tracklet combine to specify a single speaker's face. On the other hand, the likelihood term on the RHS can be calculated as",
                "where we have assumed the spatial and magnitude features of the audio, represented as $A_s$ and $A_m$, respectively, to be independent of each other. The RHS first term, $p(A_s | h; M)$, is a spatial speaker model, measuring the likelihood of speaker $h$ being active given spatial features $A_s$. We make no assumption on the speaker positions. Hence, $p(A_s | h; M)$ is constant and can be ignored. The RHS second term, $p(A_m | h; M)$, is a generative model for speaker identification.",
                "Returning to (DISPLAY_FORM8), the RHS second term, describing the probability of the speaking person's face being $r$ (recall that each tracklet captures a single person's face), may be factorized as",
                "The first term is the likelihood of tracklet $r$ generating a sound with spatial features $A_s$ and therefore related to SSL. The second term is the probability with which the tracklet $r$ is active given the audio magnitude features and the video. Calculating this requires lip sync to be performed for each tracklet, which is hard in our application due to low resolution resulting from speaker-to-camera distances and compression artifacts. Thus, we ignore this term.",
                "Putting the above equations together, the speaker-tracklet joint posterior needed in (DISPLAY_FORM7) can be obtained as",
                "where the ingredients of the RHS relate to face identification, speaker identification, and SSL, respectively, in the order of appearance. The rest of this section describes our implementations of these models."
            ],
            "gold_section": [
                "Speaker Diarization ::: Face tracking and identification",
                "Speaker Diarization",
                "Speaker Diarization ::: Sound source localization"
            ],
            "predicted": [
                "Face identification calculates person ID posterior probabilities for each tracklet. Guest IDs (e.g., 'Speaker1') are produced online, each representing a unique person in the meeting who is not on the invitee list. We utilize a discriminative face embedding which converts face images into fixed-dimensional feature vectors, or 128-dimensional vectors obtained as output layer activations of a convolutional neural network. For the face embedding and detection components, we use the algorithms from Microsoft Cognitive Services Face API BIBREF51, BIBREF52. Face identification of a tracklet is performed by comparing the set of face features extracted from its face instances, to the set of features from a gallery of each person's faces. For invited people, the galleries are taken from their enrollment videos, while for guests, the gallery pictures are accumulated online from the meeting video. We next describe our set-to-set similarity measure designed to perform this comparison.",
                "where $\\mathcal {R}$ includes all face position trajectories detected by the face tracking module within the input period. We call a face position trajectory a tracklet. The joint posterior probability on the right hand side (RHS) can be factorized as",
                "Our vision processing module (see Fig. FIGREF1) locates and identifies all persons in a room for each frame captured by the camera. The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable."
            ],
            "predicted_section": [
                "Speaker Diarization ::: Face tracking and identification",
                "Speaker Diarization"
            ]
        },
        "d147117ef24217c43252d917d45dff6e66ff807c": {
            "question_text": "How do they model external knowledge? ",
            "from_paper": "1712.00733",
            "gold": [
                "Given an image, we apply the Fast-RCNN BIBREF27 to detect the visual objects of the input image, and extract keywords of the corresponding questions with syntax analysis. Based on these information, we propose to learn a mechanism to retrieve the candidate knowledge by querying the large-scale knowledge graph, yielding a subgraph of relevant knowledge to facilitate the question answering. During the past years, a substantial amount of large-scale knowledge bases have been developed, which store common sense and factual knowledge in a machine readable fashion. In general, each piece of structured knowledge is represented as a triple $(subject, rel, object)$ with $subject$ and $object$ being two entities or concepts, and $rel$ corresponding to the specific relationship between them. In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
                "In general, the underlying symbolic nature of a Knowledge Graph (KG) makes it difficult to integrate with DNNs. The usual knowledge graph embedding models such as TransE BIBREF26 focus on link prediction, which is different from VQA task aiming to fuse knowledge. To tackle this issue, we propose to embed the entities and relations of a KG into a continuous vector space, such that the factual knowledge can be used in a more simple manner. Each knowledge triple is treated as a three-word SVO $(subject, verb, object)$ phrase, and embedded into a feature space by feeding its word-embedding through an RNN architecture. In this case, the proposed knowledge embedding feature shares a common space with other textual elements (questions and answers), which provides an additional advantage to integrate them more easily."
            ],
            "gold_section": [
                "Overview",
                "Our Proposal"
            ],
            "predicted": [
                "KDMN: our full model. External knowledge triples are incorporated in Dynamic Memory Network.",
                "We further make comprehensive comparisons among our ablative models. To make it fair, all the experiments are implemented on the same basic network structure and share the same hyper-parameters. In general, our KDMN model on average gains $1.6\\%$ over the KDMN-NoMem model and $4.0\\%$ over the KDMN-NoKG model, which further implies the effectiveness of dynamic memory networks in exploiting external knowledge. Through iterative attention processes, the episodic memory vector captures background knowledge distilled from external knowledge embeddings. The KDMN-NoMem model gains $2.4\\%$ over the KDMN-NoKG model, which implies that the incorporated external knowledge brings additional advantage, and act as a supplementary information for predicting the final answer. The indicative examples in Fig. 3 also demonstrate the impact of external knowledge, such as the 4th example of \u201cWhy is the light red?\u201d. It would be helpful if we could retrieve the function of the traffic lights from the external knowledge effectively.",
                "where $\\hat{y_{i}}=p_{i}(A^{(i)}|I^{(i)},Q^{(i)},K^{(i)};\\theta )$ represents the probability of predicting the answer $A^{(i)}$ , given the $i_{\\text{th}}$ image $I^{(i)}$ , question $Q^{(i)}$ and external knowledge $K^{(i)}$ ; $\\theta $ represents the model parameters; $D$ is the number of training samples; and $y_{i}$ is the label for the $i_{\\text{th}}$ sample. The model can be trained in an end-to-end manner once we have the candidate knowledge triples are retrieved from the original knowledge graph."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Implementation Details",
                "Attention-based Knowledge Fusion with DNNs"
            ]
        },
        "59a3d4cdd1c3797962bf8d72c226c847e06e1d44": {
            "question_text": "What are the post-processing approaches applied to the output?",
            "from_paper": "1908.05925",
            "gold": [
                "The quotes are fixed to keep them the same as the source sentences.",
                "For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses BIBREF22 is applied to convert the translations to the real cases.",
                "From our observation, the ensemble NMT model lacks the ability to translate name entities correctly. We find that words with capital characters are named entities, and those named entities in the source language may have the same form in the target language. Hence, we capture and copy these entities at the end of the translation if they does not exist in our translation."
            ],
            "gold_section": [
                "Experiments ::: Data Post-processing ::: Recaser",
                "Experiments ::: Data Post-processing ::: Quotes Fixing",
                "Experiments ::: Data Post-processing ::: Patch-up"
            ],
            "predicted": [
                "In the pre-processing, we use the special tokens <NUMBER> and <DATE> to replace numbers that express a specific quantity and date respectively. Therefore, in the post-processing, we need to restore those numbers. We simply detect the pattern <NUMBER> and <DATE> in the original source sentences and then replace the special tokens in the translated sentences with the corresponding numbers detected in the source sentences. In order to make the replacement more accurate, we will detect more complicated patterns like <NUMBER> / <NUMBER> in the original source sentences. If the translated sentences also have the pattern, we replace this pattern <NUMBER> / <NUMBER> with the corresponding numbers in the original source sentences.",
                "We note that in the corpus, there are tokens representing quantity or date. Therefore, we delexicalize the tokens using two special tokens: (1) <NUMBER> to replace all the numbers that express a specific quantity, and (2) <DATE> to replace all the numbers that express a date. Then, we retrieve these numbers in the post-processing. There are two advantages of data pre-processing. First, replacing numbers with special tokens can reduce vocabulary size. Second, the special tokens are more easily processed by the model.",
                "The language model is a denoising auto-encoder, which is trained by reconstructing original sentences from noisy sentences. The process of language modeling can be expressed as minimizing the following loss:"
            ],
            "predicted_section": [
                "Experiments ::: Data Pre-processing",
                "Experiments ::: Data Post-processing ::: Special Token Replacement",
                "Methodology ::: Unsupervised Machine Translation ::: Word-level Unsupervised NMT"
            ]
        },
        "30870a962cf88ac8c8e6b7b795936fd62214f507": {
            "question_text": "Which neural network architecture do they use for the dialog agent and user simulator?",
            "from_paper": "1709.06136",
            "gold": [
                "Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn. At the INLINEFORM0 th turn of a dialog, the dialog agent takes in (1) the previous agent output encoding INLINEFORM1 , (2) the user input encoding INLINEFORM2 , (3) the retrieved KB result encoding INLINEFORM3 , and updates its internal state conditioning on the previous agent state INLINEFORM4 . With the updated agent state INLINEFORM5 , the dialog agent emits (1) a system action INLINEFORM6 , (2) an estimation of the belief state, and (3) a pointer to an entity in the retrieved query results. These outputs are then passed to an NLG module to generate the final agent response.",
                "Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance."
            ],
            "gold_section": [
                "User Simulator",
                "Dialog Agent"
            ],
            "predicted": [
                "Our contribution in this work is two-fold. Firstly, we propose an iterative dialog policy learning method that jointly optimizes the dialog agent and the user simulator in end-to-end trainable neural dialog systems. Secondly, we design a novel neural network based user simulator for task-oriented dialogs that can be trained in a data-driven manner without requiring the design of complex rules.",
                "In supervised pre-training, the dialog agent and the user simulator are trained separately against dialog corpus. We use the same set of neural network model configurations for both agents. Hidden layer sizes of the dialog-level LSTM for dialog modeling and utterance-level LSTM for utterance encoding are both set as 150. We perform mini-batch training using Adam optimization method BIBREF41 . Initial learning rate is set as 1e-3. Dropout BIBREF42 ( INLINEFORM0 ) is applied during model training to prevent to model from over-fitting.",
                "Recently, people have proposed neural network based methods for task-oriented dialogs, motivated by their superior performance in modeling chit-chat type of conversations BIBREF24 , BIBREF1 , BIBREF2 , BIBREF25 . Bordes and Weston BIBREF26 proposed modeling task-oriented dialogs with a reasoning approach using end-to-end memory networks. Their model skips the belief tracking stage and selects the final system response directly from a list of response candidates. Comparing to this approach, our model explicitly tracks dialog belief state over the sequence of turns, as robust dialog state tracking has been shown BIBREF27 to boost the success rate in task completion. Wen et al. BIBREF16 proposed an end-to-end trainable neural network model with modularity connected system components. This system is trained in supervised manner, and thus may not be robust enough to handle diverse dialog situations due to the limited varieties in dialog corpus. Our system is trained by a combination of SL and deep RL methods, as it is shown that RL training may effectively improved the system robustness and dialog success rate BIBREF28 , BIBREF19 , BIBREF29 . Moreover, other than having separated dialog components as in BIBREF16 , we use a unified network for belief tracking, knowledge base (KB) operation, and dialog management, to fully explore the knowledge that can be shared among different tasks."
            ],
            "predicted_section": [
                "Training Procedure",
                "Related Work",
                "Introduction"
            ]
        },
        "f94cea545f745994800c1fb4654d64d1384f2c26": {
            "question_text": "Is this done in form of unsupervised (clustering) or suppervised learning?",
            "from_paper": "2003.08769",
            "gold": [
                "METHODOLOGY",
                "The real task lies in converting the image into interpretable data that can be parsed and used. To help with this, a data processing pipeline is built. The details of the pipeline are discussed below. The data pipeline extensively uses the ClarifaiBIBREF8 image recognition model. The 3 models used extensively are:",
                "The General Model : It recognizes over 11,000 different concepts and is a great all purpose solution. We have used this model to distinguish between Food images and Non-Food images.",
                "The Food Model : It recognizes more than 1,000 food items in images down to the ingredient level. This model is used to identify the ingredients in a food image.",
                "The General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai\u2019s \u2018General\u2019 model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item.",
                "A dataset of 275 images of different food items from different cuisines was compiled. These images were used as input to the Clarifai Food Model. The returned tags were used to create a knowledge database. When the general model labels for an image with high probability were a part of this database, the image was classified as a food image. The most commonly occurring food labels are visualized in Fig 3.",
                "To build a clean database for the user, images with people are excluded. This includes images with people holding or eating food. This is again done with the help of the descriptive labels returned by the Clarifai General Model. Labels such as \"people\" or \"man/woman\" indicate the presence of a person and such images are discarded.",
                "From the food images(specific to each user), each image's descriptive labels are obtained from the Food Model. The Clarifai Food Model returns a list of concepts/labels/tags with corresponding probability scores on the likelihood that these concepts are contained within the image. The sum of the probabilities of each of these labels occurring in each image is plotted against the label in Fig 4.",
                "A more sophisticated approach to classify based on the ingredients was adopted by using the K Nearest Neighbors Model. The Yummly dataset from Kaggle is used to train the model. The ingredients extracted from the images are used as a test set. The model was run successfully for k-values ranging from 1-25. The radar charts for some of the k values are shown in Fig 7, 8 and 9."
            ],
            "gold_section": [
                "METHODOLOGY ::: DATA PRE PROCESSING ::: To Remove Images with People",
                "METHODOLOGY ::: Basic Observations",
                "METHODOLOGY ::: KNN Model for Classification",
                "METHODOLOGY",
                "METHODOLOGY ::: DATA PRE PROCESSING ::: To Classify Images as Food Images"
            ],
            "predicted": [
                "Existence of huge cultural diffusion among cuisines is shown by the work carried out by S Jayaraman et al in BIBREF4. They explore the performance of each classifier for a given type of dataset under unsupervised learning methods(Linear support Vector Classifier (SVC), Logistic Regression, Random Forest Classifier and Naive Bayes).",
                "Yang et al BIBREF3 believed the key to recognizing food is exploiting the spatial relationships between different ingredients (such as meat and bread in a sandwich). They propose a new representation for food items that calculates pairwise statistics between local features computed over a soft pixel-level segmentation of the image into eight ingredient types. Then they accumulate these statistics in a multi-dimensional histogram, which is then used as a feature vector for a discriminative classifier.",
                "The General Embedding Model : It analyzes images and returns numerical vectors that represent the input images in a 1024-dimensional space. The vector representation is computed by using Clarifai\u2019s \u2018General\u2019 model. The vectors of visually similar images will be close to each other in the 1024-dimensional space. This is used to eliminate multiple similar images of the same food item."
            ],
            "predicted_section": [
                "RELATED WORK",
                "METHODOLOGY"
            ]
        },
        "54b25223ab32bf8d9205eaa8a570e99c683f0077": {
            "question_text": "What baselines do they compare to?",
            "from_paper": "1909.13466",
            "gold": [
                "Extensive experimentation over four language pairs of different dataset sizes (from small to large) with both word and sentence regularization. We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.",
                "In this section, we describe the NMT model that has been used as the basis for the proposed regularizer. It is a neural encoder-decoder architecture with attention BIBREF1 that can be regarded as a strong baseline as it incorporates both LSTMs and transformers as modules. Let us assume that $\\textbf {x}:\\lbrace x_1 \\dots x_n\\rbrace $ is the source sentence with $n$ tokens and $\\textbf {y}:\\lbrace y_1 \\dots y_m\\rbrace $ is the target translated sentence with $m$ tokens. First, the words in the source sentence are encoded into their word embeddings by an embedding layer:"
            ],
            "gold_section": [
                "The Baseline NMT model",
                "Introduction"
            ],
            "predicted": [
                "We have carried out a number of experiments with both baselines. The scores reported are an average of the BLEU scores (in percentage points, or pp) BIBREF46 over the test sets of 5 independently trained models. Table TABREF44 shows the results over the en-fr dataset. In this case, the models with ReWE have outperformed the LSTM and transformer baselines consistently. The LSTM did not benefit from using BPE, but the transformer+ReWE with BPE reached $36.30$ BLEU pp (a $+0.99$ pp improvement over the best model without ReWE). For this dataset we did not use ReSE because French was the target language.",
                "For the eu-en dataset (Table TABREF46), the results show that, again, ReWE outperforms the baselines by a large margin. Moreover, ReWE+ReSE has been able to improve the results even further ($+3.15$ BLEU pp when using BPE and $+5.15$ BLEU pp at word level over the corresponding baselines). Basque is, too, a morphologically-rich language and using BPE has proved very beneficial ($+4.27$ BLEU pp over the best word-level model). As noted before, the eu-en dataset is very low-resource (less than $100,000$ sentence pairs) and it is more likely that the baseline models generalize poorly. Consequently, regularizers such as ReWE and ReSE are more helpful, with larger margins of improvement with respect to the baselines. On a separate note, the transformer has unexpectedly performed well below the LSTM on this dataset, and especially so with BPE. We speculate that it may be more sensitive than the LSTM to the dataset's much smaller size, or in need of more refined hyper-parameter tuning.",
                "Four different language pairs have been selected for the experiments. The datasets' size varies from tens of thousands to millions of sentences to test the regularizers' ability to improve translation over a range of low-resource and high-resource language pairs."
            ],
            "predicted_section": [
                "Experiments ::: Datasets",
                "Experiments ::: Results"
            ]
        },
        "42279c3a202a93cfb4aef49212ccaf401a3f8761": {
            "question_text": "Which three variants of sequential validation are examined?",
            "from_paper": "1803.05160",
            "gold": [
                "Throughout our experiments we use only one training algorithm (subsection sec:data), and two performance measures (subsection sec:measures). During training, the performance of the trained model can be estimated only on the in-sample data. However, there are different estimation procedures which yield these approximations. In machine learning, a standard procedure is cross-validation, while for time-ordered data, sequential validation is typically used. In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc). The goal is to find the in-sample estimation procedure that best approximates the out-of-sample gold standard. The error an estimation procedure makes is defined as the difference to the gold standard.",
                "In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:",
                "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,",
                "seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,",
                "seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.",
                "The Twitter data shares some characteristics of time series and some of static data. A time series is an array of observations at regular or equidistant time points, and the observations are in general dependent on previous observations BIBREF0 . On the other hand, Twitter data is time-ordered, but the observations are short texts posted by Twitter users at any time and frequency. It can be assumed that original Twitter posts are not directly dependent on previous posts. However, there is a potential indirect dependence, demonstrated in important trends and events, through influential users and communities, or individual user's habits. These long-term topic drifts are typically not taken into account by the sentiment analysis models."
            ],
            "gold_section": [
                "Methods and experiments",
                "Estimation procedures",
                "Introduction"
            ],
            "predicted": [
                "We therefore compare two classes of estimation procedures. Cross-validation, commonly used in machine learning for model evaluation on static data, and sequential validation, commonly used for time-series data. There are many variants and parameters for each class of procedures. Our datasets are relatively large and an application of each estimation procedure takes several days to complete. We have selected three variants of each procedure to provide answers to some relevant questions.",
                "Throughout our experiments we use only one training algorithm (subsection sec:data), and two performance measures (subsection sec:measures). During training, the performance of the trained model can be estimated only on the in-sample data. However, there are different estimation procedures which yield these approximations. In machine learning, a standard procedure is cross-validation, while for time-ordered data, sequential validation is typically used. In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc). The goal is to find the in-sample estimation procedure that best approximates the out-of-sample gold standard. The error an estimation procedure makes is defined as the difference to the gold standard.",
                "In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:"
            ],
            "predicted_section": [
                "Methods and experiments",
                "Estimation procedures"
            ]
        }
    },
    "full": {
        "b584739622d0c53830e60430b13fd3ae6ff43669": {
            "question_text": "What are the evaluation metrics and criteria used to evaluate the model performance?",
            "from_paper": "1911.10742",
            "gold": [
                "Experiments ::: Automatic Evaluation Metrics",
                "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.",
                "Response-Intent Prediction (RIP) $\\&$ Response-Slot Prediction (RSP) Different from open-domain dialog systems, we care about the intents of the system response in non-collaborative tasks as we hope to know if the system response satisfies user intents. For example, in the anti-scam task, if the attacker elicits information from the system, we need to know if the system refuses or agrees to provide the information. Therefore we care about intent prediction for the generated system response. Since our baselines are more suited for social chat as they cannot produce system intents, we use the system intent and slot classifiers trained in our model to predict their responses' intents and slots. The intent predictor achieves a $84\\%$ accuracy and the semantic slot predictor achieves $77\\%$ on the AntiScam dataset. Then we compare the predicted values with human-annotated ground truth in the dataset to compute the response-intent prediction (RIP) and response-slot prediction (RSP).",
                "Extended Response-Intent Prediction (ERIP) $\\&$ Extended Response-Slot Prediction (ERSP) With Response-Intent Prediction, we verify the predicted intents to evaluate the coherence of the dialog. However, the real mapping between human-intent and system-intent is much more complicated as there might be multiple acceptable system-intents for the same human-intent. Therefore, we also design a metric to evaluate if the predicted system-intent is in the set of acceptable intents. Specifically, we estimate the transition probability $p(I_i|I_j)$ by counting the frequency of all the bi-gram human-intent and system-intent pairs in the training data. During the test stage, if the predicted intent matches the ground truth, we set the score as 1, otherwise we set the score as $p(I_{predict}|I_i)$ where $I_i$ is the intent of the input human utterance. We then report the average value of those scores over turns as the final extended response-intent prediction result.",
                "Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.",
                "Fluency Fluency is used to explore different models' language generation quality.",
                "Coherence Different from single sentence's fluency, coherence focuses more on the logical consistency between sentences in each turn.",
                "Engagement In the anti-scam scenario, one of our missions is to keep engaging with the attackers to waste their time. So we directly ask volunteers (attackers) to what extend they would like to continue chatting with the system.",
                "Dialog length (Length) Engagement is a subjective metric. Anti-scam system's goal is to engage user in the conversation longer in order to limit their harm to other potential victims. So we count the dialog length as another metric to evaluate system performance.",
                "Task Success Score (TaskSuc) The other goal of the anti-scam system is to elicit attacker's personal information. We count the average type of information (name, address and phone number) that the system obtained from attackers as the task success score."
            ],
            "gold_section": [
                "Experiments ::: Automatic Evaluation Metrics",
                "Experiments ::: Human Evaluation Metrics"
            ],
            "predicted": [
                "Perplexity Since the canonical measure of a good language model is perplexity, which indicates the error rate of the expected word. We choose perplexity to evaluate the model performance.",
                "Automatic metrics only validate the system\u2019s performance on a single dimension at a time. The ultimate holistic evaluation should be conducted by having the trained system interact with human users. Therefore we also conduct human evaluations for the dialog system built on AntiScam. We test our models and baselines with 15 college-student volunteers. Each of them is asked to pretend to be an attacker and interact with all the models for at least three times to avoid randomness. We in total collect 225 number of dialogs. Each time, volunteers are required to use similar sentences and strategies to interact with all five models and score each model based on the metrics listed below at the end of the current round. Each model receives a total of 45 human ratings, and the average score is reported as the final human-evaluation score. In total, we design five different metrics to assess the models' conversational ability whilst interacting with humans. The results are shown in Table TABREF19.",
                "Fluency Fluency is used to explore different models' language generation quality."
            ],
            "predicted_section": [
                "Experiments ::: Automatic Evaluation Metrics",
                "Experiments ::: Human Evaluation Metrics"
            ]
        },
        "bc8526d4805e2554adb2e9c01736d3f3a3b19895": {
            "question_text": "What baselines did they compare with?",
            "from_paper": "1604.02038",
            "gold": [
                "The following baselines were used in our experiments:",
                "LDA BIBREF2 . LDA is the classic topic model, and we used GibbsLDA++ for its implementation.",
                "Doc-NADE BIBREF24 . Doc-NADE is a representative neural network based topic model. We used the open-source code provided by the authors.",
                "HTMM BIBREF9 . HTMM models consider the sentence level Markov transitions. Similar to Doc-NADE, the implementation was provided by the authors.",
                "GMNTM BIBREF12 . GMNTM considers models the order of words within a sentence by a feedforward neural network. We implemented GMNTM according the descriptions in their papers by our own."
            ],
            "gold_section": [
                "Quantitative Results"
            ],
            "predicted": [
                "The following baselines were used in our experiments:",
                "We base our experiments on two benchmark datasets:",
                "In this experiment, we fed the document vectors (e.g., the INLINEFORM0 values in SLRTM) learnt by different topic models to supervised classifiers, to compare their representation power. For 20Newsgroup, we used the multi-class logistic regression classifier and used accuracy as the evaluation criterion. For Wiki10+, since multiple labels (tags) might be associated with each document, we used logistic regression for each label and the classification result is measured by Micro- INLINEFORM1 score BIBREF33 . For both datasets, we use INLINEFORM2 of the original training set for validation, and the remaining for training."
            ],
            "predicted_section": [
                "Quantitative Results"
            ]
        },
        "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd": {
            "question_text": "Which NER dataset do they use?",
            "from_paper": "1911.04474",
            "gold": [
                "We evaluate our model in two English NER datasets and four Chinese NER datasets.",
                "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.",
                "(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.",
                "(3) BIBREF35 released OntoNotes 4.0. In this paper, we use the Chinese part. We adopted the same pre-process as BIBREF36.",
                "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37.",
                "(5) Weibo NER was built based on text in Chinese social media Sina Weibo BIBREF38, and it contained 4 kinds of entities.",
                "(6) Resume NER was annotated by BIBREF33."
            ],
            "gold_section": [
                "Experiment ::: Data"
            ],
            "predicted": [
                "(1) CoNLL2003 is one of the most evaluated English NER datasets, which contains four different named entities: PERSON, LOCATION, ORGANIZATION, and MISC BIBREF34.",
                "(2) OntoNotes 5.0 is an English NER dataset whose corpus comes from different domains, such as telephone conversation, newswire. We exclude the New Testaments portion since there is no named entity in it BIBREF8, BIBREF7. This dataset has eleven entity names and seven value types, like CARDINAL, MONEY, LOC.",
                "(4) The corpus of the Chinese NER dataset MSRA came from news domain BIBREF37."
            ],
            "predicted_section": [
                "Experiment ::: Data"
            ]
        },
        "371433bd3fb5042bacec4dfad3cfff66147c14f0": {
            "question_text": "How do data-driven models usually respond to abuse?",
            "from_paper": "1909.04387",
            "gold": [
                "4 Data-driven approaches:",
                "Cleverbot BIBREF12;",
                "NeuralConvo BIBREF13, a re-implementation of BIBREF14;",
                "an implementation of BIBREF15's Information Retrieval approach;",
                "a vanilla Seq2Seq model trained on clean Reddit data BIBREF1.",
                "Finally, we consider appropriateness per system. Following related work by BIBREF21, BIBREF24, we use Trueskill BIBREF25 to cluster systems into equivalently rated groups according to their partial relative rankings. The results in Table TABREF36 show that the highest rated systen is Alley, a purpose build bot for online language learning. Alley produces \u201cpolite refusal\u201d (2b) - the top ranked strategy - 31% of the time. Comparatively, commercial systems politely refuse only between 17% (Cortana) and 2% (Alexa). Most of the time commercial systems tend to \u201cplay along\u201d (3a), joke (3b) or don't know how to answer (1e) which tend to receive lower ratings, see Figure FIGREF38. Rule-based systems most often politely refuse to answer (2b), but also use medium ranked strategies, such as deflect (2c) or chastise (2d). For example, most of Eliza's responses fall under the \u201cdeflection\u201d strategy, such as \u201cWhy do you ask?\u201d. Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only ones that ever politely refuse and we attribute their improved ratings to this. In turn, the \u201cclean\u201d seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses ( 16% of the time). Ritter:2010:UMT:1857999.1858019's IR approach is rated similarly to Capt Howdy and both produce a majority of retaliatory (2e) responses - 38% and 58% respectively - followed by flirtatious responses. Finally, Dr Love and Sophia69 produce almost exclusively flirtatious responses which are consistently ranked low by users."
            ],
            "gold_section": [
                "Results ::: Systems",
                "Data Collection"
            ],
            "predicted": [
                "Ethical challenges related to dialogue systems and conversational agents raise novel research questions, such as learning from biased data sets BIBREF0, and how to handle verbal abuse from the user's side BIBREF1, BIBREF2, BIBREF3, BIBREF4. As highlighted by a recent UNESCO report BIBREF5, appropriate responses to abusive queries are vital to prevent harmful gender biases: the often submissive and flirty responses by the female-gendered systems reinforce ideas of women as subservient. In this paper, we investigate the appropriateness of possible strategies by gathering responses from current state-of-the-art systems and ask crowd-workers to rate them.",
                "Our results show that: (1) The user's age has an significant effect on the ratings. For example, older users find jokes as a response to harassment highly inappropriate. (2) Perceived appropriateness also depends on the type of previous abuse. For example, avoidance is most appropriate after sexual demands. (3) All system were rated significantly higher than our negative adult-only baselines - except two data-driven systems, one of which is a Seq2Seq model trained on \u201cclean\" data where all utterances containing abusive words were removed BIBREF1. This leads us to believe that data-driven response generation need more effective control mechanisms BIBREF30.",
                "We first gather abusive utterances from 600K conversations with US-based customers. We search for relevant utterances by simple keyword spotting and find that about 5% of the corpus includes abuse, with mostly sexually explicit utterances. Previous research reports even higher levels of abuse between 11% BIBREF2 and 30% BIBREF6. Since we are not allowed to directly quote from our corpus in order to protect customer rights, we summarise the data to a total of 109 \u201cprototypical\" utterances - substantially extending the previous dataset of 35 utterances from Amanda:EthicsNLP2018 - and categorise these utterances based on the Linguistic Society's definition of sexual harassment BIBREF7:"
            ],
            "predicted_section": [
                "Data Collection",
                "Introduction",
                "Conclusion"
            ]
        },
        "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162": {
            "question_text": "Was the automatic annotation evaluated?",
            "from_paper": "2003.13016",
            "gold": [
                "The dataset includes two different versions of annotations, one with a set of 19 fine-grained semantic classes and another one with a set of 7 coarse-grained classes (Table ). There are 53,632 annotated entities in total, the majority of which (74.34 %) are legal entities, the others are person, location and organization (25.66 %). Overall, the most frequent entities are law GS (34.53 %) and court decision RS (23.46 %). The other legal classes (ordinance VO, European legal norm EUN, regulation VS, contract VT, and legal literature LIT) are much less frequent (1\u20136 % each). Even less frequent (less than 1 %) are lawyer AN, street STR, landscape LDS, and brand MRK.",
                "The dataset was thoroughly evaluated, see leitner2019fine for more details. As state of the art models, Conditional Random Fields (CRFs) and bidirectional Long-Short Term Memory Networks (BiLSTMs) were tested with the two variants of annotation. For CRFs, these are: CRF-F (with features), CRF-FG (with features and gazetteers), CRF-FGL (with features, gazetteers and lookup). For BiLSTM, we used models with pre-trained word embeddings BIBREF22: BiLSTM-CRF BIBREF23, BiLSTM-CRF+ with character embeddings from BiLSTM BIBREF24, and BiLSTM-CNN-CRF with character embeddings from CNN BIBREF25. To evaluate the performance we used stratified 10-fold cross-validation. As expected, BiLSTMs perform best (see Table ). The F$_1$ score for the fine-grained classification reaches 95.46 and 95.95 for the coarse-grained one. CRFs reach up to 93.23 F$_1$ for the fine-grained classes and 93.22 F$_1$ for the coarse-grained ones. Both models perform best for judge, court and law."
            ],
            "gold_section": [
                "Evaluation",
                "Description of the Dataset ::: Annotation of Named Entities"
            ],
            "predicted": [
                "The dataset was originally annotated by the first author. To evaluate and potentially improve the quality of the annotations, part of the dataset was annotated by a second linguist (using the annotation guidelines specifically prepared for its construction). We selected a small part that could be annotated in approx. two weeks. For the sentence extraction we paid special attention to the anonymised mentions of person, location or organization entities, because these are usually explained at their first mention. The resulting sample consisted of 2005 sentences with a broad variety of different entities (3 % of all sentences from each federal court). The agreement between the two annotators was measured using Kappa on a token basis. All class labels were taken into account in accordance with the IOB2 scheme BIBREF18. The inter-annotator agreement is 0.89, i. e., there is mostly very good agreement between the two annotators. Differences were in the identification of court decision and legal literature. Some unusual references of court decision (consisting only of decision type, court, date, file number) were not annotated such as `Urteil des Landgerichts Darmstadt vom 16. April 2014 \u2013 7 S 8/13 \u2013'. Apart from missing legal literature annotations, author names and law designations were annotated according to their categories (i. e., `Schoch, in: Schoch/Schneider/Bier, VwGO \u00a7 123 Rn. 35', `Bekanntmachung des BMG gem\u00e4\u00df \u00a7\u00a7 295 und 301 SGB V zur Anwendung des OPS vom 21.10.2010').",
                "The second annotator had difficulties annotating the class law, not all instances were identified (`\u00a7 272 Abs. 1a und 1b HGB', `\u00a7 3c Abs. 2 Satz 1 EStG'), others only partially (`\u00a7 716 in Verbindung mit' in `\u00a7 716 in Verbindung mit \u00a7\u00a7 321 , 711 ZPO'). Some titles of contract were not recognised and annotated (`BAT', `TV-L', `TV\u00dc-L\u00e4nder' etc.).",
                "This evaluation has revealed deficiencies in the annotation guidelines, especially regarding court decision and legal literature as well as non-entities. It would also be helpful for the identification and classification to list well-known sources of law, court decision, legal literature etc."
            ],
            "predicted_section": [
                "Description of the Dataset ::: Annotation of Named Entities"
            ]
        },
        "887d7f3edf37ccc6bf2e755dae418b04d2309686": {
            "question_text": "What type of morphological features are used?",
            "from_paper": "1805.11937",
            "gold": [
                "We use three types of units: (1) words (2) characters and character sequences and (3) outputs of morphological analysis. Words serve as a lower bound; while morphology is used as an upper bound for comparison. Table 1 shows sample outputs of various $\\rho $ functions.",
                "Here, char function simply splits the token into its characters. Similar to n-gram language models, char3 slides a character window of width $n=3$ over the token. Finally, gold morphological features are used as outputs of morph-language. Throughout this paper, we use morph and oracle interchangably, i.e., morphology-level models (MLM) have access to gold tags unless otherwise is stated. For all languages, morph outputs the lemma of the token followed by language specific morphological tags. As an exception, it outputs additional information for some languages, such as parts-of-speech tags for Turkish. Word segmenters such as Morfessor and Byte Pair Encoding (BPE) are other commonly used subword units. Due to low scores obtained from our preliminary experiments and unsatisfactory results from previous studies BIBREF13 , we excluded these units."
            ],
            "gold_section": [
                "Subword Units"
            ],
            "predicted": [
                "Morphological analysis already provides the aforementioned information about the words. However access to useful morphological features may be problematic due to software licensing issues, lack of robust morphological analyzers and high ambiguity among analyses. Character-level models (CLM), being a cheaper and accessible alternative to morphology, have been reported as performing competitively on various NLP tasks BIBREF0 , BIBREF1 , BIBREF2 . However the extent to which these tasks depend on morphology is small; and their relation to semantics is weak. Hence, little is known on their true ability to reveal the underlying morphological structure of a word and their semantic capabilities. Furthermore, their behaviour across languages from different families; and their limitations and strengths such as handling of long-range dependencies, reaction to model complexity or performance on out-of-domain data are unknown. Analyzing such issues is a key to fully understanding the character-level models.",
                "We use a simple method based on bidirectional LSTMs to train three types of base semantic role labelers that employ (1) words (2) characters and character sequences and (3) gold morphological analysis. The gold morphology serves as the upper bound for us to compare and analyze the performances of character-level models on languages of varying morphological typologies. We carry out an exhaustive error analysis for each language type and analyze the strengths and limitations of character-level models compared to morphology. In regard to the diversity hypothesis which states that diversity of systems in ensembles lead to further improvement, we combine character and morphology-level models and measure the performance of the ensemble to better understand how similar they are.",
                "Although models with access to gold morphological tags achieve better F1 scores than character models, they can be less useful a in real-life scenario since they require gold tags at test time. To predict the performance of morphology-level models in such a scenario, we train the same models with the same parameters with predicted morphological features. Predicted tags were only available for German, Spanish, Catalan and Czech. Our results given in Fig. 5 , show that (except for Czech), predicted morphological tags are not as useful as characters alone."
            ],
            "predicted_section": [
                "Introduction",
                "Predicted Morphological Tags"
            ]
        },
        "780c7993d446cd63907bb38992a60bbac9cb42b1": {
            "question_text": "What language are the captions in?",
            "from_paper": "1909.09070",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "We evaluate the language and visual representations emerging from FCC in the context of two classification tasks that aim to identify the scientific field an arbitrary text fragment (a caption) or a figure belong to, according to the SciGraph taxonomy. The latter is a particularly hard task due to the whimsical nature of the figures that appear in our corpus: figure and diagram layout is arbitrary; charts, e.g. bar and pie charts, are used to showcase data in any field from health to engineering; figures and natural images appear indistinctly, etc. Also, note that we only rely on the actual figure, not the text fragment where it is mentioned in the paper.",
                "A corpus of scientific figures and captions extracted from SN SciGraph and AI2 Semantic Scholar.",
                "The main idea of our approach is to learn a correspondence task between scientific figures and their captions as they appear in a scientific publication. The information captured in the caption explains the corresponding figure in natural language, providing guidance to identify the key features of the figure and vice versa. By seeing a figure and reading the textual description in its caption we ultimately aim to learn representations that capture e.g. what it means that two plots are similar or what gravity looks like."
            ],
            "predicted_section": [
                "Figure-Caption Correspondence",
                "Introduction",
                "Results and Discussion ::: Caption and Figure Classification"
            ]
        },
        "5ed02ae6c534cd49d405489990f0e4ba0330ff1b": {
            "question_text": "Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?",
            "from_paper": "2004.04124",
            "gold": [
                "The overall pipeline of LadaBERT (Lightweight Adaptation of BERT) is illustrated in Figure FIGREF8. As shown in the figure, the pre-trained BERT model (e.g., BERT-Base) is served as the teacher as well as the initial status of the student model. Then, the student model is compressed towards smaller parameter size through a hybrid model compression framework in an iterative manner until the target compression ratio is reached. Concretely, in each iteration, the parameter size of student model is first reduced by $1-\\Delta $ based on weight pruning and matrix factorization, and then the parameters are fine-tuned by the loss function of knowledge distillation. The motivation behind is that matrix factorization and weight pruning are complementary with each other. Matrix factorization calculates the optimal approximation under a certain rank, while weight pruning introduces additional sparsity to the decomposed matrices. Moreover, weight pruning and matrix factorization generates better initial and intermediate status of the student model, which improve the efficiency and effectiveness of knowledge distillation. In the following subsections, we will introduce the algorithms in detail.",
                "The evaluation results of LadaBERT and state-of-the-art approaches are listed in Table TABREF40, where the models are ranked by parameter sizes for feasible comparison. As shown in the table, LadaBERT consistently outperforms the strongest baselines under similar model sizes. In addition, the performance of LadaBERT demonstrates the superiority of hybrid combination of SVD-based matrix factorization, weight pruning and knowledge distillation."
            ],
            "gold_section": [
                "Lightweight Adaptation of BERT ::: Overview",
                "Experiments ::: Performance Comparison"
            ],
            "predicted": [
                "With model size of $2.5\\times $ reduction, LadaBERT-1 performs significantly better than BERT-PKD, boosting the performance by relative 8.9, 8.1, 6.1, 3.8 and 5.8 percentages on MNLI-m, MNLI-mm, SST-2, QQP and QNLI datasets respectively. Recall that BERT-PKD initializes the student model by selecting 3 of 12 layers in the pre-trained BERT-Base model. It turns out that the discarded layers have huge impact on the model performance, which is hard to be recovered by knowledge distillation. On the other hand, LadaBERT generates the student model by iterative pruning on the pre-trained teacher. In this way, the original knowledge in the teacher model can be preserved to the largest extent, and the benefit of which is complementary to knowledge distillation.",
                "To further demonstrate the efficiency of LadaBERT, we visualize the learning curves on MNLI-m and QQP datasets in Figure FIGREF42 and FIGREF42, where LadaBERT-3 is compared to the strongest baseline, TinyBERT, under $7.5 \\times $ compression ratio. As shown in the figures, LadaBERT-3 achieves good performances much faster and results in a better convergence point. After training $2 \\times 10^4$ steps (batches) on MNLI-m dataset, the performance of LadaBERT-3 is already comparable to TinyBERT after convergence (approximately $2 \\times 10^5$ steps), achieving nearly $10 \\times $ acceleration. And on QQP dataset, both performance improvement and training speed acceleration is very significant. This clearly shows the superiority of combining matrix factorization, weight pruning and knowledge distillation in a reinforce manner. Instead, TinyBERT is based on pure knowledge distillation, so the learning speed is much slower.",
                "We conduct extensive experiments on five public datasets of natural language understanding. As an example, the performance comparison of LadaBERT and state-of-the-art models on MNLI-m dataset is illustrated in Figure FIGREF1. We can see that LadaBERT outperforms other BERT-oriented model compression baselines at various model compression ratios. Especially, LadaBERT-1 outperforms BERT-PKD significantly under $2.5\\times $ compression ratio, and LadaBERT-3 outperforms TinyBERT under $7.5\\times $ compression ratio while the training speed is accelerated by an order of magnitude."
            ],
            "predicted_section": [
                "Experiments ::: Performance Comparison",
                "Introduction",
                "Experiments ::: Learning curve comparison"
            ]
        },
        "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea": {
            "question_text": "What are the parts of the \"multimodal\" resources?",
            "from_paper": "1912.02866",
            "gold": [
                "From the perspective of computational processing, diagrammatic representations present a formidable challenge, as they involve tasks from both computer vision and natural language processing. On the one hand, diagrams have a spatial organisation \u2013 layout \u2013 which needs to be segmented to identify meaningful units and their position. Making sense of how diagrams exploit the 2D layout space falls arguably within the domain of computer vision. On the other hand, diagrams also have a discourse structure, which uses the layout space to set up discourse relations between instances of natural language, various types of images, arrows and lines, thus forming a unified discourse organisation. The need to parse this discourse structure shifts the focus towards the field of natural language processing."
            ],
            "gold_section": [
                "Introduction"
            ],
            "predicted": [
                "This section introduces the two multimodal resources compared in this study and discusses related work, beginning with the crowd-sourced annotations in AI2D and continuing with the alternative expert annotations in AI2D-RST, which are built on top of the crowd-sourced descriptions and cover a 1000-diagram subset of the original data. Figure FIGREF1 provides an overview of the two datasets, explains their relation to each other and provides an overview of the experiments reported in Section SECREF4",
                "Understanding and making inferences about the structure of diagrams and other forms of multimodal discourse may be broadly conceptualised as multimodal discourse parsing. Recent examples of work in this area include alikhanietal2019 and ottoetal2019, who model discourse relations between natural language and photographic images, drawing on linguistic theories of coherence and text\u2013image relations, respectively. In most cases, however, predicting a single discourse relation covers only a part of the discourse structure. sachanetal2019 note that there is a need for comprehensive theories and models of multimodal communication, as they can be used to rethink tasks that have been previously considered only from the perspective of natural language processing.",
                "Unlike many other areas, the study of diagrammatic representations is particularly well-resourced, as several multimodal resources have been published recently to support research on computational processing of diagrams BIBREF10, BIBREF8, BIBREF11. This study compares two such resources, AI2D BIBREF10 and AI2D-RST BIBREF11, which both feature the same diagrams, as the latter is an extension of the former. Whereas AI2D features crowd-sourced, non-expert annotations, AI2D-RST provides multiple layers of expert annotations, which are informed by state-of-the-art approaches to multimodal communication BIBREF12 and annotation BIBREF13, BIBREF14."
            ],
            "predicted_section": [
                "Introduction",
                "Data"
            ]
        },
        "a57e266c936e438aeeab5e8d20d9edd1c15a32ee": {
            "question_text": "Are annotators familiar with the science topics annotated?",
            "from_paper": "1912.02866",
            "gold": [
                "The AI2D annotation schema models four types of diagram elements: text, graphics, arrows and arrowheads, whereas the semantic relations that hold between these elements are described using ten relations from a framework for analysing diagrammatic representations in engelhardt2002. Each diagram is represented using a Diagram Parse Graph (DPG), whose nodes stand for diagram elements while the edges between the nodes carry information about their semantic relations. The annotation for AI2D, which includes layout segmentations for the diagram images, DPGs and a multiple choice question-answer set, was created by crowd-sourced non-expert annotators on Amazon Mechanical Turk BIBREF10.",
                "AI2D-RST covers a subset of 1000 diagrams from AI2D, which have been annotated by trained experts using a new multi-layer annotation schema for describing the diagrams in AI2D BIBREF11. The annotation schema, which draws on state-of-the-art theories of multimodal communication BIBREF12, adopts a stand-off approach to describing the diagrams. Hence the three annotation layers in AI2D-RST are represented using three different graphs, which use the same identifiers for nodes across all three graphs to allow combining the descriptions in different graphs. AI2D-RST contains three graphs:"
            ],
            "gold_section": [
                "Data ::: Crowd-sourced Annotations from AI2D",
                "Data ::: Expert Annotations from AI2D-RST"
            ],
            "predicted": [
                "This provides an interesting setting for comparison and evaluation, as non-expert annotations are cheap to produce and easily outnumber the expert-annotated data, whose production consumes both time and resources. Expert annotations, however, incorporate domain knowledge from multimodality theory, which is unavailable via crowd-sourcing. Whether expert annotations provide better representations of diagrammatic structures and thus justify their higher cost is one question that this study seeks to answer.",
                "The promising results AI2D-RST suggest is that domain experts in multimodal communication should be involved in planning crowd-sourced annotation tasks right from the beginning. Segmentation, in particular, warrants attention as this phase defines the units of analysis: cut-outs and cross-sections, for instance, use labels and lines to pick out sub-regions of graphical objects, whereas in illustrations the labels often refer to entire objects. Such distinctions should preferably be picked out at the very beginning to be incorporated fully into the annotation schema.",
                "Tasks related to grouping and connectivity annotation could be crowd-sourced relatively easily, whereas annotating diagram types and discourse relations may require multi-step procedures and assistance in the form of prompts, as yungetal2019 have recently shown for RST. Involving both expert and crowd-sourced annotators could also alleviate problems related to circularity by forcing domain experts to frame the tasks in terms understandable to crowd-sourced workers BIBREF24."
            ],
            "predicted_section": [
                "Discussion",
                "Introduction"
            ]
        },
        "088d42ecb1e15515f6a97a0da2fed81b61d61a23": {
            "question_text": "Is this more effective for low-resource than high-resource languages?",
            "from_paper": "1909.00437",
            "gold": [
                "While mBERT outperforms MMTE on in-language training by a small margin of 0.16 points, MMTE beats mBERT by nearly 0.6 points in the zero-shot setting. Similar to results in XNLI, we see MMTE outperform mBERT on low resource languages. Since mBERT is SOTA for zero-shot cross-lingual transfer on POS tagging task BIBREF18, we also establish state-of-the-art on this dataset by beating mBERT in this setting.",
                "MMTE outperforms mBERT on 9 out of 15 languages and by 1.2 points on average. BERT achieves excellent results on English, outperforming our system by 2.5 points but its zero-shot cross-lingual transfer performance is weaker than MMTE. We see most gains in low resource languages such as ar, hi, ur, and sw. MMTE however falls short of the current state-of-the-art (SOTA) on XNLI BIBREF19. We hypothesize this might be because of 2 reasons: (1) They use only the 15 languages associated with the XNLI task for pre-training their model, and (2) They use both monolingual and parallel data for pre-training while we just use parallel data. We confirm our first hypothesis later in Section SECREF4 where we see that decreasing the number of languages in mNMT improves the performance on XNLI.",
                "We use BLEU score BIBREF15 to evaluate the quality of our translation model(s). Our mNMT model performs worse than the bilingual baseline on high resource language pairs but improves upon it on low resource language pairs. The average drop in BLEU score on 204 language pairs as compared to bilingual baselines is just 0.25 BLEU. This is impressive considering we are comparing one multilingual model to 204 different bilingual models. Table TABREF14 compares the BLEU scores achieved by mNMT to that of the bilingual baselines on 10 representative language pairs. These scores are obtained on an internal evaluation set which contains around 5k examples per language pair."
            ],
            "gold_section": [
                "Experiments and Results ::: XNLI: Cross-lingual NLI",
                "Massively Multilingual Neural Machine Translation Model ::: Pre-training ::: Model quality",
                "Experiments and Results ::: POS Tagging"
            ],
            "predicted": [
                "We find that there is a trade off between the number of languages in the multilingual model and efficiency of the learned representations due to the limited capacity. Scaling up the model to include more languages without diminishing transfer learning capability is a direction for future work. Finally, one could also consider integrating mBERT's objective with the translation objective to pre-train the mNMT system.",
                "Given the wide distribution of data across language pairs, we used a temperature based data balancing strategy. For a given language pair, $l$, let $D_l$ be the size of the available parallel corpus. Then if we adopt a naive strategy and sample from the union of the datasets, the probability of the sample being from language pair $l$ will be $p_l=\\frac{D_l}{\\Sigma _lD_l}$. However, this strategy would starve low resource language pairs. To control for the ratio of samples from different language pairs, we sample a fixed number of sentences from the training data, with the probability of a sentence belonging to language pair $l$ being proportional to $p_l^{\\frac{1}{T}}$, where $T$ is the sampling temperature. As a result, $T=1$ would correspond to a true data distribution, and, $T=100$ yields an (almost) equal number of samples for each language pair (close to a uniform distribution with over-sampling for low-resource language-pairs). We set $T=5$ for a balanced sampling strategy. To control the contribution of each language pair when constructing the vocabulary, we use the same temperature based sampling strategy with $T=5$. Our SPM vocabulary has a character coverage of $0.999995$.",
                "MMTE outperforms mBERT on 9 out of 15 languages and by 1.2 points on average. BERT achieves excellent results on English, outperforming our system by 2.5 points but its zero-shot cross-lingual transfer performance is weaker than MMTE. We see most gains in low resource languages such as ar, hi, ur, and sw. MMTE however falls short of the current state-of-the-art (SOTA) on XNLI BIBREF19. We hypothesize this might be because of 2 reasons: (1) They use only the 15 languages associated with the XNLI task for pre-training their model, and (2) They use both monolingual and parallel data for pre-training while we just use parallel data. We confirm our first hypothesis later in Section SECREF4 where we see that decreasing the number of languages in mNMT improves the performance on XNLI."
            ],
            "predicted_section": [
                "Experiments and Results ::: XNLI: Cross-lingual NLI",
                "Conclusion and Future Work",
                "Massively Multilingual Neural Machine Translation Model ::: Pre-training ::: Data sampling policy"
            ]
        },
        "cfdd583d01abaca923f5c466bb20e1d4b8c749ff": {
            "question_text": "what context aware models were experimented?",
            "from_paper": "1810.02268",
            "gold": [
                "This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.",
                "baseline Our baseline model is a standard bidirectional RNN model with attention, trained with Nematus. It operates on the sentence level and does not see any additional context. The input and output embeddings of the decoder are tied, encoder embeddings are not.",
                "concat22 We concatenate each sentence with one preceding sentence, for both the source and target side of the corpus. Then we train on this new data set without any changes to the model architecture. This very simple method is inspired by BIBREF5 .",
                "s-hier A multi-encoder architecture with hierarchical attention. This model has access to one additional context: the previous source sentence. It is read by a separate encoder, and attended to by an additional attention network. The output of the resulting two attention vectors is combined with yet another attention network.",
                "s-t-hier Identical to s-hier, except that it considers two additional contexts: the previous source sentence and previous target sentence. Both are read by separate encoders, and sequences from all encoders are combined with hierarchical attention.",
                "s-hier-to-2 The model has an additional encoder for source context, whereas the target side of the corpus is concatenated, in the same way as for concat22. This model achieved the best results in BIBREF9 .",
                "concat22 A simple concatentation model where only the training data is modified, in the same way as for the recurrent concat22 model.",
                "concat21 Trained on data where the preceding sentence is concatenated to the current one only on the source side. This model is also taken from BIBREF5 .",
                "BIBREF8 A more sophisticated context-aware Transformer that uses source context only. It has a separate encoder for source context, but all layers except the last one are shared between encoders. A source and context sentence are first encoded independently, and then a single attention layer and a gating function are used to produce a context-aware representation of the source sentence. Such restricted interaction with context is shown to be beneficial for analysis of contextual phenomena captured by the model. For details the reader is referred to their work."
            ],
            "gold_section": [
                "Context-Aware NMT Models",
                "Transformer Models",
                "Recurrent Models"
            ],
            "predicted": [
                "This section describes several context-aware NMT models that we use in our experiments. They fall into two major categories: models based on RNNs and models based on the Transformer architecture BIBREF2 . We experiment with additional context on the source side and target side.",
                "Our experiments are based on models from BIBREF9 , who have released their source code. We extend their models with parameter sharing, which was shown to be beneficial by BIBREF8 . Additionally, we consider a concatenative baseline, similar to BIBREF5 , and Transformer-based models BIBREF8 .",
                "We hope the test set will prove useful for empirically validating novel architectures for context-aware NMT. So far, we have only evaluated models that consider one sentence of context, but the nominal antecedent is more distant for a sizable proportion of the test set, and the evaluation of variable-size context models BIBREF7 , BIBREF10 is interesting future work."
            ],
            "predicted_section": [
                "Conclusions",
                "Context-Aware NMT Models"
            ]
        },
        "6295951fda0cfa2eb4259d544b00bc7dade7c01e": {
            "question_text": "Which model architecture do they use?",
            "from_paper": "1909.12079",
            "gold": [
                "Our FET approach is illustrated in Figure FIGREF4. It first constructs three representations: context representation, mention string representation, and KB type representation. Note that the KB type representation is obtained from a knowledge base through entity linking and is independent of the context of the mention.",
                "To obtain the context representation, we first use a special token $w_m$ to represent the mention (the token \u201c[Mention]\u201d in Figure FIGREF4). Then, the word sequence of the sentence becomes $w_1,...,w_{p_l-1},w_m,w_{p_l+1},...,w_n$. Their corresponding word embeddings are fed into two layers of BiLSTMs. Let $\\mathbf {h}_m^1$ and $\\mathbf {h}_m^2$ be the output of the first and the second layer of BiLSTMs for $w_m$, respectively. We use $\\mathbf {f}_c=\\mathbf {h}_m^1+\\mathbf {h}_m^2$ as the context representation vector.",
                "Apart from the three representations, we also obtain the score returned by our entity linking algorithm, which indicates its confidence on the linking result. We denote it as a one dimensional vector $\\mathbf {g}$. Then, we get $\\mathbf {f}=\\mathbf {f}_c\\oplus \\mathbf {f}_s\\oplus \\mathbf {f}_e\\oplus \\mathbf {g}$, where $\\oplus $ means concatenation. $\\mathbf {f}$ is then fed into an MLP that contains three dense layers to obtain $\\mathbf {u}_m$, out final representation for the current mention sample $m$. Let $t_1,t_2,...,t_k$ be all the types in $T$, where $k=|T|$. We embed them into the same space as $\\mathbf {u}_m$ by assigning each of them a dense vector BIBREF15. These vectors are denoted as $\\mathbf {t}_1,...,\\mathbf {t}_k$. Then the score of the mention $m$ having the type $t_i\\in T$ is calculated as the dot product of $\\mathbf {u}_m$ and $\\mathbf {t}_i$:"
            ],
            "gold_section": [
                "Method ::: Fine-grained Entity Typing Model ::: Input",
                "Method ::: Fine-grained Entity Typing Model ::: Prediction",
                "Method ::: Fine-grained Entity Typing Model ::: Context Representation"
            ],
            "predicted": [
                "We use Ours (Full) to represent our full model, and also compare with five variants of our own approach: Ours (DirectTrain) is trained without adding random person types while obtaining the KB type representation, and $\\lambda _P$ is set to 1; Ours (NoEL) does not use entity linking, i.e., the KB type representation and the entity linking confidence score are removed, and the model is trained in DirectTrain style; Ours (NonDeep) uses one BiLSTM layer and replaces the MLP with a dense layer; Ours (NonDeep NoEL) is the NoEL version of Ours (NonDeep); Ours (LocAttEL) uses the entity linking approach proposed in BIBREF19 instead of our own commonness based approach. Ours (Full), Ours (DirectTrain), and Ours (NonDeep) all use our own commonness based entity linking approach.",
                "BIBREF13 and BIBREF14 are two studies that are most related to this paper. BIBREF13 propose an unsupervised FET system where EL is an importat component. But they use EL to help with clustering and type name selection, which is very different from how we use it to improve the performance of a supervised FET model. BIBREF14 finds related entities based on the context instead of directly applying EL. The types of these entities are then used for inferring the type of the mention.",
                "We propose a deep neural fine-grained entity typing model that utilizes type information from KB obtained through entity linking."
            ],
            "predicted_section": [
                "Related Work",
                "Introduction",
                "Experiments ::: Compared Methods"
            ]
        },
        "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa": {
            "question_text": "How does labeling scheme look like?",
            "from_paper": "2003.11687",
            "gold": [
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme",
                "abb: represents abbreviations such as TRL representing Technology Readiness Level.",
                "grp: represents a group of people or an individual such as Electrical Engineers, Systems Engineers or a Project Manager.",
                "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.",
                "opcon: represents operational concepts such as decision analysis process, technology maturity assessment, system requirements review, etc.",
                "seterm: represents generic terms that are frequently used in SE text and those that do not fall under syscon or opcon such as project, mission, key performance parameter, audit etc.",
                "event: represents event-like information in SE text such as Pre-Phase A, Phase A, Phase B, etc.",
                "org: represents an organization such as `NASA', `aerospace industry', etc.",
                "art: represents names of artifacts or instruments such as `AS1300'",
                "cardinal: represents numerical values such as `1', `100', 'one' etc.",
                "loc: represents location-like entities such as component facilities or centralized facility.",
                "mea: represents measures, features, or behaviors such as cost, risk, or feasibility."
            ],
            "gold_section": [
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme"
            ],
            "predicted": [
                "cardinal: represents numerical values such as `1', `100', 'one' etc.",
                "syscon: represents any system concepts such as engineering unit, product, hardware, software, etc. They mostly represent physical concepts.",
                "loc: represents location-like entities such as component facilities or centralized facility."
            ],
            "predicted_section": [
                "CONCEPT RECOGNITION ::: BIO Labelling Scheme"
            ]
        },
        "31e6062ba45d8956791e1b86bad7efcb6d1b191a": {
            "question_text": "What word embeddings are used?",
            "from_paper": "1703.10152",
            "gold": [
                "The third model is constructed for the purpose of improving classification results for a certain category. In this study specifically, the optimization task was focused on identifying the category INLINEFORM0 . In this study, INLINEFORM1 specific word embeddings were trained ( INLINEFORM2 ) inspired by Tang et al. BIBREF21 's model: Sentiment-Specific Word Embedding (unified model: INLINEFORM3 ). After obtaining the word vectors via INLINEFORM4 , the same scheme was used to average the vectors in one sentence as in the model INLINEFORM5 ."
            ],
            "gold_section": [
                "Models"
            ],
            "predicted": [
                "The characteristics of word embeddings based on different model and dataset are listed in Table. TABREF12 .",
                "The learned word embeddings are input into a classifier as features under a supervised machine learning framework. Similar to sentiment classification using word embeddings BIBREF21 , where they try to predict each tweet to be either positive or negative, in the task of AZ, the embeddings are used to classify each sentence into one of the seven categories.",
                "In general, the classification performance of word embeddings is competitive in terms of F-measure for most of the categories. But for classifying the categories AIM, BAS and OWN, the manually crafted features proposed by Teufel et al. BIBREF2 gave better results."
            ],
            "predicted_section": [
                "Discussion",
                "Classification and evaluation",
                "Training strategy"
            ]
        },
        "c0355afc7871bf2e12260592873ffdb5c0c4c919": {
            "question_text": "What is their baseline?",
            "from_paper": "1909.10481",
            "gold": [
                "We first conduct experiments on the supervised English-English QG setting. We compare our model to the following baselines:",
                "CorefNqg BIBREF33 A sequence-to-sequence model with attention mechanism and a feature-rich encoder.",
                "Mp-Gsn BIBREF31 A sequence-to-sequence model with gated self-attention and maxout pointer mechanism.",
                "Xlm BIBREF5 The current state-of-the-art cross-lingual pre-training model. We initialize the Transformer-based sequence-to-sequence model with pre-trained XLM.",
                "We conduct experiments on the zero-shot Chinese-Chinese QG task to evaluate the cross-lingual transfer ability. In this task, models are trained with English QG data but evaluated with Chinese QG examples. We include the following models as our baselines:",
                "Xlm Fine-tuning XLM with the English QG data.",
                "Pipeline (Xlm) The pipeline of translating input Chinese sentences into English first, then performing En-En-QG with the XLM model, and finally translating back to the Chinese. We use the Transformer as the translator, which is also trained on the MultiUN dataset.",
                "Pipeline (Xlm) with Google Translator Same to Pipeline (Xlm) but using Google Translator to translate the texts."
            ],
            "gold_section": [
                "Experiments ::: Question Generation ::: English-English Question Generation",
                "Experiments ::: Question Generation ::: Chinese-Chinese Question Generation"
            ],
            "predicted": [
                "In the zero-shot English-Chinese question generation experiments, we use Xlm and Pipeline (Xlm) as our baselines. Pipeline (Xlm) is a pipeline method that uses En-En-QG with Xlm to generate questions, and then translates the results to Chinese. Because there is no annotations for En-Zh-QG, we perform human evaluation studies for this setting. Table TABREF19 shows the human evaluation results, where our model surpasses all the baselines especially in terms of relatedness and correctness.",
                "We evaluate models with BLEU-4 (BL-4), ROUGE (RG) and METEOR (MTR) metrics. As shown in Table TABREF16, our model outperforms the baselines, which demonstrates that our pre-trained model provides a good initialization for NLG.",
                "In the zero-shot setting, we only use English data for training, and directly evaluate the model on other languages. In Table TABREF22 and Table TABREF23, we present the results for French/Chinese AS, which are evaluated by the ROUGE-1, ROUGE-2 and ROUGE-L metrics. We also report the results of supervised AS in Table TABREF21 for reference. We find that Xnlg outperforms all the baseline models on both French and Chinese AS. Comparing with French, there is a larger gap between baselines and our model on zero-shot Chinese AS, which indicates that the error propagation issue is more serious on distant language pairs."
            ],
            "predicted_section": [
                "Experiments ::: Question Generation ::: English-English Question Generation",
                "Experiments ::: Question Generation ::: English-Chinese Question Generation",
                "Experiments ::: Abstractive Summarization ::: Zero-Shot Summarization"
            ]
        },
        "67cb001f8ca122ea859724804b41529fea5faeef": {
            "question_text": "what are the state of the art methods they compare with?",
            "from_paper": "1805.07882",
            "gold": [
                "We observed that no word embedding has strong results on all the tasks. Although trained on the paraphrase database and having the highest INLINEFORM0 , the SL999 embedding could not outperform the Glove embedding in SICK-R. HCTI BIBREF5 , which is the current state-of-the-art in the group of neural representation models on STSB, also used the Glove embedding. However, the performance of HTCI in STSB ( INLINEFORM1 ) is lower than our model using the Glove embedding. In SICK-R, InferSent BIBREF23 achieves a strong performance ( INLINEFORM2 ) using the Glove embedding with transfer knowledge, while our model with only the Glove embedding achieves a performance close to the performance of InferSent. These results confirm the efficiency of Multi-level comparison.",
                "We evaluate our M-MaxLSTM-CNN model on three tasks: STS, textual entailment recognition, paraphrase identification. The advantages of M-MaxLSTM-CNN are: i) simple but efficient for combining various pre-trained word embeddings with different dimensions; ii) using Multi-level comparison shows better performances compared to using only sentence-sentence comparison; iii) does not require hand-crafted features (e.g., alignment features, Ngram overlaps, syntactic features, dependency features) compared to the state-of-the-art ECNU BIBREF6 on STS Benchmark dataset."
            ],
            "gold_section": [
                "Evaluation of exploiting multiple pre-trained word embeddings",
                "Introduction"
            ],
            "predicted": [
                "We report the results of these methods in Table TABREF49 . Overall, our M-MaxLSTM-CNN shows competitive performances in these tasks. Especially in the STS task, M-MaxLSTM-CNN outperforms the state-of-the-art methods on the two datasets. Because STSB includes complicated samples compared to SICK, the performances of methods on STSB are quite lower. In STSB, the prior top performance methods use ensemble approaches mixing hand-crafted features (word alignment, syntactic features, N-gram overlaps) and neural sentence representations, while our approach is only based on a neural sentence modeling architecture. In addition, we observed that InferSent shows the strong performance on SICK-R but quite low on STSB while our model consistently obtains the strong performances on both of the datasets. InferSent uses transfer knowledge on textual entailment data, consequently it obtains the strong performance on this entailment task.",
                "At SemEval-2017 STS task, hybrid approaches obtain strong performances. BIBREF24 train a linear regression model with WordNet, alignment features and the word embedding word2vec. BIBREF6 develop an ensemble model with multiple boosting techniques (i.e., Random Forest, Gradient Boosting, and XGBoost). This model incorporates traditional features (i.e., n-gram overlaps, syntactic features, alignment features, bag-of-words) and sentence modeling methods (i.e., Averaging Word Vectors, Projecting Averaging Word Vectors, LSTM).",
                "Besides existing methods, we also compare our model with several sentence modeling approaches using multiple pre-trained word embeddings:"
            ],
            "predicted_section": [
                "Related work",
                "Overall evaluation"
            ]
        },
        "18fbfb1f88c5487f739aceffd23210a7d4057145": {
            "question_text": "what models did they compare with?",
            "from_paper": "1907.05338",
            "gold": [
                "In the sequence labeling task,we explore sub-task named entity recognition using CoNLL03 dataset BIBREF5 , which is a public available used in many studies to test the accuracy of their proposed methods BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF1 . For strategy finetune-only and strategy stack-and-finetune, we implemented two models: one with BERT and the other with BERT adding a Bi-LSTM on top. Eval measure is accuracy and F1 score.",
                "In the task of text categorization, we used Yahoo Answer Classification Dataset. The Dataset is consists of 10 classes, but due to the huge amount of the dataset, we just select two class of them. As for the upper model,we choose DenseNet BIBREF33 and HighwayLSTM BIBREF34 .",
                "The DenseNet structure contains four independent blocks and each block has four CNNs connected by residual. We initialize word embedding in the word representation layer with BERT. We initialize each character as a 768-dimension vector. In the experiment of training DenseNet,we concat the output vector of DenseNet with [CLS] for prediction.",
                "We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score.",
                "Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again."
            ],
            "gold_section": [
                "Experiment A: Sequence Labeling",
                "Experiment B: Text Classification",
                "Experiment C: Semantic Similarity Tasks"
            ],
            "predicted": [
                "Apart from the BERT fine-tuning only model and BERT+ BIMPM model, we also devise two new network structures by modifying the BIMPM model. In the first model is to remove the first bi-LSTM of BIMPM, which is the input layer for the matching layer in BIMPM. In the second model, we combine the matching layer of BIMPM and with a transformer BIBREF16 , a model we call Sim-Transformer by replacing the output layer of the matching layer, originally a bi-LSTM model, with a transformer model. From the experimental results shown in Table 4, we can see that due to the strong expressive ability of the BERT, there is almost no difference in the experimental results of removing the first bi-LSTM and BIMPM. In addition, we also find that Sim-Transformer's performance without fine-tuning is nearly four percentage points lower than BIMPM, but it out-performs BIMPM after fine-tuning. In general, the results show that BERT + Sim-Transformer out-performs BERT-only model by 4.7%, thus confirming our hypotheses again.",
                "The introduction of pre-trained language models, such as BERT BIBREF1 and Open-GPT BIBREF2 , among many others, has brought tremendous progress to the NLP research and industrial communities. The contribution of these models can be categorized into two aspects. First, pre-trained language models allow modelers to achieve reasonable accuracy without the need an excessive amount of manually labeled data. This strategy is in contrast with the classical deep learning methods, which requires a multitude more data to reach comparable results. Second, for many NLP tasks, including but not limited to, SQuAD BIBREF3 , CoQA BIBREF4 , named entity recognition BIBREF5 , Glue BIBREF6 , machine translation BIBREF7 , pre-trained model allows the creation of new state-of-art, given a reasonable amount of labelled data.",
                "We find the ensembled model enjoys a 0.72% improvements compared to the fine-tune only model and 0.005 improvement for the F1 score."
            ],
            "predicted_section": [
                "Experiment B: Text Classification",
                "Experiment C: Semantic Similarity Tasks",
                "Introduction"
            ]
        },
        "9c1f70affc87024b4280f0876839309b8dddd579": {
            "question_text": "How did they annotate the corpus?",
            "from_paper": "2003.08437",
            "gold": [
                "Corpus Annotation ::: Preprocessing ::: Tokenization",
                "After automatic tokenization using Jieba, we conducted manual corrections to ensure that all potential adpositions occur as separate tokens, closely following the Chinese Penn Treebank segmentation guidelines BIBREF39. The final corpus includes all 27 chapters of The Little Prince, with a total of 20k tokens.",
                "Corpus Annotation ::: Preprocessing ::: Adposition Targets",
                "All annotators jointly identified adposition targets according to the criteria discussed in subsec:adpositioncriteria. Manual identification of adpositions was necessary as an automatic POS tagger was found unsuitable for our criteria (sec:adpositionidentification).",
                "Corpus Annotation ::: Preprocessing ::: Data Format",
                "Though parsing is not essential to this annotation project, we ran the StanfordNLP BIBREF40 dependency parser to obtain POS tags and dependency trees. These are stored alongside supersense annotations in the CoNLL-U-Lex format BIBREF41, BIBREF0. CoNLL-U-Lex extends the CoNLL-U format used by the Universal Dependencies BIBREF42 project to add additional columns for lexical semantic annotations.",
                "Corpus Annotation ::: Reliability of Annotation",
                "The corpus is jointly annotated by three native Mandarin Chinese speakers, all of whom have received advanced training in theoretical and computational linguistics. Supersense labeling was performed cooperatively by 3 annotators for 25% (235/933) of the adposition targets, and for the remainder, independently by the 3 annotators, followed by cooperative adjudication. Annotation was conducted in two phases, and therefore we present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese."
            ],
            "gold_section": [
                "Corpus Annotation ::: Preprocessing ::: Adposition Targets",
                "Corpus Annotation ::: Preprocessing ::: Tokenization",
                "Corpus Annotation ::: Preprocessing ::: Data Format",
                "Corpus Annotation ::: Reliability of Annotation"
            ],
            "predicted": [
                "We chose to annotate the novella The Little Prince because it has been translated into hundreds of languages and dialects, which enables comparisons of linguistic phenomena across languages on bitexts. This is the first Chinese corpus to undergo SNACS annotation. Ongoing adpositional supersense projects on The Little Prince include English, German, French, and Korean. In addition, The Little Prince has received large attention from other semantic frameworks and corpora, including the English BIBREF38 and Chinese BIBREF26 AMR corpora.",
                "",
                "Though parsing is not essential to this annotation project, we ran the StanfordNLP BIBREF40 dependency parser to obtain POS tags and dependency trees. These are stored alongside supersense annotations in the CoNLL-U-Lex format BIBREF41, BIBREF0. CoNLL-U-Lex extends the CoNLL-U format used by the Universal Dependencies BIBREF42 project to add additional columns for lexical semantic annotations."
            ],
            "predicted_section": [
                "Corpus Annotation",
                "Corpus Annotation ::: Preprocessing ::: Data Format",
                "Corpus Annotation ::: Reliability of Annotation"
            ]
        },
        "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a": {
            "question_text": "How does the proposed training framework mitigate the bias pattern?",
            "from_paper": "1909.04242",
            "gold": [
                "When the annotation artifacts of the training set cannot be generalized to the testing set, which should be more common in the real-world, predicting by artifacts may hurt models' performance. Centering on the results of JOCI, in which the bias pattern of MultiNLI is misleading, we find that Norm trained with MultiNLI outperforms baseline after debiasing with all smooth values tested.",
                "Furthermore, debiasing can reduce models' dependence on the bias pattern during training, thus force models to better learn semantic information to make predictions. Norm trained with SNLI exceed baseline in JOCI with smooth terms $0.01$ and $0.1$. With larger smooth terms, Norm trained with both SNLI and MultiNLI exceeds baseline in SICK. Given the fact that JOCI is almost neutral to artifacts in SNLI, and the bias pattern of both SNLI and MultiNLI are even predictive in SICK, we owe these promotions to that our method improves models' semantic learning ability."
            ],
            "gold_section": [
                "Experimental Results ::: Debiasing Results ::: Benefits of Debiasing"
            ],
            "predicted": [
                "Classifiers trained on NLI datasets are supposed to make predictions by understanding the semantic relationships between given sentence pairs. However, it is shown that models are unintentionally utilizing the annotation artifacts BIBREF4, BIBREF2. If the evaluation is conducted under a similar distribution as the training data, e.g., with the given testing set, models will enjoy additional advantages, making the evaluation results over-estimated. On the other hand, if the bias pattern cannot be generalized to the real-world, it may introduce noise to models, thus hurting the generalization ability.",
                "In this paper, we take a close look at the annotation artifacts in NLI datasets. We find that the bias pattern could be predictive or misleading in cross-dataset testing. Furthermore, we propose a debiasing framework and experiments demonstrate that it can effectively mitigate the impacts of the bias pattern and improve the cross-dataset generalization ability of models. However, it remains an open problem that how we should treat the annotation artifacts. We cannot assert whether the bias pattern should not exist at all or it is actually some kind of nature. We hope that our findings will encourage more explorations on reliable evaluation protocols for NLI models.",
                "In this paper, we use cross-dataset testing to better assess models' generalization ability. We investigate the impacts of annotation artifacts in cross-dataset testing. Furthermore, we propose an easy-adopting debiasing training framework, which doesn't require any additional data or annotations, and apply it to the high-performing Densely Interactive Inference Network BIBREF5. Experiments show that our method can effectively mitigate the bias pattern and improve the cross-dataset generalization ability of models. To the best of our knowledge, our work is the first attempt to alleviate the annotation artifacts without any extra resources."
            ],
            "predicted_section": [
                "Introduction",
                "Conclusion"
            ]
        },
        "5d03a82a70f7b1ab9829891403ec31607828cbd5": {
            "question_text": "Is the morphology detection task evaluated?",
            "from_paper": "1912.10162",
            "gold": [
                "At the results, POS and morph classes refer to the tag labels explained in SECREF4, whilst only POS classes relate to annotated labels that describe only the part of speech. It is evident that even though the CC vectors are noisy, coming from a web source, they lead to better results than Wikipedia, possibly because they have a larger variety of tokens.",
                "Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7."
            ],
            "gold_section": [
                "Creating a Greek POS Tagger using spaCy ::: Evaluation and comparison of results",
                "Creating a Greek POS Tagger using spaCy ::: Creation of the Tag Map with reference to Universal Dependencies"
            ],
            "predicted": [
                "In the following chapters the process for implementing Part of Speech Tagging and Named Entity Recognition for the Greek Language is explained. A dataset with extended POS Tags was found and matched to a set of morphological rules, according to a treebank. The dataset was then processed, fed to the spaCy model and used for training. Similarly, for Named Entity Recognition, datasets from different sources were compared to a custom set of rules for named entities. Finally, different experiments were conducted for evaluating the accuracy of the models.",
                "It seemed that the average F1 score was higher for the Makedonia corpus, as it was the basis of the configuration for the keyword list. In order to have an objective evaluation, the results of each corpus per entity class were observed.",
                "In an experiment worth mentioning the correlation of the part of speech with the performance of the recognizer was explored. In this experiment, both pipelines (part of speech, entity recognition) were used for training with 30 iterations and the model was trained twice: with and without the usage of the part of speech information for recognition."
            ],
            "predicted_section": [
                "Creating a state of the art Named Entity Recognizer using spaCy ::: Evaluation and comparison of results",
                "Introduction"
            ]
        },
        "6cad6f074b0486210ffa4982c8d1632f5aa91d91": {
            "question_text": "How does the model proposed extend ENAMEX?",
            "from_paper": "1912.10162",
            "gold": [
                "In order to gain more information about the context of the Greek entities, a percentage of Greek Wikipedia was used. After applying sentence and token segmentation on Wikipedia text and using a pretrained model from polyglot, the keyword list increased. The keyword list had at this point about 350,000 records and consisted of 4 classes: location (LOC), organization (ORG), person (PERSON) and facility (FAC). A percentage of Greek Wikipedia was parsed and used for training in spaCy. The results from the training are presented in SECREF13."
            ],
            "gold_section": [
                "Creating a state of the art Named Entity Recognizer using spaCy ::: Usage of Wikipedia dataset for training"
            ],
            "predicted": [
                "In the following chapters the process for implementing Part of Speech Tagging and Named Entity Recognition for the Greek Language is explained. A dataset with extended POS Tags was found and matched to a set of morphological rules, according to a treebank. The dataset was then processed, fed to the spaCy model and used for training. Similarly, for Named Entity Recognition, datasets from different sources were compared to a custom set of rules for named entities. Finally, different experiments were conducted for evaluating the accuracy of the models.",
                "SpaCy uses a deep learning formula for implementing NLP models, summarised as \u201cembed, encode, attend, predict\u201d. In spaCy's approach text is inserted in the model in the form of unique numerical values (ID) for every input that can represent a token of a corpus or a class of the NLP task (part of speech tag, named entity class). At the embedding stage, features such as the prefix, the suffix, the shape and the lowercase form of a word are used for the extraction of hashed values that reflect word similarities.",
                "At prediction, a Softmax function is used for the prediction of a super tag with part of speech and morphology information. Similarly for named entities, the available class is predicted. After the training process of the model, the CNN is able to be used for NLP tasks."
            ],
            "predicted_section": [
                "SpaCy's deep learning model for POS tagging and Named Entity Recognition",
                "Introduction"
            ]
        },
        "d38b3e0896b105d171e69ce34c689e4a7e934522": {
            "question_text": "Which morphological features are extracted?",
            "from_paper": "1912.10162",
            "gold": [
                "Different labels were found at the dataset and were matched to a label map, where for each label the part of the speech and their morphology are analyzed. In more detail, the first two characters refer to the part of speech and accordingly extend to more information about it. The label map supports 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other. Each tag describes morphological features of the word, depending on the part of the speech to which it refers like the gender, the number, and the case BIBREF6. It must be mentioned that the extraction of morphological rules and the matching with the tags was done using the Greek version of the Universal Dependencies BIBREF7."
            ],
            "gold_section": [
                "Creating a Greek POS Tagger using spaCy ::: Creation of the Tag Map with reference to Universal Dependencies"
            ],
            "predicted": [
                "In the following chapters the process for implementing Part of Speech Tagging and Named Entity Recognition for the Greek Language is explained. A dataset with extended POS Tags was found and matched to a set of morphological rules, according to a treebank. The dataset was then processed, fed to the spaCy model and used for training. Similarly, for Named Entity Recognition, datasets from different sources were compared to a custom set of rules for named entities. Finally, different experiments were conducted for evaluating the accuracy of the models.",
                "Part of Speech Tagging for highly inflective languages, such as Greek is quite a difficult task. In the Greek Language, words can have different morphological forms, depending on the part of speech (verbs have up to ten different forms). For that purpose, there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1.",
                "Another main task for extracting semantic information is Named Entity Recognition (NER). Named Entity Recognition is a process where a word or a set of words reference to a world object. Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX) BIBREF2."
            ],
            "predicted_section": [
                "Introduction"
            ]
        },
        "74396ead9f88a9efc7626240ce128582ab69ef2b": {
            "question_text": "by how much did their approach outperform previous work?",
            "from_paper": "1806.03369",
            "gold": [
                "The results, including each of the training scenarios noted earlier, are presented in Table TABREF18 . Precision, recall, and F1 on the positive (sarcastic) class were recorded. The highest F1 achieved (0.780) among all cases was from training on the EasyAdapted Twitter and Amazon data. In comparison, training only on the Amazon reviews produced an F1 of 0.713 (training and testing only on Amazon reviews with our features but with the same classifier and cross-validation settings as BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 led to an F1 of 0.752, outperforming prior best results on that dataset). Training on both without EasyAdapt led to an F1 of 0.595 (or 0.715 when training only on Amazon-specific features), and finally, training only on Twitter data led to an F1 of 0.276. Training and testing on Twitter produced an F1 of 0.583 when training on all features."
            ],
            "gold_section": [
                "Results"
            ],
            "predicted": [
                "Overall, the system cut the error rate from .256 to .220, representing a 14% relative reduction in error over prior best results on the Amazon dataset. Our results testing on Twitter are not directly comparable to others, since prior work's datasets could not be released; however, our results ( INLINEFORM0 =0.583) are in line with those reported previously ( BIBREF4 RiloffSarcasm: INLINEFORM1 =0.51; BIBREF13 davidov-tsur-rappoport:2010:CONLL: INLINEFORM2 =0.545). Additionally, our Twitter data did not contain many indicators shown to be discriminative in the past (leading our general features to be better predictors of sarcasm even when training/testing entirely within the domain), and our focus in developing features was on general performance rather than performance on Twitter specifically.",
                "Finally, we include the best results reported by BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 on the same Amazon dataset. For a more direct comparison between our work and theirs, we also report the results from using all of our features under the same classification conditions as theirs (10-fold cross-validation using scikit-learn's Logistic Regression, tuning with an F1 objective). We refer to the latter case as Our Results, Same Classifier as Prior Best.",
                "Regarding the general features developed for this work, the polarity- and subjectivity-based features performed well, while performance using only PMI features was lower. PMI scores in particular may have been negatively impacted by common Twitter characteristics, such as the trend to join keywords together in hashtags, and the use of acronyms that are unconventional in other domains. These issues could be addressed to some extent in the future via word segmentation tools, spell-checkers, and acronym expansion."
            ],
            "predicted_section": [
                "Discussion",
                "Evaluation"
            ]
        },
        "87c00edc497274ae6a972c3097818de85b1b384f": {
            "question_text": "How does sentence construction component works?",
            "from_paper": "1909.08250",
            "gold": [
                "To generate a sentence, we need a sentence structure and vocabularies. Our system is developed to emulate the process of a person learning a new language and has to make guesses to understand new sentences from time to time. For example, someone, who understands the sentence \u201cBill plays a game\u201d would not fully understand the sentence \u201cBill plays a popular board game\u201d without knowing the meaning of \u201cpopular\u201d and \u201cboard game\u201d but could infer that the latter sentence indicates that its subject plays a type of game.",
                "The overall design of our system is given in Figure FIGREF7. Given a paragraph, our system produces a GF program (a pair of an abstract and a concrete syntax), which can be used for sentence generation. The system consists of two components, understanding sentences and generating GF grammar. The first component is divided into two sub-components, one for recognizing the sentence structure and one for recognizing the sentence components. The second component consists of a GF grammar encoder and a GF grammar exporter. The encoder is responsible for generating a GF grammar for each sentence, while the exporter aggregates the grammars generated from the encoder, and produces a comprehensive grammar for the whole paragraph.",
                "Method ::: Sentence Structure Recognition",
                "The sentence structure recognition process involves 2 modules: natural language processing (NLP) module and logical reasoning on result from NLP module. In this paper, we make use of the Stanford Parser tools described in BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14",
                "The NLP module tokenizes the input free text to produce a dependency-based parse tree and part-of-speech tag (POS tag). The dependency-based parse tree and the POS tag are then transform into an answer set program (ASP) BIBREF15 which contains only facts. Table TABREF13 shows the transformation of the result of NLP module into an ASP program for the sentence \u201cBill plays a game\u201d. In this table, nsubj, det, dobj and punct denote relations in the dependency-based parse tree, and mean nominal subject, determiner, direct object and punctuation respectively. Full description of all relations in a dependency-based parse tree can be found in the Universal Dependency website. The second set of notations are the POS tag PRP, VBP, DT and NN corresponding to pronoun, verb, determiner and noun. Readers can find the full list of POS tag in Penn Treebank Project.",
                "From the collection of the dependency atoms from the dependency-based parse tree, we determine the structure of a sentence using an ASP program, called $\\Pi _1$ (Listing ).",
                "Each of the rule above can be read as if the right-hand side is true then the left-hand side must be true. These rules define five possible structures of a sentence represented by the atom structure(x,y). $x$ and $y$ in the atom structure(x,y) denote the type of the structure and the number of dependency relations applied to activate the rule generating this atom, respectively. We refer to $y$ as the $i$-value of the structure. For example, $structure(1,1)$ will be recognized if the nsubj relation is in the dependency-based parse tree; $structure(3,3)$ needs 3 dependency relations to be actived: nsubj, xcomp and dobj. We often use structure #$x$ to indicate a structure of type $x$.",
                "Together with the collection of the atoms encoding the relations in the dependency-based parse tree, $\\Pi _1$ generates several atoms of the form $structure(x,y)$ for a sentence. Among all these atoms, an atom with the highest $i$-value represents the structure constructed using the highest number of dependency relations. And hence, that structure is the most informative structure that is recoginized for the sentence. Observe that $structure(1,1)$ is the most simplified structure of any sentence.",
                "Method ::: Sentence Components Recognition",
                "The goal of this step is to identify the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the dependency-based parse tree. For example, the sentence \u201cBill plays a game\u201d is encoded by a structure #2 and we expect that Bill, plays, and game correspond to the subject, verb, and object, respectively.",
                "We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\Pi _2$ (Listing ). The first four rules of $\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\Pi _2$.",
                "The result of program $\\Pi _2$ is an one-to-one mapping of some of the words in the sentence into the importaint components of a sentence, called main components, i.e. subject, object and verb. The mapping is constructed by using the core arguments in Universal Dependency Relations . Since not every word in the sentence is in a core argument relation, there are some words in the sentence that are not in the domain of the mapping that $\\Pi _2$ produces. We denote these words are complement components. To identify these words, we encode the Non-core dependents and Nominal dependents from Universal Dependency Relations into the set of rules in program $\\Pi _3$.",
                "Program $\\Pi _3$ (Listing ), together with the atoms extracted from the dependency-based parse tree such as $compound(P,N)$ ($N$ is compound noun at the position $P$ in the sentence), $amod(P,J)$ ($J$ is an adjective modifier), etc., is used to identify the complement components of the main components computed by $\\Pi _2$ while maintaining the structure of the sentence created by $\\Pi _1$. For example, a complement of a noun could be another noun (as \u201cboard\u201d in \u201cboard game\u201d), or an adjective (as \u201cpopular\u201d in \u201cpopular board game\u201d), or a preposition (as \u201cfor adults\u201d in \u201cboard game for adults\u201d).",
                "The input of Program $\\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps.",
                "Method ::: GF Grammar Encoder",
                "The goal of the encoder is to identify appropriate GF rules for the construction of a GF grammar of a sentence given its structure and its components identified in the previous two modules. This is necessary since a sentence can be encoded in GF by more than one set of rules; for example, the sentence \u201cBill wants to play a game\u201d can be encoded by the rules",
                "In GF, NP, VV, V2, VP, and Cl stand for noun phrase, verb-phrase-complement verb, two-place verb, verb phrase and clause, respectively. Note that although the set of GF grammatical rules can be used to construct a constituency-based parse tree , the reverse direction is not always true. To the best of our knowledge, there exists no algorithm for converting a constituency-based parse tree to a set GF grammar rules. We therefore need to identify the GF rules for each sentence structure.",
                "In our system, a GF rule is assigned to a structure initially (Table TABREF19). Each rule in Table TABREF19 represents the first level of the constituency-based parse tree. It acts as the coordinator for all other succeeding rules.",
                "Given the seed components identified in Section SECREF15 and the above GF rules, a GF grammar for each sentence can be constructed. However, this grammar can only be used to generate fairly simple sentences. For example, for the sentence \u201cBill plays a popular board game with his close friends.\u201d, a GF grammar for structure #2 can be constructed, which can only generate the sentence \u201cBill plays game.\u201d because it does not contain any complement components identified in Section SECREF15. Therefore, we assgin a set of GF rules for the construction of each parameter in the GF rules in Table TABREF19. The set of GF rules has to follow two conventions. The first one is after applying the set of rules to some components of the sentence, the type of the production is one of the type in Table TABREF19, e.g. $NP$, $VP$, $Cl$, $V2$, .... The second convention is that the GF encoder will select the rules as the order from top to bottom in Table TABREF20. Note that the encoder always has information of what type of input and output for the rule it is looking for.",
                "For instance, we have \u201cgame\u201d is the object (main components), and we know that we have to construct \u201cgame\u201d in the result GF grammar to be a NP (noun phrase). Program $\\Pi _2$ identifies that there are two complement components for the word \u201cgame\u201d, which are \u201cboard\u201d and \u201cpopular\u201d, a noun and an adjective respectively. The GF encoder then select the set of rules: N $\\rightarrow $ N $\\rightarrow $ CN and A $\\rightarrow $ AP to create the common noun \u201cboard game\u201d and the adjective phrase first. The next rule is AP $\\rightarrow $ CN $\\rightarrow $ CN. The last rule to be applied is CN $\\rightarrow $ NP. The selection is easily decided since the input and the output of the rules are pre-determined, and there is no ambiguity in the selection process.",
                "The encoder uses the GF rules and the components identified by the previous subsections to produce different constructors for different components of a sentence. A part of the output of the GF encoder for the object \u201cgame\u201d is",
                "The encoder will also create the operators that will be included in the oper section of the GF grammar for supporting the new constructor. For example, the following operators will be generated for serving the Game constructor above:",
                "Method ::: GF Grammar Exporter",
                "The GF Grammar Exporter has the simplest job among all modules in the system. It creates a GF program for a paragraph using the GF grammars created for the sentences of the paragraph. By taking the union of all respective elements of each grammar for each sentence, i.e., categories, functions, linearizations and operators, the Grammar Exporter will group them into the set of categories (respectively, categories, functions, linearizations, operators) of the final grammar."
            ],
            "gold_section": [
                "Method ::: Sentence Components Recognition",
                "Method",
                "Method ::: Sentence Structure Recognition",
                "Method ::: GF Grammar Exporter",
                "Method ::: GF Grammar Encoder"
            ],
            "predicted": [
                "The result of program $\\Pi _2$ is an one-to-one mapping of some of the words in the sentence into the importaint components of a sentence, called main components, i.e. subject, object and verb. The mapping is constructed by using the core arguments in Universal Dependency Relations . Since not every word in the sentence is in a core argument relation, there are some words in the sentence that are not in the domain of the mapping that $\\Pi _2$ produces. We denote these words are complement components. To identify these words, we encode the Non-core dependents and Nominal dependents from Universal Dependency Relations into the set of rules in program $\\Pi _3$.",
                "The input of Program $\\Pi _3$ is the position ($pos$) of the word in the sentence. Program $\\Pi _3$ is called whenever there is a new complement component discovered. That way of recursive calls is to identify the maximal chunk of the words that support the main components of the sentence. The result of this module is a list of vocabularies for the next steps.",
                "We begin with recognizing the main words (components) that play the most important roles in the sentence based on a given sentence structure. This is achieved by program $\\Pi _2$ (Listing ). The first four rules of $\\Pi _2$ determine the main subject and verb of the sentence whose structure is #1, #2, #3, or #5. Structure #4 requires a special treatment since the components following tobe can be of different forms. For instance, in \u201cCathy is gorgeous,\u201d the part after tobe is an adjective, but in \u201cCathy is a beautiful girl,\u201d the part after tobe is a noun, though, with adjective beautiful. This is done using the four last rules of $\\Pi _2$."
            ],
            "predicted_section": [
                "Method ::: Sentence Components Recognition"
            ]
        },
        "7380e62edcb11f728f6d617ee332dc8b5752b185": {
            "question_text": "Which neural language model architecture do they use?",
            "from_paper": "1612.07486",
            "gold": [
                "Our model is based on a standard stacked character-based LSTM BIBREF4 with two layers, followed by a hidden layer and a final output layer with softmax activations. The only modification made to accommodate the fact that we train the model with text in nearly a thousand languages, rather than one, is that language embedding vectors are concatenated to the inputs of the LSTMs at each time step and the hidden layer before the softmax. We used three separate embeddings for these levels, in an attempt to capture different types of information about languages. The model structure is summarized in fig:model.",
                "In contrast to related work, we focus on massively multilingual data sets to cover for the first time a substantial amount of the linguistic diversity in the world in a project related to data-driven language modeling. We do not presuppose any prior knowledge about language similarities and evolution and let the model discover relations on its own purely by looking at the data. The only supervision that is giving during training is a language identifier as a one-hot encoding. From that and the actual training examples, the system learns dense vector representations for each language included in our data set along with the character-level RNN parameters of the language model itself."
            ],
            "gold_section": [
                "Introduction",
                "Methods"
            ],
            "predicted": [
                "Neural language models BIBREF0 , BIBREF1 , BIBREF2 have become an essential component in several areas of natural language processing (NLP), such as machine translation, speech recognition and image captioning. They have also become a common benchmarking application in machine learning research on recurrent neural networks (RNN), because producing an accurate probabilistic model of human language is a very challenging task which requires all levels of linguistic analysis, from pragmatics to phonology, to be taken into account.",
                "In our experiments we use 1024-dimensional LSTMs, 128-dimensional character embeddings, and 64-dimensional language embeddings. Layer normalization BIBREF5 is used, but no dropout or other regularization since the amount of data is very large (about 3 billion characters) and training examples are seen at most twice. For smaller models early stopping is used. We use Adam BIBREF6 for optimization. Training takes between an hour and a few days on a K40 GPU, depending on the data size.",
                "Concurrent with this work, Johnson2016zeroshot conducted a study using neural machine translation (NMT), where a sub-word decoder is told which language to generate by means of a special language identifier token in the source sentence. This is close to our model, although beyond a simple interpolation experiment (as in our sec:generating) they did not further explore the language vectors, which would have been challenging to do given the small number of languages used in their study."
            ],
            "predicted_section": [
                "Related Work",
                "Introduction",
                "Methods"
            ]
        },
        "71413505d7d6579e2a453a1f09f4efd20197ab4b": {
            "question_text": "How is the system constructed to be linear in the size of the narrative input and the terminology?",
            "from_paper": "1612.03762",
            "gold": [
                "INLINEFORM0 scans the text word-by-word (remember that each word corresponds to a token) once and performs a \u201cvoting task\u201d: at the INLINEFORM1 -th step, it marks (i.e., \u201cvotes\u201d) with index INLINEFORM2 each LLT INLINEFORM3 containing the current ( INLINEFORM4 -th) word of the ADR description. Moreover, it keeps track of the position where the INLINEFORM5 -th word occurs in INLINEFORM6 ."
            ],
            "gold_section": [],
            "predicted": [
                "Let INLINEFORM0 be the input size (the length, in terms of words, of the narrative description). Let INLINEFORM1 be the cardinality of the dictionary (i.e., the number of terms). Moreover, let INLINEFORM2 be the number of distinct words occurring in the dictionary and let INLINEFORM3 be the length of the longest term in the dictionary. For MedDRA, we have about 75K terms ( INLINEFORM4 ) and 17K unique words ( INLINEFORM5 ). Notice that, reasonably, INLINEFORM6 is a small constant for any dictionary; in particular, for MedDRA we have INLINEFORM7 . We assume that all update operations on auxiliary data structures require constant time INLINEFORM8 .",
                "From an abstract point of view, we try to recognize, in the narrative description, single words belonging to LLTs, which do not necessarily occupy consecutive positions in the text. This way, we try to \u201creconstruct\u201d MedDRA terms, taking into account the fact that in a description the reporter can permute or omit words. As we will show, MagiCoder has not to deal with computationally expensive tasks, such as taking into account subroutines for permutations and combinations of words (as, for example, in BIBREF19 ).",
                "Word-by-word linear scan of the description and \u201cvoting task\u201d: a word \u201cvotes\u201d LLTs it belongs to. For each term voted by one or more words, we store some information about the retrieved syntactical matching."
            ],
            "predicted_section": [
                "MagiCoder complexity analysis",
                "MagiCoder: overview"
            ]
        },
        "c2ce25878a17760c79031a426b6f38931cd854b2": {
            "question_text": "What is the source of the training/testing data?",
            "from_paper": "2003.11528",
            "gold": [
                "Chinese Classical poetry can be classified into two primary categories, SHI and CI. According to the statistical data from CCPC1.0, a Chinese Classical Poetry Corpus consisting of 834,902 poems in total (We believe it is almost a full collection of Chinese Classical poems). 92.87% poems in CCPC1.0 fall into the category of SHI and 7.13% fall into the category of CI. SHI and CI can be further divided into many different types in terms of their forms. We briefly introduce the related background knowledge as follows.",
                "We implement the GPT-2 model based on the transformers library BIBREF8. The model configuration is 8 attention heads per layer, 8 layers, 512 embedding dimensions, and 1024 feed-forward layer dimensions. We employ the OpenAIAdam optimizer and train the model with 400,000 steps in total on 4 NVIDIA 1080Ti GPUs. The characters with frequency less than 3 in CCPC1.0 are treated as UNK and a vocabulary with 11259 tokens (characters) is finally built up."
            ],
            "gold_section": [
                "Introduction",
                "Experiment ::: Experiment Setup"
            ],
            "predicted": [
                "After pre-processing, all the formatted poem samples will be sent to the poetry generation model for training, as illustrated in Figure 3.",
                "State Key Lab on Intelligent Technology and Systems, Tsinghua University, Beijing, China",
                "Institute for Artificial Intelligence, Tsinghua University, Beijing, China"
            ],
            "predicted_section": [
                " :::  ::: ",
                "Model ::: Pre-processing"
            ]
        },
        "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d": {
            "question_text": "what is the previous work they are comparing to?",
            "from_paper": "1801.03615",
            "gold": [
                "Our RNN and Transformer baseline systems utilize BPE BIBREF3 to transfer the original word sequence to subword sequence on both the source and the target sides, since the subword method had a stable improvement compared with word based system, especially on morphologically rich languages.",
                "Besides, we compared our system with a fully character-based baseline system, which is an implementation of BIBREF4 ( BIBREF4 )'s work, and is available on github."
            ],
            "gold_section": [
                "Baselines"
            ],
            "predicted": [
                "We use BLEU BIBREF26 as our evaluation metric. The performance of different systems are shown in Table TABREF34 and TABREF35 . On both the news and e-commerce domains, our system performs better than baseline systems.",
                "On news domain, the average improvement of our method is 1.75 and 0.97 BLEU score when implemented on RNN-based NMT, compared with subword BIBREF3 method and fully character-based BIBREF4 method, respectively. When implemented on Transformer BIBREF15 , average improvement is 1.47 BLEU compared with subword method. On the e-commerce domain, which use 50M sentences as training corpus, the average improvement of our method is 0.68 BLEU compared with the subword method.",
                "We evaluate stem accuracies and suffix accuracies separately. For stem, we use BLEU as evaluation metric, Table TABREF34 shows stem BLEU of different methods on \u201cNews2014\u201d test set, our method can gain significant improvement compared with baselines, since our method can reduce data sparsity better than baselines. Our method can effectively reduce suffix error, Figure FIGREF43 gives some examples both on e-commerce and news domains:"
            ],
            "predicted_section": [
                "Results and Analysis"
            ]
        },
        "4e2b12cfc530a4682b06f8f5243bc9f64bd41135": {
            "question_text": "How is quality of the word vectors measured?",
            "from_paper": "1910.09362",
            "gold": [
                "Experiments ::: Task 1: Word Similarity Task ::: Task Description",
                "The task computes the correlation between the word similarity scores by human judgment and the word distances in vector space. We use Pearson correlation coefficient $\\rho _p$ as the metric, the higher of which the better the word vectors are. The expression of $\\rho _p$ is",
                "Experiments ::: Task 2: Synonym Selection Task ::: Task Description",
                "This task attempts to select the semantically closest word, from the candidate answers, to the stem word. For example, given the stem word \u201ccostly\u201d and the candidate answers \u201cexpensive, beautiful, popular, complicated\u201d, the most similar word should be \u201cexpensive\u201d. For each candidate answer, we compute the cosine similarity score between its word vector and that of the stem word. The candidate answer with the highest score is our final answer for a question. Here we use the TOEFL dataset BIBREF36 with 80 synonym questions and the LEX dataset with 303 questions collected by ourselves."
            ],
            "gold_section": [
                "Experiments ::: Task 2: Synonym Selection Task ::: Task Description",
                "Experiments ::: Task 1: Word Similarity Task ::: Task Description"
            ],
            "predicted": [
                "where $v_{w}$ and $v_{w}^{\\prime }$ are the vectors of the \u201cinput\u201d and \u201coutput\u201d words, and $|V|$ is the size of vocabulary.",
                "The recent decade has witnessed the great success achieved by word representation in natural language processing (NLP). It proves to be an integral part of most other NLP tasks, in which words have to be vectorized before input to the models. High quality word vectors have boosted the performance of many tasks, such as named entity recognition BIBREF0, BIBREF1, sentence completion BIBREF2, BIBREF3, part-of-speech tagging BIBREF4, BIBREF5, sentiment analysis BIBREF6, BIBREF7, and machine translation BIBREF8, BIBREF9. In a conventional way, word vectors are obtained from word-context co-occurrence matrices by either cascading the row and column vectors BIBREF10 or applying singular value decomposition (SVD) BIBREF11. However, these approaches are limited by their sub-optimal linear structure of vector space and the highly increased memory requirement when confronting huge vocabularies. Both problems have been solved by a popular model called Word2Vec BIBREF12, which utilizes two shallow neural networks, i.e., skip-gram and continuous bag-of-words, to learn word vectors from large corpora. The model is also capable of capturing interesting linear relationships between word vectors.",
                "Smoothed Unigram. The smoothed unigram distribution in Word2Vec BIBREF12 solves this problem because it gives more chances for infrequent words to be sampled. However, the required power rate is decided empirically, and may need adjustment for different scenarios BIBREF24, BIBREF25. BIBREF23 even propose to use a bigram distribution after studying the power rate, but it is infeasible for large corpora. Besides, the smoothed unigram distribution also changes the lexical structure of infrequent words, which could be a reason for the limited quality of word vectors."
            ],
            "predicted_section": [
                "Related Work",
                "Introduction",
                "Word2Vec ::: Architectures"
            ]
        },
        "ccec4f8deff651858f44553f8daa5a19e8ed8d3b": {
            "question_text": "What are the datasets used in the paper?",
            "from_paper": "1909.05190",
            "gold": [
                "We also use ATOMIC BIBREF7 as the event sentiment labeled dataset. In this dataset, the sentiment of the event is labeled as words. For example, the sentiment of \u201cPersonX broke vase\u201d is labeled as \u201c(sad, be regretful, feel sorry, afraid)\u201d. We use SenticNet BIBREF14 to normalize these emotion words ($W=\\lbrace w_1, w_2, \\dots , w_n\\rbrace $) as the positive (labeled as 1) or the negative (labeled as -1) sentiment. The sentiment polarity of the event $P_e$ is dependent on the polarity of the labeled emotion words $P_W$: $P_e=1$, if $\\sum _i P_{w_i}>0$, or $P_e=-1$, if $\\sum _i P_{w_i}<0$. We use the softmax binary classifier to learn sentiment enhanced event embeddings. The input of the classifier is event embeddings, and the output is its sentiment polarity (positive or negative). The model is trained in a supervised manner by minimizing the cross entropy error of the sentiment classification, whose loss function is given below.",
                "Extensive experiments show that incorporating external commonsense knowledge brings promising improvements to event embeddings, achieving 78% and 200% improvements on hard similarity small and big dataset, respectively. With better embeddings, we can achieve superior performances on script event prediction and stock market prediction compared to state-of-the-art baseline methods.",
                "Except for the hard similarity task, we also evaluate our approach on the transitive sentence similarity dataset BIBREF19, which contains 108 pairs of transitive sentences: short phrases containing a single subject, object and verb (e.g., agent sell property). It also has another dataset which consists of 200 sentence pairs. In this dataset, the sentences to be compared are constructed using the same subject and object and semantically correlated verbs, such as `spell\u2019 and `write\u2019; for example, `pupils write letters\u2019 is compared with `pupils spell letters\u2019. As this dataset is not suitable for our task, we only evaluate our approach and baselines on 108 sentence pairs.",
                "Following BIBREF21 (BIBREF21), we evaluate on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2. As SGNN proposed by BIBREF21 (BIBREF21) achieved state-of-the-art performances for this task, we use the framework of SGNN, and only replace their input event embeddings with our intent and sentiment-enhanced event embeddings."
            ],
            "gold_section": [
                "Commonsense Knowledge Enhanced Event Representations ::: Sentiment Embedding",
                "Experiments ::: Event Similarity Evaluation ::: Transitive Sentence Similarity",
                "Experiments ::: Script Event Prediction",
                "Introduction"
            ],
            "predicted": [
                "We compare the performance of our approach against a variety of event embedding models developed in recent years. These models can be categorized into three groups:",
                "Averaging Baseline (Avg) This represents each event as the average of the constituent word vectors using pre-trained GloVe embeddings BIBREF8.",
                "Experimental results of hard similarity and transitive sentence similarity are shown in Table TABREF23. We find that:"
            ],
            "predicted_section": [
                "Experiments ::: Baselines",
                "Experiments ::: Event Similarity Evaluation ::: Results"
            ]
        },
        "649e77ac2ecce42ab2efa821882675b5a0c993cb": {
            "question_text": "What languages do they apply the model to?",
            "from_paper": "1606.02601",
            "gold": [
                "Another approach to go beyond words is based on on character-level neural network models. Both recurrent and convolutional architectures for deriving word representations from characters have been used, and results in downstream tasks such as language modelling and POS tagging have been promising, with reductions in word perplexity for language modelling and state-of-the-art English POS tagging accuracy BIBREF8 , BIBREF9 . Ballesteros et al. ballesteros train a character-level model for parsing. Zhang et al. zhang do away with words completely, and train a convolutional neural network to do text classification directly from characters.",
                "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks.",
                "What is less clear is how well these models learn word semantics. Classical word embedding models seem to capture word semantics, and the nearest neighbours of a given word are typically semantically related words BIBREF3 , BIBREF19 . In addition, the correlation between model word similarity scores and human similarity judgments is typically high BIBREF20 . However, no previous work (to our knowledge) evaluates the similarity judgments of character-level models against human annotators.",
                "The Char2Vec model",
                "We hypothesize that by incorporating morphological knowledge directly into a character-level model, one can improve the ability of character-level models to learn compositional word semantics. In addition, we hypothesize that incorporating morphological knowledge helps structure the embedding space in such a way that affixation corresponds to a regular shift in the embedding space. We test both hypotheses directly in \u00a7 \"Capturing semantic similarity\" and \u00a7 \"Capturing syntactic and semantic regularity\" respectively.",
                "The starting point for our model is the skip-gram with negative sampling (SGNS) objective of Mikolov et al. word2vec2. For a vocabulary $V$ of size $|V|$ and embedding size $N$ , SGNS learns two embedding tables $W, C \\in \\mathbb {R}^{N \\times |V|}$ , the target and context vectors. Every time a word $w$ is seen in the corpus with a context word $c$ , the tables are updated to maximize",
                "$$\\log \\sigma (w \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-w \\cdot \\tilde{c}_i)]$$ (Eq. 7)",
                "where $P(w)$ is a noise distribution from which we draw $k$ negative samples. In the end, the target vector for a word $w$ should have high inner product with context vectors for words with which it is typically seen, and low inner products with context vectors for words it is not typically seen with. Figure 1 illustrates this for a particular example. In Mikolov et al. word2vec2, the noise distribution $P(w)$ is proportional to the unigram probability of a word raised to the 3/4th power BIBREF11 .",
                "Our innovation is to replace $W$ with a trainable function $f$ that accepts a sequence of characters and returns a vector of length $N$ (i.e. $f: A^{<\\omega } \\rightarrow \\mathbb {R}^N$ , where $A$ is the alphabet we are considering and $A^{<\\omega }$ denotes the finite length strings over the alphabet $A$ ). We still keep the table of context embeddings $C$ , and our model objective is still to minimize",
                "$$\\log \\sigma (f(w) \\cdot c) + \\sum _{i = 1}^{k} \\mathbb {E}_{\\tilde{c}_i \\sim P(w)} [\\log \\sigma (-f(w) \\cdot \\tilde{c}_i)]$$ (Eq. 8)",
                "where we now treat $w$ as a sequence of characters. After training, $f$ can be used to produce an embedding for any sequence of characters, even if it was not previously seen in training.",
                "The process of calculating $f$ on a word is illustrated in Figure 2 . We first pad the word with beginning and end of word tokens, and then pass the characters of the word into a character lookup table. As the link between characters and morphemes is non-compositional and requires essentially memorizing a sequence of characters, we use LSTMs BIBREF21 to encode the letters in the word, as they have been shown to capture non-local and non-linear dependencies. We run a forward and a backward LSTM over the character embeddings. The forward LSTM reads the beginning of word symbol, but not the end of word symbol, and the backward LSTM reads the end of word symbol but not the beginning of word symbol. This is necessary to align the resulting embeddings, so that the LSTM hidden states taken together correspond to a partition of the word into two without overlap.",
                "The LSTMs output two sequences of vectors $h_0^{f}, \\dots , h_n^f$ and $h_n^{b}, \\dots , h_0^b$ . We then concatenate the resulting vectors, and pass them through a shared feed-forward layer to obtain a final sequence of vectors $h_i$ . Each vector corresponds to two half-words: one half read by the forward LSTM, and the other by the backward LSTM.",
                "We then learn an attention model over these hidden states: given a hidden state $h_i$ , we calculate a weight $\\alpha _i = a(h_i)$ such that $\\sum \\alpha _i = 1$ , and then calculate the resulting vector for the word $w$ as $f(w) = \\sum \\alpha _i h_i$ . Following Bahdanau et al. bahdanau, we calculate $a$ as",
                "$$a(h_i) = \\frac{\\exp (v^{T} \\tanh (Wh_i))}{\\sum _j \\exp (v^{T} \\tanh (Wh_j))}$$ (Eq. 10)",
                "i.e. a softmax over the hidden states.",
                "Capturing morphology via attention",
                "Previous work on bidirectional LSTM character-level models used both LSTMs to read the entire word BIBREF8 , BIBREF22 . This can lead to redundancy, as both LSTMs are used to capture the full word. In contrast, our model is capable of splitting the words and optimizing the two LSTMs for modelling different halves. This means one of the LSTMs can specialize on word prefixes and roots, while the other memorizes possible suffixes. In addition, when dealing with an unknown word, it can be split into known and unknown components. The model can then use the semantic knowledge it has learnt for a known component to predict a representation for the unknown word as a whole.",
                "We hypothesize that the natural place to split words is on morpheme boundaries, as morphemes are the smallest unit of language which carry semantic meaning. We test the splitting capabilities of our model in \u00a7 \"Morphological awareness\" .",
                "Experiments",
                "We evaluate our model on three tasks: morphological analysis (\u00a7 \"Morphological awareness\" ), semantic similarity (\u00a7 \"Capturing semantic similarity\" ), and analogy retrieval (\u00a7 \"Capturing syntactic and semantic regularity\" ). We trained all of the models once, and then use the same trained model for all three tasks \u2013 we do not perform hyperparameter tuning to optimize performance on each task.",
                "We trained our Char2Vec model on the Text8 corpus, consisting of the first 100MB of a 2006 cleaned-up dump of Wikipedia. We only trained on words which appeared more than 5 times in our corpus. We used a context window size of 3 words either side of the target word, and took 11 negative samples per positive sample, using the same smoothed unigram distribution as word2vec. The model was trained for 3 epochs using the Adam optimizer BIBREF23 . All experiments were carried out using Keras BIBREF24 and Theano BIBREF25 , BIBREF26 . We initialized the context lookup table using word2vec, and kept it fixed during training. In all character-level models, the character embeddings have dimension $d_C = 64$ , while the forward and backward LSTMs have dimension $d_{LSTM} = 256$ . The concatenation of both therefore has dimensionality $d = 512$ . The concatenated LSTM hidden states are then compressed down to $d_{word} = 256$ by a feed-forward layer.",
                "As baselines, we trained a SGNS model on the same dataset with the same parameters. To test how much the attention model helps the character-level model to generalize, we also trained the Char2Vec model without the attention layer, but with the same parameters. In this model, the word embeddings are just the concatenation of the final forward and backward states, passed through a feedforward layer. We refer to this model as C2V-NO-ATT. We also constructed count-based vectors using SVD on PPMI-weighted co-occurence counts, with a window size of 3. We kept the top 256 principal components in the SVD decomposition, to obtain embeddings with the same size as our other models.",
                "To evaluate our model, we evaluate its use as a morphological analyzer (\u00a7 \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (\u00a7 \"Capturing semantic similarity\" ), and examine the structure of the embedding space (\u00a7 \"Capturing syntactic and semantic regularity\" ).",
                "The main innovation of our Char2Vec model compared to existing recurrent character-level models is the capability to split words and model each half independently. Here we test whether our model segmentations correspond to gold-standard morphological analyses.",
                "We obtained morphological analyses for all the words in our training vocabulary which were in the English Lexicon Project BIBREF27 . We then converted these into surface-level segmentations using heuristic affix-matching, and used this as a gold-standard morphemic analysis. We ended up with 14682 words, of which 7867 have at least two morphemes and 1138 have at least three.",
                "Evaluating morphological segmentation is a long-debated issue BIBREF28 . Traditional hard morphological analyzers are normally evaluated on border $F_1$ \u2013 that is, how many morpheme borders are recovered. However, our model does not actually posit any hard morpheme borders. Instead, it just associates each character boundary with a weight. Therefore, we treat the problem of recovering intra-word morpheme boundaries as a ranking problem. We rank each inter-character boundary of a word according to our model weights, and then evaluate whether our model ranks morpheme boundaries above non-morpheme boundaries.",
                "We use mean average precision (MAP) as our evaluation metric. We first calculate precision at $N$ for each word, until all the gold standard morpheme boundaries have been recovered. Then, we average over $N$ to obtain the average precision (AP) for that word. We then calculate the mean of the APs across all words to obtain the MAP for the model.",
                "We report results of a random baseline as a point of comparison, which randomly places morpheme boundaries inside the word. We also report the results of the Porter stemmer, where we place a morpheme boundary at the end of the stem, then randomly thereafter.",
                "Finally, we trained Morfessor 2.0 BIBREF13 on our corpus, using an initial random split value of 0.9, and stopping training when the difference in loss between successive epochs is less than 0.1% of the total loss. We then used our trained Morfessor model to predict morpheme boundaries, and randomly permuted the morpheme boundaries and ranked them ahead of randomly permuted non-morpheme boundaries to calculate MAP.",
                "As the test set is dominated by words with simple morphology, we also extracted all the morphologically rich words with 3 or more morphemes, and created a separate evaluation on this subsection. We report the results in Table 1 .",
                "As the results show, our model performs the best out of all the methods at analysing morphologically rich words with multiple morphemes. On these words, our model even outperforms Morfessor, which is explicitly designed as a morphological analyzer. This shows that our model learns splits which correspond well to human morphological analysis, even though we build no morphological knowledge into our model. However, when evaluating on all words, the Porter stemmer has a great advantage, as it is rule-based and able to give just the stem of words with great precision, which is effectively giving a canonical segmentation for words with just 2 morphemes.",
                "We show some model analyses against the gold standard in Table 2 .",
                "Next, we tested our model similarity scores against human similarity judgments. For these datasets, human annotators are asked to judge how similar two words are on a fixed scale. Model word vectors are evaluated based on ranking the word pairs according to their cosine similarity, and then measuring the correlation (using Spearman's $\\rho $ ) between model judgments and human judgments BIBREF20 .",
                "We use the WordSim353 dataset BIBREF29 , the test split of the MEN dataset BIBREF30 , and the Rare Word (RW) dataset BIBREF31 . The word pairs in the WordSim353 and MEN datasets are typically simple, commonly occurring words denoting basic concepts, whereas the RW dataset contains many morphologically derived words which have low corpus frequencies. This is reflected by how many of the test pairs in each dataset contain out of vocabulary (OOV) items: 3/353 and 6/1000 of the word pairs in WordSim353 and MEN, compared with 1083/2034 for the RW dataset.",
                "We report results for in-corpus word pairs in Table 3 , and for all word pairs for those models able to predict vectors for unseen words in Table 4 .",
                "Overall, word-based embedding models learn vectors that correlate better with human judgments, particularly for morphologically simple words. However, character-based models are competitive with word-based models on the RW dataset. While the words in this dataset appear rarely in our corpus (of the in-corpus words, over half appear fewer than 100 times), each morpheme may be common, and the character-level models can use this information. We note that on the entire RW dataset (of which over half contain an OOV word), the character-based models still perform reasonably. We also note that on word pairs in the RW test containing at least one OOV word, the full Char2Vec model outperforms the C2V model without morphology. This suggests that character-based embedding models are learning to morphologically analyse complex word forms, even on unseen words, and that giving the model the capability to learn word segments independently helps this process.",
                "We also present some word nearest neighbours for our Char2Vec model in Table 5 , both on the whole vocabulary and then filtering the nearest neighbours to only include words which appear 100 times or more in our corpus. This corresponds to keeping the top 10k words, which is common among language models BIBREF8 , BIBREF9 . We note that nearest neighbour predictions include words that are orthographically distant but semantically similar, showing that our model has the capability to learn to compose characters into word meanings.",
                "We also note that word nearest neighbours seem to be more semantically coherent when rarely-observed words are filtered out of the vocabulary, and more based on orthographic overlap when the entire vocabulary is included. This suggests that for rarely-observed words, the model is basing its predictions on orthographic analysis, whereas for more commonly observed words it can `memorize' the mapping between the orthography and word semantics.",
                "Finally, we evaluate the structure of the embedding space of our various models. In particular, we test whether affixation corresponds to regular linear shifts in the embedding space.",
                "To do this, we use the Google analogy dataset BIBREF3 . This consists of 19544 questions of the form \u201cA is to B as C is to X\u201d. We split this collection into semantic and syntactic sections, based on whether the analogies between the words are driven by morphological changes or deeper semantic shifts. Example semantic questions are on capital-country relationships (\u201cParis is to France as Berlin is to X) and currency-country relationships. Example syntactic questions are adjective-adverb relationships (\u201camazing is to amazingly as apparent is to X\u201d) and opposites formed by prefixing a negation particle (\u201cacceptable is to unacceptable as aware is to X\u201d). This results in 5537 semantic analogies and 10411 syntactic analogies.",
                "We use the method of Mikolov et al. word2vec1 to answer these questions. We first $\\ell _2$ -normalize all of our word vectors. Then, to answer a question of the form \u201cA is to B as C is to X\u201d, we find the word $w$ which satisfies",
                "$$w = \\operatornamewithlimits{argmax}_{w \\in V - \\lbrace a, b, c\\rbrace } \\cos (w, b - a + c)$$ (Eq. 28)",
                "where $a,\\, b,\\, c$ are the word vectors for the words A, B and C respectively.",
                "We report the results in Table 6 . The most intriguing result is that character-level models are competitive with word-level models for syntactic analogy, with our Char2Vec model holding the best result for syntactic analogy answering. This suggests that incorporating morphological knowledge explicitly rather than latently helps the model learn morphological features. However, on the semantic analogies, the character-based models do much worse than the word-based models. This is perhaps unsurprising in light of the previous section, where we demonstrate that character-based models do worse at the semantic similarity task than word-level models.",
                "Discussion",
                "We only report results for English. However, English is a morphologically impoverished language, with little inflection and relatively few productive patterns of derivation. Our morphology test set reflects this, with over half the words consisting of a simple morpheme, and over 90% having at most 2 morphemes.",
                "This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.",
                "Conclusion",
                "In this paper, we present a model which learns morphology and word embeddings jointly. Given a word, it splits the word in to segments and ranks the segments based on their context-predictive power. Our model can segment words into morphemes, and also embed the word into a representation space.",
                "We show that our model is competitive at the task of morpheme boundary recovery compared to a dedicated morphological analyzer, beating dedicated analyzers on words with a rich morphology. We also show that in the representation space word affixation corresponds to linear shifts, demonstrating that our model can learn morphological features.",
                "Finally, we show that character-level models, while outperformed by word-level models generally at the task of semantic similarity, are competitive at representing rare morphologically rich words. In addition, the character-level models can predict good quality representations for unseen words, with the morphologically aware character-level model doing slightly better."
            ],
            "gold_section": [
                "Capturing syntactic and semantic regularity",
                "Capturing semantic similarity",
                "Experiments",
                "Morphological awareness",
                "The Char2Vec model",
                "Character-level models",
                "Discussion",
                "Introduction",
                "Capturing morphology via attention",
                "Conclusion"
            ],
            "predicted": [
                "This is unfortunate for our model, as it performs better on words with richer morphology. It gives consistently more accurate morphological analyses for these words compared to standard baselines, and matches word-level models for semantic similarity on rare words with rich morphology. In addition, it seems to learn morphosyntactic features to help solve the syntactic analogy task. Most of all, it is language-agnostic, and easy to port across different languages. We thus expect our model to perform even better for languages with a richer morphology than English, such as Turkish and German.",
                "To evaluate our model, we evaluate its use as a morphological analyzer (\u00a7 \"Morphological awareness\" ), test how well it learns word semantics, including for unseen words (\u00a7 \"Capturing semantic similarity\" ), and examine the structure of the embedding space (\u00a7 \"Capturing syntactic and semantic regularity\" ).",
                "Excitingly, character-level models seem to capture morphological effects. Examining nearest neighbours of morphologically complex words in character-aware models often shows other words with the same morphology BIBREF8 , BIBREF9 . Furthermore, morphosyntactic features such as capitalization and suffix information have long been used in tasks such as POS tagging BIBREF17 , BIBREF18 . By explicitly modelling these features, one might expect good performance gains in many NLP tasks."
            ],
            "predicted_section": [
                "Discussion",
                "Character-level models",
                "Introduction"
            ]
        },
        "f85520bbc594918968d7d9f33d11639055458344": {
            "question_text": "What are the deep learning architectures used?",
            "from_paper": "1909.11232",
            "gold": [
                "Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal). Unlike traditional image classification or object detection models where neural networks learn hierarchical spatial features from data, sign recognition requires capture of temporal body motion.",
                "RNN has shown success in modeling sequential pattern in dataBIBREF6. It can capture temporal dynamics in data by maintaining an internal state. However, the basic RNN has problems dealing with long term dependencies in data due to the vanishing gradient problem BIBREF21. Some solutions to the vanishing gradient problem involve careful initialization of network parameters or early stopping BIBREF22. But the most effective solution is to modify the RNN architecture in such a way that there exists a memory state (cell state) at every time step that can identify what to remember and what to forget. This architecture is referred to as long short term memory (LSTM) network BIBREF23. While the basic RNN is a direct transformation of the previous state and the current input, the LSTM maintains an internal memory and has mechanisms to update and use that memory. This is achieved by deploying four separate neural networks also called gates. Figure FIGREF12 depicts a cell of an LSTM network which shows input at the current time step ${x_t}$ and the previous state ${h_{t-1}}$ enter into the cell; and get concatenated. The forget gate processes it to remove unnecessary information, and outputs ${f_t}$ which gets multiplied with the previously stored memory ${C_{t-1}}$ and produces a refined memory for the current time.",
                "Given a sample skeletal data of $R^{T \\times J \\times 3}$, where $T$ denotes time axis, $J$ is the number of body joints and the last dimension is the 3D coordinates of each joint. We flatten every dimension except time and at each time step we can feed a vector of size $R^{3 \\times J}$ as input. However, we have empirically verified that learning a sequential pattern for each coordinate axis independently and combining them later shows stronger classification performance. Based on this, we trained three different 2 layer LSTMs for data from x, y, and z coordinates separately; and concatenate their final embedding to produce softmax output. In this setting, each separate LSTM receives data as $R^{T \\times J}$ and final embedding size is $R^{3\\times S}$ where $S$ is the state size of LSTM cell. Figure FIGREF15 (a) shows the architecture where as a sample arrives, just before entering into main network, data along separate axis is split and entered into three different LSTM networks. The model concatenates the final state from each of the separate LSTM networks; followed by feeding this into the softmax layer for classification. This approach is referred by Axis Independent Architecture (AI-LSTM). Implementation details such as values of T and J are provided in the `Experiments' section.",
                "AI-LSTM, described in last section, works by modeling temporal dynamics of body joints' data over time. However, there can be spatial interactions with joints at a specific time step. It fails to capture any such interaction among joints in a given time. To incorporate spatial relationship among joints, we propose a simple novel data augmentation technique for skeletal data. We do this by origin transfer. For each frame in a gesture sample, we use each wrist joints as origin and transform all other joints' data by subtracting that origin from them. In this way spatial information is added to the input. We refer this model with spatial data augmentation as Spatial AI-LSTM. This augmentation technique is depicted in Figure FIGREF21. A sample data of form $R^{T \\times 6 \\times 3}$ results in a representation of $R^{T \\times 5 \\times 3}$ after subtracting left wrist joint (origin transfer). After this augmentation process, each sample is a $R^{20 \\times 16 \\times 3}$ matrix. Hence, each separate LSTM networks in our Spatial AI-LSTM network receives an input of $R^{20 \\times 16}$.",
                "We hypothesize that, some signs that have mostly similar skeletal motion pattern could be distinguishable using hand shape information. We propose a combination of LSTM and 3D CNN networks. We call this Max CNN-LSTM network. Figure FIGREF15 (b) represents the the Max CNN-LSTM. The details of 3D CNN module is shown in Figure FIGREF14. This architecture has two parts: one for left hand patches and other for right hand patches. Each part has four 3D convolutional layers (second and fourth layers have following maximum pooling layers) followed by 2 fully connected layers. Final embeddings from these two parts are concatenated and by using a softmax layer, from which a classification score is produced. The other AI-LSTM network is fed with skeletal time series data. At the final time step, the LSTM state vector is taken and using a softmax layer another probability score is produced. The final classification score is created by taking element wise maximum of the output scores from the two networks. During back\u2013propagation, both networks are trained on their own score. The combined network acts like a model ensemble and some sign classes which are confused by RNN network alone might have an improved recognition accuracy with this approach.",
                "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train."
            ],
            "gold_section": [
                "Our Approach ::: Combined Network",
                "Our Approach ::: Axis Independent LSTM",
                "Our Approach",
                "Experiments ::: Training Details",
                "Our Approach ::: Recurrent Neural Networks (RNN)",
                "Our Approach ::: Spatial AI-LSTM"
            ],
            "predicted": [
                "Inspired by the success of deep learning approaches in computer vision BIBREF20, we applied different deep learning architectures to model sign languages from both input modes (RGB and skeletal). Unlike traditional image classification or object detection models where neural networks learn hierarchical spatial features from data, sign recognition requires capture of temporal body motion.",
                "To deal with over-fitting, dropout was used for all networks except convolutional layers with probability of 0.5. In addition to dropout, L2 regularization was used for LSTM networks and for dense layers; $\\beta $ was set to 0.008 which controls the impact of regularization on the network. State size and number of layers of LSTM networks were 50 and 2, respectively. Learning rate for Max CNN-LSTM and LSTM networks were set to $0.00001$ and $0.00005$, respectively. We used Adam Optimizer for training our networks BIBREF24. All networks were run for a certain number of epochs (200-300) with a batch size of 64. We developed all of our models with Tensorflow 1.10 (python). Average time taken to train an AI-LSTM and an Spatial AI-LSTM are 25 and 30 minutes on an Intel(R) Core(TM) i5-7600 (3.50GHz) processor respectively. We trained 3D CNN and Max 3D CNN models on GPU (Tesla K80) and each model took around 20 hours to train.",
                "We present a deep learning based approach for ASL recognition that leverages skeletal and video data. The proposed model captures the underlying temporal dynamics associated with sign language and also identifies specific hand shape patterns from video data to improve recognition performance. A new data augmentation technique was introduced that allowed the LSTM networks to capture spatial dynamics among joints. Finally, a large public dataset for ASL recognition will be released to the community to spur research in this direction; and bring benefits of digital assistants to the deaf and hard of hearing community. For future research direction, we are looking into the problem of sentence level ASL recognition. We also plan to use other data modality such as wifi signals which can be complimentary to video in sign language recognition."
            ],
            "predicted_section": [
                "Our Approach",
                "Experiments ::: Training Details",
                "Conclusion"
            ]
        },
        "f7b91b99279833f9f489635eb8f77c6d13136098": {
            "question_text": "Which sentence compression technique works best?",
            "from_paper": "1912.11980",
            "gold": [
                "To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.",
                "According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:"
            ],
            "gold_section": [
                "Experiments ::: Main Results ::: Machine Translation",
                "Experiments ::: Main Results ::: Sentence Compression"
            ],
            "predicted": [
                "Different with the length marker or length countdown input, to induce our model to output the compression sequence with desired length, we use beam search during generation to find the sequence $S^{^{\\prime }}$ that maximizes a score function $s(S^{^{\\prime }}, S)$ given a trained ESC model. The length normalization is introduced to account for the fact that we have to compare hypotheses of different length. Without some form of length-normalization regular $ln$, beam search will favor shorter sequences over longer ones on average since a negative log-probability is added at each step, yielding lower (more negative) scores for longer sentences. Moreover, a coverage penalty $cp$ is also added to favor the sequence that cover the source sentence meaning as much as possible according to the attention weights BIBREF20.",
                "Generally, sentence compression is a typical sequence generation task which aims to maximize the absorption and long-term retention of large amounts of data over a relatively short sequence for text understanding BIBREF5, BIBREF6. To distinguish the importance of words in the sentence and, more importantly, to dig out the most salient part in the sentence representation, we utilize the sentence compression method to explicitly distill the key knowledge that can retain the key meaning of the sentence, termed explicit sentence compression (ESC) in this paper. Depending on whether or not the sentence compression is trained using human annotated data, the proposed method can be implemented in three ways: supervised ESC, unsupervised ESC, and semi-supervised ESC.",
                "Different from these work, our proposed sentence compression model does not rely on any known linguistics motivated (such as syntax) skeleton simplification, but directly trains a computation motivated sentence compression model to learn to compress sentences and re-paraphrase them directly in seq2seq model. Though with a pure computation source, our sentence compression model can surprisingly generate more grammatically correct and refined sentences, and the words in the compressed sentence do not have to be the same as the original sentence. In the meantime, our sentence compression model can stably give source backbone representation exempt from unstable performance of a syntactic parser which is essential for syntactic skeleton simplification. Our sentence compression model can perform unsupervised training on large-scale data sets, and then use the supervised data for finetune, which is more promising from the results."
            ],
            "predicted_section": [
                "Explicit Sentence Compression",
                "Related Work",
                "Explicit Sentence Compression ::: Compression Rate Control"
            ]
        },
        "99e514acc0109b7efa4e3860ce1e8c455f5bb790": {
            "question_text": "Do they compare performance against state of the art systems?",
            "from_paper": "1912.11980",
            "gold": [
                "The proposed NMT model was evaluated on the WMT14 English-to-German (EN-DE) and English-to-French (EN-FR) tasks, which are both standard large-scale corpora for NMT evaluation. For the EN-DE translation task, 4.43 M bilingual sentence pairs from the WMT14 dataset were used as training data, including Common Crawl, News Commentary, and Europarl v7. The newstest2013 and newstest2014 datasets were used as the dev set and test set, respectively. For the EN-FR translation task, 36 M bilingual sentence pairs from the WMT14 dataset were were used as training data. Newstest12 and newstest13 were combined for validation and the newstest14 was the test set, following the setting of BIBREF29. The BPE algorithm BIBREF23 was also adopted, and the joint vocabulary size was set at 40 K. For the hyper-parameters of our Transformer (base/large) models, we followed the settings used in BIBREF0's work.",
                "In addition, we also reported the state-of-the-art results in recent literatures, including modelling local dependencies (Localness) BIBREF30, fusing multiple-layer representations in SANs (Context-Aware) BIBREF31, and fusing all global context representations in SANs (global-deep context) BIBREF32. MultiBLEU was used to evaluate the translation task.",
                "According to the results in Table TABREF34, we chose the semi-supervised ESC model (which performed the best) to generate compressed sentences for the machine translation task. The main results on the WMT14 EN-DE and EN-FR translation tasks are shown in Table TABREF35. In the EN-DE task, we made the following observations:"
            ],
            "gold_section": [
                "Experiments ::: Setup ::: Machine Translation",
                "Experiments ::: Main Results ::: Machine Translation"
            ],
            "predicted": [
                "To evaluate the quality of our sentence compression model, we conducted a horizontal comparison between the proposed sentence compression model and other sentence compression models in different settings. Table TABREF34 shows the comparison results. We observed that the proposed unsupervised ESC model performed substantially better than Fevry and BIBREF17's unsupervised method. The proposed supervised ESC model also substantially outperformed the RNN-based Seq2seq and BIBREF11's baseline method. That is, our supervised model gave +2.0 improvements on R-1, R-2, and R-L scores over the RNN-based Seq2seq. This means that the proposed Transformer-based approaches can generate compressed sentences of high quality.",
                "4) BBFNMT (based) is comparable to the +global-deep context, the best comparison system, while BBFNMT (big) slightly outperformed +global-deep context by $0.16$ BLEU scores. In particular, the parameters of BBFNMT (base/big) model, which just increased $12.1/7.9$M over the Transformer (base/big), were only 70% of the +global-deep context model. This denotes that the BBFNMT model is more efficient than the +global-deep context model. In addition, the training speed of the proposed models slightly decreased ($8\\%$), compared to the corresponding baselines.",
                "For the EN-FR translation task, the proposed models gave similar improvements over the baseline systems and comparing methods (except that the Transformer (big) performed much more better than Transformer (base)). These results show that our method is robust for improving the translation of other language pairs."
            ],
            "predicted_section": [
                "Experiments ::: Main Results ::: Machine Translation",
                "Experiments ::: Main Results ::: Sentence Compression"
            ]
        },
        "ac87dd34d28c3edd9419fa0145f3d38c87d696aa": {
            "question_text": "What is the dataset that is used to train the embeddings?",
            "from_paper": "1807.08089",
            "gold": [
                "We used LibriSpeech BIBREF46 as the audio corpus in the experiments, which is a corpus of read speech in English derived from audiobooks. This corpus contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. We used the \u201cclean\" and \u201cothers\" sets with a total of 960 hours, and extracted 39-dim MFCCs as the acoustic features."
            ],
            "gold_section": [
                "Dataset"
            ],
            "predicted": [
                "For parallelizing the text and audio embeddings in Subsection SECREF14 , we projected the embeddings to the top 100 principle components, so the affine transformation matrices were INLINEFORM0 . The mini-batch size was 200, and INLINEFORM1 in ( EQREF15 ) was set to 0.5.",
                "In Stage 2, the two encoders INLINEFORM0 and INLINEFORM1 were both 2-hidden-layer fully-connected feedforward networks with size 256. The size of embedding vectors was set to be 128. The context window size was 5, and the negative sampling number was 5.",
                "Word embedding or Word2Vec BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 has been widely used in the area of natural language processing BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , in which text words are transformed into vector representations of fixed dimensionality BIBREF11 , BIBREF12 , BIBREF13 . This is because these vector representations carry plenty of semantic information learned from the context of the considered words in the text training corpus. Similarly, audio Word2Vec has also been proposed in the area of speech signal processing, in which spoken words (signal segments for words without knowing the underlying word it represents) are transformed into vector representations of fixed dimensionality BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 , BIBREF22 , BIBREF23 , BIBREF24 . These vector representations carry the phonetic structures of the spoken words learned from the signals within the spoken words, and have been shown to be useful in spoken term detection, in which the spoken terms are detected simply based on the phonetic structures. Such Audio Word2Vec representations do not carry semantics, because they are learned from individual spoken words only without considering the context."
            ],
            "predicted_section": [
                "Introduction",
                "Model Implementation"
            ]
        },
        "50bcbb730aa74637503c227f022a10f57d43f1f7": {
            "question_text": "what is the baseline model",
            "from_paper": "1703.05320",
            "gold": [
                "We ran SVM-Rank with different combinations of features listed in Table 2 , but due to limited space, we only report the result of those combinations which achieved highest F1-score. We compared our method to two baseline models TF-IDF and LSI which only use Cosine similarity to retrieve the relevant articles. Results from Table 3 indicate that (LSI, Manhattan, Jaccard) is the triple of features which achieves the best result and the most stability."
            ],
            "gold_section": [
                "Information Retrieval"
            ],
            "predicted": [
                "HUKB-2 BIBREF23 used a fundamental feature BM25 and applied mutatis mutandis for articles. If both an article and a query have conditional parts, they are divided into two parts like conditional parts and the rest part before measuring their similarity. This investigation in conditional parts is valuable since it is a common structure in laws. Their F1-score in formal rune is the second highest (0.5532), which is slightly higher than our system (0.5478) using SVM-Rank and a set of features LSI, Manhattan, Jaccard. This shows that for phase 1, our model with a set of defined features is relatively competitive.",
                "where: $S_0$ is the highest relevant score. In other words, the score ratio of a relevant article and the most relevant article should not be lower than 85% (choosing the value 0.85 for this threshold is simply heuristic based). This is to prevent a relevant article to have a very low score as opposed to the most relevant article.",
                "For information retrieval task, 20% of query-article pairs are used for evaluating our model while the rest is for training. As we only consider single-paragraph articles in the training phase, if a multiple-paragraph article is relevant, all of its generated single-paragraph articles will be marked as relevant. In addition, the label for each query-article pair is set either 1 (relevant) or 0 (irrelevant). In our experiment, instead of selecting top $k$ retrieved articles as relevant articles, we consider a retrieved article $A_i$ as a relevant article if its score $S_i$ satisfies Equation ( 26 ): "
            ],
            "predicted_section": [
                "Information Retrieval",
                "Formal run phase 1 - COLIEE 2016"
            ]
        },
        "8246d1eee1482555d075127ac84f2e1d0781a446": {
            "question_text": "what datasets were used?",
            "from_paper": "1805.11598",
            "gold": [
                "We evaluate our system on the semantic role labeling portion of the CoNLL-2009 shared task BIBREF0 , on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates."
            ],
            "gold_section": [
                "Data"
            ],
            "predicted": [
                "We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors BIBREF7 from the news, web, and Wikipedia text of the Leipzig Corpora Collection BIBREF8 . We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency.",
                "We also note that, due to semi-automatic projection of annotations to construct the German dataset, more than half of German sentences do not include labeled predicate and arguments. Thus while German has almost as many sentences as Czech, it has by far the fewest training examples (predicate-argument structures); see Table TABREF3 .",
                "Despite the consistency of this format, there are significant differences between the training sets across languages. English uses PropBank role labels BIBREF2 . Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as \u201carg INLINEFORM0 -agt\u201d (for \u201cagent\u201d) or \u201cA INLINEFORM1 \u201d that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; BIBREF3 ), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other."
            ],
            "predicted_section": [
                "Data",
                "Monolingual Baseline"
            ]
        },
        "1e11e74481ead4b7635922bbe0de041dc2dde28d": {
            "question_text": "How many hand-crafted templates did they have to make?",
            "from_paper": "1610.03807",
            "gold": [
                "We first compare our system with serban-EtAl:2016:P16-1 on 500 randomly selected triples from Freebase BIBREF7 . For the 500 triples, we hand-crafted 106 templates, as these triples share only 53 distinct predicates (we made 2 templates for each predicate on average). 991 seed questions are generated by applying the templates on the triples, and 1529 more questions are retrieved from Google. To evaluate the fluency of the candidate questions, we train a 4-gram language model (LM) on gigaword (LDC2011T07) with Kneser Ney smoothing. Using the averaged language model score as index, the top 500 questions are selected to compare with the results from serban-EtAl:2016:P16-1. We ask three native English speakers to evaluate the fluency and the naturalness of both results based on a 4-point scheme where 4 is the best.",
                "The last experiment is on our in-house KB in the power tool domain. It contains 67 distinct predicates, 293 distinct subjects and 279 distinct objects respectively. For the 67 predicates, we hand-craft 163 templates. Here we use the same language model as in our first experiment, and learn a skip-gram model BIBREF11 on Wikipedia for evaluating domain relevance."
            ],
            "gold_section": [
                "Evaluation on Freebase",
                "Evaluation on the Domain-specific KB"
            ],
            "predicted": [
                "The only human labor in this work is the question template construction. Our system does not require a large number of templates because: (1) the iterative question expansion can produce a large number of questions even with a relatively small number of seed questions, as we see in the experiments, (2) multiple entities in the KB share the same predicates. Another advantage is that our system can easily generate updated questions as web is self-updating consistently. In our experiment, we compare with serban-EtAl:2016:P16-1 on 500 random selected triples from Freebase BIBREF7 . Evaluated by 3 human graders, questions generated by our system are significantly better then serban-EtAl:2016:P16-1 on grammaticality and naturalness.",
                "Shown in Figure 1 , our system contains the sub-modules of question template construction, seed question generation, question expansion and selection. Given an input KB, a small set of question templates is first constructed such that each template is associated with a predicate, then a seed question set is generated by applying the template set on the input KB, before finally more questions are generated from related questions that are iteratively retrieved from a search engine with already-obtained questions as search queries (section \"Experiments\" ). Taking our in-house KB of power tool domain as an example, template \u201chow to use #X#\u201d is first constructed for predicate \u201cperformsActivity\u201d. In addition, seed question \u201chow to use jigsaw\u201d is generated by applying the template on triple \u201c $\\langle $ jigsaw, performsActivity, CurveCut $\\rangle $ \u201d, before finally questions (Figure 2 ) are retrieved from Google with the seed question.",
                "We propose a system for generating questions from KB that significantly reduces the human effort by leveraging the massive web resources. Given a KB, a small set of question templates are first hand-crafted based on the predicates in the KB. These templates consist of a transcription of the predicate in the KB (e.g. performsActivity $\\Rightarrow $ how to) and placeholders for the subject (#X#) and the object (#Y#). A seed question set is then generated by applying the templates on the KB. The seed question set is further expanded through a search engine (e.g., Google, Bing), by iteratively forming each generated question as a search query to retrieve more related question candidates. Finally a selection step is applied by estimating the fluency and domain relevance of each question candidate."
            ],
            "predicted_section": [
                "System",
                "Introduction"
            ]
        },
        "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c": {
            "question_text": "How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?",
            "from_paper": "1707.06519",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Besides analyzing the cosine similarity of the learned representations, we also apply them to the query-by-example STD task. Here we compare the retrieval performance in MAP of INLINEFORM0 with different levels of accessibility to the low-resource target language along with two baseline models, INLINEFORM1 and INLINEFORM2 trained purely by the target languages. For the four target languages, the total available amount of audio word segments in the training set were 4 thousands for each language. In Table TABREF20 , we took different partitions of the target language training sets to fine tune the INLINEFORM3 pretrained by the source languages. The amount of audio word segments in these partitions are: 1K, 2K, 3K, 4K, and 0, which means no fine-tuning.",
                "To evaluate the quality of language transfer, we trained the Audio Word2Vec model by INLINEFORM0 from the source language, English, and applied it on different target languages, French (FRE), German (GER), Czech (CZE), and Spanish (ESP). We computed the average cosine similarity of the vector representations for each pair of the audio segments in the retrieval database of the target languages (20K segments for each language), and compare it with the phoneme sequence edit distance (PSED). The average and variance (the length of the black line on each bar) of the cosine similarity for groups of pairs clustered by the phoneme sequence edit distances (PSED) between the two words are shown in Fig. FIGREF14 . For comparison, we also provide the results obtained from the English retrieval database (250K segments), where the segments were not seen by the model in training procedure.",
                "From Table TABREF20 , INLINEFORM0 trained by source language generally outperforms the INLINEFORM1 trained by the limited amount of target language (\" INLINEFORM2 No Transfer\"), proving that with enough audio segments, INLINEFORM3 can identify and encode universal phonetic structure. Comparing with NE, INLINEFORM4 surpasses INLINEFORM5 in German and French even without fine-tuning, whereas in Czech, INLINEFORM6 also achieves better score than INLINEFORM7 with fine-tuning. However, in Spanish, INLINEFORM8 achieved a MAP score of 0.13 with fine-tuning, slightly lower than 0.17 obtained by INLINEFORM9 . Back to Fig. FIGREF14 , the gap between phoneme sequence edit distances 2 and 3 in Spanish is smaller than other languages. Also, as discussed earlier in Section 6.2, the variance in Spanish is also bigger. The smaller gap and bigger variance together indicate that the model is weaker on Spanish at identifying audio segments of different words and thus affects the MAP performance in Spanish."
            ],
            "predicted_section": [
                "Analysis of Language Transfer",
                "Language Transferring on STD"
            ]
        },
        "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14": {
            "question_text": "Which languages do they explore?",
            "from_paper": "1911.12848",
            "gold": [
                "Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages.",
                "Code-mixing is mixing two or more languages while communicating in person or over the web. Code-mixing is basically observed in the multilingual speakers. Code-mixed languages are a challenge to the sentiment analysis problem. A classic example of the code-mix language is Hinglish which is combination of English and Hindi words present in a sentence. Hinglish is widely used language in India to communicate over the web. For e.g. movie review in Hinglish is \u201cyeh movie kitni best hai.. Awesome.\u201d In this sentence movie, best and awesome are English words but the remaining words are Hindi words, so the language identification becomes the first step in code mix languages followed by the SA which indirectly increases the overhead for the researchers and becomes time consuming process.",
                "Pandey et al. BIBREF12 defined a framework to carry out the SA task on the Hindi movie reviews. BIBREF12 observed that the lower accuracy was obtained by using SWN as a classification technique and hence suggested using synset replacement algorithm along with the SWN. Synset replacement algorithms groups the synonymous words having same concepts together. It helped in increasing the accuracy of the system because if the word was not present in the Hindi SWN then it found the closest word and assigned the score of that word BIBREF12. In the study, Bhargava et al. BIBREF13 completed the SA task on the FIRE 2015 dataset. The dataset consisted of code-mixed sentences in English along with 4 Indian languages (Hindi, Bengali, Tamil, Telugu). The architecture consisted of 2 main steps Language Identification and Sentiment Classification. Punctuations, hashtags were identified and handled by the CMU Ark tagger. Machine learning techniques like logistic regression and SVM were used for language identification. SWN\u2019s of each language were used for sentiment classification. The results of the implemented system were compared with the previous language translation technique and 8% better precision was observed BIBREF13."
            ],
            "gold_section": [
                "Introduction ::: Code Mix Languages",
                "Sentiment Analysis Levels ::: Sentence Level",
                "Introduction ::: Indigenous Languages"
            ],
            "predicted": [
                "According to the authors Medagoda et al. BIBREF0 there has being a continuous research going on in the English language but the research carried out in the indigenous languages is less. Also, the researches in indigenous languages follow the techniques used for the English language but this has one disadvantage which is, techniques have properties which are specific to a language. Hence It is really important to understand and analyze Indigenous language data because it can give meaningful insights to the companies. For example, India and China have world's largest population and are rich in diverse languages, analysing these indigenous language will be useful to companies because they have large share of users in India and China. In the current study, the types of languages i.e. indigenous languages and code mix languages are discussed prior to the approaches, methodologies used by the researchers and challenges faced by them.",
                "Majority of the research carried out for indigenous languages is performed using Machine Learning algorithms except the research carried out by the authors in BIBREF12, BIBREF24, BIBREF26, BIBREF25. Deep learning algorithms have time and again proved to be much better than the traditional machine learning techniques.",
                "Indigenous languages are the languages that are native to a region or spoken by a group of people in a particular state. It is not necessarily a national language. For e.g. Irish, Tibetan, Spanish, Hindi, Marathi, Gujarati, Telugu, Tamil are the indigenous languages."
            ],
            "predicted_section": [
                "Discussions and Analysis",
                "Introduction",
                "Introduction ::: Indigenous Languages"
            ]
        },
        "5d790459b05c5a3e6f1e698824444e55fc11890c": {
            "question_text": "What are two baseline methods?",
            "from_paper": "1911.01770",
            "gold": [
                "Similarly to BIBREF19 and BIBREF17, we evaluated our model on 10 subsets of 1000 samples each. One sample of these subsets is composed of text embedding and image embedding in the shared latent space. Since our interest lies in the recipe retrieval task, we optimized and evaluated our model by using each image embedding in the subsets as query against all text embeddings. By ranking the query and the candidate embeddings according to their cosine distance, we estimate the median rank. The model's performance is best, if the matching text embedding is found at the first rank. Further, we estimate the recall percentage at the top K percent over all queries. The recall percentage describes the quantity of queries ranked amid the top K closest results. In Table TABREF11 the results are presented, in comparison to baseline methods.",
                "The proposed model architecture is based on a multi-path approach for each of the involved input data types namely, instructions, ingredients and images, similarly to BIBREF19. In Figure FIGREF4, the overall structure is presented. For the instruction encoder, we utilized a self-attention mechanism BIBREF20, which learns which words of the instructions are relevant with a certain ingredient. In order to encode the ingredients, a bidirectional RNN is used, since ingredients are an unordered list of words. All RNNs in the ingredients path were implemented with Long Short-Term Memory (LSTM) cells BIBREF21. We fixed the ingredient representation to have a length of 600, independent of the amount of ingredients. Lastly, the outputs of the self-attention-instruction encoder with ingredient attention and the output of the bidirectional LSTM ingredient-encoder are concatenated and mapped to the joint embedding space. The image analysis path is composed of a ResNet-50 model BIBREF22, pretrained on the ImageNet Dataset BIBREF23, with a custom top layer for mapping the image features to the joint embedding space. All word embeddings are pretrained with the word2vec algorithm BIBREF24 and fine tuned during the joint embedding learning phase. We chose 512-dimensional word embedding for our model with self-attention, whereas BIBREF19 and BIBREF17 chose a vector length of 300. In the following sections, more details about the aforementioned paths are presented.",
                "The emergence of multi-modal databases has led to novel approaches for meal image analysis. The fusion of visual features learned from images by deep Convolution Neural Networks (CNN) and textual features lead to outstanding results in food recognition applications. An early approach for recipe retrieval was based on jointly learning to predict food category and its ingredients using deep CNN BIBREF16. In a following step, the predicted ingredients are matched against a large corpus of recipes. More recent approach is proposed by BIBREF15 and is based on jointly learning recipe-text and image representations in a shared latent space. Recurrent Neural Networks (RNN) and CNN are mainly used to map text and image into the shared space. To align the text and image embedding vectors between matching recipe-image pairs, cosine similarity loss with margin was applied. Carvalho et al. BIBREF17 proposed a similar multi-modal embedding method for aligning text and image representations in a shared latent space. In contrast to Salvador et al. BIBREF15, they formulated a joint objective function which incorporates the loss for the cross-modal retrieval task and a classification loss, instead of using the latent space for a multitask learning setup. To address the challenge of encoding long sequences (like recipe instructions), BIBREF15 chose to represent single instructions as sentence embedding using the skip-thought technique BIBREF18. These encoded instruction sentences are referred to as skip-instructions and their embedding is not fine tuned when learning the image-text joint embedding."
            ],
            "gold_section": [
                "Experimental Setup and Results",
                "Introduction",
                "Materials and Methods ::: Model Architecture"
            ],
            "predicted": [
                "Qualitative results such as recipe retrieval, quality of the cluster formation in the joint embedding space and heat maps of instruction words are more important than the previously mentioned benchmarking scores. Depending on meal type, all baseline implementations as well as our Ingredient Attention based model exhibit a broad range of retrieval accuracy. In Figure FIGREF16 we present a few typical results on the intended recipe retrieval task.",
                "where $\\beta \\in [0,1]$ weights between quadratic and linear loss, $\\alpha \\in [0,2]$ is the margin and $\\gamma \\in [0,1]$ weights between semantic- and sample-loss. The triplet loss encourages the embedding vectors of a matching pair to be larger by a margin above its non-matching counterpart. Further, the semantic loss encourages the model to form clusters of dishes, sharing the same class. We chose $\\beta $ to be $0.1$, $\\alpha $ to be $0.3$ and $\\gamma $ to be $0.3$.",
                "We have trained our model using cosine similarity loss with margin as in BIBREF19 and with the triplet loss proposed by BIBREF17. Both objective functions and the semantic regularization by BIBREF19 aim at maximizing intra-class correlation and minimizing inter-class correlation."
            ],
            "predicted_section": [
                "Experimental Setup and Results",
                "Materials and Methods ::: Loss function"
            ]
        },
        "c176eb1ccaa0e50fb7512153f0716e60bf74aa53": {
            "question_text": "Are results reported only on English data?",
            "from_paper": "1910.01340",
            "gold": [
                "We used the IRA dataset that was released by Twitter after identifying the Russian trolls. The original dataset contains $3,841$ accounts, but we use a lower number of accounts and tweets after filtering them. We focus on accounts that use English as main language. In fact, our goal is to detect Russian accounts that mimic a regular US user. Then, we remove from these accounts non-English tweets, and maintain only tweets that were tweeted originally by them. Our final IRA accounts list contains 2,023 accounts.",
                "Recent years have seen a large increase in the amount of disinformation and fake news spread on social media. False information was used to spread fear and anger among people, which in turn, provoked crimes in some countries. The US in the recent years experienced many similar cases during the presidential elections, such as the one commonly known as \u201cPizzagate\" . Later on, Twitter declared that they had detected a suspicious campaign originated in Russia by an organization named Internet Research Agency (IRA), and targeted the US to affect the results of the 2016 presidential elections. The desired goals behind these accounts are to spread fake and hateful news to further polarize the public opinion. Such attempts are not limited to Twitter, since Facebook announced in mid-2019 that they detected a similar attempt originating from UAE, Egypt and Saudi Arabia and targeting other countries such as Qatar, Palestine, Lebanon and Jordan. This attempt used Facebook pages, groups, and user accounts with fake identities to spread fake news supporting their ideological agendas. The automatic detection of such attempts is very challenging, since the true identity of these suspicious accounts is hidden by imitating the profiles of real persons from the targeted audience; in addition, sometimes they publish their suspicious idea in a vague way through their tweets' messages.",
                "In this work, we identify online trolls in Twitter, namely IRA trolls, from a textual perspective. We study the effect of a set of text-based features and we propose a machine learning model to detect them. We aim to answer three research questions: RQ1. Does the thematic information improve the detection performance?, RQ2. Can we detect IRA trolls from only a textual perspective? and RQ3. How IRA campaign utilized the emotions to affect the public opinions?"
            ],
            "gold_section": [
                "Data ::: Russian Trolls (IRA)",
                "Introduction"
            ],
            "predicted": [
                "To understand more the NLI features performance, given their high performance comparing to the other features, we extract the top important tokens for each of the NLI feature subsets (see Figure FIGREF37). Some of the obtained results confirmed what was found previously. For instance, the authors in BIBREF19 found that Russians write English tweets with more prepositions comparing to native speakers of other languages (e.g. as, about, because in (c) Stop-words and RP in (a) POS in Figure FIGREF37). Further research must be conducted to investigate in depth the rest of the results.",
                "Table TABREF32 presents the classification results showing the performance of each feature set independently. Generally, we can see that the thematic information improves the performance of the proposed features clearly (RQ1), and with the largest amount in the Emotions features (see $-_{themes}$ and $+_{themes}$ columns). This result emphasizes the importance of the thematic information. Also, we see that the emotions performance increases with the largest amount considering F1$_{macro}$ value; this motivates us to analyze the emotions in IRA tweets (see the following section).",
                "Linguistic Analysis. We measure statistically significant differences in the cues markers of Morality, LIWC, Bias and Subjectivity, Stance, and Bad and Sexual words across IRA trolls and regular users. These findings presented in Table TABREF38 allows for a deeper understanding of IRA trolls."
            ],
            "predicted_section": [
                "Experiments and Analysis ::: Results",
                "Experiments and Analysis ::: Analysis"
            ]
        },
        "595fe416a100bc7247444f25b11baca6e08d9291": {
            "question_text": "What profile features are used?",
            "from_paper": "1910.01340",
            "gold": [
                "Native Language Identification (NLI): This feature was inspired by earlier works on identifying native language of essays writers BIBREF22. We aim to detect IRA trolls by identifying their way of writing English tweets. As shown in BIBREF19, English tweets generated by non-English speakers have a different syntactic pattern . Thus, we use state-of-the-art NLI features to detect this unique pattern BIBREF23, BIBREF24, BIBREF25; the feature set consists of bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL). We extract the POS and the DEPREL information using spaCy, an off-the-shelf POS tagger. We clean the tweets from the special characters and maintained dots, commas, and first-letter capitalization of words. We use regular expressions to convert a sequence of dots to a single dot, and similarly for sequence of characters.",
                "Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.",
                "Textual Representation ::: Profiling IRA Accounts"
            ],
            "gold_section": [
                "Textual Representation ::: Profiling IRA Accounts"
            ],
            "predicted": [
                "Stylistic: We extract a set of stylistic features following previous works in the authorship attribution domain BIBREF27, BIBREF28, BIBREF29, such as: the count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions. In addition, we extract the uppercase ratio and the tweet length.",
                "Similar to the feature representation of the theme-based features, we represent each user's tweets by considering the average and standard deviation of her tweets' $V_{1,2,..N}$, given $V_i$ as the concatenation of the previous two features vectors of a tweet$_i$. A user $x$ final feature vector is defined as follows:",
                "IRA dataset provided by Twitter contains less information about the accounts details, and they limited to: profile description, account creation date, number of followers and followees, location, and account language. Therefore, as another baseline we use the number of followers and followees to assess their identification ability (we will mention them as Network Features in the rest of the paper)."
            ],
            "predicted_section": [
                "Experiments and Analysis ::: Baselines",
                "Textual Representation ::: Profiling IRA Accounts"
            ]
        },
        "3a25f82512d56d9e1ffba72f977f515ae3ba3cca": {
            "question_text": "Does programme plans gathering and open sourcing some large dataset for Icelandic language?",
            "from_paper": "2003.09244",
            "gold": [
                "As mentioned above, a number of language resources have been made available at the repository m\u00e1lf\u00f6ng. Most of these are now also available at the CLARIN-IS website and will be integrated into the CLARIN Virtual Language Observatory. Below we give a brief and non-exhaustive overview of language resources for Icelandic which will be developed in the programme.",
                "We will update the IGC with new data from more sources and continue collecting data from rights holders who have given their permission for using their material. A new version will be released each year during the five-year programme.",
                "Treebanks. The largest of the syntactically parsed treebanks that exist is the Icelandic Parsed Historical Corpus (IcePaHC; Wallenberg et al., 2011; R\u00f6gnvaldsson et al., 2011, 2012), which contains one million words from the 12th to the 21st century. The scheme used for the syntactic annotation is based on the Penn Parsed Corpora of Historical English BIBREF24, BIBREF25. On the other hand, no Universal Dependencies (UD)-treebanks are available for Icelandic. Within the programme, a UD-treebank will by built, based on IcePaHC, and extended with new material.",
                "Morphological database. The Database of Icelandic Morphology (DIM; Bjarnad\u00f3ttir et al., 2019) contains inflectional paradigms of about 287,000 lemmas. A part of the database, DMII-Core, only includes data in a prescriptive context and is suited for language learners, creating teaching material and other prescriptive uses. It consists of the inflection of approx. 50,000 words. We will extend it by reviewing ambiguous inflection forms. We will define format for data publication as the core will be available for use by a third party. For the sake of simplifying the process of adding material to the database and its maintenance, we will take advantage of the lexicon acquisition tool described in Section SECREF16 and adapt it for DIM."
            ],
            "gold_section": [
                "Core Projects ::: Language Resources"
            ],
            "predicted": [
                "The Icelandic Government decided in 2017 to fund a five-year programme for Icelandic LT, based on a report written by a group of LT experts BIBREF0. After more than two years of preparation, a consortium consisting of universities, institutions, associations, and private companies started the work on the programme on the 1st of October 2019. The goal of the programme is to ensure that Icelandic can be made available in LT applications, and thus will be usable in all areas of communication. Furthermore, that access to information and other language-based communication and interaction in Icelandic will be accessible to all, e.g. via speech synthesis or speech-to-text systems.",
                "Parallel data. Icelandic's rich morphology and relatively free word order is likely to demand large amount of training data in order to develop MT systems that produce adequate and fluent translations. The ParIce corpus currently consists of only 3.5 million sentence pairs which is rather small in relation to parallel corpora in general. The goal of this phase is to create an aligned and filtered parallel corpus of translated documents from the European Economic Area (EEA) domain (e.g. regulations and directives). As of 2017, around 7,000 documents were available in Icelandic with corresponding documents in English. The aim is to pair all accessible documents in the course of the project.",
                "Icelandic wordnet. The Icelandic wordnet BIBREF26, which contains 200,000 phrasemes of various kinds and about 100,000 compounds, is not a traditional dictionary as it analyses internal connections semantically and syntactically within Icelandic vocabulary. We will define a more appropriate data format and convert the wordnet data to that format. In addition, we will work on improving the wordnet itself by filling in gaps in various categories."
            ],
            "predicted_section": [
                "Core Projects ::: Language Resources",
                "Core Projects ::: Machine Translation",
                "Introduction"
            ]
        },
        "b59f3a58939f7ac007d3263a459c56ebefc4b49a": {
            "question_text": "What concrete software is planned to be developed by the end of the programme?",
            "from_paper": "2003.09244",
            "gold": [
                "Previously, two tokenisers have been built for Icelandic, one is a part of IceNLP and the other a part of Greynir. As Greynir is still in active development, it will be used as a base for the LT project's development. In order to be able to test the tokenisers' accuracy, a test set that takes different tokeniser settings into account will be developed.",
                "Software development. The software development tasks of the spell and grammar checking project will build a working open source correction system whose development is informed by the analysis of the data sets created within the project. The spell and grammar checker will be based on the foundation for processing Icelandic text provided by the Greynir system.",
                "Software implementation and research. The research areas are chosen so to enhance the language resource development for Icelandic. A punctuation system for Icelandic will be analysed and implemented. Compound words are common in Icelandic and the language also has a relatively rich inflection structure so it is important to address those features for language modeling. Pronunciation analysis, speaker diarization and speech analysis will also be addressed especially for Icelandic, and acoustic modelling for children and teenagers receive attention in the project."
            ],
            "gold_section": [
                "Core Projects ::: NLP Tools",
                "Core Projects ::: Automatic Speech Recognition (ASR)",
                "Core Projects ::: Spell and Grammar Checking"
            ],
            "predicted": [
                "After studying somewhat similar national programmes in other European countries, we have defined the most important factors that in our opinion will help lead to the success of the programme: First, we have defined core projects that comprise the most important language resources and software tools necessary for various LT applications. Second, all deliverables will be published under as open licenses as possible and all resources and software will be easily accessible. The deliverables will be packaged and published for use in commercial applications, where applicable. Third, from the beginning of the programme, we encourage innovation projects from academia and industry through a competitive R&D fund, and fourth, constant communication with users and industry through conferences, events and direct interaction will be maintained, with the aim of putting deliverables to use in products as soon as possible. The cooperation between academia and industry is also reflected in the consortium of universities, institutions, associations, and private companies, performing the R&D work for all core projects.",
                "The focus of the programme will be on the development of text and speech-based language resources, on the development of core natural language processing (NLP) tools like tokenisers, taggers and parsers, and finally, to publish open-source software in the areas of speech recognition, speech synthesis, machine translation, and spell and grammar checking. All deliverables of the programme will be published under open licenses, to encourage use of resources and software in commercial products.",
                "We will work on these four aspects of semantic analysis listed above. In recent years, not much work has been carried out in this field for Icelandic. This part of the LT programme will thus start with researching the current state-of-the-art and defining realistic goals."
            ],
            "predicted_section": [
                "Core Projects ::: NLP Tools",
                "Introduction",
                "Conclusion"
            ]
        },
        "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5": {
            "question_text": "Which of their proposed domain adaptation methods proves best overall?",
            "from_paper": "1710.06923",
            "gold": [
                "We ran our Evo-Devo mechanism with the 486 ASR sentences (see Table TABREF14 ) and measured the accuracy after each repair. On an average we have achieved about 5 to 10% improvements in the accuracy of the sentences. Fine-tuning the repair and fitness functions, namely Equation (), would probably yield much better performance accuracies. However, experimental results confirm that the proposed Evo-Devo mechanism is an approach that is able to adapt INLINEFORM0 to get closer to INLINEFORM1 . We present a snapshot of the experiments with Google ASR (Ga) and calculate accuracy with respect to the user spoken question as shown in Table TABREF16 .",
                "In the machine learning technique of adaptation, we considers INLINEFORM0 pairs as the predominant entity and tests the accuracy of classification of errors.",
                "The combination of features INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 namely, (bag of consonants, bag of vowels, left context, number of words, right context) gave the best results with INLINEFORM5 % improvement in accuracy in classification over 10-fold validation."
            ],
            "gold_section": [
                " Evo-Devo based experiments",
                "Machine Learning experiments"
            ],
            "predicted": [
                "General-purpose ASR engines when used for enterprise domains may output erroneous text, especially when encountering domain-specific terms. One may have to adapt/repair the ASR output in order to do further natural language processing such as question-answering. We have presented two mechanisms for adaptation/repair of ASR-output with respect to a domain. The Evo-Devo mechanism provides a bio-inspired abstraction to help structure the adaptation and repair process. This is one of the main contribution of this paper. The machine learning mechanism provides a means of adaptation and repair by examining the feature-space of the ASR output. The results of the experiments show that both these mechanisms are promising and may need further development.",
                "Table TABREF16 clearly demonstrates the promise of the evo-devo mechanism for adaptation/repair. In our experiments we observed that the adaptation/repair of sub-parts in ASR-output ( INLINEFORM0 ) that most probably referred to domain terms occurred well and were easily repaired, thus contributing to increase in accuracy. For non-domain-specific linguistic terms the method requires one to build very good linguistic repair rules, without which the method could lead to a decrease in accuracy. One may need to fine-tune the repair, match and fitness functions for linguistic terms. However, we find the abstraction of evo-devo mechanism is very apt to use.",
                "Note that this is equivalent to, albiet loosely, learning the error model of a specific ASR. Since we have a small training set, we have used the Naive Bayes classifier that is known to perform well for small datasets with high bias and low variance. We have used the NLTK BIBREF11 Naive Bayes classifier in all our experiments."
            ],
            "predicted_section": [
                "Machine Learning mechanism of adaptation",
                "Conclusions",
                " Evo-Devo based experiments"
            ]
        },
        "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86": {
            "question_text": "Which intrisic measures do they use do evaluate obtained representations?",
            "from_paper": "1811.04791",
            "gold": [
                "The results above were presented as part of an earlier conference version of this paper BIBREF3 . Here, we expand upon that work in several ways. First, we include new results on the corpora and evaluation measures used in the zrsc, to allow more direct comparisons with other work. In doing so, we also provide the first set of results on identical systems evaluated using both the same-different and ABX evaluation measures. This permits the two measures themselves to be better compared. Finally, we provide both a qualitative analysis of the differences between the different features we extract, and a quantitative evaluation on the downstream target-language task of unsupervised full-coverage speech segmentation and clustering using the system of BIBREF4 . This is the first time that multilingual features are used in such a system, which performs a complete segmentation of input speech into hypothesized words. As in our intrinsic evaluations, we find that the multilingual bnfs consistently outperform the best unsupervised cae features, which in turn outperform or do similarly to MFCCs.",
                "In this work we use the cae in our experiments on unsupervised representation learning, since it performed well in the 2015 ZRSC, achieved some of the best-reported results on the same-different task (which we also consider), and has readily available code. As noted above, the cae attempts to normalize out non-linguistic factors such as speaker, channel, gender, etc., by using top-down information from pairs of similar speech segments. Extracting cae features requires three steps, as illustrated in Figure FIGREF6 . First, an utd system is applied to the target language to extract pairs of speech segments that are likely to be instances of the same word or phrase. Each pair is then aligned at the frame level using dtw, and pairs of aligned frames are presented as the input INLINEFORM0 and target output INLINEFORM1 of a dnn. After training, a middle layer INLINEFORM2 is used as the learned feature representation."
            ],
            "gold_section": [
                "Introduction",
                "Background and Motivation"
            ],
            "predicted": [
                "In the previous experiments, we used data from GlobalPhone, which provides corpora collected and formatted similarly for a wide range of languages. However, GlobalPhone is not freely available and no previous zero-resource studies have used these corpora, so in this section we also provide results on the zrsc 2015 BIBREF0 data sets, which have been widely used in other work. The target languages are English (from the Buckeye corpus BIBREF38 ) and Xitsonga (NCHLT corpus BIBREF39 ). Table TABREF8 includes the corpus statistics. These corpora are not split into train/dev/test; since training is unsupervised, the system is simply trained directly on the unlabeled test set (which could also be done in deployment). Importantly, no hyperparameter tuning is done on the Buckeye or Xitsonga data, so these results still provide a useful test of generalization. Notably, the Buckeye English corpus contains conversational speech and is therefore different in style from the rest of our data.",
                "For comparison we also include ABX results of the official zrsc 2015 topline BIBREF0 , which are posteriorgrams obtained from a supervised speech recognition system, the current state-of-the-art system BIBREF18 which even outperforms the topline for English, and the system of BIBREF42 which is the most recent form of the ABNet BIBREF12 , an architecture that is similar to our cae.",
                "We use several metrics to compare the resulting segmented word tokens to ground truth forced alignments of the data. By mapping every discovered word token to the ground truth word with which it overlaps most, average cluster purity can be calculated as the total proportion of correctly mapped tokens in all clusters. More than one cluster may be mapped to the same ground truth word type. In a similar way, we can calculate unsupervised word error rate (WER), which uses the same cluster-to-word mapping but also takes insertions and deletions into account. Here we consider two ways to perform the cluster mapping: many-to-one, where more than one cluster can be assigned the same word label (as in purity), or one-to-one, where at most one cluster is mapped to a ground truth word type (accomplished in a greedy fashion). We also compute the gender and speaker purity of the clusters, where we want to see clusters that are as diverse as possible on these measures, i.e., low purity. To explicitly evaluate how accurate the model performs segmentation, we compare the proposed word boundary positions to those from forced alignments of the data (falling within a single true phoneme from the boundary). We calculate boundary precision and recall, and report the resulting word boundary F-scores. We also calculate word token F-score, which requires that both boundaries from a ground truth word token be correctly predicted."
            ],
            "predicted_section": [
                "Experimental Setup and Evaluation",
                "Evaluation using ZRSC Data and Measures"
            ]
        },
        "101d7a355e8bf6d1860917876ee0b9971eae7a2f": {
            "question_text": "Do they report results only for English data?",
            "from_paper": "1611.04887",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "This test essentially captures the importance of \u201cnatural word order\u201d. We found that LDA was invariant to the reordering of the words in the tweet for most of the tasks. This result is not surprising as LDA considers each word in the tweet independently. CNN, LSTM and BLSTM rely on the word order significantly to perform well for most of the prediction tasks.",
                "This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order.",
                "Fine-grained analysis of various supervised and unsupervised models discussed in Section SECREF3 , across various dimensions discussed in Section SECREF4 , is presented in Table TABREF30 . The codes used to conduct our experiments are publicly accessible at: https://github.com/ganeshjawahar/fine-tweet/."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Conclusion",
                "Sensitivity of Property Prediction Task to Word Order"
            ]
        },
        "4288621e960ffbfce59ef1c740d30baac1588b9b": {
            "question_text": "What conclusions do the authors draw from their experiments?",
            "from_paper": "1611.04887",
            "gold": [
                "This work proposed a set of elementary property prediction tasks to understand different tweet representations in an application independent, fine-grained fashion. The open nature of social media not only poses a plethora of opportunities to understand the basic characteristics of the posts, but also helped us draw novel insights about different representation models. We observed that among supervised models, CNN, LSTM and BLSTM encapsulates most of the syntactic and social properties with a great accuracy, while BOW, DSSM, STV and T2V does that among the unsupervised models. Tweet length affects the task prediction accuracies, but we found that all models behave similarly under variation in tweet length. Finally while LDA is insensitive to input word order, CNN, LSTM and BLSTM are extremely sensitive to word order."
            ],
            "gold_section": [
                "Conclusion"
            ],
            "predicted": [
                "In this section we perform an extensive evaluation of all the models in an attempt to find the significance of different representation models. Essentially we study every model (with optimal settings reported in the corresponding paper) with respect to the following three perspectives.",
                "The paper is organized as follows. Sections 2 and 3 discuss the set of proposed elementary property prediction tasks and the models considered for this study respectively. Section 4 and 5 presents the experiment setup and result analysis respectively. We conclude the work with a brief summary in Section 5.",
                "Fine-grained analysis of various supervised and unsupervised models discussed in Section SECREF3 , across various dimensions discussed in Section SECREF4 , is presented in Table TABREF30 . The codes used to conduct our experiments are publicly accessible at: https://github.com/ganeshjawahar/fine-tweet/."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Introduction",
                "Experiments"
            ]
        },
        "042800c3336ed5f4826203616a39747c61382ba6": {
            "question_text": "Which commonsense knowledge base are they using?",
            "from_paper": "1709.05453",
            "gold": [
                "In our experiment, ConceptNet is used as the commonsense knowledge base. Preprocessing of this knowledge base involves removing assertions containing non-English characters or any word outside vocabulary $V$ . 1.4M concepts remain. 0.8M concepts are unigrams, 0.43M are bi-grams and the other 0.17M are tri-grams or more. Each concept is associated with an average of 4.3 assertions. More than half of the concepts are associated with only one assertion."
            ],
            "gold_section": [
                "ConceptNet"
            ],
            "predicted": [
                "Several commonsense knowledge bases have been constructed during the past decade, such as ConceptNet BIBREF17 and SenticNet BIBREF18 . The aim of commonsense knowledge representation and reasoning is to give a foundation of real-world knowledge to a variety of AI applications, e.g., sentiment analysis BIBREF19 , handwriting recognition BIBREF20 , e-health BIBREF21 , aspect extraction BIBREF22 , and many more. Typically, a commonsense knowledge base can be seen as a semantic network where concepts are nodes in the graph and relations are edges (Figure 2 ). Each $<concept1, relation, concept2 >$ triple is termed an assertion.",
                "In this paper, we assume that a commonsense knowledge base is composed of assertions $A$ about concepts $C$ . Each assertion $a \\in A$ takes the form of a triple $<c_1,r,c_2 >$ , where $r \\in R$ is a relation between $c_1$ and $c_2$ , such as IsA, CapableOf, etc. $c_1,c_2$ are concepts in $C$ . The relation set $R$ is typically much smaller than $C$0 . $C$1 can either be a single word (e.g., \u201cdog\u201d and \u201cbook\u201d) or a multi-word expression (e.g., \u201ctake_a_stand\u201d and \u201cgo_shopping\u201d). We build a dictionary $C$2 out of $C$3 where every concept $C$4 is a key and a list of all assertions in $C$5 concerning $C$6 , i.e., $C$7 or $C$8 , is the value. Our goal is to retrieve commonsense knowledge about every concept covered in the message.",
                "In the context of artificial intelligence (AI), commonsense knowledge is the set of background information that an individual is intended to know or assume and the ability to use it when appropriate BIBREF3 , BIBREF4 , BIBREF5 . Due to the vastness of such kind of knowledge, we speculate that this goal is better suited by employing an external memory module containing commonsense knowledge rather than forcing the system to encode it in model parameters as in traditional methods."
            ],
            "predicted_section": [
                "Commonsense Knowledge Retrieval",
                "Introduction",
                "Commonsense Knowledge"
            ]
        },
        "74fcb741d29892918903702dbb145fef372d1de3": {
            "question_text": "What is the model trained?",
            "from_paper": "1909.02322",
            "gold": [
                "We propose an alternative to the Extract first, Abstract later (EA) approach which eliminates the need for an extractive model and enables the use of all input documents when generating the summary. Figure FIGREF5 illustrates our Condense-Abstract (CA) framework. In lieu of an integrated encoder-decoder, we generate summaries using two separate models. The Condense model returns document encodings for $N$ input documents, while the Abstract model uses these encodings to create an abstractive summary. This two-step approach has at least three advantages for multi-document summarization. Firstly, optimization is easier since parameters for the encoder and decoder weights are learned separately. Secondly, CA-based models are more space-efficient, since $N$ documents in the cluster are not treated as one very large instance but as $N$ separate instances when training the Condense model. Finally, it is possible to generate customized summaries targeting specific aspects of the input since the Abstract model operates over the encodings of all available documents.",
                "Let $\\mathcal {D}$ denote a cluster of $N$ documents about a specific target (e.g., a movie or product). For each document $X=\\lbrace w_1,w_2,...,w_M\\rbrace \\in \\mathcal {D}$, the Condense model learns an encoding $d$, and word-level encodings $h_1, h_2, ..., h_M$. We use a BiLSTM autoencoder as the Condense model. Specifically, we employ a Bidirectional Long Short Term Memory (BiLSTM) encoder BIBREF31:",
                "The decoder generates summaries conditioned on the reduced document encoding $d^{\\prime }$ and reduced word-level encodings $h^{\\prime }_1,h^{\\prime }_2,...,h^{\\prime }_V$. We use a simple LSTM decoder enhanced with attention BIBREF14 and copy mechanisms BIBREF32. We set the first hidden state $s_0$ to $d^{\\prime }$, and run an LSTM to calculate the current hidden state using the previous hidden state $s_{t-1}$ and word $y^{\\prime }_{t-1}$ at time step $t$:"
            ],
            "gold_section": [
                "Condense-Abstract Framework ::: The Abstract Model ::: Decoder",
                "Condense-Abstract Framework",
                "Condense-Abstract Framework ::: The Condense Model"
            ],
            "predicted": [
                "For all experiments, our model used word embeddings with 128 dimensions, pretrained on the training data using GloVe BIBREF34. We set the dimensions of all hidden vectors to 256, the batch size to 8, and the beam search size to 5. We applied dropout BIBREF35 at a rate of 0.5. The model was trained using the Adam optimizer BIBREF36 and $l_2$ constraint BIBREF37 of 2. We performed early stopping based on model performance on the development set. Our model is implemented in PyTorch.",
                "",
                "We use two objective functions to train the Abstract model. Firstly, we use a maximum likelihood loss to optimize the generation probability distribution $p(y^{\\prime }_t)$ based on gold summaries $Y=\\lbrace y_1,y_2,...,y_L\\rbrace $ provided at training time:"
            ],
            "predicted_section": [
                "Experimental Setup ::: Training Configuration",
                "Condense-Abstract Framework ::: The Abstract Model ::: Training"
            ]
        },
        "6a20a3220c4edad758b912e2d3e5b99b0b295d96": {
            "question_text": "How exactly do they weigh between different statistical models?",
            "from_paper": "1805.04579",
            "gold": [
                "After generating summary from a particular model, our aim is to compute summaries through overlap of different models. Let us have INLINEFORM0 summaries from INLINEFORM1 different models. For INLINEFORM2 summarization model, let the INLINEFORM3 sentences contained be:-",
                "Given a document INLINEFORM0 we tokenize it into sentences as < INLINEFORM1 >.",
                "Now for our list of sentences INLINEFORM0 we define cWeight as weight obtained for each sentence using INLINEFORM1 models.",
                "Here, INLINEFORM0 is a function which returns 1 if sentence is in summary of INLINEFORM1 model, otherwise zero. INLINEFORM2 is weight assigned to each model without training, INLINEFORM3"
            ],
            "gold_section": [
                "Single Document Summarization",
                "Prepossessing"
            ],
            "predicted": [
                "We use two similarity measures :",
                "In the Table 1, we try different model pairs with weights trained on corpus for Task 2. We have displayed mean ROUGE-2 scores for base Models. We have calculated final scores taking into consideration all normalizations, stemming, lemmatizing and clustering techniques, and the ones providing best results were used. We generally expected WordNet, Glove based semantic models to perform better given they better capture crux of the sentence and compute similarity using the same, but instead, they performed average. This is attributed to the fact they assigned high similarity scores to not so semantically related sentences. We also observe that combinations with TF/IDF and Similarity Matrices(Jaccard/Cosine) offer nearly same results. The InferSent based Summarizer performed exceptionally well. We initially used pre-trained features to generate sentence vectors through InferSent.",
                "Applying positional weighing . INLINEFORM0 INLINEFORM1 "
            ],
            "predicted_section": [
                "Using Stastical Models",
                "Experiments"
            ]
        },
        "a1a0365bf6968cbdfd1072cf3923c26250bc955c": {
            "question_text": "What type of neural models are used?",
            "from_paper": "1803.08419",
            "gold": [
                "Sequence to Sequence approaches for dialogue modelling",
                "The problem with rule-based models was that they were often domain dependent and could not be easily ported to a new domain. They also depended on hand crafted rules which was both expensive and required domain expertise. Two factors which when combined spell doom for scalbility. All of this changed in 2015 when Vinyals et al proposed an approach BIBREF2 inspired from the recent progress in machine translation BIBREF1 . Vinyals et al used the sequence to sequence learning architecture for conversation agents. Their model was the first model which could be trained end-to-end, and could generate a new output utterance based on just the input sentence and no other hand crafted features.",
                "Language Model based approaches for dialogue modelling",
                "Though sequence-to-sequence based models have achieved a lot of success, another push in the field has been to instead train a language model over the entire dialogue as one single sequence BIBREF18 . These works argue that a language model is better suited to dialogue modeling, as it learns how the conversation evolves as information progresses."
            ],
            "gold_section": [
                "Language Model based approaches for dialogue modelling",
                "Sequence to Sequence approaches for dialogue modelling"
            ],
            "predicted": [
                "After exploring the neural methods in a lot of detail, the researchers have also begun exploring, in the current decade, how to use the reinforcement learning methods in the dialogue and personal agents.",
                "For each utterance, they calculated features like n-grams of the words and their POS tags, dialog act and task/subtask label. Then they put those features in the binary MaxEnt classifier. For this, their model was able to achieve an error rate of 25.1% for the dialog act classification which was better than the best performing models at that time. Although, according to the modern standards, the results are not that great but the approach they suggested (of using data to build machine learning models) forms the basis of the techniques that are currently used in this area.",
                "Building on works like this the Emotional Chatting Machine model proposed by Zhou et al BIBREF30 is a model which generates responses that are not only grammatically consistent but also emotionally consistent. To achieve this their approach models the high-level abstraction of emotion expressions by embedding emotion categories. They also capture the change of implicit internal emotion states and use explicit emotion expressions with an external emotion vocabulary."
            ],
            "predicted_section": [
                "Reinforcement Learning based models",
                "Approaches to Human-ize agents",
                "Machine Learning Methods"
            ]
        },
        "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550": {
            "question_text": "What was the proposed use of conversational agents in pioneering work?",
            "from_paper": "1803.08419",
            "gold": [
                "Early Techniques",
                "Initially, the interactive dialogue systems were based on and limited to speaker independent recognition of isolated words and phrases or limited continuous speech such as digit strings. In August 1993, there came the ESPRIT SUNDIAL project (Peckham et al, 1993 BIBREF3 ) which was aimed at allowing spontaneous conversational inquiries over the telephone for the train timetable and flight enquiries. The linguistic processing component in it was based on natural language parsing. The parser made use of alternative word hypotheses represented in a lattice or graph in constructing a parse tree and allowance was made for gaps and partially parsable strings. It made use of both syntactic and semantic knowledge for the task domain. It was able to achieve a 96% success rate for the flight inquiry application in English. However, the issue was that the given conversational agent was heavily limited to the types of applications it can perform and its high success rate was more due to that instead of great natural language techniques (relative to recent times)."
            ],
            "gold_section": [
                "Early Techniques"
            ],
            "predicted": [
                "The researchers thought that if they can create assistant models specific to the corresponding models, they can achieve better accuracy for those applications instead of creating a common unified personal assistant which at that time performed quite poorly. There was a surge in application-specific assistants like in-car intelligent personal assistant (Schillo et al, 1996 BIBREF5 ), spoken-language interface to execute military exercises (Stent et al, 1999 BIBREF6 ), etc. Since it was difficult to develop systems with high domain extensibility, the researchers came up with a distributed architecture for cooperative spoken dialogue agents (Lin et al, 1999 BIBREF7 ).",
                "In this survey paper we explored the exciting and rapidly changing field of conversational agents. We talked about the early rule-based methods that depended on hand-engineered features. These methods laid the ground work for the current models. However these models were expensive to create and the features depended on the domain that the conversational agent was created for. It was hard to modify these models for a new domain. As computation power increased, and we developed neural networks that were able to capture long range dependencies (RNNs,GRUs,LSTMs) the field moved towards neural models for building these agents. Sequence to sequence model created in 2015 was capable of handling utterances of variable lengths, the application of sequence to sequence to conversation agents truly revolutionized the domain. After this advancement the field has literally exploded with numerous application in the last couple of years. The results have been impressive enough to find their way into commercial applications such that these agents have become truly ubiquitous. We attempt to present a broad view of these advancements with a focus on the main challenges encountered by the conversational agents and how these new approaches are trying to mitigate them.",
                "Starting with pattern matching programs like ELIZA developed at MIT in 1964 to the current commercial conversational agents and personal assistants (Siri, Allo, Alexa, Cortana et al) that all of us carry in our pockets, conversational agents have come a long way. In this paper we look at this incredible journey. We start by looking at early rule-based methods which consisted of hand engineered features, most of which were domain specific. However, in our view, the advent of neural networks that were capable of capturing long term dependencies in text and the creation of the sequence to sequence learning model BIBREF1 that was capable of handling utterances of varying length is what truly revolutionized the field. Since the sequence to sequence model was first used to build a neural conversational agent BIBREF2 in 2016 the field has exploded. With a multitude of new approaches being proposed in the last two years which significantly impact the quality of these conversational agents, we skew our paper towards the post 2016 era. Indeed one of the key features of this paper is that it surveys the exciting new developments in the domain of conversational agents."
            ],
            "predicted_section": [
                "Early Techniques",
                "Introduction",
                "Conclusion"
            ]
        },
        "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a": {
            "question_text": "what was their newly established state of the art results?",
            "from_paper": "1707.05589",
            "gold": [
                "We compare models on three datasets. The smallest of them is the Penn Treebank corpus by BIBREF13 with preprocessing from BIBREF14 . We also include another word level corpus: Wikitext-2 by BIBREF15 . It is about twice the size of Penn Treebank with a larger vocabulary and much lighter preprocessing. The third corpus is Enwik8 from the Hutter Prize dataset BIBREF16 . Following common practice, we use the first 90 million characters for training, and the remaining 10 million evenly split between validation and test.",
                "We tested LSTMs of various depths and an RHN of depth 5 with parameter budgets of 10 and 24 million matching the sizes of the Medium and Large LSTMs by BIBREF18 . The results are summarised in Table TABREF9 .",
                "Notably, in our experiments even the RHN with only 10M parameters has better perplexity than the 24M one in the original publication. Our 24M version improves on that further. However, a shallow LSTM-based model with only 10M parameters enjoys a very comfortable margin over that, with deeper models following near the estimated noise range. At 24M, all depths obtain very similar results, reaching exp(4.065) [fixed,zerofill,precision=1] at depth 4. Unsurprisingly, NAS whose architecture was chosen based on its performance on this dataset does almost equally well, even better than in BIBREF1 .",
                "Wikitext-2 is not much larger than Penn Treebank, so it is not surprising that even models tuned for Penn Treebank perform reasonably on this dataset, and this is in fact how results in previous works were produced. For a fairer comparison, we also tune hyperparameters on the same dataset. In Table TABREF14 , we report numbers for both approaches. All our results are well below the previous state of the are for models without dynamic evaluation or caching. That said, our best result, exp(4.188) [fixed,zerofill,precision=1] compares favourably even to the Neural Cache BIBREF6 whose innovations are fairly orthogonal to the base model."
            ],
            "gold_section": [
                "Datasets",
                "Wikitext-2",
                "Penn Treebank"
            ],
            "predicted": [
                "During the transitional period when deep neural language models began to supplant their shallower predecessors, effect sizes tended to be large, and robust conclusions about the value of the modelling innovations could be made, even in the presence of poorly controlled \u201chyperparameter noise.\u201d However, now that the neural revolution is in full swing, researchers must often compare competing deep architectures. In this regime, effect sizes tend to be much smaller, and more methodological care is required to produce reliable results. Furthermore, with so much work carried out in parallel by a growing research community, the costs of faulty conclusions are increased.",
                "Naturally, NAS benefitted only to a limited degree from our tuning, since the numbers of BIBREF1 were already produced by employing similar regularisation methods and a grid search. The small edge can be attributed to the suboptimality of grid search (see Section SECREF23 ).",
                "In contrast to the previous datasets, our numbers on this task (reported in BPC, following convetion) are slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs which is about a tenth of what the model of BIBREF0 was trained for. Nevertheless, we match their smaller RHN with our models which are very close to each other. NAS lags the other models by a surprising margin at this task."
            ],
            "predicted_section": [
                "Analysis",
                "Enwik8",
                "Conclusion"
            ]
        },
        "636ac549cf4917c5922cd09a655abf278924c930": {
            "question_text": "how was the experiment evaluated?",
            "from_paper": "1910.03943",
            "gold": [
                "A robust metric for evaluating a set of hotel embeddings (or, more generally, any set of items displayed to a user in response to an information need) is its ability to predict a user's next click/selection. In this section, we compare our model based on the hits@k metric in various scenarios. Hits@k measures the average number of times the correct selection appears in the top k predictions."
            ],
            "gold_section": [
                "Experimental Results ::: Quantitative Analysis ::: Hits@k for hotel context prediction"
            ],
            "predicted": [
                "In this section, we present several experiments to evaluate the performance of the trained hotel2vec embeddings. Before diving into the details of the experiments, we first describe the dataset and model parameters.",
                "Our dataset contains more than 40M user click sessions, which includes more than 1.1 million unique hotels. A click session is defined as a span of clicks performed by a user with no gap of more than 7 days. We randomly split the sessions into training, validation, and test with a ratio of 8:1:1.",
                ""
            ],
            "predicted_section": [
                "Experimental Results",
                "Experimental Results ::: Experimental Framework",
                "The Proposed Framework ::: Neural Network Architecture"
            ]
        },
        "78c7318b2218b906a67d8854f3e511034075f79a": {
            "question_text": "Which dialogue data do they use to evaluate on?",
            "from_paper": "1909.03087",
            "gold": [
                "We perform experiments on two tasks, PersonaChat and Wizard of Wikipedia, which evaluate different aspects of conversational ability. We first optimize the questions to maximize worker agreement, and then benchmark existing state-of-the-art models on each task."
            ],
            "gold_section": [
                "Experiments"
            ],
            "predicted": [
                "Dialogue between human and machine is an important end-goal of natural language research. The open-ended nature of generating sequences in a multi-turn setup naturally makes the task difficult to evaluate \u2013 with full evaluation possessing many of the difficulties of the task itself as it requires deep understanding of the content of the conversation. As in many other natural language generation (NLG) tasks, automatic metrics have not been shown to have a clear correlation with human evaluations BIBREF0, BIBREF1. This means the current standard for all dialogue research involves human trials, which slows down research and greatly increases the cost of model development.",
                "Currently the standard approach in chitchat dialogue is to perform human evaluations BIBREF2, BIBREF20, BIBREF21, BIBREF4, BIBREF5, BIBREF7, typically reporting a judgment such as conversation quality or appropriateness via a Likert scale or pairwise comparison. While conversations are naturally multi-turn, pairwise setups typically consider single turn evaluations, taking the \u201cgold\u201d dialogue history from human-human logs, and only consider altering a single utterance. A more complete multi-turn evaluation is typically measured with a Likert scale (usually 1-4 or 1-5) after the conversation takes place. Some works such as BIBREF6 ask a series of questions relating to different aspects of conversational ability. There are some notable variants from these standard setups. BIBREF22 provide a method that combines continuous scales and relative assessments, but in single-turn, rather than multi-turn evaluation. BIBREF19 compare human evaluations to automatic metrics computed on self-chats. Note that we also use self-chats in this work, but we evaluate these with humans, rather than automatic metrics.",
                "The annotator is posed a question phrasing (e.g. \u201cwhich speaker is more knowledgeable\u201d or \u201cwhich speaker sounds more human?\u201d), and asked to make a binary choice between model $A$ and model $B$. They are strongly encouraged to provide a short text justification for their choice. We collect $N$ trials of such pairwise judgments, and use them to decide which model wins. Statistical significance can be computed using a binomial test."
            ],
            "predicted_section": [
                "Related Work",
                "Method: Acute-eval ::: Human-Model chats",
                "Introduction"
            ]
        },
        "35c01dc0b50b73ee5ca7491d7d373f6e853933d2": {
            "question_text": "Which dataset do they use for text altering attributes matching to image parts?",
            "from_paper": "1912.06203",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "To achieve effective image manipulation guided by text descriptions, the key is to exploit both text and image cross-modality information, generating new attributes matching the given text and also preserving text-irrelevant contents of the original image. To fuse text and image information, existing methods BIBREF8, BIBREF9 typically choose to directly concatenate image and global sentence features along the channel direction. Albeit simple, the above heuristic may suffer from some potential issues. Firstly, the model cannot precisely correlate fine-grained words with corresponding visual attributes that need to be modified, leading to inaccurate and coarse modification. For instance, shown in the first row of Fig. FIGREF1, both models cannot generate detailed visual attributes like black eye rings and a black bill. Secondly, the model cannot effectively identify text-irrelevant contents and thus fails to reconstruct them, resulting in undesirable modification of text-irrelevant parts in the image. For example, in Fig. FIGREF1, besides modifying the required attributes, both models BIBREF8, BIBREF9 also change the texture of the bird (first row) and the structure of the scene (second row).",
                "where diff is the $L_{1}$ pixel difference between the input image and the corresponding modified image, sim is the text-image similarity, which is calculated by using pretrained text and image encoders BIBREF16 based on a text-image matching score to extract global feature vectors of a given text description and the corresponding modified image, and then the similarity value is computed by applying cosine similarity between these two global vectors. Specifically, the design is based on the intuition that if the manipulated image is generated from an identity mapping network, then the text-image similarity should be low, as the synthetic image cannot perfectly keep a semantic consistency with the given text description.",
                "Text-guided image manipulation. There are few studies focusing on image manipulation using natural language descriptions. Dong et al. BIBREF8 proposed a GAN-based encoder-decoder architecture to disentangle the semantics of both input images and text descriptions. Nam et al. BIBREF9 implemented a similar architecture, but introduced a text-adaptive discriminator that can provide specific word-level training feedback to the generator. However, both methods are limited in performance due to a less effective text-image concatenation method and a coarse sentence condition."
            ],
            "predicted_section": [
                "Related Work",
                "Introduction",
                "Experiments"
            ]
        },
        "2df3cd12937591481e85cf78c96a24190ad69e50": {
            "question_text": "What are existing baseline models on these benchmark datasets?",
            "from_paper": "2004.02214",
            "gold": [
                "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:",
                "Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:",
                "To examine the effect of leveraging the pre-trained language model for the task of dialogue generation, we directly fine-tune the GPT-2 model on the dialogue data without any designed adaptations.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:",
                "Model proposed by BIBREF16 which incorporates distributed style embeddings into the structure of decoding cells to control the generation process.",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:",
                "Model proposed by BIBREF11 which uses memory modules to control the stylistic expressions in the generated responses.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):",
                "Model proposed by BIBREF27 which modifies the retrieved response based on the lexical difference between the input and the retrieved query. This approach does not take the style aspect into consideration.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):",
                "For this approach, we apply the state-of-the-art style transfer BIBREF23 model on the retrieved response. This approach does not consider the input query information during the transfer process.",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):",
                "Given the input query, a style classifier is used to rerank the top 10 retrieved responses. The response with the highest score on the desired style is selected."
            ],
            "gold_section": [
                "Experiments ::: Model Comparison ::: Generative Approaches ::: ECM:",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Style Transfer (RST):",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Retrieval + Reranking (RRe):",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: GPT2-FT:",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Speaker:",
                "Experiments ::: Model Comparison",
                "Experiments ::: Model Comparison ::: Retrieval-Based Approaches ::: Skeleton-to-Response (SR):"
            ],
            "predicted": [
                "We compare the proposed approach with several competitive baselines that can be categorized into two classes: generative approaches and retrieval-based approaches.",
                "To fully evaluate the proposed approach, we conduct extensive experiments on three benchmark datasets. Results of both human and automatic evaluation show that the proposed approach significantly outperforms several strong baselines. In addition, we also conduct an extensive cross-domain experiment to demonstrate that the proposed approach is more robust than such baselines.",
                "Standard sequence-to-sequence model with attention mechanism BIBREF39, BIBREF40."
            ],
            "predicted_section": [
                "Experiments ::: Model Comparison",
                "Introduction",
                "Experiments ::: Model Comparison ::: Generative Approaches ::: Seq2seq:"
            ]
        },
        "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a": {
            "question_text": "Other than privacy, what are the other major ethical challenges in clinical data?",
            "from_paper": "1703.10090",
            "gold": [
                "Unlocking knowledge from free text in the health domain has a tremendous societal value. However, discrimination can occur when individuals or groups receive unfair treatment as a result of automated processing, which might be a result of biases in the data that were used to train models. The question is therefore what the most important biases are and how to overcome them, not only out of ethical but also legal responsibility. Related to the question of bias is so-called algorithm transparency BIBREF44 , BIBREF45 , as this right to explanation requires that influences of bias in training data are charted. In addition to sampling bias, which we introduced in section 2, we discuss in this section further sources of bias. Unlike sampling bias, which is a corpus-level bias, these biases here are already present in documents, and therefore hard to account for by introducing larger corpora.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Data quality Texts produced in the clinical settings do not always tell a complete or accurate patient story (e.g. due to time constraints or due to patient treatment in different hospitals), yet important decisions can be based on them. As language is situated, a lot of information may be implicit, such as the circumstances in which treatment decisions are made BIBREF47 . If we fail to detect a medical concept during automated processing, this can not necessarily be a sign of negative evidence. Work on identifying and imputing missing values holds promise for reducing incompleteness, see Lipton et al. LiptonEtAl2016 for an example in sequential modeling applied to diagnosis classification.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Reporting bias Clinical texts may include bias coming from both patient's and clinician's reporting. Clinicians apply their subjective judgments to what is important during the encounter with patients. In other words, there is separation between, on the one side, what is observed by the clinician and communicated by the patient, and on the other, what is noted down. Cases of more serious illness may be more accurately documented as a result of clinician's bias (increased attention) and patient's recall bias. On the other hand, the cases of stigmatized diseases may include suppressed information. In the case of traffic injuries, documentation may even be distorted to avoid legal consequences BIBREF48 .",
                "We need to be aware that clinical notes may reflect health disparities. These can originate from prejudices held by healthcare practitioners which may impact patients' perceptions; they can also originate from communication difficulties in the case of ethnic differences BIBREF49 . Finally, societal norms can play a role. Brady et al. BradyEtAl2016 find that obesity is often not documented equally well for both sexes in weight-addressing clinics. Young males are less likely to be recognized as obese, possibly due to societal norms seeing them as \u201cstocky\" as opposed to obese. Unless we are aware of such bias, we may draw premature conclusions about the impact of our results.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Observational bias Although variance in health outcome is affected by social, environmental and behavioral factors, these are rarely noted in clinical reports BIBREF13 . The bias of missing explanatory factors because they can not be identified within the given experimental setting is also known as the streetlight effect. In certain cases, we could obtain important prior knowledge (e.g. demographic characteristics) from data other than clinical notes.",
                "paragraph4 0.9ex plus1ex minus.2ex-1em Dual use We have already mentioned linking personal health information from online texts to clinical records as a motivation for exploring surrogate data sources. However, this and many other applications also have potential to be applied in both beneficial and harmful ways. It is easy to imagine how sensitive information from clinical notes can be revealed about an individual who is present in social media with a known identity. More general examples of dual use are when the NLP tools are used to analyze clinical notes with a goal of determining individuals' insurability and employability."
            ],
            "gold_section": [
                "Social impact and biases"
            ],
            "predicted": [
                "In this paper, we reviewed some challenges that we believe are central to the work in clinical NLP. Difficult access to data due to privacy concerns has been an obstacle to progress in the field. We have discussed how the protection of privacy through sanitization measures and the requirement for informed consent may affect the work in this domain. Perhaps, it is time to rethink the right to privacy in health in the light of recent work in ethics of big data, especially its uneasy relationship to the right to science, i.e. being able to benefit from science and participate in it BIBREF51 , BIBREF52 . We also touched upon possible sources of bias that can have an effect on the application of NLP in the health domain, and which can ultimately lead to unfair or harmful treatment.",
                "Related to difficult access to raw clinical data is the lack of available annotated datasets for model training and benchmarking. The reality is that annotation projects do take place, but are typically constrained to a single healthcare organization. Therefore, much of the effort put into annotation is lost afterwards due to impossibility of sharing with the larger research community BIBREF6 , BIBREF14 . Again, exceptions are either few\u2014e.g. THYME BIBREF15 , a corpus annotated with temporal information\u2014or consist of small datasets resulting from shared tasks like the i2b2 and ShARe/CLEF. In addition, stringent access policies hamper reproduction efforts, impede scientific oversight and limit collaboration, not only between institutions but also more broadly between the clinical and NLP communities.",
                "The ethics discussion is gaining momentum in general NLP BIBREF8 . We aim in this paper to gather the ethical challenges that are especially relevant for clinical NLP, and to stimulate discussion about those in the broader NLP community. Although enhancing privacy through restricted data access has been the norm, we do not only discuss the right to privacy, but also draw attention to the social impact and biases emanating from clinical notes and their processing. The challenges we describe here are in large part not unique to clinical NLP, and are applicable to general data science as well."
            ],
            "predicted_section": [
                "Sensitivity of data and privacy",
                "Introduction",
                "Conclusion"
            ]
        },
        "8b1af67e3905244653b4cf66ba0acec8d6bff81f": {
            "question_text": "How were the ngram models used to generate predictions on the data?",
            "from_paper": "1704.08390",
            "gold": [
                "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0",
                "After training the N-gram language models, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. Table 1 shows an example of two scored tweets from hashtag file Bad_Job_In_5_Words.tsv based on the tweet data trigram language model. Note that KenLM reports the log of the probability of the N-grams rather than the actual probabilities so the value closer to 0 (-19) has the higher probability and is associated with the tweet judged to be funnier."
            ],
            "gold_section": [
                "Tweet Scoring"
            ],
            "predicted": [
                "An N-gram model can predict the next word from a sequence of N-1 previous words. A trigram Language Model (LM) predicts the conditional probability of the next word using the following approximation: DISPLAYFORM0 ",
                "Once we had the corpora ready, we used the KenLM Toolkit to train the N-gram language models on each corpus. We trained using both bigrams and trigrams on the tweet and news data. Our language models accounted for unknown words and were built both with and without considering sentence or tweet boundaries.",
                "Training Language Models (LMs) is a straightforward way to collect a set of rules by utilizing the fact that words do not appear in an arbitrary order; we in fact can gain useful information about a word by knowing the company it keeps BIBREF7 . A statistical language model estimates the probability of a sequence of words or an upcoming word. An N-gram is a contiguous sequence of N words: a unigram is a single word, a bigram is a two-word sequence, and a trigram is a three-word sequence. For example, in the tweet"
            ],
            "predicted_section": [
                "Background",
                "Language Model Training"
            ]
        },
        "a3a867f7b3557c168d05c517c468ff6c7337bff9": {
            "question_text": "What dataset did they use?",
            "from_paper": "1701.03578",
            "gold": [
                "To resolve these limitations, we propose fast transfer learning schemes. It trains a base model with a large dataset and copies its first n-many layers to the first n-many layers of a target model. Then the target model is fine-tuned with relatively small target data. Several learning schemes such as freezing a certain layer or adding a surplus layer are proposed for achieving the result. In experiments, we trained a general language model with huge corpus such as an Workshop on Statistical Machine Translation (WMT) data and a movie script data by using powerful computing machines, and then transferred the model to target environment for updating to be a personalized language model. With this approach, the final model can mimic target user's language style with proper syntax.",
                "In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users' dialogues.",
                "We also apply the transfer learning schemes with some of the English bible data. The same general language model, which involved previously training with the WMT'14 corpus for 10 days, is used. English bible data is added and employed in training for another 4 hours using proposed transfer learning schemes."
            ],
            "gold_section": [
                "Literary-Style to Spoken-Style Sentence Completion",
                "Introduction"
            ],
            "predicted": [
                "We mainly conduct two types of experiments. The first one is a sentence completion experiment, and the other one is a message-reply prediction experiment. In the former case, we train a general language model with literary-style data and apply a proposed transfer learning scheme with spoken-style data to achieve a personalized language model. With this setting, the difference between general and personalized language models can be measured in a quantitative and a qualitative manner. For the latter case, we use dialogue-style data such as drama scripts to train a general language model. From the drama scripts, some characters' data are taken apart and are used to train the personalized language model. With this setting, the output of the personalized model is compared to the original dialogue of the same character.",
                "In the experiments, we trained the general language model with literary-style data and applied the transfer learning with spoken-style data. Then we evaluated the model output for sentence completion task in a qualitative and a quantitative manner. The test result showed that the model learned the style of the target language properly. Another test was conducted by training the general language model with the script of the drama, \u201cFriends,\" and by applying transfer learning with main character corpora from the script to generate the personalized language model. The message-reply prediction task was evaluated with this model. The test result shows higher similarity between the output of the personalized language model and the same user dialogue than the one between the output of the personalized language model and other users' dialogues.",
                "To check the influence of training data size (number of sentences) in personalized language model, we trained the general language model (trained with \u201cFriends\" corpus, message-reply prediction model) with different sizes of personal (\u201cchandler\" and \u201crachel\") dataset. The proposed scheme_2 method was used for this test. Table 4 shows evaluation results of the trained models. Dataset '0' means the model is not trained with personal dataset. The perplexity shows lower value as we use more dataset in training, and it outperforms \u201cfriends 5-gram\u201d model from the 2,000 dataset cases."
            ],
            "predicted_section": [
                "General-Style to Personal-Style Message-Reply Prediction",
                "Introduction",
                "Experiments"
            ]
        },
        "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c": {
            "question_text": "What is the improvement in performance compared to the linguistic gold standard?",
            "from_paper": "2003.03131",
            "gold": [
                "Table contains the error analysis for English, Finnish and North S\u00e1mi. For English and North S\u00e1mi, EM+Prune results in less under-segmentation but worse over-segmentation. For Finnish these results are reversed. However, the suffixes are often better modeled, as shown by lower under-segmentation on SUF-SUF (all languages) and STM-SUF (English and North S\u00e1mi)."
            ],
            "gold_section": [
                "Results"
            ],
            "predicted": [
                "Figure shows the Precision\u2013Recall curves for the primary systems, for all four languages. While increasing the Morfessor cost, forced splitting improves BPR. Tables to show test set Boundary Precision, Recall and F$_{1}$-score (BPR) results at the optimal tuning point (selected using a development set) for each model, for English, Finnish, Turkish and North S\u00e1mi, respectively. The default Morfessor EM+Prune configuration (\u201csoft\u201d EM, full prior, forcesplit) significantly outperforms Morfessor Baseline w.r.t. the F-score for all languages except North S\u00e1mi, for which there is no significant difference between the methods.",
                "The closeness to linguistic segmentation is evaluated by comparison with annotated morph boundaries using boundary precision, boundary recall, and boundary $F_{1}$-score BIBREF21. The boundary $F_{1}$-score (F-score for short) equals the harmonic mean of precision (the percentage of correctly assigned boundaries with respect to all assigned boundaries) and recall (the percentage of correctly assigned boundaries with respect to the reference boundaries). Precision and recall are calculated using macro-averages over the word types in the test set. In the case that a word has more than one annotated segmentation, we take the one that gives the highest score.",
                "Pre-pruning of redundant substrings gives mixed results. For Turkish, both Morfessor cost and BPR are degraded by the pre-pruning, but for the other three languages the pre-pruning is beneficial or neutral. When tuning $\\alpha $ to very high values (less segmentation), pre-pruning of redundant substrings improves the sensitivity to tuning. The same effect may also be achievable by using a larger seed lexicon. We perform most of our experiments with pre-pruning turned on."
            ],
            "predicted_section": [
                "Results",
                "Experimental Setup ::: Evaluation"
            ]
        },
        "252599e53f52b3375b26d4e8e8b66322a42d2563": {
            "question_text": "Which data augmentation techniques do they use?",
            "from_paper": "1709.06365",
            "gold": [
                "To sample INLINEFORM0 , we first marginalise out INLINEFORM1 in the right part of Eq. ( SECREF4 ) with the Dirichlet multinomial conjugacy: +rCl+x* d=1D (d,)(d, + md,)Gamma ratio 1 k=1K (d,k + md,k)(d,k)Gamma ratio 2 where INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 is the gamma function. Gamma ratio 1 in Eq. ( SECREF17 ) can be augmented with a set of Beta random variables INLINEFORM5 as: +rCl+x* (d,)(d, + md,)Gamma ratio 1 qd qdd,-1 (1-qd)md,-1 where for each document INLINEFORM6 , INLINEFORM7 . Given a set of INLINEFORM8 for all the documents, Gamma ratio 1 can be approximated by the product of INLINEFORM9 , i.e., INLINEFORM10 .",
                "Gamma ratio 2 in Eq. ( SECREF17 ) is the Pochhammer symbol for a rising factorial, which can be augmented with an auxiliary variable INLINEFORM0 BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 as follows: +rCl+x* (d,k + md,k)(d,k)Gamma ratio 2 = td,k=0md,k Smd,ktd,k d,ktd,k where INLINEFORM1 indicates an unsigned Stirling number of the first kind. Gamma ratio 2 is a normalising constant for the probability of the number of tables in the Chinese Restaurant Process (CRP) BIBREF28 , INLINEFORM2 can be sampled by a CRP with INLINEFORM3 as the concentration and INLINEFORM4 as the number of customers: +rCl+x* td,k = i=1md,k Bern(d,kd,k+i) where INLINEFORM5 samples from the Bernoulli distribution. The complexity of sampling INLINEFORM6 by Eq. ( SECREF17 ) is INLINEFORM7 . For large INLINEFORM8 , as the standard deviation of INLINEFORM9 is INLINEFORM10 BIBREF28 , one can sample INLINEFORM11 in a small window around the current value in complexity INLINEFORM12 .",
                "Unlike most existing methods, our way of incorporating the meta information facilitates the derivation of an efficient Gibbs sampling algorithm. With two data augmentation techniques (i.e., the introduction of auxiliary variables), MetaLDA admits the local conjugacy and a close-form Gibbs sampling algorithm can be derived. Note that MetaLDA incorporates the meta information on the Dirichlet priors, so we can still use LDA's collapsed Gibbs sampling algorithm for the topic assignment INLINEFORM0 . Moreover, Step UID12 and UID9 show that one only needs to consider the non-zero entries of INLINEFORM1 and INLINEFORM2 in computing the full conditionals, which further reduces the inference complexity."
            ],
            "gold_section": [
                "Inference",
                "Sampling \u03bb l,k \\lambda _{l,k}:"
            ],
            "predicted": [
                "In the experiments, three regular text datasets and three short text datasets were used:",
                "In this section, we evaluate the proposed MetaLDA against several recent advances that also incorporate meta information on 6 real datasets including both regular and short texts. The goal of the experimental work is to evaluate the effectiveness and efficiency of MetaLDA's incorporation of document and word meta information both separately and jointly compared with other methods. We report the performance in terms of perplexity, topic coherence, and running time per iteration.",
                "We conduct extensive experiments with several real datasets including regular and short texts in various domains. The experimental results demonstrate that MetaLDA achieves improved performance in terms of perplexity, topic coherence, and running time."
            ],
            "predicted_section": [
                "Datasets",
                "Introduction",
                "Experiments"
            ]
        },
        "275b2c22b6a733d2840324d61b5b101f2bbc5653": {
            "question_text": "How are the tweets selected?",
            "from_paper": "1901.10619",
            "gold": [
                "Using the DataSift Firehose, we collected historical tweets from public accounts with geographical coordinates located in a 15-counties region surrounding a medium sized US city from July 2013 to June 2014. This one-year data set contains over 7 million geo-tagged tweets (approximately 90% written in English) from around 85,000 unique Twitter accounts. This particular locality has geographical diversity, covering both urban and rural areas and providing mixed and balanced demographics. We could apply local knowledge into the construction of our final job-related corpus, which has been approved very helpful in the later experiments.",
                "Initial Classifier \ud835\udc02 0 \\mathbf {C_0}",
                "In order to identify probable job-related tweets which are talking about paid positions of regular employment while excluding noises (such as students discussing homework or school-related activities, or people complimenting others), we defined a simple term-matching classifier with inclusion and exclusion terms in the first step (see Table TABREF9 ).",
                "Classifier INLINEFORM0 consists of two rules: the matched tweet must contain at least one word in the Include lexicon and it cannot contain any word in the Exclude lexicon. Before applying filtering rules, we pre-processed each tweet by (1) converting all words to lower cases; (2) stripping out punctuation and special characters; and (3) normalizing the tweets by mapping out-of-vocabulary phrases (such as abbreviations and acronyms) to standard phrases using a dictionary of more than 5,400 slang terms in the Internet."
            ],
            "gold_section": [
                "Data Collection",
                "Initial Classifier \ud835\udc02 0 \\mathbf {C_0}"
            ],
            "predicted": [
                "Through observation we noticed some patterns like:",
                "We used INLINEFORM0 to detect (not) job-related tweets, and applied our linguistic heuristics to further separate accounts into personal and business groups automatically.",
                "This motivated a simple heuristic that appeared surprisingly effective at determining which kind of accounts each job-related tweet was posted from: if an account had more job-related tweets matching the \u201chashtags + URL\u201d patterns than tweets in other topics, we labeled it a business account; otherwise it is a personal account. We validated its effectiveness using the job-related tweets sampled by the models in crowdsourced evaluations phase. It is essential to note that when crowdsourcing annotators made judgment about the type of accounts as personal or business, they were shown only one target tweet\u2014without any contexts or posts history which our heuristics rely on."
            ],
            "predicted_section": [
                "Determining Sources of Job-Related Tweets"
            ]
        },
        "f1f7a040545c9501215d3391e267c7874f9a6004": {
            "question_text": "what dataset was used?",
            "from_paper": "1612.09535",
            "gold": [
                "The program was developed in R BIBREF16 and makes use of some specific text mining packages. We have implemented our method using the following R packages: tm BIBREF17 , cwhmisc BIBREF18 , memoise BIBREF19 , openNLP BIBREF20 , Hmisc BIBREF21 . The OpenNLP POS Tagger uses a probability model to predict the correct POS tag and, for Portuguese language, it was trained on CoNLL_X bosque data.",
                "In this work, we evaluate our NER approach using two news corpora. One corpus is a set of 227 texts published on December 31, 2010 by the Lusa agency (portuguese agency of news) and will be referred to as `News'. The other corpus (named here `Sports news') is a set of 881 sports news. The texts were manually annotated according to the enamex designation and the type `miscellaneous'."
            ],
            "gold_section": [
                "Comparing PAMPO with other NER tools",
                "Implementation"
            ],
            "predicted": [
                "Table TABREF27 shows the most frequent `candidate entities' from the whole book, as extracted by Algorithm 1 and which of those candidate entities were considered as actual `named entities' by Algorithm 2.",
                "Figure FIGREF42 presents scatter plots of INLINEFORM0 vs INLINEFORM1 for the four extractors, PAMPO, AlchemyAPI, Rembrandt and Zemanta for the `Sports news' and `News' corpora, first four panels and four bottom panels, respectively. It is noteworthy that almost all the 881 points of the `Sports news' for PAMPO extractor are in the upper right corner of the scatter plot, as well as almost all the 227 points of the `News'. The other tools present a more dispersed solution quality.",
                "From this book, a total of 12120 named entities were extracted by PAMPO, corresponding to 5159 unique named entities. To assess the quality of this process, the first 125 pages of the book were manually labelled (1/3 of the text book). The values of the computed measures are shown in Table TABREF29 . This part of the book contains 3836 named entities. INLINEFORM0 and INLINEFORM1 are estimated for the two phases based on the results obtained on the 125 pages of the book. A total of 5089 terms were labelled `candidate entities' in the first phase and 3075 were identified as `named entities' in the second phase. The true positives were 3205 in the first phase and 2982 in the second phase (partial identifications count as 1/2). This means that the INLINEFORM2 , given by Equation ( EQREF30 ), decreases from 0.84 to 0.78, and the INLINEFORM3 , given by Equation ( EQREF31 ), increases from 0.63 to 0.97. DISPLAYFORM0 DISPLAYFORM1 "
            ],
            "predicted_section": [
                "Evaluation",
                "Analysis of results"
            ]
        },
        "20e2b517fddb0350f5099c39b16c2ca66186d09b": {
            "question_text": "What baseline do they compare against?",
            "from_paper": "1603.01547",
            "gold": [
                "Several recent deep neural network architectures BIBREF1 , BIBREF3 , BIBREF7 , BIBREF12 were applied to the task of text comprehension. The last two architectures were developed independently at the same time as our work. All of these architectures use an attention mechanism that allows them to highlight places in the document that might be relevant to answering the question. We will now briefly describe these architectures and compare them to our approach.",
                "Attentive and Impatient Readers were proposed in BIBREF1 . The simpler Attentive Reader is very similar to our architecture. It also uses bidirectional document and query encoders to compute an attention in a similar way we do. The more complex Impatient Reader computes attention over the document after reading every word of the query. However, empirical evaluation has shown that both models perform almost identically on the CNN and Daily Mail datasets.",
                "Chen et al. 2016",
                "A model presented in BIBREF7 is inspired by the Attentive Reader. One difference is that the attention weights are computed with a bilinear term instead of simple dot-product, that is INLINEFORM0 . The document embedding INLINEFORM1 is computed using a weighted sum as in the Attentive Reader, INLINEFORM2 . In the end INLINEFORM3 , where INLINEFORM4 is a new embedding function.",
                "Memory Networks",
                "MenNN BIBREF13 were applied to the task of text comprehension in BIBREF3 .",
                "Dynamic Entity Representation",
                "The Dynamic Entity Representation model BIBREF12 has a complex architecture also based on the weighted attention mechanism and max-pooling over contextual embeddings of vectors for each named entity.",
                "One can also see how the task complexity varies with the type of the omitted word (named entity, common noun, verb, preposition). BIBREF3 have shown that while standard LSTM language models have human level performance on predicting verbs and prepositions, they lack behind on named entities and common nouns. In this article we therefore focus only on predicting the first two word types."
            ],
            "gold_section": [
                "Datasets",
                "Attentive and Impatient Readers",
                "Related Work",
                "Chen et al. 2016",
                "Dynamic Entity Representation",
                "Memory Networks"
            ],
            "predicted": [
                "CBT. In named entity prediction our best single model with accuracy of 68.6% performs 2% absolute better than the MenNN with self supervision, the averaging ensemble performs 4% absolute better than the best previous result. In common noun prediction our single models is 0.4% absolute better than MenNN however the ensemble improves the performance to 69% which is 6% absolute better than MenNN.",
                "Performance of our models on the CNN and Daily Mail datasets is summarized in Table TABREF27 , Table TABREF28 shows results on the CBT dataset. The tables also list performance of other published models that were evaluated on these datasets. Ensembles of our models set new state-of-the-art results on all evaluated datasets.",
                "In this section we evaluate our model on the CNN, Daily Mail and CBT datasets. We show that despite the model's simplicity its ensembles achieve state-of-the-art performance on each of these datasets."
            ],
            "predicted_section": [
                "Evaluation",
                "Results"
            ]
        },
        "058b6e3fdbb607fa7dbfc688628b3e13e130c35a": {
            "question_text": "Does the paper list other heuristic biases in the LSTMs?",
            "from_paper": "1912.00239",
            "gold": [
                "Interestingly, even though the case orders preferred by the LSTM correlate with those of humans, there are also subtle differences: we also find that models tend to prefer argument orders that start with dative to those that start with accusative, when the opposite is true for human grammaticality scores. The origin of such differences is unclear. Understanding it more fully would require to obtain distributional statistics on the order of such phrases in the original corpus.",
                "To check for the existence of such effect, we categorized the nouns in all of our sentences as animate and inanimate, and computed the human and machine scores of our grammatical sentences as a function of the association between case and animacy. Table TABREF22 shows that indeed, both humans and machines are biased by animacy-case associations: all share a preference for animate for nominative (subject) and dative (indirect object). By contrast, negative AUC values for accusative indicate that direct objects are preferred as inanimate."
            ],
            "gold_section": [
                "Results ::: Animacy Preferences",
                "Results ::: Argument Order Preferences"
            ],
            "predicted": [
                "We set up a well controlled grammaticality test for the processing of argument structure in neural language models and in humans. The results show that LSTMs are better than chance in detecting an abnormal argument structure, despite the fact that the arguments could occur in any position, due to the generally free word order of phrases in German relative clauses. The average performance of models, though, is far from 100% correct and lower than humans, and the error patterns differ markedly. Contrary to humans, neural language models are overly sensitive to frequency distribution of phrase types. For instance, they assign a higher probability to sentences containing multiple nominative phrases than a correct sentence with only one nominative phrase. This frequency bias directly reflects the frequency of nominative, accusative and dative in the language, as the same bias is found in unigram and bigram models. Similar to the conclusion reached by BIBREF21 in their investigation of the error patterns made by RNNs and humans on syntactic agreement, we find that the syntactic representations of humans and LSTMs differ in some respects.",
                "We evaluate three LMs on our dataset, the two-layer LSTM of BIBREF8 trained on German Wikipedia text, as well as n-gram baselines using the same corpus. We ask proficient German speakers to annotate our sentences for grammaticality, providing a human comparison. Since some of these sentences are rather implausible because of the permutations, we also collect human meaningfulness scores. We find that our dataset is challenging for both LMs and humans and that LMs lag behind human performance.",
                "Figure FIGREF11 shows the distribution of the log probability scores predicted by the LSTM and the distribution of the grammaticality scores given by humans. Figure FIGREF16 presents the distributions and average of the AUC values computed per template (50 in total), both for the models' log probability scores and the human grammaticality scores. Performances are rather modest, with a mean AUC of 0.56 for the LTSM and of 0.58 for humans, compared to the chance score of 0.5 for the unigram and bigram models. As expected, the n-gram baselines perform exactly at chance, confirming that they do not represent verb argument structures and that LMs need a deeper encoding to be able capture syntax within sentences. We also notice that AUC varies relatively little across different templates for our models, indicating that the particular choice of template has little impact. For humans, the wider spread in results can be attributed partially to the fact that 55 random permutations out of the 144 permutations were annotated for each template. Therefore, it might have been easier to distinguish grammatical sentences from ungrammatical ones for some templates than others."
            ],
            "predicted_section": [
                "Conclusions",
                "Introduction",
                "Results ::: Main Classification Task"
            ]
        },
        "bdae851d4cf1d05506cf3e8359786031ac4f756f": {
            "question_text": "What models have been evaluated?",
            "from_paper": "1911.12237",
            "gold": [
                "We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):",
                "Pointer generator network BIBREF4. In the case of Pointer Generator, we use a default configuration, changing only the minimum length of the generated summary from 35 (used in news) to 15 (used in dialogues).",
                "Transformer BIBREF16. The model is trained using OpenNMT library. We use the same parameters for training both on news and on dialogues, changing only the minimum length of the generated summary \u2013 35 for news and 15 for dialogues.",
                "Fast Abs RL BIBREF5. It is trained using its default parameters. For dialogues, we change the convolutional word-level sentence encoder (used in extractor part) to only use kernel with size equal 3 instead of 3-5 range. It is caused by the fact that some of utterances are very short and the default setting is unable to handle that.",
                "Fast Abs RL Enhanced. The additional variant of the Fast Abs RL model with slightly changed utterances i.e. to each utterance, at the end, after artificial separator, we add names of all other interlocutors. The reason for that is that Fast Abs RL requires text to be split into sentences (as it selects sentences and then paraphrase each of them). For dialogues, we divide text into utterances (which is a natural unit in conversations), so sometimes, a single utterance may contain more than one sentence. Taking into account how this model works, it may happen that it selects an utterance of a single person (each utterance starts with the name of the author of the utterance) and has no information about other interlocutors (if names of other interlocutors do not appear in selected utterances), so it may have no chance to use the right people's names in generated summaries.",
                "LightConv and DynamicConv BIBREF17. The implementation is available in fairseq BIBREF18. We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation BIBREF19; (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small BIBREF20."
            ],
            "gold_section": [
                "Experimental setup ::: Models"
            ],
            "predicted": [
                "We carry out experiments with the following summarization models (for all architectures we set the beam size for beam search decoding to 5):",
                "The results for the news summarization task are shown in Table TABREF25 and for the dialogue summarization \u2013 in Table TABREF26. In both domains, the best models' ROUGE-1 exceeds 39, ROUGE-2 \u2013 17 and ROUGE-L \u2013 36. Note that the strong baseline for news (Lead-3) is outperformed in all three metrics only by one model. In the case of dialogues, all tested models perform better than the baseline (LONGEST-3).",
                "According to ROUGE metrics, the best performing model is DynamicConv with GPT-2 embeddings, trained on joined news and dialogue data with an utterance separation token."
            ],
            "predicted_section": [
                "Results",
                "Experimental setup ::: Models"
            ]
        },
        "07d98dfa88944abd12acd45e98fb7d3719986aeb": {
            "question_text": "Are all generated examples semantics-preserving perturbations to the original text?",
            "from_paper": "1909.07873",
            "gold": [
                "Alzantot et al. BIBREF20 proposed a black-box targeted attack using a population-based optimization via genetic algorithm BIBREF21. The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier. While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy. Our attacks can be a test of how robust the text classification models are to word and character-level perturbations.",
                "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "The primary purpose of pretraining AEG is to enable our hybrid encoder-decoder to encode both character and word information from the input example and produce both word and character-level transformations in the form of paraphrases or misspellings. Though the pretraining helps us mitigate the cold-start issue, it does not guarantee that these perturbed texts will fool the target model. There are large number of valid perturbations that can be applied due to multiple ways of arranging text units to produce paraphrases or different misspellings. Thus, minimizing $J_{mle}$ is not sufficient to generate adversarial examples.",
                "The reward $r(\\hat{y})$ for the sequence generated is a weighted sum of different constraints required for generating adversarial examples. Since our model operates at word and character levels, we therefore compute three rewards: adversarial reward, semantic similarity and lexical similarity reward. The reward should be high when: (a) the generated sequence causes the target model to produce a low classification prediction probability for its ground truth category, (b) semantic similarity is preserved and (c) the changes made to the original text are minimal.",
                "Inspired by the work of Li et al. BIBREF37, we train a deep matching model that can represent the degree of match between two texts. We use character based biLSTM models with attention BIBREF38 to handle word and character level perturbations. The matching model will help us compute the the semantic similarity $R_S$ between the text generated and the original input text.",
                "Since our model functions at both character and word level, we compute the lexical similarity. The purpose of this reward is to keep the changes as minimal as possible to just fool the target classifier. Motivated by the recent work of Moon et al. BIBREF39, we pretrain a deep neural network to compute approximate Levenshtein distance $R_{L}$ composed of character based bi-LSTM model. We replicate that model by generating a large number of text with perturbations in the form of insertions, deletions or replacements. We also include words which are prominent nicknames, abbreviations or inconsistent notations to have more lexical similarity. This is generally not possible using direct Levenshtein distance computation. Once trained, it can produce a purely lexical embedding of the text without semantic allusion. This can be used to compute the lexical similarity between the generated text $y$ and the original input text $x$ for our purpose.",
                "Table TABREF29 summarizes the data and models used in our experiments. We compare our proposed model with the following black-box non-targeted attacks:",
                "Random: We randomly select a word in the text and introduce some perturbation to that word in the form of a character replacement or synonymous word replacement. No specific strategy to identify importance of words.",
                "NMT-BT: We generate paraphrases of the sentences of the text using a back-translation approach BIBREF23. We used pretrained English$\\leftrightarrow $German translation models to obtain back-translations of input examples.",
                "DeepWordBug BIBREF24: A scoring function is used to determine the important tokens to change. The tokens are then modified to evade a target model.",
                "No-RL: We use our pretrained model without the reinforcement learning objective.",
                "Given different settings of the adversary, there are other works that have designed attacks in \u201cgray-box\u201d settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of \u201cgray-box\u201d attacks are quite different in each of these approaches. In this paper, we focus on \u201cblack-box\u201d setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization. The adversarial sequences generated are able to effectively fool the classifiers without changing the semantics of the text. Our contributions are as follows:"
            ],
            "gold_section": [
                "Related Work",
                "Training ::: Training with Reinforcement learning ::: Rewards ::: Semantic Similarity",
                "Training ::: Training with Reinforcement learning ::: Rewards ::: Lexical Similarity",
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "Training ::: Training with Reinforcement learning ::: Rewards",
                "Introduction",
                "Proposed Attack Strategy",
                "Experiments ::: Setup"
            ],
            "predicted": [
                "Most of the prior work has focused on image classification models where adversarial examples are obtained by introducing imperceptible changes to pixel values through optimization techniques BIBREF4, BIBREF5. However, generating natural language adversarial examples can be challenging mainly due to the discrete nature of text samples. Continuous data like image or speech is much more tolerant to perturbations compared to text BIBREF6. In textual domain, even a small perturbation is clearly perceptible and can completely change the semantics of the text. Another challenge for generating adversarial examples relates to identifying salient areas of the text where a perturbation can be applied successfully to fool the target classifier. In addition to fooling the target classifier, the adversary is designed with different constraints depending on the task and its motivations BIBREF7. In our work, we focus on constraining our adversary to craft examples with semantic preservation and minimum perturbations to the input text.",
                "We also evaluated our model based on human judgments. We conducted an experiment where the workers were presented with randomly sampled 100 adversarial examples generated by our model which were successful in fooling the target classifier. The examples were shuffled to mitigate ordering bias, and every example was annotated by three workers. The workers were asked to label the sentiment of the sampled adversarial example. For every adversarial example shown, we also showed the original text and asked them to rate their similarity on a scale from 0 (Very Different) to 3 (Very Similar). We found that the perturbations produced by our model do not affect the human judgments significantly as $94.6\\%$ of the human annotations matched with the ground-truth label of the original text. The average similarity rating of $1.916$ also indicated that the generated adversarial sequences are semantics-preserving.",
                "Alzantot et al. BIBREF20 proposed a black-box targeted attack using a population-based optimization via genetic algorithm BIBREF21. The perturbation procedure consists of random selection of words, finding their nearest neighbours, ranking and substitution to maximize the probability of target category. In this method, random word selection in the sequence to substitute were full of uncertainties and might be meaningless for the target label when changed. Since our model focuses on black-box non-targeted attack using an encoder-decoder approach, our work is closely related to the following techniques in the literature: Wong (2017) BIBREF22, Iyyer et al. BIBREF23 and Gao et al. BIBREF24. Wong (2017) BIBREF22 proposed a GAN-inspired method to generate adversarial text examples targeting black-box classifiers. However, this approach was restricted to binary text classifiers. Iyyer et al. BIBREF23 crafted adversarial examples using their proposed Syntactically Controlled Paraphrase Networks (SCPNs). They designed this model for generating syntactically adversarial examples without compromising on the quality of the input semantics. The general process is based on the encoder-decoder architecture of SCPN. Gao et al. BIBREF24 implemented an algorithm called DeepWordBug that generates small text perturbations in a black box setting forcing the deep learning model to make mistakes. DeepWordBug used a scoring function to determine important tokens and then applied character-level transformations to those tokens. Though the algorithm successfully generates adversarial examples by introducing character-level attacks, most of the introduced perturbations are constricted to misspellings. The semantics of the text may be irreversibly changed if excessive misspellings are introduced to fool the target classifier. While SCPNs and DeepWordBug primary rely only on paraphrases and character transformations respectively to fool the classifier, our model uses a hybrid word-character encoder-decoder approach to introduce both paraphrases and character-level perturbations as a part of our attack strategy. Our attacks can be a test of how robust the text classification models are to word and character-level perturbations."
            ],
            "predicted_section": [
                "Experiments ::: Human Evaluation",
                "Related Work",
                "Introduction"
            ]
        },
        "7f90e9390ad58b22b362a57330fff1c7c2da7985": {
            "question_text": "Do they use already trained model on some task in their reinforcement learning approach?",
            "from_paper": "1909.07873",
            "gold": [
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "The primary purpose of pretraining AEG is to enable our hybrid encoder-decoder to encode both character and word information from the input example and produce both word and character-level transformations in the form of paraphrases or misspellings. Though the pretraining helps us mitigate the cold-start issue, it does not guarantee that these perturbed texts will fool the target model. There are large number of valid perturbations that can be applied due to multiple ways of arranging text units to produce paraphrases or different misspellings. Thus, minimizing $J_{mle}$ is not sufficient to generate adversarial examples.",
                "Training ::: Training with Reinforcement learning",
                "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm."
            ],
            "gold_section": [
                "Training ::: Supervised Pretraining with Teacher Forcing",
                "Training ::: Training with Reinforcement learning"
            ],
            "predicted": [
                "No-RL: We use our pretrained model without the reinforcement learning objective.",
                "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.",
                "We propose a black-box non-targeted attack strategy by combining ideas of substitute network and adversarial example generation. We formulate it as a reinforcement learning task."
            ],
            "predicted_section": [
                "Introduction",
                "Experiments ::: Setup",
                "Training ::: Training with Reinforcement learning"
            ]
        },
        "3e3e45094f952704f1f679701470c3dbd845999e": {
            "question_text": "How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?",
            "from_paper": "1909.07873",
            "gold": [
                "Proposed Attack Strategy",
                "Let us consider a target model $T$ and $(x,l)$ refers to the samples from the dataset. Given an instance $x$, the goal of the adversary is to generate adversarial examples $x^{\\prime }$ such that $T(x^{\\prime }) \\ne l$, where $l$ denotes the true label i.e take one of the $K$ classes of the target classification model. The changes made to $x$ to get $x^{\\prime }$ are called perturbations. We would like to have $x^{\\prime }$ close to the original instance $x$. In a black box setting, we do not have knowledge about the internals of the target model or its training data. Previous work by Papernot et al. BIBREF14 train a separate substitute classifier such that it can mimic the decision boundaries of the target classifier. The substitute classifier is then used to craft adversarial examples. While these techniques have been applied for image classification models, such methods have not been explored extensively for text.",
                "We implement both the substitute network training and adversarial example generation using an encoder-decoder architecture called Adversarial Examples Generator (AEG). The encoder extracts the character and word information from the input text and produces hidden representations of words considering its sequence context information. A substitute network is not implemented separately but applied using an attention mechanism to weigh the encoded hidden states based on their relevance to making predictions closer to target model outputs. The attention scores provide certain level of interpretability to the model as the regions of text that need to perturbed can be identified and visualized. The decoder uses the attention scores obtained from the substitute network, combines it with decoder state information to decide if perturbation is required at this state or not and finally emits the text unit (a text unit may refer to a word or character). Inspired by a work by Luong et al. BIBREF25, the decoder is a word and character-level recurrent network employed to generate adversarial examples. Before the substitute network is trained, we pretrain our encoder-decoder model on common misspellings and paraphrase datasets to empower the model to produce character and word perturbations in the form of misspellings or paraphrases. For training substitute network and generation of adversarial examples, we randomly draw data that is disjoint from the training data of the black-box model since we assume the adversaries have no prior knowledge about the training data or the model. Specifically, we consider attacking a target classifier by generating adversarial examples based on unseen input examples. This is done by dividing the dataset into training, validation and test using 60-30-10 ratio. The training data is used by the target model, while the unseen validation samples are used with necessary data augmentation for our AEG model. We further improve our model by using a self-critical approach to finally generate better adversarial examples. The rewards are formulated based on the following goals: (a) fool the target classifier, (b) minimize the number of perturbations and (c) preserve the semantics of the text. In the following sections, we explain the encoder-decoder model and then describe the reinforcement learning framing towards generation of adversarial examples.",
                "Training ::: Training with Reinforcement learning",
                "We fine-tune our model to fool a target classifier by learning a policy that maximizes a specific discrete metric formulated based on the constraints required to generate adversarial examples. In our work, we use the self-critical approach of Rennie et al. BIBREF36 as our policy gradient training algorithm.",
                "Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)",
                "In SCST approach, the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts. First, we compute two sequences: (a) $y^{\\prime }$ sampled from the model's distribution $p(y^{\\prime }_j|y^{\\prime }_{<j},h)$ and (b) $\\hat{y}$ obtained by greedily decoding ($argmax$ predictions) from the distribution $p(\\hat{y}_j|\\hat{y}_{<j},h)$ Next, rewards $r(y^{\\prime }_j),r(\\hat{y}_j)$ are computed for both the sequences using a reward function $r(\\cdot )$, explained in Section SECREF26. We train the model by minimizing:",
                "Here $r(\\hat{y})$ can be viewed as the baseline reward. This approach, therefore, explores different sequences that produce higher reward compared to the current best policy."
            ],
            "gold_section": [
                "Training ::: Training with Reinforcement learning ::: Self-critical sequence training (SCST)",
                "Training ::: Training with Reinforcement learning",
                "Proposed Attack Strategy"
            ],
            "predicted": [
                "Given different settings of the adversary, there are other works that have designed attacks in \u201cgray-box\u201d settings BIBREF8, BIBREF9, BIBREF10. However, the definitions of \u201cgray-box\u201d attacks are quite different in each of these approaches. In this paper, we focus on \u201cblack-box\u201d setting where we assume that the adversary possesses a limited set of labeled data, which is different from the target's training data, and also has an oracle access to the system, i.e., one can query the target classifier with any input and get its corresponding predictions. We propose an effective technique to generate adversarial examples in a black-box setting. We develop an Adversarial Example Generator (AEG) model that uses a reinforcement learning framing to generate adversarial examples. We evaluate our models using a word-based BIBREF11 and character-based BIBREF12 text classification model on benchmark classification tasks: sentiment classification and news categorization. The adversarial sequences generated are able to effectively fool the classifiers without changing the semantics of the text. Our contributions are as follows:",
                "Adversarial examples are generally minimal perturbations applied to the input data in an effort to expose the regions of the input space where a trained model performs poorly. Prior works BIBREF0, BIBREF1 have demonstrated the ability of an adversary to evade state-of-the-art classifiers by carefully crafting attack examples which can be even imperceptible to humans. Following such approaches, there has been a number of techniques aimed at generating adversarial examples BIBREF2, BIBREF3. Depending on the degree of access to the target model, an adversary may operate in one of the two different settings: (a) black-box setting, where an adversary doesn't have access to target model's internal architecture or its parameters, (b) white-box setting, where an adversary has access to the target model, its parameters, and input feature representations. In both these settings, the adversary cannot alter the training data or the target model itself. Depending on the purpose of the adversary, adversarial attacks can be categorized as (a) targeted attack and (b) non-targeted attack. In a targeted attack, the output category of a generated example is intentionally controlled to a specific target category with limited change in semantic information. While a non-targeted attack doesn't care about the category of misclassified results.",
                "In this work, we have introduced a $AEG$, a model capable of generating adversarial text examples to fool the black-box text classification models. Since we do not have access to gradients or parameters of the target model, we modelled our problem using a reinforcement learning based approach. In order to effectively baseline the REINFORCE algorithm for policy-gradients, we implemented a self-critical approach that normalizes the rewards obtained by sampled sentences with the rewards obtained by the model under test-time inference algorithm. By generating adversarial examples for target word and character-based models trained on IMDB reviews and AG's news dataset, we find that our model is capable of generating semantics-preserving perturbations that leads to steep decrease in accuracy of those target models. We conducted ablation studies to find the importance of individual components of our system. Extremely low values of the certain reward coefficient constricts the quantitative performance of the model can also lead to semantic divergence. Therefore, the choice of a particular value for this model should be motivated by the demands of the context in which it is applied. One of the main challenges of such approaches lies in the ability to produce more synthetic data to train the generator model in the distribution of the target model's training data. This can significantly improve the performance of our model. We hope that our method motivates a more nuanced exploration into generating adversarial examples and adversarial training for building robust classification models."
            ],
            "predicted_section": [
                "Introduction",
                "Conclusion"
            ]
        },
        "8e9561541f2e928eb239860c2455a254b5aceaeb": {
            "question_text": "What language pairs are affected?",
            "from_paper": "1906.01502",
            "gold": [
                "M-Bert's ability to transfer between languages that are written in different scripts, and thus have effectively zero lexical overlap, is surprising given that it was trained on separate monolingual corpora and not with a multilingual objective. To probe deeper into how the model is able to perform this generalization, Table TABREF14 shows a sample of pos results for transfer across scripts.",
                "Among the most surprising results, an M-Bert model that has been fine-tuned using only pos-labeled Urdu (written in Arabic script), achieves 91% accuracy on Hindi (written in Devanagari script), even though it has never seen a single pos-tagged Devanagari word. This provides clear evidence of M-Bert's multilingual representation ability, mapping structures onto new vocabularies based on a shared representation induced solely from monolingual language model training data.",
                "However, cross-script transfer is less accurate for other pairs, such as English and Japanese, indicating that M-Bert's multilingual representation is not able to generalize equally well in all cases. A possible explanation for this, as we will see in section SECREF18 , is typological similarity. English and Japanese have a different order of subject, verb and object, while English and Bulgarian have the same, and M-Bert may be having trouble generalizing across different orderings.",
                "Generalizing across typological features ",
                "Table TABREF20 shows macro-averaged pos accuracies for transfer between languages grouped according to two typological features: subject/object/verb order, and adjective/noun order BIBREF11 . The results reported include only zero-shot transfer, i.e. they do not include cases training and testing on the same language. We can see that performance is best when transferring between languages that share word order features, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order.",
                "Our results show that M-Bert is able to perform cross-lingual generalization surprisingly well. More importantly, we present the results of a number of probing experiments designed to test various hypotheses about how the model is able to perform this transfer. Our experiments show that while high lexical overlap between languages improves transfer, M-Bert is also able to transfer between languages written in different scripts\u2014thus having zero lexical overlap\u2014indicating that it captures multilingual representations. We further show that transfer works best for typologically similar languages, suggesting that while M-Bert's multilingual representation is able to map learned structures onto new vocabularies, it does not seem to learn systematic transformations of those structures to accommodate a target language with different word order."
            ],
            "gold_section": [
                "Generalizing across typological features ",
                "Introduction",
                "Generalization across scripts"
            ],
            "predicted": [
                "To further verify that En-Bert's inability to generalize is due to its lack of a multilingual representation and not an inability of its English-specific word piece vocabulary to represent data in other languages, we evaluate on non-cross-lingual ner and see that it performs comparably to a previous state of the art model (see Table TABREF12 ).",
                "For script-corrected inputs, i.e., when Hindi is written in Devanagari, M-Bert's performance when trained only on monolingual corpora is comparable to performance when training on code-switched data, and it is likely that some of the remaining difference is due to domain mismatch. This provides further evidence that M-Bert uses a representation that is able to incorporate information from multiple languages.",
                "Following BIBREF10 , we compare languages on a subset of the WALS features BIBREF11 relevant to grammatical ordering. Figure FIGREF17 plots pos zero-shot accuracy against the number of common WALS features. As expected, performance improves with similarity, showing that it is easier for M-Bert to map linguistic structures when they are more similar, although it still does a decent job for low similarity languages when compared to En-Bert."
            ],
            "predicted_section": [
                "Effect of vocabulary overlap",
                "Effect of language similarity",
                "Code switching and transliteration"
            ]
        },
        "860257956b83099cccf1359e5d960289d7d50265": {
            "question_text": "Which neural network architecture do they use?",
            "from_paper": "1702.02367",
            "gold": [
                "The sequences of dense representations for INLINEFORM0 and INLINEFORM1 are encoded using a bidirectional recurrent neural network encoder with Gated Recurrent Units (GRU) as in BIBREF3 which represents each word INLINEFORM2 as the concatenation of a forward encoding INLINEFORM3 and a backward encoding INLINEFORM4 . From now on, we denote the contextual representation for the word INLINEFORM5 by INLINEFORM6 and the contextual representation for the word INLINEFORM7 in the document INLINEFORM8 by INLINEFORM9 . Differently from BIBREF3 , we build a unique representation for the whole set of documents INLINEFORM10 related to the query INLINEFORM11 by stacking each contextual representation INLINEFORM12 obtaining a matrix INLINEFORM13 , where INLINEFORM14 .",
                "This phase uncovers a possible inference chain which models meaningful relationships between the query and the set of related documents. The inference chain is obtained by performing, for each inference step INLINEFORM0 , the attention mechanisms given by the query attentive read and the document attentive read keeping a state of the inference process given by an additional recurrent neural network with GRU units. In this way, the network is able to progressively refine the attention weights focusing on the most relevant tokens of the query and the documents which are exploited by the prediction neural network to select the correct answers among the candidate ones."
            ],
            "gold_section": [
                "Encoding phase",
                "Inference phase"
            ],
            "predicted": [
                "The neural network weights are supposed to learn latent features which encode relationships between the most relevant words for the given query to predict the correct answers. The outer sigmoid activation function is used to treat the problem as a multi-label classification problem, so that each candidate answer is independent and not mutually exclusive. In this way the neural network generates a score which represents the probability that the candidate answer is correct. Moreover, differently from BIBREF3 , the candidate answer INLINEFORM0 can be any word, even those which not belong to the documents related to the query.",
                "According to the experimental evaluations conducted on the above-mentioned datasets, high-level performance can be obtained exploiting complex attention mechanisms which are able to focus on relevant evidences in the processed content. One of the earlier approaches used to solve these tasks is given by the general Memory Network BIBREF21 , BIBREF22 framework which is one of the first neural network models able to access external memories to extract relevant information through an attention mechanism and to use them to provide the correct answer. A deep Recurrent Neural Network with Long Short-Term Memory units is presented in BIBREF18 , which solves CNN/Daily Mail datasets by designing two different attention mechanisms called Impatient Reader and Attentive Reader. Another way to incorporate attention in neural network models is proposed in BIBREF23 which defines a pointer-sum loss whose aim is to maximize the attention weights which lead to the correct answer.",
                "In this work we propose a novel model based on Artificial Neural Networks to answer questions with multiple answers by exploiting multiple facts retrieved from a knowledge base. The proposed model can be considered a relevant building block of a conversational recommender system. Differently from BIBREF3 , our model can consider multiple documents as a source of information in order to generate multiple answers which may not belong to the documents. As presented in this work, common tasks such as QA and top-n recommendation can be solved effectively by our model."
            ],
            "predicted_section": [
                "Related work",
                "Conclusions and Future Work",
                "Prediction phase"
            ]
        },
        "d126d5d6b7cfaacd58494f1879547be9e91d1364": {
            "question_text": "What similarity metrics have been tried?",
            "from_paper": "2004.04478",
            "gold": [
                "In table TABREF6, we present the n-gram percent match among the domain data used in our experiments. We observe that the n-gram match from among this corpora is relatively low and simple corpus similarity measures which use orthographic techniques cannot be used to obtain domain similarity. Hence, we propose the use of the metrics detailed below to perform our experiments.",
                "We use a total of 11 metrics over two scenarios: the first that uses labelled data, while the second that uses unlabelled data.",
                "We explain all our metrics in detail later in this section. These 11 metrics can also be classified into two categories:",
                "Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings.",
                "Asymmetric Metrics - The metrics which are 2-way in nature i.e., $(D_1,D_2)$ and $(D_2,D_1)$ have different similarity values viz. Entropy Change, Doc2Vec embeddings, and FastText sentence embeddings. These metrics offer additional advantage as they can help decide which domain to train from and which domain to test on amongst $D_1$ and $D_2$.",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap",
                "All words in a domain are not significant for sentiment expression. For example, comfortable is significant in the `Clothing' domain but not as significant in the `Movie' domain. In this metric, we build upon existing work by sharma2018identifying and extract significant words from each domain using the $\\chi ^2$ test. This method relies on computing the statistical significance of a word based on the polarity of that word in the domain. For our experiments, we consider only the words which appear at least 10 times in the corpus and have a $\\chi ^2$ value greater than or equal to 1. The $\\chi ^2$ value is calculated as follows:",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)",
                "KL Divergence can be used to compare the probabilistic distribution of polar words in two domains BIBREF10. A lower KL Divergence score indicates that the probabilistic distribution of polar words in two domains is identical. This implies that the domains are close to each other, in terms of sentiment similarity. Therefore, to rank source domains for a target domain using this metric, we inherit the concept of symmetric KL Divergence proposed by murthy2018judicious and use it to compute average Symmetric KL-Divergence of common polar words shared by a domain-pair. We label a word as `polar' for a domain if,",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity",
                "This metric is our novel contribution for domain adaptability evaluation. It helps in detection of `Chameleon Word(s)' which change their polarity across domains BIBREF11. The motivation comes from the fact that chameleon words directly affect the CDSA accuracy. For example, poignant is positive in movie domain whereas negative in many other domains viz. Beauty, Clothing etc.",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change",
                "Entropy is the degree of randomness. A relatively lower change in entropy, when two domains are concatenated, indicates that the two domains contain similar topics and are therefore closer to each other. This metric is also our novel contribution. Using this metric, we calculate the percentage change in the entropy when the target domain is concatenated with the source domain. We calculate the entropy as the combination of entropy for unigrams, bigrams, trigrams, and quadrigrams. We consider only polar words for unigrams. For bi, tri and quadrigrams, we give priority to polar words by using a weighted entropy function and this weighted entropy $E$ is calculated as:",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec",
                "We train SKIPGRAM models on all the domains to obtain word embeddings. We build models with 50 dimensions where the context window is chosen to be 5. For each domain pair, we then compare embeddings of common adjectives in both the domains by calculating Angular Similarity BIBREF17. It was observed that cosine similarity values were very close to each other, making it difficult to clearly separate domains. Since Angular Similarity distinguishes nearly parallel vectors much better, we use it instead of Cosine Similarity. We obtain a similarity value by averaging over all common adjectives. For the final similarity value of this metric, we use Jaccard Similarity Coefficient here as well:",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec",
                "Doc2Vec represents each sentence by a dense vector which is trained to predict words in the sentence, given the model. It tries to overcome the weaknesses of the bag-of-words model. Similar to Word2Vec, we train Doc2Vec models on each domain to extract sentence vectors. We train the models over 100 epochs for 100 dimensions, where the learning rate is 0.025. Since we can no longer leverage adjectives for sentiment, we use SentiWordnet for assigning sentiment scores (ranging from -1 to +1 where -1 denotes a negative sentiment, and +1 denotes a positive sentiment) to reviews (as detailed above) and select reviews which have a score above a certain threshold. We have empirically arrived at $\\pm 0.01$ as the threshold value. Any review with a score outside this window is selected. We also restrict the length of reviews to a maximum of 100 words to reduce sparsity.",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe",
                "Both Word2Vec and GloVe learn vector representations of words from their co-occurrence information. However, GloVe is different in the sense that it is a count-based model. In this metric, we use GloVe embeddings for adjectives shared by domain-pairs. We train GloVe models for each domain over 50 epochs, for 50 dimensions with a learning rate of 0.05. For computing similarity of a domain-pair, we follow the same procedure as described under the Word2Vec metric. The final similarity value is obtained using equation (DISPLAY_FORM29).",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText",
                "We train monolingual word embeddings-based models for each domain using the FastText library. We train these models with 100 dimensions and 0.1 as the learning rate. The size of the context window is limited to 5 since FastText also uses sub-word information. Our model takes into account character n-grams from 3 to 6 characters, and we train our model over 5 epochs. We use the default loss function (softmax) for training.",
                "We devise two different metrics out of FastText models to calculate the similarity between domain-pairs. In the first metric (ULM4), we compute the Angular Similarity between the word vectors for all the common adjectives, and for each domain pair just like Word2Vec and GloVe. Overall, similarity for a domain pair is calculated using equation (DISPLAY_FORM29). As an additional metric (ULM5), we extract sentence vectors for reviews and follow a procedure similar to Doc2Vec. SentiWordnet is used to filter out train and test data using the same threshold window of $\\pm 0.01$.",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo",
                "We use the pre-trained deep contextualized word representation model provided by the ELMo library. Unlike Word2Vec, GloVe, and FastText, ELMo gives multiple embeddings for a word based on different contexts it appears in the corpus."
            ],
            "gold_section": [
                "Similarity Metrics",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM4: Entropy Change",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM2: Symmetric KL-Divergence (SKLD)",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM3: Chameleon Words Similarity",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM6: ELMo",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM1: Word2Vec",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM4 and ULM5: FastText",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM2: Doc2Vec",
                "Similarity Metrics ::: Metrics: Unlabelled Data ::: ULM3: GloVe",
                "Similarity Metrics ::: Metrics: Labelled Data ::: LM1: Significant Words Overlap"
            ],
            "predicted": [
                "We use a total of 11 metrics over two scenarios: the first that uses labelled data, while the second that uses unlabelled data.",
                "Symmetric Metrics - The metrics which consider domain-pairs $(D_1,D_2)$ and $(D_2,D_1)$ as the same and provide similar results for them viz. Significant Words Overlap, Chameleon Words Similarity, Symmetric KL Divergence, Word2Vec embeddings, GloVe embeddings, FastText word embeddings, ELMo based embeddings and Universal Sentence Encoder based embeddings.",
                "In table TABREF6, we present the n-gram percent match among the domain data used in our experiments. We observe that the n-gram match from among this corpora is relatively low and simple corpus similarity measures which use orthographic techniques cannot be used to obtain domain similarity. Hence, we propose the use of the metrics detailed below to perform our experiments."
            ],
            "predicted_section": [
                "Similarity Metrics"
            ]
        },
        "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654": {
            "question_text": "What high-resource language pair is the parent model trained on?",
            "from_paper": "1604.02201",
            "gold": [
                "We start by a brief description of our NMT model in Section \"NMT Background\" . Section \"Transfer Results\" gives some background on transfer learning and explains how we use it to improve machine translation performance. Our main experiments translating Hausa, Turkish, Uzbek and Urdu into English with the help of a French-English parent model are presented in Section \"Experiments\" . Section \"Analysis\" explores alternatives to our model to enhance understanding. We find that the choice of parent language pair affects performance, and provide an empirical upper bound on transfer performance using an artificial language. We experiment with English-only language models, copy models, and word-sorting models to show that what we transfer goes beyond monolingual information and using a translation model trained on bilingual corpora as a parent is essential. We show the effects of freezing, fine-tuning, and smarter initialization of different components of the attention-based NMT system during transfer. We compare the learning curves of transfer and no-transfer models showing that transfer solves an overfitting problem, not a search problem. We summarize our contributions in Section \"Conclusion\" .",
                "The transfer learning approach we use is simple and effective. We first train an NMT model on a dataset where there is a large amount of bilingual data (e.g., French-English), which we call the parent model. Next, we initialize an NMT model with the already-trained parent model. This new model is then trained on a dataset with very little bilingual data (e.g., Uzbek-English), which we call the child model. This means that the low-data NMT model will not start with random weights, but with the weights from the parent model."
            ],
            "gold_section": [
                "Introduction",
                "Transfer Learning"
            ],
            "predicted": [
                "For all of our experiments with low-resource languages we use French as the parent source language and for child source languages we use Hausa, Turkish, Uzbek, and Urdu. The target language is always English. Table 1 shows parallel training data set sizes for the child languages, where the language with the most data has only 1.8m English tokens. For comparison, our parent French-English model uses a training set with 300 million English tokens and achieves 26 Bleu on the development set. Table 1 also shows the SBMT system scores along with the NMT baselines that do not use transfer. There is a large gap between the SBMT and NMT systems without using our transfer method.",
                "Our experimental results are shown in Table 5 , where we use French and German as parent languages. If we just train a model with no transfer on a small Spanish-English training set we get a Bleu score of 16.4. When using our transfer method using French and German as parent languages, we get Bleu scores of 31.0 and 29.8 respectively. As expected, French is a better parent than German for Spanish, which could be the result of the parent language being more similar to the child language.",
                "In the above experiments, we use a parent model trained on a large French/English bilingual corpus. One might hypothesize that our gains come from exploiting the English half of the corpus as an additional language model resource. Therefore, we explore transfer learning for the child model with parent models that only use the English side of the bilingual corpus. Table 8 shows the results for these experiments where we train one parent model to copy English sentences (English-English) and another parent model to un-permute scrambled English sentences (EngPerm-English). Additionally, we train a parent model that is just an RNN language model. These results show that our transfer learning is not simply importing an English language model, but making use of translation parameters learned from the parent's large bilingual text."
            ],
            "predicted_section": [
                "Different Parent Models",
                "Experiments",
                "Different Parent Languages"
            ]
        },
        "5ae005917efc17a505ba1ba5e996c4266d6c74b6": {
            "question_text": "Did they use the same dataset as Skip-gram to train?",
            "from_paper": "1806.06571",
            "gold": [
                "We train our NN on words and their contexts extracted from the English wikipedia dump from May 2015. We have cleaned the data by replacing all numbers with 0 and removing special characters except those usually present in the English text like dots, brackets, apostrophes etc. For the final training data we have randomly selected only 2.5M segments (mostly sentences). It consist of 96M running words with the vocabulary size of 1.09M distinct word forms.",
                "We consider only the 141K most frequent word forms to simplify the training. The remaining word forms fall out of vocabulary (OOV), so the original Skip-gram cannot provide them with any vector representation. Our SubGram relies on known substrings and always provides at least some approximation."
            ],
            "gold_section": [
                "Evaluation and Data Sets"
            ],
            "predicted": [
                "The Skip-gram model is a classic NN, where activation functions are removed and hierarchical soft-max BIBREF8 is used instead of soft-max normalization. The input representation is one-hot so the activation function is not needed on hidden layer, there is nothing to be summed up. This way, the model is learned much faster than comparable non-linear NNs and lends itself to linear vector operations possibly useful for finding semantically or syntactically related words.",
                "Comparing Skip-gram and SubGram on the original test set (Table TABREF18 ), we see that our SubGram outperforms Skip-gram in several morpho-syntactic question sets but over all performs similarly (42.5% vs. 42.3%). On the other hand, it does not capture the tested semantic relations at all, getting a zero score on average.",
                "When comparing models on our test set (Table TABREF19 ), we see that given the same training set, SubGram significantly outperforms Skip-gram model (22.4% vs. 9.7%). The performance of Skip-gram trained on the much larger dataset is higher (43.5%) and it would be interesting to see the SubGram model, if we could get access to such training data. Note however, that the Rule-based baseline is significantly better on both test sets."
            ],
            "predicted_section": [
                "Skip-gram Model",
                "Experiments and Results"
            ]
        },
        "72c04eb3fc323c720f7f8da75c70f09a35abf3e6": {
            "question_text": "How much were the gains they obtained?",
            "from_paper": "1806.06571",
            "gold": [
                "We test our model on the original test set BIBREF7 . The test set consists of 19544 \u201cquestions\u201d, of which 8869 are called \u201csemantic\u201d and 10675 are called \u201csyntactic\u201d and further divided into 14 types, see Table TABREF4 . Each question contains two pairs of words ( INLINEFORM0 ) and captures relations like \u201cWhat is to `woman' ( INLINEFORM1 ) as `king' ( INLINEFORM2 ) is to `man' ( INLINEFORM3 )?\u201d, together with the expected answer `queen' ( INLINEFORM4 ). The model is evaluated by finding the word whose representation is the nearest (cosine similarity) to the vector INLINEFORM5 . If the nearest neighbor is INLINEFORM6 , we consider the question answered correctly.",
                "We have decided to create more general test set which would consider more than 35 pairs per question set. Since we are interested in morphosyntactic relations, we extended only the questions of the \u201csyntactic\u201d type with exception of nationality adjectives which is already covered completely in original test set.",
                "The accuracy is computed as the number of correctly answered questions divided by the total number of questions in the set. Because the Skip-gram cannot answer questions containing OOV words, we also provide results with such questions excluded from the test set (scores in brackets).",
                "Table TABREF18 and Table TABREF19 report the results. The first column shows the rule-based approach. The column \u201cReleased Skip-gram\u201d shows results of the model released by Mikolov and was trained on a 100 billion word corpus from Google News and generates 300 dimensional vector representation. The third column shows Skip-gram model trained on our training data, the same data as used for the training of the SubGram. Last column shows the results obtained from our SubGram model.",
                "We consider only the 141K most frequent word forms to simplify the training. The remaining word forms fall out of vocabulary (OOV), so the original Skip-gram cannot provide them with any vector representation. Our SubGram relies on known substrings and always provides at least some approximation.",
                "We propose a substring-oriented extension of Skip-gram model which induces vector embeddings from character-level structure of individual words. This approach gives the NN more information about the examined word with no drawbacks in data sparsity or reliance on explicit linguistic annotation.",
                "Comparing Skip-gram and SubGram on the original test set (Table TABREF18 ), we see that our SubGram outperforms Skip-gram in several morpho-syntactic question sets but over all performs similarly (42.5% vs. 42.3%). On the other hand, it does not capture the tested semantic relations at all, getting a zero score on average."
            ],
            "gold_section": [
                "Experiments and Results",
                "Our Test Set",
                "Evaluation and Data Sets",
                "SubGram"
            ],
            "predicted": [
                "Table TABREF18 and Table TABREF19 report the results. The first column shows the rule-based approach. The column \u201cReleased Skip-gram\u201d shows results of the model released by Mikolov and was trained on a 100 billion word corpus from Google News and generates 300 dimensional vector representation. The third column shows Skip-gram model trained on our training data, the same data as used for the training of the SubGram. Last column shows the results obtained from our SubGram model.",
                "This work has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement no. 645452 (QT21), the grant GAUK 8502/2016, and SVV project number 260 333.",
                "The last column suggests that the performance of our model on OOV words is not very high, but it is still an improvement over flat zero of the Skip-gram model. The performance on OOVs is expected to be lower, since the model has no knowledge of exceptions and can only benefit from regularities in substrings."
            ],
            "predicted_section": [
                "Experiments and Results",
                "Acknowledgment"
            ]
        },
        "ab8b0e6912a7ca22cf39afdac5531371cda66514": {
            "question_text": "By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s ",
            "from_paper": "1912.03010",
            "gold": [
                "The experiment results are listed in Table TABREF21, showing a similar trend as the results in Librispeech dataset. Semantic mask is complementary to specagumentation, which enables better S2S language modeling training in an E2E model, resulting in a relative 4.5$\\%$ gain. The experiment proves the effectiveness of semantic mask on a different and smaller dataset.",
                "As far as we know, our model is the best E2E ASR system on the Librispeech testset, which achieves a comparable result with wav2letter Transformer on test-clean dataset and a better result on test-other dataset, even though our model (75M parameters) is much smaller than the wav2letter Transformer (210M parameters). The reason might be that our semantic masking is more suitable on a noisy setting, because the input features are not reliable and the model has to predict the next token relying on previous ones and the whole context of the input. Our model is built upon the code base of ESPnet, and achieves relative $10\\%$ gains due to the better architecture and masking strategy. Comparing with hybrid methods, our model obtains a similar performance on the test-clean set, but is still worse than the best hybrid model on the test-other dataset."
            ],
            "gold_section": [
                "EXPERIMENT ::: Librispeech 960h",
                "EXPERIMENT ::: TedLium2"
            ],
            "predicted": [
                "This paper presents a semantic mask method for E2E speech recognition, which is able to train a model to better consider the whole audio context for the disambiguation. Moreover, we elaborate a new architecture for E2E model, achieving state-of-the-art performance on the Librispeech test set in the scope of E2E models.",
                "End-to-end (E2E) acoustic models, particularly with the attention-based encoder-decoder framework BIBREF0, have achieved a competitive recognition accuracy in a wide range of speech datasets BIBREF1. This model directly learns the mapping from the input acoustic signals to the output transcriptions without decomposing the problems into several different modules such as lexicon modeling, acoustic modeling and language modeling as in the conventional hybrid architecture. While this kind of E2E approach significantly simplifies the speech recognition pipeline, the weakness is that it is difficult to tune the strength of each component. One particular problem from our observations is that the attention based E2E model tends to make grammatical errors, which indicates that the language modeling power of the model is weak, possibly due to the small amount of training data, or the mismatch between the training and evaluation data. However, due to the jointly model approach in the attention model, it is unclear how to improve the strength of the language modeling power, i.e., attributing more weights to the previous output tokens in the decoder, or to improve the strength of the acoustic modeling power, i.e., attributing more weights to the context vector from the encoder.",
                "In principle, our approach is applicable to the attention-based E2E framework with any type of neural network encoder. To constrain our research scope, we focus on the transformer architecture BIBREF5, which is originally proposed for neural machine translation. Recently, it has been shown that the transformer model can achieve competitive or even higher recognition accuracy compared with the recurrent neural network (RNN) based E2E model for speech recognition BIBREF6. Compared with RNNs, the transformer model can capture the long-term correlations with a computational complexity of $O(1)$, instead of using many steps of back-propagation through time (BPTT) as in RNNs. We evaluate our transformer model with semantic masking on Librispeech and TedLium datasets. We show that semantic masking can achieve significant word error rate reduction (WER) on top of SpecAugment, and we report the lowest WERs on the test sets of the Librispeech corpus with an E2E model."
            ],
            "predicted_section": [
                "Introduction",
                "Conclusion"
            ]
        },
        "75c221920bee14a6153bd5f4c1179591b2f48d59": {
            "question_text": "What impact on performance is shown for different choices of optimizers and learning rate policies?",
            "from_paper": "2004.02401",
            "gold": [
                "Applying CLR has positive impacts on NMT training for both Adam and SGD. When applied to SGD, CLR exempts the needs for a big initial learning rate as it enables the optimizer to explore the local minima better. Shrinking on CLR for SGD is not desirable as a higher learning rate is required (Figure FIGREF16). It is noted that applying CLR to Adam produces consistent improvements regardless of shrink options (Figure FIGREF15). Furthermore, it can be observed that the effects of applying CLR to Adam are more significant than those of SGD, as shown in Figure FIGREF17. Similar results are obtained from our experiments on \u201cIWSLT2017-de-en\" and \u201cIWSLT2014-fr-en\" corpora (Figures FIGREF30 and FIGREF31 in Appendix SECREF7). The corresponding BLEU scores are presented in Table TABREF18, in which the above-mentioned effects of CLR on Adam can also be established. The training takes fewer epochs to converge to reach a local minimum with better BLEU scores (i.e., bold fonts in Table TABREF18)."
            ],
            "gold_section": [
                "Experiments ::: Effects of Applying CLR to NMT Training"
            ],
            "predicted": [
                "While the aforementioned works have helped to contribute our understanding of the nature of the various optimizers, their learning rates and batch size effects, they are mainly focused on computer vision (CV) related deep learning networks and datasets. In contrast, the rich body of works in Neural Machine Translation (NMT) and other Natural Language Processing (NLP) related tasks have been largely left untouched. Recall that CV deep learning networks and NMT deep learning networks are very different. For instance, the convolutional network that forms the basis of many successful CV deep learning networks is translation invariant, e.g., in a face recognition network, the convolutional filters produce the same response even when the same face is shifted or translated. In contrast, Recurrent Neural Networks (RNN) BIBREF12, BIBREF13 and transformer-based deep learning networks BIBREF14, BIBREF15 for NMT are specifically looking patterns in sequences. There is no guarantee that the results from the CV based studies can be carried across to NMT. There is also a lack of awareness in the NMT community when it comes to optimizers and other related issues such as learning rate policy and batch size. It is often assumed that using the mainstream optimizer (Adam) with the default settings is good enough. As our study shows, there is significant room for improvement.",
                "The other hyperparameter to take care of is the learning rate decay rate, shown in Figure FIGREF8. For the various optimizers, the learning rate is usually decayed to a small value to ensure convergence. There are various commonly used decay schemes such as piece-wise constant step function, inverse (reciprocal) square root. This study adopts two learning rate decay policies:",
                "There has been many interests in deep learning optimizer research recently BIBREF0, BIBREF1, BIBREF2, BIBREF3. These works attempt to answer the question: what is the best step size to use in each step of the gradient descent? With the first order gradient descent being the de facto standard in deep learning optimization, the question of the optimal step size or learning rate in each step of the gradient descent arises naturally. The difficulty in choosing a good learning rate can be better understood by considering the two extremes: 1) when the learning rate is too small, training takes a long time; 2) while overly large learning rate causes training to diverge instead of converging to a satisfactory solution."
            ],
            "predicted_section": [
                "The Proposed Approach",
                "Introduction"
            ]
        },
        "cfc73e0c82cf1630b923681c450a541a964688b9": {
            "question_text": "How do they operationalize socioeconomic status from twitter user data?",
            "from_paper": "1804.01155",
            "gold": [
                "To obtain a unique representative location of each user, we extracted the sequence of all declared locations from their geolocated tweets. Using this set of locations we selected the most frequent to be the representative one, and we took it as a proxy for the user's home location. Further we limited our users to ones located throughout the French territory thus not considering others tweeting from places outside the country. This selection method provided us with $110,369$ geolocated users who are either detected as French speakers or assigned to be such by Twitter and all associated to specific 'home' GPS coordinates in France. To verify the spatial distribution of the selected population, we further assessed the correlations between the true population distributions (obtained from census data BIBREF28 ) at different administrative level and the geolocated user distribution aggregated correspondingly. More precisely, we computed the $R^2$ coefficient of variation between the inferred and official population distributions (a) at the level of 22 regions. Correlations at this level induced a high coefficient of $R^2\\simeq 0.89$ ( $p<10^{-2}$ ); (b) At the arrondissement level with 322 administrative units and coefficient $R^2\\simeq 0.87$ ( $p<10^{-2}$ ); and (c) at the canton level with 4055 units with a coefficient $R\\simeq 0.16$ ( $p<10^{-2}$ ). Note that the relatively small coefficient at this level is due to the interplay of the sparsity of the inferred data and the fine grained spatial resolution of cantons. All in all, we can conclude that our sample is highly representative in terms of spatial population distribution, which at the same time validate our selection method despite the potential inherent biases induced by the method taking the most frequented GPS coordinates as the user's home location.",
                "The second dataset we used was released in December 2016 by the National Institute of Statistics and Economic Studies (INSEE) of France. This data corpus BIBREF29 contains a set of sociodemographic aggregated indicators, estimated from the 2010 tax return in France, for each 4 hectare ( $200m \\times 200m$ ) square patch across the whole French territory. Using these indicators, one can estimate the distribution of the average socioeconomic status (SES) of people with high spatial resolution. In this study, we concentrated on three indicators for each patch $i$ , which we took to be good proxies of the socioeconomic status of the people living within them. These were the $S^i_\\mathrm {inc}$ average yearly income per capita (in euros), the $S^i_{\\mathrm {own}}$ fraction of owners (not renters) of real estate, and the $S^i_\\mathrm {den}$ density of population defined respectively as",
                "To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators."
            ],
            "gold_section": [
                "Twitter dataset: sociolinguistic features",
                "Combined dataset: individual socioeconomic features"
            ],
            "predicted": [
                "Many studies have overcome this limitation by taking advantage of the geolocation feature allowing Twitter users to include in their posts the location from which they were tweeted. Based on this metadata, studies have been able to assign home location to geolocated users with varying degrees of accuracy BIBREF15 . Subsequent work has also been devoted to assigning to each user some indicator that might characterize their socioeconomic status based on their estimated home location. These indicators are generally extracted from other datasets used to complete the Twitter one, namely census data BIBREF16 , BIBREF12 , BIBREF17 or real estate online services as Zillow.com BIBREF18 . Other approaches have also relied on sources of socioeconomic information such as the UK Standard Occupation Classification (SOC) hierarchy, to assign socioeconomic status to users with occupation mentions BIBREF19 . Despite the relative success of these methods, their common limitation is to provide observations and predictions based on a carefully hand-picked small set of users, letting alone the problem of socioeconomic status inference on larger and more heterogeneous populations. Our work stands out from this well-established line of research by expanding the definition of socioeconomic status to include several demographic features as well as by pinpointing potential home location to individual users with an unprecedented accuracy. Identifying socioeconomic status and the network effects of homophily BIBREF20 is an open question BIBREF21 . However, recent results already showed that status homophily, i.e. the tendency of people of similar socioeconomic status are better connected among themselves, induce structural correlations which are pivotal to understand the stratified structure of society BIBREF22 . While we verify the presence of status homophily in the Twitter social network, we detect further sociolinguistic correlations between language, location, socioeconomic status, and time, which may inform novel methods to infer socioeconomic status for a broader set of people using common information available on Twitter.",
                "Data collected from Twitter provides a large variety of information about several users including their tweets, which disclose their interests, vocabulary, and linguistic patterns; their direct mentions from which their social interactions can be inferred; and the sequence of their locations, which can be used to infer their representative location. However, no information is directly available regarding their socioeconomic status, which can be pivotal to understand the dynamics and structure of their personal linguistic patterns.",
                "To overcome this limitation we combined our Twitter data with the socioeconomic maps of INSEE by assigning each geolocated Twitter user to a patch closest to their estimated home location (within 1 km). This way we obtained for all $110,369$ geolocated users their dynamical linguistic data, their egocentric social network as well as a set of SES indicators."
            ],
            "predicted_section": [
                "Related Work",
                "Combined dataset: individual socioeconomic features"
            ]
        },
        "98b97d24f31e9c535997e9b6cb126eb99fc72a90": {
            "question_text": "What empirical evaluation was used?",
            "from_paper": "1910.01160",
            "gold": [
                "We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.",
                "First, we consider the semantic representation with BERT. Our experiments included multiple pre-trained models of BERT with different sizes and cases sensitivity, among which the large uncased model, bert_uncased_L-24_H-1024_A-16, gave the best results. We use the recommended settings of hyper-parameters in BERT's Github repository and use the fake news and satire data to fine-tune the model. Furthermore, we tested separate models based on the headline and body text of a story, and in combination. Results are shown in Table TABREF6. The models based on the headline and text body give a similar F1 score. However, while the headline model performs poorly on precision, perhaps due to the short text, the model based on the text body performs poorly on recall. The model based on the full text of headline and body gives the best performance.",
                "With regard to research question RQ2 on the understanding of semantic and linguistic nuances between fake news and satire - a key advantage of studying the coherence metrics is explainability. While the pre-trained model of BERT gives the best result, it is not easily interpretable. The coherence metrics allow us to study the differences between fake news and satire in a straightforward manner."
            ],
            "gold_section": [
                "Evaluation ::: Insights on Linguistic Nuances",
                "Evaluation ::: Classification Between Fake News and Satire"
            ],
            "predicted": [
                "We evaluate the performance of our method based on the dataset of fake news and satire articles and using the F1 score with a ten-fold cross-validation as in the baseline work BIBREF1.",
                "Consequently, we use the set of text coherence metrics as implemented by Coh-Metrix BIBREF12. Coh-Metrix is a tool for producing linguistic and discourse representations of a text. As a result of applying the Coh-Metrix to the input documents, we have 108 indices related to text statistics, such as the number of words and sentences; referential cohesion, which refers to overlap in content words between sentences; various text readability formulas; different types of connective words and more. To account for multicollinearity among the different features, we first run a Principal Component Analysis (PCA) on the set of Coh-Metrix indices. Note that we do not apply dimensionality reduction, such that the features still correspond to the Coh-Metrix indices and are thus explainable. Then, we use the PCA scores as independent variables in a logistic regression model with the fake and satire labels as our dependent variable. Significant features of the logistic regression model are shown in Table TABREF3 with the respective significance levels. We also run a step-wise backward elimination regression. Those components that are also significant in the step-wise model appear in bold.",
                "In the following sub sections, we evaluate our classification model and share insights on the nuances between fake news and satire, while addressing our two research questions."
            ],
            "predicted_section": [
                "Method ::: Linguistic Analysis with Coh-Metrix",
                "Evaluation",
                "Evaluation ::: Classification Between Fake News and Satire"
            ]
        },
        "998fa38634000f2d7b52d16518b9e18e898ce933": {
            "question_text": "Does the SESAME dataset include discontiguous entities?",
            "from_paper": "1908.05758",
            "gold": [
                "The next step consists of detecting mentions to entities in the raw text. To do this, we tag character segments that exactly match one of the known names of an entity. For instance, we can tag two different entities in the following text:"
            ],
            "gold_section": [
                "Preprocessing ::: Identifying entity mentions in text"
            ],
            "predicted": [
                "We only consider sentences that have annotated entities. After all, sentences with only parser extraction entities do not take advantage of the human discernment invested in the structuring of the data of DBpedia.",
                "Not surprisingly, the vast majority of words are not related to an entity mention at all. The statistics among words that are part of an entity mention are given in Table TABREF46, where over half of the entity mentions are of the type location.",
                "SESAME consists of 87,769,158 tokens in total. The count and proportion of each entity tag (not a named entity, organization, person, location) is given in TABREF45."
            ],
            "predicted_section": [
                "Preprocessing ::: SESAME",
                "Preprocessing ::: SESAME ::: Tokens"
            ]
        },
        "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6": {
            "question_text": "What is the Semantic Web?",
            "from_paper": "1911.01248",
            "gold": [
                "NLG is the process of automatically generating coherent NL text from non-linguistic data BIBREF0. Recently, the field has seen an increased interest in the development of NLG systems focusing on verbalizing resources from SW data BIBREF1. The SW aims to make information available on the Web easier to process for machines and humans. However, the languages underlying this vision, i.e., RDF, SPARQL and OWL, are rather difficult to understand for non-expert users. For example, while the meaning of the OWL class expression Class: Professor SubClassOf: worksAt SOME University is obvious to every SW expert, this expression (\u201cEvery professor works at a university\u201d) is rather difficult to fathom for lay persons.",
                "OWL BIBREF15 is the de-facto standard for machine processable and interoperable ontologies on the SW. In its second version, OWL is equivalent to the description logic $\\mathcal {SROIQ}(D)$. Such expressiveness has a higher computational cost but allows the development of interesting applications such as automated reasoning BIBREF16. OWL 2 ontologies consist of the following three different syntactic categories:",
                "RDF BIBREF18 uses a graph-based data model for representing knowledge. Statements in RDF are expressed as so-called triples of the form (subject, predicate, object). RDF subjects and predicates are IRI and objects are either IRI or literals. RDF literals always have a datatype that defines its possible values. A predicate denotes a property and can also be seen as a binary relation taking subject and object as arguments. For example, the following triple expresses that Albert Einstein was born in Ulm:"
            ],
            "gold_section": [
                "Background ::: OWL",
                "Introduction",
                "Background ::: RDF"
            ],
            "predicted": [
                "In this paper, we present an open-source holistic NLG framework for the SW, named LD2NL, which facilitates the verbalization of the three key languages of the SW, i.e., RDF, OWL, and SPARQL into NL. Our framework is based on a bottom-up paradigm for verbalizing SW data. Additionally, LD2NL builds upon SPARQL2NL as it is open-source and the paradigm it follows can be reused and ported to RDF and OWL. Thus, LD2NL is capable of generating either a single sentence or a summary of a given resource, rule, or query. To validate our framework, we evaluated LD2NL using experts 66 in NLP and SW as well as 20 non-experts who were lay users or non-users of SW. The results suggest that LD2NL generates texts which can be easily understood by humans. The version of LD2NL used in this paper, all experimental results will be publicly available.",
                "NLG is the process of automatically generating coherent NL text from non-linguistic data BIBREF0. Recently, the field has seen an increased interest in the development of NLG systems focusing on verbalizing resources from SW data BIBREF1. The SW aims to make information available on the Web easier to process for machines and humans. However, the languages underlying this vision, i.e., RDF, SPARQL and OWL, are rather difficult to understand for non-expert users. For example, while the meaning of the OWL class expression Class: Professor SubClassOf: worksAt SOME University is obvious to every SW expert, this expression (\u201cEvery professor works at a university\u201d) is rather difficult to fathom for lay persons.",
                "where $\\text{\\em synset}(p)$ is the set of all synsets of $p$, $\\text{\\em synset}(p|X)$ is the set of all synsets of $p$ that are of the syntactic class $X \\in \\lbrace \\texttt {noun},\\texttt {verb}\\rbrace $ and $f(t)$ is the frequency of use of $p$ in the sense of the synset $t$ according to WordNet. For"
            ],
            "predicted_section": [
                "LD2NL Framework ::: From RDF to NL ::: Lexicalization",
                "Introduction"
            ]
        },
        "1128a600a813116cba9a2cf99d8568ae340f327a": {
            "question_text": "What datasets do they use in the experiment?",
            "from_paper": "1802.08969",
            "gold": [
                "For classification task, we test our model on 16 classification datasets, the first 14 datasets are product reviews that collected based on the dataset, constructed by BIBREF27 , contains Amazon product reviews from different domains: Books, DVDs, Electronics and Kitchen and so on. The goal in each domain is to classify a product review as either positive or negative. The datasets in each domain are partitioned randomly into training data, development data and testing data with the proportion of 70%, 10% and 20% respectively. The detailed statistics are listed in Table 1 .",
                "The remaining two datasets are two sub-datasets about movie reviews.",
                "IMDB The movie reviews with labels of subjective or objective BIBREF28 .",
                "MR The movie reviews with two classes BIBREF29 .",
                "For sequence tagging task, we use the Wall Street Journal(WSJ) portion of Penn Treebank (PTB) BIBREF33 , CoNLL 2000 chunking, and CoNLL 2003 English NER datasets. The statistics of these datasets are described in Table 4 ."
            ],
            "gold_section": [
                "Exp-II: Multi-task Learning of Sequence Tagging",
                "Exp-I: Multi-task Learning of text classification"
            ],
            "predicted": [
                "In this section, we investigate the empirical performances of our proposed model on two multi-task datasets. Each dataset contains several related tasks.",
                "We first conduct our experiment on classification tasks.",
                "It is worth noticing that labeled data for training each task can come from completely different datasets. Following BIBREF9 , the training is achieved in a stochastic manner by looping over the tasks:"
            ],
            "predicted_section": [
                "Exp-I: Multi-task Learning of text classification",
                "Experiment",
                "Training"
            ]
        },
        "d64fa192a7e9918c6a22d819abad581af0644c7d": {
            "question_text": "What new tasks do they use to show the transferring ability of the shared meta-knowledge?",
            "from_paper": "1802.08969",
            "gold": [
                "To test the transferability of our learned Meta-LSTM, we also design an experiment, in which we take turns choosing 15 tasks to train our model with multi-task learning, then the learned Meta-LSTM are transferred to the remaining one task. The parameters of transferred Meta-LSTM, $\\theta ^{(s)}_m$ in Eq.( 33 ), are fixed and cannot be updated on the new task.",
                "We demonstrate the effectiveness of our architectures on two kinds of NLP tasks: text classification and sequence tagging. Experimental results show that jointly learning of multiple related tasks can improve the performance of each task relative to learning them independently.",
                "Table 5 shows the accuracies or F1 scores on the sequence tagging datasets of our models, compared to some state-of-the-art results. As shown, our proposed Meta-LSTM performs better than our competitor models whether it is single or multi-task learning."
            ],
            "gold_section": [
                "Exp-II: Multi-task Learning of Sequence Tagging",
                "Exp-I: Multi-task Learning of text classification",
                "Introduction"
            ],
            "predicted": [
                "In this paper, we introduce a novel knowledge sharing scheme for multi-task learning. The difference from the previous models is the mechanisms of sharing information among several tasks. We design a meta network to store the knowledge shared by several related tasks. With the help of the meta network, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. Experimental results show that our model can improve the performances of several related tasks by exploring common features and outperforms the representational sharing scheme. The knowledge captured by the meta network can be transferred across other new tasks.",
                "In this paper, we take a very different multi-task architecture from meta-learning perspective BIBREF25 . One goal of meta-learning is to find efficient mechanisms to transfer knowledge across domains or tasks BIBREF26 .",
                "The meta network can be considered as off-the-shelf knowledge and then be used for unseen new tasks."
            ],
            "predicted_section": [
                "Exp-I: Multi-task Learning of text classification",
                "Conclusion and Future Work",
                "Meta Multi-Task Learning"
            ]
        },
        "3d1ad8a4aaa2653d0095bafba74738bd20795acf": {
            "question_text": "what dataset were used?",
            "from_paper": "1909.07158",
            "gold": [
                "We use three data sets related to the hate speech.",
                "Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval",
                "data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech).",
                "Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic",
                "data set is a manually labeled text toxicity data, originally containing 1000 comments crawled from YouTube videos about the Ferguson unrest in 2014. Apart from the main label describing if the comment is hate speech, there are several other labels characterizing each comment, e.g., if it is a threat, provocative, racist, sexist, etc. (not used in our study). There are 138 comments labeled as a hate speech and 862 as non-hate speech. We produced a data set of 300 comments using all 138 hate speech comments and randomly sampled 162 non-hate speech comments.",
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets."
            ],
            "gold_section": [
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval",
                "Experimental Setting ::: Hate Speech Data Sets ::: 2 - YouToxic",
                "Experimental Setting ::: Hate Speech Data Sets"
            ],
            "predicted": [
                "We use three data sets related to the hate speech.",
                "data set originates in a study regarding hate speech detection and the problem of offensive language BIBREF3. Our data set consists of 3000 tweets. We took 1430 tweets labeled as hate speech and randomly sampled 1670 tweets from the collection of remaining 23353 tweets.",
                "data set is taken from the SemEval task \"Multilingual detection of hate speech against immigrants and women in Twitter (hatEval)\". The competition was organized for two languages, Spanish and English; we only processed the English data set. The data set consists of 100 tweets labeled as 1 (hate speech) or 0 (not hate speech)."
            ],
            "predicted_section": [
                "Experimental Setting ::: Hate Speech Data Sets ::: 3 - OffensiveTweets",
                "Experimental Setting ::: Hate Speech Data Sets ::: 1 - HatEval",
                "Experimental Setting ::: Hate Speech Data Sets"
            ]
        },
        "344238de7208902f7b3a46819cc6d83cc37448a0": {
            "question_text": "Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?",
            "from_paper": "1908.06024",
            "gold": [
                "Razavi et al. razavi were the first to adopt lexicon-based abuse detection. They constructed an insulting and abusing language dictionary of words and phrases, where each entry had an associated weight indicating its abusive impact. They utilized semantic rules and features derived from the lexicon to build a three-level Naive Bayes classification system and apply it to a dataset of INLINEFORM0 messages ( INLINEFORM1 flame and the rest okay) extracted from the Usenet newsgroup and the Natural Semantic Module company's employee conversation thread ( INLINEFORM2 accuracy). Njagi et al. gitari also employed such a lexicon-based approach and, more recently, Wiegand et al. wiegand proposed an automated framework for generating such lexicons. While methods based on lexicons performed well on explicit abuse, the researchers noted their limitations on implicit abuse.",
                "Bag-of-words (bow) features have been integral to several works on abuse detection. Sood et al. sood2012 showed that an svm trained on word bi-gram features outperformed a word-list baseline utilizing a Levenshtein distance-based heuristic for detecting profanity. Their best classifier (combination of SVMs and word-lists) yielded an F INLINEFORM0 of INLINEFORM1 . Warner and Hirschberg warner employed a template-based strategy alongside Brown clustering to extract surface-level bow features from a dataset of paragraphs annotated for antisemitism, and achieved an F INLINEFORM2 of INLINEFORM3 using svms. Their approach is unique in that they framed the task as a word-sense disambiguation problem, i.e., whether a term carried an anti-semitic sense or not. Other examples of bow-based methods are those of Dinakar et al. dinakar2011modeling, Burnap and Williams burnap and Van Hee et al. vanhee who use word n-grams in conjunction with other features, such as typed-dependency relations or scores based on sentiment lexicons, to train svms ( INLINEFORM4 F INLINEFORM5 on the data-bully dataset). Recenlty, Salminen et al. salminen2018anatomy showed that a linear SVM using tf\u2013idf weighted n-grams achieves the best performance (average F INLINEFORM6 of INLINEFORM7 ) on classification of hateful comments (from a YouTube channel and Facebook page of an online news organization) as one of 29 different hate categories (e.g., accusation, promoting violence, humiliation, etc.).",
                "Several researchers have directly incorporated features and identity traits of users in order to model the likeliness of abusive behavior from users with certain traits, a process known as user profiling. Dadvar et al. davdar included the age of users alongside other traditional lexicon-based features to detect cyber-bullying, while Gal\u00e1n-Garc\u00eda et al. galan2016supervised utilized the time of publication, geo-position and language in the profile of Twitter users. Waseem and Hovy waseemhovy exploited gender of Twitter users alongside character n-gram counts to improve detection of sexism and racism in tweets from data-twitter-wh (F INLINEFORM0 increased from INLINEFORM1 to INLINEFORM2 ). Using the same setup, Unsv\u00e5g and Gamb\u00e4ck unsvaag2018effects showed that the inclusion of social network-based (i.e., number of followers and friends) and activity-based (i.e., number of status updates and favorites) information of users alongside their gender further enhances performance ( INLINEFORM3 gain in F INLINEFORM4 ).",
                "Building on the work of Djuric et al., Nobata et al. nobata evaluated the performance of a large range of features on the Yahoo! datasets (data-yahoo-*) using a regression model: (1) word and character n-grams; (2) linguistic features, e.g., number of polite/hate words and punctuation count; (3) syntactic features, e.g., parent and grandparent of node in a dependency tree; (4) distributional-semantic features, e.g., paragraph2vec comment representations. Although the best results were achieved with all features combined (F INLINEFORM0 INLINEFORM1 on data-yahoo-fin-a, INLINEFORM2 on data-yahoo-news-a), character n-grams on their own contributed significantly more than other features due to their robustness to noise (i.e., obfuscations, misspellings, unseen words). Experimenting with the data-yahoo-fin-dj dataset, Mehdad and Tetreault mehdad investigated whether character-level features are more indicative of abuse than word-level ones. Their results demonstrated the superiority of character-level features, showing that svm classifiers trained on Bayesian log-ratio vectors of average counts of character n-grams outperform the more intricate approach of Nobata et al. nobata in terms of AUC ( INLINEFORM3 vs. INLINEFORM4 ) as well as other rnn-based character and word-level models.",
                "Samghabadi et al. W17-3010 utilized a similar set of features as Nobata et al. and augmented it with hand-engineered ones such as polarity scores derived from SentiWordNet, measures based on the LIWC program, and features based on emoticons. They then applied their method to three different datasets: data-wiki-att, a Kaggle dataset annotated for insult, and a dataset of questions and answers (each labeled as invective or neutral) that they created by crawling ask.fm. Distributional-semantic features combined with the aforementioned features constituted an effective feature space for the task ( INLINEFORM0 , INLINEFORM1 , INLINEFORM2 F INLINEFORM3 on data-wiki-att, Kaggle, ask.fm respectively). In line with the findings of Nobata et al. and Mehdad and Tetreault, character n-grams performed well on these datasets too."
            ],
            "gold_section": [
                "Neural network based approaches",
                "Feature engineering based approaches"
            ],
            "predicted": [
                "With the advent of social media, anti-social and abusive behavior has become a prominent occurrence online. Undesirable psychological effects of abuse on individuals make it an important societal problem of our time. Munro munro2011 studied the ill-effects of online abuse on children, concluding that children may develop depression, anxiety, and other mental health problems as a result of their encounters online. Pew Research Center, in its latest report on online harassment BIBREF0 , revealed that INLINEFORM0 of adults in the United States have experienced abusive behavior online, of which INLINEFORM1 have faced severe forms of harassment, e.g., that of sexual nature. The report goes on to say that harassment need not be experienced first-hand to have an impact: INLINEFORM2 of American Internet users admitted that they stopped using an online service after witnessing abusive and unruly behavior of their fellow users. These statistics stress the need for automated abuse detection and moderation systems. Therefore, in the recent years, a new research effort on abuse detection has sprung up in the field of NLP.",
                "Online abuse stands as a significant challenge before society. Its nature and characteristics constantly evolve, making it a complex phenomenon to study and model. Automated abuse detection methods have seen a lot of development in recent years: from simple rule-based methods aimed at identifying directed, explicit abuse to sophisticated methods that can capture rich semantic information and even aspects of user behavior. By comprehensively reviewing the investigated methods to date, our survey aims to provide a platform for future research, facilitating progress in this important area. While we see an array of challenges that lie ahead, e.g., modeling extra-propositional aspects of language, user behavior and wider conversation, we believe that recent progress in the areas of semantics, dialogue modeling and social media analysis put the research community in a strong position to address them. Summaries of public datasets In table TABREF4 , we summarize the datasets described in this paper that are publicly available and provide links to them. A discussion of metrics The performance results we have reported highlight that, throughout work on abuse detection, different researchers have utilized different evaluation metrics for their experiments \u2013 from area under the receiver operating characteristic curve (auroc) BIBREF79 , BIBREF48 to micro and macro F INLINEFORM0 BIBREF28 \u2013 regardless of the properties of their datasets. This makes the presented techniques more difficult to compare. In addition, as abuse is a relatively infrequent phenomenon, the datasets are typically skewed towards non-abusive samples BIBREF6 . Metrics such as auroc may, therefore, be unsuitable since they may mask poor performance on the abusive samples as a side-effect of the large number of non-abusive samples BIBREF52 . Macro-averaged precision, recall, and F INLINEFORM1 , as well as precision, recall, and F INLINEFORM2 on specifically the abusive classes, may provide a more informative evaluation strategy; the primary advantage being that macro-averaged metrics provide a sense of effectiveness on the minority classes BIBREF73 . Additionally, area under the precision-recall curve (auprc) might be a better alternative to auroc in imbalanced scenarios BIBREF46 . ",
                "In this paper, we survey the methods that have been developed for automated detection of online abuse, analyzing their strengths and weaknesses. We first describe the datasets that exist for abuse. Then we review the various detection methods that have been investigated by the NLP community. Finally, we conclude with the main trends that emerge, highlight the challenges that remain, outline possible solutions, and propose guidelines for ethics and explainability. To the best of our knowledge, this is the first comprehensive survey in this area. We differ from previous surveys BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 in the following respects: 1) we discuss the categorizations of abuse based on coarse-grained vs. fine-grained taxonomies; 2) we present a detailed overview of datasets annotated for abuse; 3) we provide an extensive review of the existing abuse detection methods, including ones based on neural networks (omitted by previous surveys); 4) we discuss the key outstanding challenges in this area; and 5) we cover aspects of ethics and explainability."
            ],
            "predicted_section": [
                "Conclusions",
                "Introduction"
            ]
        },
        "22225ba18a6efe74b1315cc08405011d5431498e": {
            "question_text": "Do they use external financial knowledge in their approach?",
            "from_paper": "1705.00571",
            "gold": [
                "Domain specific terminology is expected to play a key part in this task, as reporters, investors and analysts in the financial domain will use a specific set of terminology when discussing financial performance. Potentially, this may also vary across different financial domains and industry sectors. Therefore, we took an exploratory approach and investigated how various features and learning algorithms perform differently, specifically SVR and BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper we are releasing our source code and the finance specific BLSTM word embedding model.",
                "The training data published by the organisers for this track was a set of headline sentences from financial news articles where each sentence was tagged with the company name (which we treat as the aspect) and the polarity of the sentence with respect to the company. There is the possibility that the same sentence occurs more than once if there is more than one company mentioned. The polarity was a real value between -1 (negative sentiment) and 1 (positive sentiment).",
                "We additionally trained a word2vec BIBREF10 word embedding model on a set of 189,206 financial articles containing 161,877,425 tokens, that were manually downloaded from Factiva. The articles stem from a range of sources including the Financial Times and relate to companies from the United States only. We trained the model on domain specific data as it has been shown many times that the financial domain can contain very different language."
            ],
            "gold_section": [
                "Introduction",
                "Data"
            ],
            "predicted": [
                "Domain specific terminology is expected to play a key part in this task, as reporters, investors and analysts in the financial domain will use a specific set of terminology when discussing financial performance. Potentially, this may also vary across different financial domains and industry sectors. Therefore, we took an exploratory approach and investigated how various features and learning algorithms perform differently, specifically SVR and BLSTMs. We found that BLSTMs outperform an SVR without having any knowledge of the company that the sentiment is with respect to. For replicability purposes, with this paper we are releasing our source code and the finance specific BLSTM word embedding model.",
                "There is a growing amount of research being carried out related to sentiment analysis within the financial domain. This work ranges from domain-specific lexicons BIBREF2 and lexicon creation BIBREF3 to stock market prediction models BIBREF4 , BIBREF5 . BIBREF4 used a multi layer neural network to predict the stock market and found that incorporating textual features from financial news can improve the accuracy of prediction. BIBREF5 showed the importance of tuning sentiment analysis to the task of stock market prediction. However, much of the previous work was based on numerical financial stock market data rather than on aspect level financial textual data. In aspect based sentiment analysis, there have been many different techniques used to predict the polarity of an aspect as shown in SemEval-2016 task 5 BIBREF1 . The winning system BIBREF6 used many different linguistic features and an ensemble model, and the runner up BIBREF7 used uni-grams, bi-grams and sentiment lexicons as features for a Support Vector Machine (SVM). Deep learning methods have also been applied to aspect polarity prediction. BIBREF8 created a hierarchical BLSTM with a sentence level BLSTM inputting into a review level BLSTM thus allowing them to take into account inter- and intra-sentence context. They used only word embeddings making their system less dependent on extensive feature engineering or manual feature creation. This system outperformed all others on certain languages on the SemEval-2016 task 5 dataset BIBREF1 and on other languages performed close to the best systems. BIBREF9 also created an LSTM based model using word embeddings but instead of a hierarchical model it was a one layered LSTM with attention which puts more emphasis on learning the sentiment of words specific to a given aspect.",
                "We are grateful to Nikolaos Tsileponis (University of Manchester) and Mahmoud El-Haj (Lancaster University) for access to headlines in the corpus of financial news articles collected from Factiva. This research was supported at Lancaster University by an EPSRC PhD studentship."
            ],
            "predicted_section": [
                "Related Work",
                "Acknowledgements",
                "Introduction"
            ]
        },
        "7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6": {
            "question_text": "Which data sources do they use?",
            "from_paper": "1602.03483",
            "gold": [
                "Unless stated above, all models were trained on the Toronto Books Corpus, which has the inter-sentential coherence required for SkipThought and FastSent. The corpus consists of 70m ordered sentences from over 7,000 books.",
                "We also measure how well representation spaces reflect human intuitions of the semantic sentence relatedness, by computing the cosine distance between vectors for the two sentences in each test pair, and correlating these distances with gold-standard human judgements. The SICK dataset BIBREF36 consists of 10,000 pairs of sentences and relatedness judgements. The STS 2014 dataset BIBREF37 consists of 3,750 pairs and ratings from six linguistic domains. Example ratings are shown in Table TABREF15 . All available pairs are used for testing apart from the 500 SICK `trial' pairs, which are held-out for tuning hyperparameters (representation size of log-linear models, and noise parameters in SDAE). The optimal settings on this task are then applied to both supervised and unsupervised evaluations."
            ],
            "gold_section": [
                "Unsupervised Evaluations",
                "Training and Model Selection"
            ],
            "predicted": [
                "The following models rely on (freely-available) data that has more structure than raw text.",
                "CaptionRep Using the same overall architecture, we trained (BOW and RNN) models to map captions in the COCO dataset BIBREF19 to pre-trained vector representations of images. The image representations were encoded by a deep convolutional network BIBREF20 trained on the ILSVRC 2014 object recognition task BIBREF21 . Multi-modal distributed representations can be encoded by feeding test sentences forward through the trained model.",
                "Non-Distributed Baseline We implement a TFIDF BOW model in which the representation of sentence INLINEFORM0 encodes the count in INLINEFORM1 of a set of feature-words weighted by their tfidf in INLINEFORM2 , the corpus. The feature-words are the 200,000 most common words in INLINEFORM3 ."
            ],
            "predicted_section": [
                "Existing Models Trained on Text",
                "Models Trained on Structured Resources"
            ]
        },
        "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55": {
            "question_text": "what four learning strategies are investigated?",
            "from_paper": "1908.11664",
            "gold": [
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@",
                "This is a simple but effective model for multi-domain learning, in which all domains are aggregated together and will be further used for training a set of shared parameters. Notably, domains in this model are not explicitly informed of their differences.",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@",
                "More recently, unsupervised pre-training has achieved massive success in NLP community BIBREF28, BIBREF29, which usually provides tremendous external knowledge. However, there are few works on building the connection between large-scale pre-trained models and multi-domain learning. In this model, we explore how the external knowledge unsupervised pre-trained models bring can contribute to multi-domain learning and new domain adaption .",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@",
                "The domain type can also be introduced directly as a feature vector, which can augment learned representations with domain-aware ability.",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@",
                "In order to overcome the above limitations, we try to bridge the communication gap between different domains when updating shared parameters via meta-learning BIBREF33, BIBREF34, BIBREF35."
            ],
            "gold_section": [
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{IV}_{Meta}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{I}_{Base}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@",
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{III}_{Tag}$@!END@"
            ],
            "predicted": [
                "Analysis: This model instructs the processing of multi-domain learning by utilizing external pre-trained knowledge. Another perspective is to address this problem algorithmically.",
                "In this paper, we explore publication in the context of the domain and investigate the domain shift problem in summarization. When verified its existence, we propose to build a multi-domain testbed for summarization that requires both training and measuring performance on a set of domains. Under these new settings, we propose four learning schemes to give a preliminary explore in characteristics of different learning strategies when dealing with multi-domain summarization tasks.",
                "More recently, unsupervised pre-training has achieved massive success in NLP community BIBREF28, BIBREF29, which usually provides tremendous external knowledge. However, there are few works on building the connection between large-scale pre-trained models and multi-domain learning. In this model, we explore how the external knowledge unsupervised pre-trained models bring can contribute to multi-domain learning and new domain adaption ."
            ],
            "predicted_section": [
                "Multi-domain Summarization ::: Four Learning Strategies ::: Model@!START@$^{II}_{BERT}$@!END@",
                "Conclusion"
            ]
        },
        "654306d26ca1d9e77f4cdbeb92b3802aa9961da1": {
            "question_text": "By how much did the new model outperform multilingual BERT?",
            "from_paper": "1912.07076",
            "gold": [
                "Table TABREF25 summarizes the results for POS tagging. We find that neither M-BERT model improves on the previous state of the art for any of the three resources, with results ranging 0.1-0.8% points below the best previously published results. By contrast, both language-specific models outperform the previous state of the art, with absolute improvements for FinBERT cased ranging between 0.4 and 1.7% points. While these improvements over the already very high reference results are modest in absolute terms, the relative reductions in errors are notable: in particular, the FinBERT cased error rate on FTB is less than half of the best CoNLL'18 result BIBREF22. We also note that the uncased models are surprisingly competitive with their cased equivalents for a task where capitalization has long been an important feature: for example, FinBERT uncased performance is within approx. 0.1% points of FinBERT cased for all corpora.",
                "Table TABREF41 shows LAS results for predicted and gold segmentation. While Udify initialized with M-BERT fails to outperform our strongest baseline BIBREF22, Udify initialized with FinBERT achieves notably higher performance on all three treebanks, establishing new state-of-the-art parsing results for Finnish with a large margin. Depending on the treebank, Udify with cased FinBERT LAS results are 2.3\u20133.6% points above the previous state of the art, decreasing errors by 24%\u201331% relatively.",
                "Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest."
            ],
            "gold_section": [
                "Evaluation ::: Dependency Parsing ::: Results",
                "Evaluation ::: Part of Speech Tagging ::: Results",
                "Evaluation ::: Text classification ::: Results"
            ],
            "predicted": [
                "Comparing M-BERT and FinBERT, we find that the language-specific models outperform the multilingual models across the full range of training data sizes for both datasets. For news, the four BERT variants have broadly similar learning curves, with the absolute advantage for FinBERT models ranging from 3% points for 1K examples to just over 1% point for 100K examples, and relative reductions in error from 20% to 13%. For online discussion, the differences are much more pronounced, with M-BERT models performing closer to the FastText baseline than to FinBERT. Here the language-specific BERT outperforms the multilingual by over 20% points for the smallest training data and maintains a 5% point absolute advantage even with 100,000 training examples, halving the error rate of the multilingual model for the smallest training set and maintaining an over 20% relative reduction for the largest.",
                "The BERT model of devlin2018bert has been particularly influential, establishing state-of-the-art results for English for a range of NLU tasks and NER when it was released. For most languages, the only currently available BERT model is the multilingual model (M-BERT) trained on pooled data from 104 languages. While M-BERT has been shown to have a remarkable ability to generalize across languages BIBREF3, several studies have also demonstrated that monolingual BERT models, where available, can notably outperform M-BERT. Such results include the evaluation of the recently released French BERT model BIBREF4, the preliminary results accompanying the release of a German BERT model, and the evaluation of ronnqvist-etal-2019-multilingual comparing M-BERT with English and German monolingual models.",
                "In this paper, we study the application of language-specific and multilingual BERT models to Finnish NLP. We introduce a new Finnish BERT model trained from scratch and perform a comprehensive evaluation comparing its performance to M-BERT on established datasets for POS tagging, NER, and dependency parsing as well as a range of diagnostic text classification tasks. The results show that 1) on most tasks the multilingual model does not represent an advance over previous state of the art, indicating that multilingual models may fail to deliver on the promise of deep transfer learning for lower-resourced languages, and 2) the custom Finnish BERT model systematically outperforms the multilingual as well as all previously proposed methods on all benchmark tasks, showing that language-specific deep transfer learning models can provide comparable advances to those reported for much higher-resourced languages."
            ],
            "predicted_section": [
                "Evaluation ::: Text classification ::: Results",
                "Introduction"
            ]
        },
        "5a7d1ae6796e09299522ebda7bfcfad312d6d128": {
            "question_text": "What previous proposed methods did they explore?",
            "from_paper": "1912.07076",
            "gold": [
                "The current transfer learning methods have evolved from word embedding techniques, such as word2vec BIBREF5, GLoVe BIBREF6 and fastText BIBREF7, to take into account the textual context of words. Crucially, incorporating the context avoids the obvious limitations stemming from the one-vector-per-unique-word assumption inherent to the previous word embedding methods. The current successful wave of work proposing and applying different contextualized word embeddings was launched with ELMo BIBREF0, a context embedding method based on bidirectional LSTM networks. Another notable example is the ULMFit model BIBREF8, which specifically focuses on techniques for domain adaptation of LSTM-based language models. Following the introduction of the attention-based (as opposed to recurrent) Transformer architecture BIBREF9, BERT was proposed by BIBREF2, demonstrating superior performance on a broad array of tasks. The BERT model has been further refined in a number of follow-up studies BIBREF10, BIBREF11 and, presently, BERT and related models form the de facto standard approach to embedding text segments as well as individual words in context."
            ],
            "gold_section": [
                "Related Work"
            ],
            "predicted": [
                "We implement the text classification methods following devlin2018bert, minimizing task-specific architecture and simply attaching a dense output layer to the initial ([CLS]) token of the top layer of BERT. We establish baseline text classification performance using fastText BIBREF7. We evaluated a range of parameter combinations and different pretrained word vectors for the method using the development data, selecting character n-gram features of lengths 3\u20137, training for 25 epochs, and initialization with subword-enriched embeddings induced from Wikipedia texts BIBREF45 for the final experiments.",
                "The FinBERT models and all of the tools and resources introduced in this paper are available under open licenses from https://turkunlp.org/finbert.",
                "Unless stated otherwise, all experiments follow the basic setup used in the experiments of devlin2018bert, selecting the learning rate, batch size and the number of epochs used for fine-tuning separately for each model and dataset combination using a grid search with evaluation on the development data. Other model and optimizer parameters were kept at the BERT defaults. Excepting for the parsing experiments, we repeat each experiment 5-10 times and report result mean and standard deviation."
            ],
            "predicted_section": [
                "Evaluation ::: Text classification ::: Methods",
                "Evaluation",
                "Conclusions"
            ]
        },
        "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8": {
            "question_text": "What is the state-of-the-art approach?",
            "from_paper": "1911.11161",
            "gold": [
                "We first compare the performance of our approach with the baseline results obtained from Rashkin et al. BIBREF3 that uses a full transformer architecture BIBREF26, consisting of an encoder and decoder. Table TABREF9 provides a comparison of our approach with to the baseline approach. In Table TABREF9, we refer our \u201cOur Model Fine-Tuned\u201d as the baseline fine-tuned GPT-2 model trained on the dialogue and \u201cOur-model Emo-prepend\u201d as the GPT-2 model that is fine-tuned on the dialogues but also conditioned on the emotion displayed in the conversation. We find that fine-tuning the GPT-2 language model using a transfer learning approach helps us achieve a lower perplexity and a higher BLEU scores. The results from our approach are consistent with the empirical study conducted by Edunov et al BIBREF27 that demonstrate the effectiveness of the using pre-trained model diminishes when added to the decoder network in an seq2seq approach. We also perform a comparison between our two models on the metrics of length, diversity, readability and coherence. We find that our baseline model produces less diverse responses compared to when the model is conditioned on emotion. We find that the our emo-prepend model also higher a slightly higher readability score that our baseline model."
            ],
            "gold_section": [
                "Results ::: Automated Metrics"
            ],
            "predicted": [
                "Our work advances the field of conversational agents by applying the transfer learning approach towards generating emotionally relevant responses that is grounded on emotion and situational context. We find that our fine-tuning based approach outperforms the current state of the art approach on the automated metrics of the BLEU and perplexity. We also show that transfer learning approach helps produce well crafted responses on smaller dialogue corpus.",
                "Evaluating the quality of responses in open domain situations where the goal is not defined is an important area of research. Researchers have used methods such as BLEU , METEOR BIBREF17, ROUGE BIBREF18 from machine translation and text summarization BIBREF19 tasks. BLEU and METEOR are based on word overlap between the proposed and ground truth responses; they do not adequately account for the diversity of responses that are possible for a given input utterance and show little to no correlation with human judgments BIBREF19. We report on the BLEU BIBREF20 and Perplexity (PPL) metric to provide a comparison with the current state-of-the-art methods. We also report our performance using other metrics such as length of responses produced by the model. Following, Mei et al BIBREF21, we also report the diversity metric that helps us measure the ability of the model to promote diversity in responses BIBREF22. Diversity is calculated as the as the number of distinct unigrams in the generation scaled by the total number of generated tokens BIBREF21, BIBREF1. We report on two additional automated metrics of readability and coherence. Readability quantifies the linguistic quality of text and the difficulty of the reader in understanding the text BIBREF23. We measure readability through the Flesch Reading Ease (FRE) BIBREF24 which computes the number of words, syllables and sentences in the text. Higher readability scores indicate that utterance is easier to read and comprehend. Similarly, coherence measures the ability of the dialogue system to produce responses consistent with the topic of conversation. To calculate coherence, we use the method proposed by Dziri et al. BIBREF25.",
                "The area of dialogue systems has been studied extensively in both open-domain BIBREF28 and goal-oriented BIBREF29 situations. Extant approaches towards building dialogue systems has been done predominantly through the seq2seq framework BIBREF0. However, prior research has shown that these systems are prone to producing dull and generic responses that causes engagement with the human to be affected BIBREF0, BIBREF2. Researchers have tackled this problem of dull and generic responses through different optimization function such as MMI BIBREF30 and through reinforcement learning approachesBIBREF31. Alternative approaches towards generating more engaging responses is by grounding them in personality of the speakers that enables in creating more personalized and consistent responses BIBREF1, BIBREF32, BIBREF13."
            ],
            "predicted_section": [
                "Related Work",
                "Experiments ::: Metrics",
                "Introduction"
            ]
        },
        "ca595151735444b5b30a003ee7f3a7eb36917208": {
            "question_text": "What type of features are extracted with this language?",
            "from_paper": "2002.03056",
            "gold": [
                "NLP Feature Specification Language ::: Feature Types ::: Linguistic Features",
                "Figure FIGREF8 depicts two levels of taxonomy for features considered as linguistic.",
                "NLP Feature Specification Language ::: Feature Types ::: Semantic Similarity and Relatedness based Features",
                "Semantic similarity can be estimated between words, between phrases, between sentences, and between documents in a corpus. Estimation could either be based upon corpus text alone by applying approaches like vector space modeling BIBREF16, latent semantic analysis BIBREF17, topic modeling BIBREF18, or neural embeddings (e.g., Word2Vec BIBREF19 or Glove BIBREF20) and their extensions to phrase, sentence, and document levels. Otherwise it can be estimated based upon ontological relationships (e.g., WordNet based BIBREF21) among concept terms appearing in the corpus.",
                "NLP Feature Specification Language ::: Feature Types ::: Statistical Features",
                "Figure FIGREF13 depicts different types of statistical features which can be extracted for individual documents or corpus of documents together with methods to extract these features at different levels."
            ],
            "gold_section": [
                "NLP Feature Specification Language ::: Feature Types ::: Linguistic Features",
                "NLP Feature Specification Language ::: Feature Types ::: Statistical Features",
                "NLP Feature Specification Language ::: Feature Types ::: Semantic Similarity and Relatedness based Features"
            ],
            "predicted": [
                "To illustrate, let us consider context based features: Table FIGREF9 gives various options which need to be specified for directing how context for an SU should be extracted. For example, Context_Window := [2, Sentence] will extract all tokens within current sentence, which are present within a distance of 2 on both sides from the current SU. However, Context_Window := [2, Sentence]; POSContext := NN$\\mid $VB will extract only those tokens within current sentence, which are present within a distance of 2 on both sides from the current SU and have POS tag either NN (noun singular) or VB (verb, base form).",
                "Analysis Unit (AU) specifies level at which features have to be extracted. At Corpus level, features are extracted for all the text documents together. At Document level, features are extracted for each document in corpus separately. At Para (paragraph) level Features are extracted for multiple sentences constituting paragraphs together. At Sentence level features to be extracted for each sentence. Figure FIGREF6 depicts classes of features considered in nlpFSpL and their association with different AUs.",
                "Figure FIGREF13 depicts different types of statistical features which can be extracted for individual documents or corpus of documents together with methods to extract these features at different levels."
            ],
            "predicted_section": [
                "NLP Feature Specification Language ::: Meta Elements",
                "NLP Feature Specification Language ::: Feature Types ::: Linguistic Features",
                "NLP Feature Specification Language ::: Feature Types ::: Statistical Features"
            ]
        },
        "1f053f338df6d238cb163af1a0b1b073e749ed8a": {
            "question_text": "Do they evaluate their parallel sentence generation?",
            "from_paper": "1806.09652",
            "gold": [
                "For the evaluation of the performance of our sentence extraction models, we looked at a few sentences manually, and have done a qualitative analysis, as there was no gold standard evaluation set for sentences extracted from Wikipedia. In Table TABREF13 , we can see the qualitative accuracy for some parallel sentences extracted from Tamil. The sentences extracted from Tamil, have been translated to English using Google Translate, so as to facilitate a comparison with the sentences extracted from English.",
                "We evaluated the quality of the extracted parallel sentence pairs, by performing machine translation experiments on the augmented parallel corpus."
            ],
            "gold_section": [
                "Evaluation Metrics",
                "Machine Translation"
            ],
            "predicted": [
                "In this section, we will describe the entire pipeline, depicted in Figure FIGREF5 , which is involved in training a parallel sentence extraction system, and also to infer and decode high-precision nearly-parallel sentence-pairs from bilingual article pages collected from Wikipedia.",
                "In this paper, we will propose a neural approach to parallel sentence extraction and compare the BLEU scores of machine translation systems with and without the use of the extracted sentence pairs to justify the effectiveness of this method. Compared to previous approaches which require specialized meta-data from document structure or significant amount of hand-engineered features, the neural model for extracting parallel sentences is learned end-to-end using only a small bootstrap set of parallel sentence pairs.",
                "This method is extremely beneficial for translating language pairs with very little parallel corpora. These parallel sentences facilitate significant improvement in machine translation quality when compared to a generic system as has been shown in our results."
            ],
            "predicted_section": [
                "Approach",
                "Introduction",
                "Conclusion"
            ]
        },
        "0dfe43985dea45d93ae2504cccca15ae1e207ccf": {
            "question_text": "What methods are used to build two other Viatnamese datsets?",
            "from_paper": "2002.00175",
            "gold": [
                "We conduct our experiments and do comparisons through three datasets with the same size and images of sportball category: Two Vietnamese datasets generated by two methods (translated by Google Translation service and annotated by human) and the original MS-COCO English dataset. The three sets are distributed into three subsets: 2,695 images for the training set, 924 images for validation set and 231 images for test set."
            ],
            "gold_section": [
                "Experiments ::: Experiment Settings ::: Dataset preparation"
            ],
            "predicted": [
                "Our human resources for dataset construction involve five writers, whose ages are from 22-25. Being native Vietnamese residents, they are fluent in Vietnamese. All five UIT-ViIC creators first research and are trained about sports knowledge as well as the specialized vocabulary before starting to work.",
                "In this section, we describes procedures of building our sportball Vietnamese dataset, called UIT-ViIC.",
                "Familiar English words such as laptop, TV, tennis, etc. are allowed."
            ],
            "predicted_section": [
                "Dataset Creation ::: Annotation Process"
            ]
        },
        "b0e894536857cb249bd75188c3ca5a04e49ff0b6": {
            "question_text": "How do attention, recurrent and convolutional networks differ on the language classes they accept?",
            "from_paper": "1906.01615",
            "gold": [],
            "gold_section": [],
            "predicted": [
                "Attention is a popular enhancement to sequence-to-sequence (seq2seq) neural networks BIBREF9 , BIBREF10 , BIBREF11 . Attention allows a network to recall specific encoder states while trying to produce output. In the context of machine translation, this mechanism models the alignment between words in the source and target languages. More recent work has found that \u201cattention is all you need\u201d BIBREF12 , BIBREF13 . In other words, networks with only attention and no recurrent connections perform at the state of the art on many tasks.",
                "Now, we analyze the effect of adding attention to an acceptor network. Because we are concerned with language acceptance instead of transduction, we consider a simplified seq2seq attention model where the output sequence has length 1:",
                "This paper follows BIBREF1 by analyzing the expressiveness of neural network acceptors under asymptotic conditions. We formalize asymptotic language acceptance, as well as an associated notion of network memory. We use this theory to derive computation upper bounds and automata-theoretic characterizations for several different kinds of recurrent neural networks section:rnns, as well as other architectural variants like attention section:attention and convolutional networks (CNNs) section:cnns. This leads to a fairly complete automata-theoretic characterization of sequential neural networks."
            ],
            "predicted_section": [
                "Attention",
                "Introduction"
            ]
        },
        "94c22f72665dfac3e6e72e40f2ffbc8c99bf849c": {
            "question_text": "What type of languages do they test LSTMs on?",
            "from_paper": "1906.01615",
            "gold": [
                "BIBREF1 show how the LSTM can simulate a simplified variant of the counter machine. Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.",
                "Another important formal language task for assessing network memory is string reversal. Reversing requires remembering a INLINEFORM0 prefix of characters, which implies INLINEFORM1 state complexity.",
                "We frame reversing as a seq2seq transduction task, and compare the performance of an LSTM encoder-decoder architecture to the same architecture augmented with attention. We also report the results of BIBREF22 for a stack neural network (StackNN), another architecture with INLINEFORM0 state complexity (thm:stackstatecomplexity).",
                "Counting",
                "The goal of this experiment is to evaluate which architectures have memory beyond finite state. We train a language model on INLINEFORM0 with INLINEFORM1 and test it on longer strings INLINEFORM2 . Predicting the INLINEFORM3 character correctly while maintaining good overall accuracy requires INLINEFORM4 states. The results reported in fig:countingresults demonstrate that all recurrent models, with only two hidden units, find a solution to this task that generalizes at least over this range of string lengths.",
                "Counting with Noise",
                "In order to abstract away from asymptotically unstable representations, our next experiment investigates how adding noise to an RNN's activations impacts its ability to count. For the SRN and GRU, noise is added to INLINEFORM0 before computing INLINEFORM1 , and for the LSTM, noise is added to INLINEFORM2 . In either case, the noise is sampled from the distribution INLINEFORM3 .",
                "Reversing"
            ],
            "gold_section": [
                "Reversing",
                "Counting with Noise",
                "Counting"
            ],
            "predicted": [
                " BIBREF1 show how the LSTM can simulate a simplified variant of the counter machine. Combining these results, we see that the asymptotic expressiveness of the LSTM falls somewhere between the general and simplified counter languages. This suggests counting is a good way to understand the behavior of LSTMs.",
                "The construction in thm:lstmmemorybound produces a counter machine whose counter and state update functions are linearly separable. Thus, we have an upper bound on the expressive power of the LSTM:",
                "thm:lstmupperbound constitutes a very tight upper bound on the expressiveness of LSTM computation. Asymptotically, LSTMs are not powerful enough to model even the deterministic context-free language INLINEFORM0 ."
            ],
            "predicted_section": [
                "Long Short-Term Memory Networks"
            ]
        },
        "ce8d8de78a21a3ba280b658ac898f73d0b52bf1b": {
            "question_text": "What is possible future improvement for proposed method/s?",
            "from_paper": "1910.10487",
            "gold": [
                "In future work, the memory module could be applied to other domains such as summary generation. While memory modules are able to capture neural vectors of information, they may not easily capture specific words for later use. A possible future approach might combine memory module architectures with pointer softmax networks BIBREF16 to allow memory models to store information about which words from previous utterances of the conversation to use in future responses."
            ],
            "gold_section": [
                "Conclusion"
            ],
            "predicted": [
                "In recent years, there have been proposals to use memory neural networks to capture long-term information. A memory module is defined as an external component of the neural network system, and it is theoretically unlimited in capacity. weston2014memory propose a sequence prediction method using a memory with content-based addressing. In their implementation for the bAbI task BIBREF9 for example, their model encodes and sequentially saves words from text in memory slots. When a question about the text is asked, the model uses content-based addressing to retrieve memories relevant to the question, in order to generate answers. They use the k-best memory slots, where k is a relative small number (1 or 2 in their paper). sukhbaatar2015end propose an end-to-end neural network model, which uses content-based addressing to access multiple memory layers. This model has been implemented in a relatively simple goal-oriented dialogue system (restaurant booking) and has decent performance BIBREF10.",
                "Other NTM variants have also been proposed recently. DBLP:journals/corr/ZhangYZ15 propose structured memory architectures for NTMs, and argue they could alleviate overfitting and increase predictive accuracy. DBLP:journals/nature/GravesWRHDGCGRA16 propose a memory access mechanism on top of NTM, which they call the Differentiable Neural Computer (DNC). DNC can store the transitions between memory locations it accesses, and thus can model some structured data. DBLP:journals/corr/GulcehreCCB16 proposed a Dynamic Neural Turing Machine (D-NTM) model, which allows more addressing mechanisms, such as multi-step addressing. DBLP:journals/corr/GulcehreCB17 further simplified the algorithm, so a single trainable matrix is used to get locations for read and write. Both models separate the address section from the content section of memory.",
                "While coherence and diversity remain the primary focus of model dialogue architectures, many have tried to incorporate additional capabilities. zhou2017mojitalk introduce emotion into generated utterances by creating a large-scale fine-grained emotion dialogue dataset that uses tagged emojis to classify utterance sentiment. Then they train a conditional variational autoencoder (CVAE) to generate responses given an input emotion. Along this line of research, li2016persona use Reddit users as a source of persona, and learn individual persona embeddings per user. The system then conditions on these embeddings to generate a response while maintaining coherence specific to the given user. pandey2018exemplar expand the context of an existing dialogue model by extracting input responses from the training set that are most similar to the current input. These \"exemplar\" responses are then conditioned on to use as reference for final response generation. In another attempt to add context, young2018augmenting utilize a relational database to extract specific entity relations that are relevant for the current input. These relations provide more context for the dialogue model and allows it to respond to the user with information it did not observe in the training set."
            ],
            "predicted_section": [
                "Recent Work"
            ]
        },
        "2b32cf05c5e736f764ceecc08477e20ab9f2f5d7": {
            "question_text": "Does the paper report F1-scores with and without post-processing for the second task?",
            "from_paper": "1908.06493",
            "gold": [
                "Many classifiers can predict a score or confidence about the prediction. Turning this score into the prediction is usually performed by setting a threshold, such as 0 and 0.5, so labels which have a score assigned greater than that are assigned to the sample. This might be not the optimal threshold in the multi-label classification setup and there are many approaches to set it (BIBREF9). Although these methods concentrate in the sample or label, we have had good results with a much more general approach.",
                "As described in BIBREF1, Read and Pfahringer BIBREF10 introduce a method (referred hereinafter to as LCA) to estimate the threshold globally. Their method chooses the threshold that minimizes the difference between the label cardinality of the training set and the predicted set.",
                "For the HMC subtask B, we used a simple threshold based on the results obtained for LCA. Especially, using multiple models per node could cause a different scaling.",
                "Table TABREF28 shows the comparison of the different examined approaches in subtask B in the preliminary phase. Both implementations, Hsklearn and our own produced very similar results, so for the sake of reproducibility, we chose to continue with Hsklearn. We can see here, in contrary to the subtask A, that -0.25 achieved for one configuration better results, indicating that -0.2 could be overfitted on subtask A and a value diverging from that could also perform better. The extended approach means that an extra feature extraction module was added (having 3 instead of only 2) with n-gram 1-2 and stopwords removal. The LCA approach yielded here a worse score in the normalized but almost comparable in the non-normalized. However, the simple threshold approach performed better and therefore more promising."
            ],
            "gold_section": [
                "Experiments ::: Preliminary Experiments on Development Set",
                "Data and Methodology ::: System Definition ::: Post-processing: Threshold"
            ],
            "predicted": [
                "In Table TABREF30, the best results by team regarding micro F-1 are shown. Our approach reached second place. The difference between the first four places were mostly 0.005 between each, showing that only a minimal change could lead to a place switching. Also depicted are not null improvements results, i.e. in a following post-processing, starting from the predictions, the highest score label is predicted for each sample, even though the score was too low. It is worth-noting that the all but our approaches had much higher precision compared to the achieved recall.",
                "The threshold set to -0.25 shown also to produce better results with micro F-1, in contrast to the simple average between recall and precision. This can be seen also by checking the average value between recall and precision, by checking the sum, our approach produced 0.7072+0.6487 = 1.3559 whereas the second team had 0.7377+0.6174 = 1.3551, so the harmonic mean gave us a more comfortable winning marge.",
                "In Fig. FIGREF26, a graph showing the dependency between the threshold set and the micro F-1 score achieved in the development set is depicted. The curve fitted was $a*x^2+b*x+c$ which has the maximum at approx. -0.2. We chose -0.25 in the expectation that the test set would not be exactly as the development set and based on our previous experience with other multi-label datasets (such as the RCv1-v2) which have an optimal threshold at -0.3. Also as we will see, the results proved us right achieving the best recall, yet not surpassing the precision score. This is a crucial aspect of the F-1 measure, as it is the harmonic mean it will push stronger and not linearly the result towards the lower end, so if decreasing the threshold, increases the recall linearly and decreases also the precision linearly, balancing both will consequently yield a better F-1 score."
            ],
            "predicted_section": [
                "Experiments ::: Subtask A",
                "Experiments ::: Preliminary Experiments on Development Set",
                "Experiments ::: Subtask B"
            ]
        },
        "fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b": {
            "question_text": "What is the difference in size compare to the previous model?",
            "from_paper": "1805.09821",
            "gold": [
                "Split the data into train, development and test corpus: for each languages, we provide training data of different sizes (1k, 2k, 5k and 10k stories), a development (1k) and a test corpus (4k);",
                "Most works in the literature use only 1 000 examples to train the document classifier. To invest the impact of more training data, we also provide training corpora of 2 000, 5 000 and 10 000 documents. The development corpus for each language is composed of 1 000 and the test set of 4 000 documents respectively. All have uniform class distributions. An important aspect of this work is to provide a framework to study and evaluate cross-lingual document classification for many language pairs. In that spirit, we will name this corpus \u201cMultilingual Document Classification Corpus\u201d, abbreviated as MLDoc. The full Reuters Corpus Volume 2 has a special license and we can not distribute it ourselves. Instead, we provide tools to extract all the subsets of MLDoc at https://github.com/facebookresearch/MLDoc.",
                "We have defined a new evaluation framework for cross-lingual document classification in eight languages. This corpus largely extends previous corpora which were also based on the Reuters Corpus Volume 2, but mainly considered the transfer between English and German. We also provide detailed baseline results using two competitive approaches (multilingual word and sentence embeddings, respectively), for cross-lingual document classification between all eight languages. This new evaluation framework is freely available at https://github.com/facebookresearch/MLDoc."
            ],
            "gold_section": [
                "Conclusion",
                "Multilingual document classification"
            ],
            "predicted": [
                "In this section, we provide comparative results on our new Multilingual Document Classification Corpus. Since the initial work by BIBREF0 many alternative approaches to cross-lingual document classification have been developed. We will encourage the respective authors to evaluate their systems on MLDoc. We believe that a large variety of transfer language pairs will give valuable insights on the performance of the various approaches.",
                "We have defined a new evaluation framework for cross-lingual document classification in eight languages. This corpus largely extends previous corpora which were also based on the Reuters Corpus Volume 2, but mainly considered the transfer between English and German. We also provide detailed baseline results using two competitive approaches (multilingual word and sentence embeddings, respectively), for cross-lingual document classification between all eight languages. This new evaluation framework is freely available at https://github.com/facebookresearch/MLDoc.",
                "A subset of the English and German sections of RCV2 was defined by BIBREF0 to evaluate cross-lingual document classification. This subset was used in several follow-up works and many comparative results are available for the transfer between German and English. BIBREF1 extended the use of RCV2 for cross-lingual document classification to the French and Spanish language (transfer from and to English). An analysis of these evaluation corpora has shown that the class prior distributions vary significantly between the classes (see Table 2 ). For German and English, more than 80% of the examples in the test set belong to the classes GCAT and MCAT and at most 2% to the class CCAT. These class prior distributions are very different for French and Spanish: the class CCAT is quite frequent with 21% and 15% of the French and Spanish test set respectively. One may of course argue that variability in the class prior distribution is typical for real-world problems, but this shifts the focus from a high quality cross-lingual transfer to \u201ctricks\u201d for how to best handle the class imbalance. Indeed, in previous research the transfer between English and German achieves accuracies higher than 90%, while the performance is below 80% for EN/FR or even 70% EN/ES. We have seen experimental evidence that these important differences are likely to be caused by the discrepancy in the class priors of the test sets."
            ],
            "predicted_section": [
                "Baseline results",
                "Cross-lingual document classification",
                "Conclusion"
            ]
        },
        "c035a011b737b0a10deeafc3abe6a282b389d48b": {
            "question_text": "What are the components of the classifier?",
            "from_paper": "1707.07212",
            "gold": [
                "We model the conditional distribution over a tweet's veridicality toward a candidate INLINEFORM0 winning a contest against a set of opponents, INLINEFORM1 , using a log-linear model: INLINEFORM2",
                "where INLINEFORM0 is the veridicality (positive, negative or neutral).",
                "To extract features INLINEFORM0 , we first preprocessed tweets retrieved for a specific event to identify named entities, using BIBREF20 's Twitter NER system. Candidate ( INLINEFORM1 ) and opponent entities were identified in the tweet as follows:",
                "We use five feature templates: context words, distance between entities, presence of punctuation, dependency paths, and negated keyword."
            ],
            "gold_section": [
                "Features",
                "Veridicality Classifier"
            ],
            "predicted": [
                "- target ( INLINEFORM0 ). A target is a named entity that matches a contender name from our queries.",
                "To extract features INLINEFORM0 , we first preprocessed tweets retrieved for a specific event to identify named entities, using BIBREF20 's Twitter NER system. Candidate ( INLINEFORM1 ) and opponent entities were identified in the tweet as follows:",
                "- entity ( INLINEFORM0 ): Any named entity which does not match the list of contenders. Figure FIGREF25 illustrates the named entity labeling for a tweet obtained from the query \u201cOscars Leonardo DiCaprio win since:2016-2-22 until:2016-2-28\". Leonardo DiCaprio is the target, while the named entity tag for Bryan Cranston, one of the losers for the Oscars, is re-tagged as opponent. These tags provide information about the position of named entities relative to each other, which is used in the features."
            ],
            "predicted_section": [
                "Veridicality Classifier"
            ]
        },
        "415014a5bcd83df52c9307ad16fab1f03d80f705": {
            "question_text": "What syntactic and semantic features are proposed?",
            "from_paper": "1605.05156",
            "gold": [
                "Semantic Features",
                "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.",
                "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.",
                "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.",
                "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.",
                "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question.",
                "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.",
                "Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.",
                "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.",
                "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "gold_section": [
                "Syntactic Features",
                "Semantic Features"
            ],
            "predicted": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.",
                "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question."
            ],
            "predicted_section": [
                "Syntactic Features",
                "Semantic Features",
                "Features"
            ]
        },
        "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c": {
            "question_text": "what are the proposed semantic features?",
            "from_paper": "1605.05156",
            "gold": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.",
                "Vulgar Words: Similar to opinion words, vulgar words can either signal great emotions or an informality mostly seen in expressions than any other kind of speech act (least seen in assertions). We used an online collection of vulgar words to collect a total of 349 vulgar words. A binary feature indicates the appearance or lack thereof of any of these words.",
                "Emoticons: Emoticons have become ubiquitous in online communication and so cannot be ignored. Like vulgar words, emoticons can also signal emotions or informality. We used an online collection of text-based emoticons to collect a total of 362 emoticons. A binary feature indicates the appearance or lack thereof of any of these emoticons.",
                "Speech Act Verbs: There are certain verbs (such as ask, demand, promise, report, etc) that typically signal certain speech acts. Wierzbicka BIBREF9 has compiled a total of 229 English speech act verbs divided into 37 groups. Since this is a collection of verbs, it is crucially important to only consider the verbs in a tweet and not any other word class (since some of these words can appear in multiple part-of-speech categories). In order to do this, we used Owoputi et al.'s BIBREF10 Twitter part-of-speech tagger to identify all the verbs in a tweet, which were then stemmed using Porter Stemming BIBREF11 . The stemmed verbs were then compared to the 229 speech act verbs (which were also stemmed using Porter Stemming). Thus, we have 229 binary features coding the appearance or lack thereof of each of these verbs.",
                "N-grams: In addition to the verbs mentioned, there are certain phrases and non-verb words that can signal certain speech acts. For example, the phrase \"I think\" signals an expression, the phrase \"could you please\" signals a request and the phrase \"is it true\" signals a question. Similarly, the non-verb word \"should\" can signal a recommendation and \"why\" can signal a question."
            ],
            "gold_section": [
                "Semantic Features",
                "Features"
            ],
            "predicted": [
                "Opinion Words: We used the \"Harvard General Inquirer\" lexicon BIBREF8 , which is a dataset used commonly in sentiment classification tasks, to identify 2442 strong, negative and positive opinion words (such as robust, terrible, untrustworthy, etc). The intuition here is that these opinion words tend to signal certain speech acts such as expressions and recommendations. One binary feature indicates whether any of these words appear in a tweet.",
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "These words and phrases are called n-grams (an n-gram is a contiguous sequence of n words). Given the relatively short sentences on Twitter, we decided to only consider unigram, bigram and trigram phrases. We generated a list of all of the unigrams, bigrams and trigrams that appear at least five times in our tweets for a total of 6,738 n-grams. From that list we selected a total of 1,415 n-grams that were most predictive of the speech act of their corresponding tweets but did not contain topic-specific terms (such as Boston, Red Sox, etc). There is a binary feature for each of these sub-trees indicating their appearance."
            ],
            "predicted_section": [
                "Semantic Features",
                "Features"
            ]
        },
        "95bbd91badbfe979899cca6655afc945ea8a6926": {
            "question_text": "what syntactic features are proposed?",
            "from_paper": "1605.05156",
            "gold": [
                "We studied many features before settling on the features below. Our features can be divided into two general categories: Semantic and Syntactic. Some of these features were motivated by various works on speech act classification, while others are novel features. Overall we selected 3313 binary features, composed of 1647 semantic and 1666 syntactic features.",
                "Syntactic Features",
                "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.",
                "Twitter-specific Characters: There are certain Twitter-specific characters that can signal speech acts. These characters are #, @, and RT.The position of these characters is also important to consider since Twitter-specific characters used in the initial position of a tweet is more predictive than in other positions. Therefore, we have three additional binary features indicating whether these symbols appear in the initial position.",
                "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations.",
                "Dependency Sub-trees: Much can be gained from the inclusion of sophisticated syntactic features such as dependency sub-trees in our speech act classifier. We used Kong et al.'s BIBREF13 Twitter dependency parser for English (called the TweeboParser) to generate dependency trees for our tweets. Dependency trees capture the relationship between words in a sentence. Each node in a dependency tree is a word with edges between words capturing the relationship between the words (a word either modifies or is modified by other words). In contrast to other syntactic trees such as constituency trees, there is a one-to-one correspondence between words in a sentence and the nodes in the tree (so there are only as many nodes as there are words). Figure FIGREF8 shows the dependency tree of an example tweet.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech."
            ],
            "gold_section": [
                "Syntactic Features",
                "Features"
            ],
            "predicted": [
                "Punctuations: Certain punctuations can be predictive of the speech act in a tweet. Specifically, the punctuation ? can signal a question or request while ! can signal an expression or recommendation. We have two binary features indicating the appearance or lack thereof of these symbols.",
                "Part-of-speech: Finally, we used the part-of-speech tags generated by the dependency tree parser to identify the use of adjectives and interjections (such as yikes, dang, etc). Interjections are mostly used to convey emotion and thus can signal expressions. Similarly adjectives can signal expressions or recommendations. We have two binary features indicating the usage of these two parts-of-speech.",
                "Abbreviations: Abbreviations are seen with great frequency in online communication. The use of abbreviations (such as b4 for before, jk for just kidding and irl for in real life) can signal informal speech which in turn can signal certain speech acts such as expression. We collected 944 such abbreviations from an online dictionary and Crystal's book on language used on the internet BIBREF12 . We have a binary future indicating the presence of any of the 944 abbreviations."
            ],
            "predicted_section": [
                "Syntactic Features"
            ]
        },
        "2bd702174e915d97884d1571539fb1b5b0b7123a": {
            "question_text": "what pretrained word embeddings are used?",
            "from_paper": "1802.02614",
            "gold": [
                "Word embedding matrix was initialized with pre-trained 300-dimensional GloVe vectors BIBREF28 . For character-level embedding, we used one hot encoding with 69 characters (68 ASCII characters plus one unknown character). Both word embedding and character embedding matrix were fixed during the training. After algorithm SECREF12 was applied, the remaining out-of-vocabulary words were initialized as zero vectors. We used Stanford PTBTokenizer BIBREF32 on the Ubuntu corpus. The same hyper-parameter settings are applied to both Ubuntu Dialogue and Douban conversation corpus. For the ensemble model, we use the average prediction output of models with different runs. On both corpuses, the dimension of word2vec vectors generated on the training set is 100.",
                "On Douban conversation corpus, FastText BIBREF7 pre-trained Chinese embedding vectors are used in ESIM + enhanced word vector whereas word2vec generated on training set is used in baseline model (ESIM). It can be seen from table TABREF23 that character embedding enhances the performance of original ESIM. Enhanced Word representation in algorithm SECREF12 improves the performance further and has shown that the proposed method is effective. Most models (RNN, CNN, LSTM, BiLSTM, Dual-Encoder) which encode the whole context (or response) into compact vectors before matching do not perform well. INLINEFORM0 directly models sequential structure of multi utterances in context and achieves good performance whereas ESIM implicitly makes use of end-of-utterance(__eou__) and end-of-turn (__eot__) token tags as shown in subsection SECREF41 ."
            ],
            "gold_section": [
                "Implementation details",
                "Overall Results"
            ],
            "predicted": [
                "Word embedding were initialized by GloVe vectors and then updated during the training.",
                "Used the fixed pre-built FastText vectors where word vectors for out-of-vocabulary words were computed based on built model.",
                "Used the fixed pre-trained GloVe vectors ."
            ],
            "predicted_section": [
                "Evaluation of several word embedding representations"
            ]
        },
        "e14e3e0944ec3290d1985e9a3da82a7df17575cd": {
            "question_text": "Which dataset do they evaluate on?",
            "from_paper": "1806.07042",
            "gold": [
                "Our experiments are conducted on a large scale Chinese conversation corpus comprised of 20 million context-response pairs. We compare our model with generative models and retrieval models in terms of fluency, relevance, diversity and originality. The experiments show that our method outperforms traditional generative models on relevance, diversity and originality. We further find that the revised response achieves better relevance compared to its prototype and other retrieval results, demonstrating that the editing process does not only promote response originality but also improve the relevance of retrieval results."
            ],
            "gold_section": [
                "Introduction"
            ],
            "predicted": [
                "In terms of ensemble models and our editing model, the validation set and the test set are the same with datasets prepared for retrieval and generation models. Besides, for each context in the validation and test sets, we select its prototypes with the method described in Section \u201cPrototype Selector\". We follow Song et al. song2016two to construct a training data set for ensemble models, and construct a training data set with the method described in Section \u201cPrototype Selector\" for our editing models. We can obtain 42,690,275 INLINEFORM0 quadruples with the proposed data preparing method. For a fair comparison, we randomly sample 19,623,374 instances for the training of our method and the ensemble method respectively. To facilitate further research, related resources of the paper can be found at https://github.com/MarkWuNLP/ResponseEdit.",
                "Regarding human side-by-side evaluation, we can find that Edit-Default and Edit-N-rerank are slightly better than Retrieval-default and Retrieval-rerank (The winning examples are more than the losing examples), indicating that the post-editing is able to improve the response quality. Ed-Default is worse than Ens-Default, but Ed-N-Rerank is better than Ens-Rerank. This is mainly because the editing model regards the prototype response as the source language, so it is highly depends on the quality of prototype response.",
                "We evaluate our model on four criteria: fluency, relevance, diversity and originality. We employ Embedding Average (Average), Embedding Extrema (Extrema), and Embedding Greedy (Greedy) BIBREF35 to evaluate response relevance, which are better correlated with human judgment than BLEU. Following BIBREF10 , we evaluate the response diversity based on the ratios of distinct unigrams and bigrams in generated responses, denoted as Distinct-1 and Distinct-2. In this paper, we define a new metric, originality, that is defined as the ratio of generated responses that do not appear in the training set. Here, \u201cappear\" means we can find exactly the same response in our training data set. We randomly select 1,000 contexts from the test set, and ask three native speakers to annotate response fluency. We conduct 3-scale rating: +2, +1 and 0. +2: The response is fluent and grammatically correct. +1: There are a few grammatical errors in the response but readers could understand it. 0: The response is totally grammatically broken, making it difficult to understand. As how to evaluate response generation automatically is still an open problem BIBREF35 , we further conduct human evaluations to compare our models with baselines. We ask the same three native speakers to do a side-by-side comparison BIBREF15 on the 1,000 contexts. Given a context and two responses generated by different models, we ask annotators to decide which response is better (Ties are permitted)."
            ],
            "predicted_section": [
                "Evaluation Results",
                "Experiment setting",
                "Evaluation Metrics"
            ]
        },
        "45e6532ac06a59cb6a90624513242b06d7391501": {
            "question_text": "What do they mean by explicit selection of most relevant segments?",
            "from_paper": "1912.11637",
            "gold": [
                "Explicit Sparse Transformer is still based on the Transformer framework. The difference is in the implementation of self-attention. The attention is degenerated to the sparse attention through top-$k$ selection. In this way, the most contributive components for attention are reserved and the other irrelevant information are removed. This selective method is effective in preserving important information and removing noise. The attention can be much more concentrated on the most contributive elements of value. In the following, we first introduce the sparsification in self-attention and then extend it to context attention."
            ],
            "gold_section": [
                "Explicit Sparse Transformer"
            ],
            "predicted": [
                "We propose a novel model called Explicit Sparse Transformer, which enhances the concentration of the Transformer's attention through explicit selection.",
                "With the top-$k$ selection, the high attention scores are selected through an explicit way. This is different from dropout which randomly abandons the scores. Such explicit selection can not only guarantee the preservation of important components, but also simplify the model since $k$ is usually a small number such as 8, detailed analysis can be found in SECREF28. The next step after top-$k$ selection is normalization:",
                "Understanding natural language requires the ability to pay attention to the most relevant information. For example, people tend to focus on the most relevant segments to search for the answers to their questions in mind during reading. However, retrieving problems may occur if irrelevant segments impose negative impacts on reading comprehension. Such distraction hinders the understanding process, which calls for an effective attention."
            ],
            "predicted_section": [
                "Introduction",
                "Explicit Sparse Transformer"
            ]
        },
        "a98ae529b47362f917a398015c8525af3646abf0": {
            "question_text": "What datasets they used for evaluation?",
            "from_paper": "1912.11637",
            "gold": [
                "To evaluate the performance of Explicit Sparse Transformer in NMT, we conducted experiments on three NMT tasks, English-to-German translation (En-De) with a large dataset, English-to-Vietnamese (En-Vi) translation and German-to-English translation (De-En) with two datasets of medium size. For En-De, we trained Explicit Sparse Transformer on the standard dataset for WMT 2014 En-De translation. The dataset consists of around 4.5 million sentence pairs. The source and target languages share a vocabulary of 32K sub-word units. We used the newstest 2013 for validation and the newstest 2014 as our test set. We report the results on the test set.",
                "For En-Vi, we trained our model on the dataset in IWSLT 2015 BIBREF20. The dataset consists of around 133K sentence pairs from translated TED talks. The vocabulary size for source language is around 17,200 and that for target language is around 7,800. We used tst2012 for validation, and tst2013 for testing and report the testing results. For De-En, we used the dataset in IWSLT 2014. The training set contains 160K sentence pairs and the validation set contains 7K sentences. Following BIBREF21, we used the same test set with around 7K sentences. The data were preprocessed with byte-pair encoding BIBREF22. The vocabulary size is 14,000.",
                "We evaluated our approach on the image captioning task. Image captioning is a task that combines image understanding and language generation. We conducted experiments on the Microsoft COCO 2014 dataset BIBREF23. It contains 123,287 images, each of which is paired 5 with descriptive sentences. We report the results and evaluate the image captioning model on the MSCOCO 2014 test set for image captioning. Following previous works BIBREF24, BIBREF25, we used the publicly-available splits provided by BIBREF26. The validation set and test set both contain 5,000 images.",
                "Enwiki8 is large-scale dataset for character-level language modeling. It contains 100M bytes of unprocessed Wikipedia texts. The inputs include Latin alphabets, non-Latin alphabets, XML markups and special characters. The vocabulary size 205 tokens, including one for unknown characters. We used the same preprocessing method following BIBREF33. The training set contains 90M bytes of data, and the validation set and the test set contains 5M respectively."
            ],
            "gold_section": [
                "Results ::: Image Captioning ::: Dataset",
                "Results ::: Neural Machine Translation ::: Dataset",
                "Results ::: Language Modeling ::: Dataset"
            ],
            "predicted": [
                "We conducted a series of experiments on three natural language processing tasks, including neural machine translation, image captioning and language modeling. Detailed experimental settings are in Appendix SECREF42.",
                "We still use the default setting of Transformer for training our proposed Explicit Sparse Transformer. We report the standard automatic evaluation metrics with the help of the COCO captioning evaluation toolkit BIBREF53, which includes the commonly-used evaluation metrics, BLEU-4 BIBREF55, METEOR BIBREF54, and CIDEr BIBREF56.",
                "We evaluated our approach on the image captioning task. Image captioning is a task that combines image understanding and language generation. We conducted experiments on the Microsoft COCO 2014 dataset BIBREF23. It contains 123,287 images, each of which is paired 5 with descriptive sentences. We report the results and evaluate the image captioning model on the MSCOCO 2014 test set for image captioning. Following previous works BIBREF24, BIBREF25, we used the publicly-available splits provided by BIBREF26. The validation set and test set both contain 5,000 images."
            ],
            "predicted_section": [
                "Appendix ::: Experimental Details ::: Image Captioning",
                "Results ::: Image Captioning ::: Dataset",
                "Results"
            ]
        },
        "a0197894ee94b01766fa2051f50f84e16b5c9370": {
            "question_text": "Do they reason why greedy decoding works better then beam search?",
            "from_paper": "1606.07947",
            "gold": [
                "We run experiments to compress a large state-of-the-art INLINEFORM0 LSTM model, and find that with sequence-level knowledge distillation we are able to learn a INLINEFORM1 LSTM that roughly matches the performance of the full system. We see similar results compressing a INLINEFORM2 model down to INLINEFORM3 on a smaller data set. Furthermore, we observe that our proposed approach has other benefits, such as not requiring any beam search at test-time. As a result we are able to perform greedy decoding on the INLINEFORM4 model 10 times faster than beam search on the INLINEFORM5 model with comparable performance. Our student models can even be run efficiently on a standard smartphone. Finally, we apply weight pruning on top of the student network to obtain a model that has INLINEFORM6 fewer parameters than the original teacher model. We have released all the code for the models described in this paper.",
                "Sequence-level interpolation (Seq-Inter), in addition to improving models trained via Word-KD and Seq-KD, also improves upon the original teacher model that was trained on the actual data but fine-tuned towards Seq-Inter data (Baseline INLINEFORM0 Seq-Inter). In fact, greedy decoding with this fine-tuned model has similar performance ( INLINEFORM1 ) as beam search with the original model ( INLINEFORM2 ), allowing for faster decoding even with an identically-sized model."
            ],
            "gold_section": [
                "Results and Discussion",
                "Introduction"
            ],
            "predicted": [
                "Run-time complexity for beam search grows linearly with beam size. Therefore, the fact that sequence-level knowledge distillation allows for greedy decoding is significant, with practical implications for running NMT systems across various devices. To test the speed gains, we run the teacher/student models on GPU, CPU, and smartphone, and check the average number of source words translated per second (Table 2). We use a GeForce GTX Titan X for GPU and a Samsung Galaxy 6 smartphone. We find that we can run the student model 10 times faster with greedy decoding than the teacher model with beam search on GPU ( INLINEFORM0 vs INLINEFORM1 words/sec), with similar performance.",
                "Finally, although past work has shown that models with lower perplexity generally tend to have higher BLEU, our results indicate that this is not necessarily the case. The perplexity of the baseline INLINEFORM0 English INLINEFORM1 German model is INLINEFORM2 while the perplexity of the corresponding Seq-KD model is INLINEFORM3 , despite the fact that Seq-KD model does significantly better for both greedy ( INLINEFORM4 BLEU) and beam search ( INLINEFORM5 BLEU) decoding.",
                "We hypothesize that sequence-level knowledge distillation is effective because it allows the student network to only model relevant parts of the teacher distribution (i.e. around the teacher's mode) instead of `wasting' parameters on trying to model the entire space of translations. Our results suggest that this is indeed the case: the probability mass that Seq-KD models assign to the approximate mode is much higher than is the case for baseline models trained on original data (Table 1: INLINEFORM0 ). For example, on English INLINEFORM1 German the (approximate) INLINEFORM2 for the INLINEFORM3 Seq-KD model (on average) accounts for INLINEFORM4 of the total probability mass, while the corresponding number is INLINEFORM5 for the baseline. This also explains the success of greedy decoding for Seq-KD models\u2014since we are only modeling around the teacher's mode, the student's distribution is more peaked and therefore the INLINEFORM6 is much easier to find. Seq-Inter offers a compromise between the two, with the greedily-decoded sequence accounting for INLINEFORM7 of the distribution."
            ],
            "predicted_section": [
                "Results and Discussion",
                "Decoding Speed"
            ]
        },
        "cbb4eba59434d596749408be5b923efda7560890": {
            "question_text": "What baselines is the neural relation extractor compared to?",
            "from_paper": "1603.00957",
            "gold": [
                "Table 1 summarizes the results on the test data along with the results from the literature. We can see that joint EL and RE performs better than the default pipelined approach, and outperforms most semantic parsing based models, except BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing. In addition, inference on unstructured data helps the default model. The joint EL and RE combined with inference on unstructured data further improves the default pipelined model by 9.2% (from 44.1% to 53.3%), and achieves a new state-of-the-art result beating the previous reported best result of yih-EtAl:2015:ACL-IJCNLP (with one-tailed t-test significance of $p < 0.05$ ).",
                "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction."
            ],
            "gold_section": [
                "Relation Extraction",
                "Results and Discussion"
            ],
            "predicted": [
                "In MCCNN, we use two channels, one for syntactic information and the other for sentential information. The network structure is illustrated in Figure 2 . Convolution layer tackles an input of varying length returning a fixed length vector (we use max pooling) for each channel. These fixed length vectors are concatenated and then fed into a softmax classifier, the output dimension of which is equal to the number of predefined relation types. The value of each dimension indicates the confidence score of the corresponding relation.",
                "where $\\theta $ represents the weights, and $\\lambda $ the $L2$ regularization parameters. The weights $\\theta $ can be efficiently computed via back-propagation through network structures. To minimize $J(\\theta )$ , we apply stochastic gradient descent (SGD) with AdaGrad BIBREF20 .",
                "Our work also intersects with relation extraction methods. While these methods aim to predict a relation between two entities in order to populate KBs BIBREF46 , BIBREF47 , BIBREF48 , we work with sentence level relation extraction for question answering. krishnamurthy2012weakly and fader2014open adopt open relation extraction methods for QA but they require hand-coded grammar for parsing queries. Closest to our extraction method is yao-jacana-freebase-acl2014 and yao-scratch-qa-naacl2015 who also uses sentence level relation extraction for QA. Unlike them, we can predict multiple relations per question, and our MCCNN architecture is more robust to unseen contexts compared to their logistic regression models."
            ],
            "predicted_section": [
                "Relation Extraction",
                "Related Work"
            ]
        },
        "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b": {
            "question_text": "What is the previous state-of-the-art?",
            "from_paper": "1603.00957",
            "gold": [
                "The state-of-the-art methods for this task can be roughly categorized into two streams. The first is based on semantic parsing BIBREF3 , BIBREF4 , which typically learns a grammar that can parse natural language to a sophisticated meaning representation language. But such sophistication requires a lot of annotated training examples that contains compositional structures, a practically impossible solution for large KBs such as Freebase. Furthermore, mismatches between grammar predicted structures and KB structure is also a common problem BIBREF4 , BIBREF5 , BIBREF6 .",
                "On the other hand, instead of building a formal meaning representation, information extraction methods retrieve a set of candidate answers from KB using relation extraction BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 or distributed representations BIBREF11 , BIBREF12 . Designing large training datasets for these methods is relatively easy BIBREF7 , BIBREF13 , BIBREF14 . These methods are often good at producing an answer irrespective of their correctness. However, handling compositional questions that involve multiple entities and relations, still remains a challenge. Consider the question what mountain is the highest in north america. Relation extraction methods typically answer with all the mountains in North America because of the lack of sophisticated representation for the mathematical function highest. To select the correct answer, one has to retrieve all the heights of the mountains, and sort them in descending order, and then pick the first entry. We propose a method based on textual evidence which can answer such questions without solving the mathematic functions implicitly.",
                "We now proceed to identify the relation between the answer and the entity in the question. Inspired by the recent success of neural network models in KB question-answering BIBREF16 , BIBREF12 , and the success of syntactic dependencies for relation extraction BIBREF17 , BIBREF18 , we propose a Multi-Channel Convolutional Neural Network (MCCNN) which could exploit both syntactic and sentential information for relation extraction."
            ],
            "gold_section": [
                "Relation Extraction",
                "Introduction"
            ],
            "predicted": [
                "Over time, the QA task has evolved into two main streams \u2013 QA on unstructured data, and QA on structured data. TREC QA evaluations BIBREF26 were a major boost to unstructured QA leading to richer datasets and sophisticated methods BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . While initial progress on structured QA started with small toy domains like GeoQuery BIBREF34 , recent focus has shifted to large scale structured KBs like Freebase, DBPedia BIBREF35 , BIBREF36 , BIBREF3 , BIBREF4 , BIBREF37 , and on noisy KBs BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 . An exciting development in structured QA is to exploit multiple KBs (with different schemas) at the same time to answer questions jointly BIBREF43 , BIBREF44 , BIBREF45 . QALD tasks and linked data initiatives are contributing to this trend.",
                "We have presented a method that could infer both on structured and unstructured data to answer natural language questions. Our experiments reveal that unstructured inference helps in mitigating representational issues in structured inference. We have also introduced a relation extraction method using MCCNN which is capable of exploiting syntax in addition to sentential features. Our main model which uses joint entity linking and relation extraction along with unstructured inference achieves the state-of-the-art results on WebQuestions dataset. A potential application of our method is to improve KB-question answering using the documents retrieved by a search engine.",
                "Our model combines the best of both worlds by inferring over structured and unstructured data. Though earlier methods exploited unstructured data for KB-QA BIBREF40 , BIBREF3 , BIBREF7 , BIBREF6 , BIBREF16 , these methods do not rely on unstructured data at test time. Our work is closely related to joshi:2014 who aim to answer noisy telegraphic queries using both structured and unstructured data. Their work is limited in answering single relation queries. Our work also has similarities to sun2015open who does question answering on unstructured data but enrich it with Freebase, a reversal of our pipeline. Other line of very recent related work include Yahya:2016:RQE:2835776.2835795 and savenkovknowledge."
            ],
            "predicted_section": [
                "Related Work",
                "Conclusion and Future Work"
            ]
        },
        "23cbf6ab365c1eb760b565d8ba51fb3f06257d62": {
            "question_text": "What are the baseline models?",
            "from_paper": "1910.02677",
            "gold": [
                "In recent years, SS was largely treated as a monolingual variant of machine translation (MT), where simplification operations are learned from complex-simple sentence pairs automatically extracted from English Wikipedia and Simple English Wikipedia BIBREF11, BIBREF12.",
                "Phrase-Based MT system with candidate reranking. Dissimilar candidates are favored based on their Levenshtein distance to the source.",
                "BIBREF33",
                "Deep semantics sentence representation fed to a monolingual MT system.",
                "Our contributions are the following: (1) We adapt a parametrization mechanism to the specific task of Sentence Simplification by choosing relevant parameters; (2) We show through a detailed analysis that our model can indeed control the considered attributes, making the simplifications potentially able to fit the needs of various end audiences; (3) With careful calibration, our controllable parametrization improves the performance of out-of-the-box Seq2Seq models leading to a new state-of-the-art score of 41.87 SARI BIBREF9 on the WikiLarge benchmark BIBREF10, a +1.42 gain over previous scores, without requiring any external resource or modified training objective.",
                "Syntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI.",
                "Seq2Seq trained with reinforcement learning, combined with a lexical simplification model.",
                "Lately, SS has mostly been tackled using Seq2Seq MT models BIBREF14. Seq2Seq models were either used as-is BIBREF15 or combined with reinforcement learning thanks to a specific simplification reward BIBREF10, augmented with an external simplification database as a dynamic memory BIBREF16 or trained with multi-tasking on entailment and paraphrase generation BIBREF17.",
                "Seq2Seq model based on the pointer-copy mechanism and trained via multi-task learning on the Entailment and Paraphrase Generation tasks.",
                "Standard Seq2Seq model. The second beam search hypothesis is selected during decoding; the hypothesis number is an hyper-parameter fine-tuned with SARI.",
                "BIBREF35",
                "Seq2Seq with a memory-augmented Neural Semantic Encoder, tuned with SARI.",
                "Seq2Seq integrating the simple PPDB simplification database BIBREF36 as a dynamic memory. The database is also used to modify the loss and re-weight word probabilities to favor simpler words."
            ],
            "gold_section": [
                "Experiments ::: Overall Performance",
                "Related Work ::: Sentence Simplification",
                "Introduction"
            ],
            "predicted": [
                "We select the model with the best SARI on the validation set and report its scores on the test set. This model only uses three parameters out of four: NbChars$_{0.95}$, LevSim$_{0.75}$ and WordRank$_{0.75}$ (optimal target ratios are in subscript).",
                "ACCESS scores best on SARI (41.87), a significant improvement over previous state of the art (40.45), and third to best FKGL (7.22). The second and third models in terms of SARI, DMASS+DCSS (40.45) and SBMT+PPDB+SARI (39.96), both use the external resource Simple PPDB BIBREF36 that was extracted from 1000 times more data than what we used for training. Our FKGL is also better (lower) than these methods. The Hybrid model scores best on FKGL (4.56) i.e. they generated the simplest (and shortest) sentences, but it was done at the expense of SARI (31.40).",
                "Syntax-based MT model augmented using the PPDB paraphrase database BIBREF34 and fine-tuned towards SARI."
            ],
            "predicted_section": [
                "Experiments ::: Overall Performance"
            ]
        },
        "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e": {
            "question_text": "What datasets are used?",
            "from_paper": "2002.04374",
            "gold": [
                "The Spanish data consider the PC-GITA corpus BIBREF5, which contains utterances from 50 PD patients and 50 HC, Colombian Spanish native speakers. The participants were asked to pronounce a total of 10 sentences, the rapid repetition of /pa-ta-ka/, /pe-ta-ka/, /pa-ka-ta/, /pa/, /ta/, and /ka/, one text with 36 words, and a monologue. All patients were in ON state at the time of the recording, i.e., under the effect of their daily medication.",
                "Speech recordings of 88 PD patients and 88 HC speakers from Germany are considered BIBREF17. The participants performed four speech task: the rapid repetition of /pa-ta-ka/, 5 sentences, one text with 81 words, and a monologue.",
                "A total of 100 native Czech speakers (50 PD, 50 HC) were considered BIBREF18. The speech tasks performed by the participants include the rapid repetition of the syllables /pa-ta-ka/, a read text with 80 words, and a monologue."
            ],
            "gold_section": [
                "Materials and methods ::: Data ::: Czech",
                "Materials and methods ::: Data ::: German",
                "Materials and methods ::: Data ::: Spanish"
            ],
            "predicted": [
                "Speech recordings of patients in three different languages are considered: Spanish, German, and Czech. All of the recordings were captured in noise controlled conditions. The speech signals were down-sampled to 16 kHz. The patients in the three datasets were evaluated by a neurologist expert according to the third section of the movement disorder society, unified Parkinson's disease rating scale (MDS-UPDRS-III) BIBREF16. Table TABREF5 summarizes the information about the patients and healthy speakers.",
                "The features extracted from the transitions include 12 Mel-Frequency Cepstral Coefficients (MFCCs) with their first and second derivatives, and the log energy of the signal distributed into 22 Bark bands. The total number of descriptors corresponds to 58. Four statistical functionals (mean, standard deviation, skewness, and kurtosis) are computed for each descriptor, obtaining a 232-dimensional feature-vector per utterance. The classification of PD patients and HC speakers is performed with a radial basis SVM with margin parameter $C=10$ and a Gaussian kernel with parameter $\\gamma =0.0001$. The SVM is tested following a 10-fold Cross-Validation strategy, speaker independent.",
                "Table TABREF13 shows the results obtained for the baseline and the CNNs trained for each language individually. Similar accuracies are obtained between the baseline and the CNN model for Spanish language, which also exhibit the highest accuracy among the three languages. Note that the highest accuracy for German language was obtained with the baseline model. Conversely, for Czech language the CNN produces the highest accuracy. Note also that for the three languages, the results are unbalanced towards one of the two classes according to the specificity and sensitivity values. The difference in the results obtained among the three languages can be explained considering the information provided in Table TABREF5. For the patients in the Spanish language, the average MDS-UPDRS-III score is higher compared with the German and Czech patients, i.e, there are patients with higher disease severity in the Spanish data compared to German and Czech patients."
            ],
            "predicted_section": [
                "Materials and methods ::: Data",
                "Experiments and results ::: Baseline and individual CNN models",
                "Materials and methods ::: Baseline model"
            ]
        },
        "ed15a593d64a5ba58f63c021ae9fd8f50051a667": {
            "question_text": "Is this model trained in unsuperized manner?",
            "from_paper": "2001.05540",
            "gold": [
                "The shifted alphabetic sequence task should be trivial to solve for a powerful sequence to sequence model implemented with Transformers. The next translation task we teach the model is Caesar's cipher. This is an old encryption method, in which each letter in the source sequence is replaced by a letter some fixed number of positions down the alphabet. The sequences do not need to be in alphabetic order, meaning the diversity of input sequences will be much larger than with the previous task. We again sample a $\\text{min}_n <= n < \\text{max}_n$, where $\\text{min}_n = 3$ and $\\text{max}_n = 25$ this time. We shift each letter in the source sequence by $\\text{max}_n = 25$. If the sampled $n$ is 5, we randomly sample 5 letters from the alphabet and shift each letter in the target to the left by one character we get the following example:",
                "Source $ h\\ k\\ b\\ e\\ t $",
                "Target $ g\\ j\\ a\\ d\\ s $",
                "The first task we train the insertion-deletion model on is shifting alphabetic sequences. For generation of data we sample a sequence length $\\text{min}_n <= n < \\text{max}_n$ from a uniform distribution where $\\text{min}_n = 3$ and $\\text{max}_n = 10$. We then uniformly sample the starting token and finish the alphabetic sequence until it has length $n$. For a sampled $n = 5$ and starting letter $\\text{c}$, shifting each letter by $\\text{max}_n$ to ensure the source and target have no overlapping sequence, here is one example sequence:",
                "Source $ c\\ d\\ e\\ f\\ g $",
                "Target $ m\\ n\\ o\\ p\\ q $"
            ],
            "gold_section": [
                "Experiments ::: Learning shifted alphabetic sequences",
                "Experiments ::: Learning Caesar's Cipher"
            ],
            "predicted": [
                "We parametrize both the insertion and deletion probability distributions with two stacked transformer decoders, where $\\theta _i$ denotes the parameters of the insertion model and $\\theta _d$ of the deletion model. The models are trained at the same time, where the deletion model's signal is dependent on the state of the current insertion model. For sampling from the insertion model we take the argument that maximizes the probability of the current sequence via parallel decoding: $\\hat{c}_l = \\arg \\max _{c}p(c, \\mid l, \\hat{x}_t)$. We do not backpropagate through the sampling process, i.e., the gradient during training can not flow from the output of the deletion model through the insertion model. Both models are trained to maximize the log-probability of their respective distributions. A graphical depiction of the model is shown in Figure FIGREF7.",
                "Since the signal for the deletion model is dependent on the insertion model's state, it is possible that the deletion model does not receive a learning signal during training. This happens when either the insertion model is too good and never inserts a wrong token, or when the insertion model does not insert anything at all. To mitigate this problem we propose an adversarial sampling method. To ensure that the deletion model always has a signal, with some probability $p_{\\text{adv}}$ we mask the ground-truth tokens in the target for the insertion model during training. This has the effect that when selecting the token to insert in the input sequence, before passing it to the deletion model, the insertion model selects the incorrect token it is most confident about. Therefore, the deletion model always has a signal and trains for a situation that it will most likely also encounter during inference.",
                "We demonstrate the capabilities of our Insertion-Deletion model through experiments on synthetic translation datasets. We show how the addition of deletion improves BLEU score, and how the insertion and deletion model interact as shown in Table TABREF9. We found that adversarial deletion training did not improve BLEU scores on these synthetic tasks. However, the adversarial training scheme can still be helpful when the deletion model does not receive a signal during training by sampling from the insertion model alone (i.e., when the insertion-model does not make any errors)."
            ],
            "predicted_section": [
                "Method ::: Learning",
                "Experiments"
            ]
        },
        "6c4e1a1ccc0c5c48115864a6928385c248f4d8ad": {
            "question_text": "What are the solutions proposed for the seq2seq shortcomings?",
            "from_paper": "1612.02695",
            "gold": [
                "We compare three strategies designed to prevent incomplete transcripts. The first strategy doesn't change the beam search criterion, but forbids emitting the EOS token unless its probability is within a set range of that of the most probable token. This strategy prevents truncations, but is inefficient against omissions in the middle of the transcript, such as the failure shown in Table TABREF17 . Alternatively, beam search criterion can be extended to promote long transcripts. A term depending on the transcript length was proposed for both CTC BIBREF3 and seq2seq BIBREF11 networks, but its usage was reported to be difficult because beam search was looping over parts of the recording and additional constraints were needed BIBREF11 . To prevent looping we propose to use a coverage term that counts the number of frames that have received a cumulative attention greater than INLINEFORM0 : DISPLAYFORM0",
                "The coverage criterion prevents looping over the utterance because once the cumulative attention bypasses the threshold INLINEFORM0 a frame is counted as selected and subsequent selections of this frame do not reduce the decoding cost. In our implementation, the coverage is recomputed at each beam search iteration using all attention weights produced up to this step.",
                "Label Smoothing Prevents Overconfidence",
                "A elegant solution to model overconfidence was problem proposed for the Inception image recognition architecture BIBREF15 . For the purpose of computing the training cost the ground-truth label distribution is smoothed, with some fraction of the probability mass assigned to classes other than the correct one. This in turn prevents the model from learning to concentrate all probability mass on a single token. Additionally, the model receives more training signal because the error function cannot easily saturate."
            ],
            "gold_section": [
                "Solutions to Partial Transcripts Problem",
                "Label Smoothing Prevents Overconfidence"
            ],
            "predicted": [
                "We have analysed the impact of model confidence by separating its effects on model accuracy and beam search effectiveness. We also propose a practical solution to the partial transcriptions problem, relating to the coverage of the input utterance.",
                "Understanding and preventing limitations specific to seq2seq models is crucial for their successful development. Discriminative training allows seq2seq models to focus on the most informative features. However, it also increases the risk of overfitting to those few distinguishing characteristics. We have observed that seq2seq models often yield very sharp predictions, and only a few hypotheses need to be considered to find the most likely transcription of a given utterance. However, high confidence reduces the diversity of transcripts obtained using beam search.",
                "Deep learning BIBREF0 has led to many breakthroughs including speech and image recognition BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . A subfamily of deep models, the Sequence-to-Sequence (seq2seq) neural networks have proved to be very successful on complex transduction tasks, such as machine translation BIBREF7 , BIBREF8 , BIBREF9 , speech recognition BIBREF10 , BIBREF11 , BIBREF12 , and lip-reading BIBREF13 . Seq2seq networks can typically be decomposed into modules that implement stages of a data processing pipeline: an encoding module that transforms its inputs into a hidden representation, a decoding (spelling) module which emits target sequences and an attention module that computes a soft alignment between the hidden representation and the targets. Training directly maximizes the probability of observing desired outputs conditioned on the inputs. This discriminative training mode is fundamentally different from the generative \"noisy channel\" formulation used to build classical state-of-the art speech recognition systems. As such, it has benefits and limitations that are different from classical ASR systems."
            ],
            "predicted_section": [
                "Solutions to Seq2Seq Failure Modes",
                "Introduction"
            ]
        },
        "523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd": {
            "question_text": "What experiments do they perform?",
            "from_paper": "2002.04745",
            "gold": [
                "Experiments ::: Experiment Settings ::: Machine Translation",
                "We conduct our experiments on two widely used tasks: the IWSLT14 German-to-English (De-En) task and the WMT14 English-to-German (En-De) task. For the IWSLT14 De-En task, we use the same model configuration as in Section 3. For the WMT14 En-De task, we use the Transformer base setting. More details can be found in the supplementary material.",
                "For training the Pre-LN Transformer, we remove the learning rate warm-up stage. On the IWSLT14 De-En task, we set the initial learning rate to be $5e^{-4}$ and decay the learning rate at the 8-th epoch by 0.1. On the WMT14 En-De task, we run two experiments in which the initial learning rates are set to be $7e^{-4}/1.5e^{-3}$ respectively. Both learning rates are decayed at the 6-th epoch followed by the inverse square root learning rate scheduler.",
                "Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)",
                "We follow BIBREF8 to use English Wikipedia corpus and BookCorpus for pre-training. As the dataset BookCorpus BIBREF40 is no longer freely distributed. We follow the suggestions from BIBREF8 to crawl and collect BookCorpus on our own. The concatenation of two datasets contains roughly 3.4B words in total, which is comparable with the data corpus used in BIBREF8. We randomly split documents into one training set and one validation set. The training-validation ratio for pre-training is 199:1.",
                "We use base model configuration in our experiments. Similar to the translation task, we train the Pre-LN BERT without the warm-up stage and compare it with the Post-LN BERT. We follow the same hyper-parameter configuration in BIBREF8 to train the Post-LN BERT using 10k warm-up steps with $\\text{lr}_{max}=1e^{-4}$. For the Pre-LN BERT, we use linear learning rate decay starting from $3e^{-4}$ without the warm-up stage. We have tried to use a larger learning rate (such as $3e^{-4}$) for the Post-LN BERT but found the optimization diverged."
            ],
            "gold_section": [
                "Experiments ::: Experiment Settings ::: Machine Translation",
                "Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)"
            ],
            "predicted": [
                "We record the model checkpoints for every epoch during training and calculate the validation loss and BLEU score. The performance of the models at different checkpoints are plotted in Figure FIGREF41 - FIGREF44.",
                "As our theory is derived based on several simplifications of the problem, we conduct experiments to study whether our theoretical insights are consistent with what we observe in real scenarios. The general model and training configuration exactly follow Section 3.2. The experiments are repeated ten times using different random seeds.",
                "We follow BIBREF8 to use English Wikipedia corpus and BookCorpus for pre-training. As the dataset BookCorpus BIBREF40 is no longer freely distributed. We follow the suggestions from BIBREF8 to crawl and collect BookCorpus on our own. The concatenation of two datasets contains roughly 3.4B words in total, which is comparable with the data corpus used in BIBREF8. We randomly split documents into one training set and one validation set. The training-validation ratio for pre-training is 199:1."
            ],
            "predicted_section": [
                "Optimization for the Transformer ::: Empirical verification of the theory and discussion",
                "Experiments ::: Experiment Settings ::: Unsupervised Pre-training (BERT)",
                "Experiments ::: Experiment Results ::: Machine Translation"
            ]
        },
        "3d662fb442d5fc332194770aac835f401c2148d9": {
            "question_text": "Do they report results only on English data?",
            "from_paper": "1911.09247",
            "gold": [
                "The English-German translation models are trained on WMT datasets, including News Commentary 13, Europarl v7, and Common Crawl, and evaluated on newstest2013 for early stopping. On the newstest2013 dev set, the En$\\rightarrow $De model reaches a BLEU-4 score of 19.6, and the De$\\rightarrow $En model reaches a BLEU-4 score of 24.6.",
                "We use 303 sub areas from Stack Exchange data dumps. The full list of area names is in the appendix. We do not include Stack Overflow because it is too specific to programming related questions. We also exclude all questions under the following language sub areas: Chinese, German, Spanish, Russian, Japanese, Korean, Latin, Ukrainian. This ensures that the questions in MQR are mostly English sentences. Having questions from 303 Stack Exchange sites makes the MQR dataset cover a broad range of domains.",
                "All well-formed questions in the pairs must start with \u201chow\u201d, \u201cwhy\u201d, \u201cwhen\u201d, \u201cwhat\u201d, \u201cwhich\u201d, \u201cwho\u201d, \u201cwhose\u201d, \u201cdo\u201d, \u201cwhere\u201d, \u201cdoes\u201d, \u201cis\u201d, \u201care\u201d, \u201cmust\u201d, \u201cmay\u201d, \u201cneed\u201d, \u201cdid\u201d, \u201cwas\u201d, \u201cwere\u201d, \u201ccan\u201d, \u201chas\u201d, \u201chave\u201d, \u201care\u201d. This step is performed to make sure the questions are explicit questions but not statements or commands.",
                "To ensure there are no sentences written in non-English languages, we keep questions that contain 80% or more of valid English characters, including punctuation."
            ],
            "gold_section": [
                "MQR Dataset Construction and Analysis",
                "Models and Experiments ::: Methods Built from Other Resources ::: Round Trip Neural Machine Translation."
            ],
            "predicted": [
                "To ensure there are no sentences written in non-English languages, we keep questions that contain 80% or more of valid English characters, including punctuation.",
                "The English-French models are trained on Common Crawl 13, Europarl v7, News Commentary v9, Giga release 2, and UN doc 2000. On the newstest2013 dev set, the En$\\rightarrow $Fr model reaches a BLEU-4 score of 25.6, and the Fr$\\rightarrow $En model reaches a BLEU-4 score of 26.1.",
                "Is the spelling correct? Misuse of third person singular or past tense in verbs are considered grammatical errors instead of spelling errors. Missing question mark in the end of a question is also considered as spelling errors."
            ],
            "predicted_section": [
                "MQR Dataset Construction and Analysis",
                "Models and Experiments ::: Methods Built from Other Resources ::: Round Trip Neural Machine Translation.",
                "MQR Dataset Construction and Analysis ::: Dataset Quality"
            ]
        },
        "34b434825f0ca3225dc8914f9da865d2b4674f08": {
            "question_text": "Does the baseline use any contextual information?",
            "from_paper": "1912.08084",
            "gold": [
                "The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement.",
                "First, there is a random baseline, followed by an SVM classifier based on a bag-of-words representation with TF.IDF weights learned on the training data. Then come three versions of the ClaimBuster system: CB-Platform uses scores from the online demo, which we accessed on December 20, 2016, and SVM$_{CBfeat}$ and FNN$_{CBfeat}$ are our re-implementations, trained on our dataset."
            ],
            "gold_section": [
                "Related Work",
                "Experiments and Evaluation ::: Evaluation Results"
            ],
            "predicted": [
                "Table TABREF27 shows the results when using all features vs. excluding the contextual features vs. using the contextual features only. We can see that the contextual features have a major impact on performance: excluding them yields major drop for all measures, e.g., MAP drops from .427 to .385, and P@5 drops from .800 to .550. The last two rows in the table show that using contextual features only performs about the same as CB Platform (which uses no contextual features at all).",
                "Modeling the context: We develop a novel approach for automatically predicting which claims should be prioritized for fact-checking, based on a rich input representation. In particular, we model not only the textual content, but also the context: how the target claim relates to the current segment, to neighboring segments and sentences, and to the debate as a whole, and also how the opponents and the public react to it.",
                "Note that the investigative journalists did not select the check-worthy claims in isolation. Our analysis shows that these include claims that were highly disputed during the debate, that were relevant to the topic introduced by the moderator, etc. We will make use of these contextual dependencies below, which is something that was not previously tried in related work."
            ],
            "predicted_section": [
                "The CW-USPD-2016 dataset on US Presidential Debates",
                "Discussion ::: Effect of Context Modeling",
                "Introduction"
            ]
        },
        "61a2599acfbd3d75de58e97ecdba2d9cf0978324": {
            "question_text": "What is the strong rivaling system?",
            "from_paper": "1912.08084",
            "gold": [
                "The previous work that is most relevant to our work here is that of BIBREF9, who developed the ClaimBuster system, which assigns each sentence in a document a score, i.e., a number between 0 and 1 showing how worthy it is of fact-checking. The system is trained on their own dataset of about eight thousand debate sentences (1,673 of them check-worthy), annotated by students, university professors, and journalists. Unfortunately, this dataset is not publicly available and it contains sentences without context as about 60% of the original sentences had to be thrown away due to lack of agreement."
            ],
            "gold_section": [
                "Related Work"
            ],
            "predicted": [
                "State-of-the-art results: We achieve state-of-the-art results, outperforming a strong rivaling system by a margin, while also demonstrating that this improvement is due primarily to our modeling of the context.",
                "Metadata (8 C features): Check-worthy claims often contain mutual accusations between the opponents, as the following example shows (from the 2nd presidential debate):",
                "We model the problem as a ranking task, and we train both Support Vector Machines (SVM) and Feed-forward Neural Networks (FNN) obtaining state-of-the-art results. We also analyze the relevance of the specific feature groups and we show that modeling the context yields a significant boost in performance. Finally, we also analyze whether we can learn to predict which facts are check-worthy with respect to each of the individual media sources, thus capturing their biases. It is worth noting that while trained on political debates, many features of our model can be potentially applied to other kinds of information sources, e.g., interviews and news."
            ],
            "predicted_section": [
                "Introduction",
                "Modeling Check-Worthiness ::: Contextual Features"
            ]
        },
        "c27b885b1e38542244f52056abf288b2389b9fc6": {
            "question_text": "How do they determine demographics on an image?",
            "from_paper": "1905.01347",
            "gold": [
                "In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias.",
                "Face Detection",
                "The FaceBoxes network BIBREF15 is employed for face detection, consisting of a lightweight CNN that incorporates novel Rapidly Digested and Multiple Scale Convolutional Layers for speed and accuracy, respectively. This model was trained on the WIDER FACE dataset BIBREF16 and achieves average precision of 95.50% on the Face Detection Data Set and Benchmark (FDDB) BIBREF17 . On a subset of 1,000 images from FDDB hand-annotated by the author for apparent age and gender, the model achieves a relative fair performance across intersectional groups, as show in Table TABREF1 .",
                "The task of apparent age annotation arises as ground-truth ages of individuals in images are not possible to obtain in the domain of web-scraped datasets. In this work, we follow Merler et al. BIBREF18 and employ the Deep EXpectation (DEX) model of apparent age BIBREF19 , which is pre-trained on the IMDB-WIKI dataset of 500k faces with real ages and fine-tuned on the APPA-REAL training and validation sets of 3.6k faces with apparent ages, crowdsourced from an average of 38 votes per image BIBREF20 . As show in Table TABREF2 , the model achieves a mean average error of 5.22 years on the APPA-REAL test set, but exhibits worse performance on younger and older age groups.",
                "We recognize that a binary representation of gender does not adequately capture the complexities of gender or represent transgender identities. In this work, we express gender as a continuous value between 0 and 1. When thresholding at 0.5, we use the sex labels of `male' and `female' to define gender classes, as training datasets and evaluation benchmarks use this binary label system. We again follow Merler et al. BIBREF18 and employ a DEX model to annotate the gender of an individual. When tested on APPA-REAL, with enhanced annotations provided by BIBREF21 , the model achieves an accuracy of 91.00%, however its errors are not evenly distributed, as shown in Table TABREF3 . The model errs more on younger and older age groups and on those with a female gender label."
            ],
            "gold_section": [
                "Apparent Age Annotation",
                "Gender Annotation",
                "Methodology",
                "Face Detection"
            ],
            "predicted": [
                "In order to provide demographic annotations at scale, there exist two feasible methods: crowdsourcing and model-driven annotations. In the case of large-scale image datasets, crowdsourcing quickly becomes prohibitively expensive; ImageNet, for example, employed 49k AMT workers during its collection BIBREF14 . Model-driven annotations use supervised learning methods to create models that can predict annotations, but this approach comes with its own meta-problem; as the goal of this work is to identify demographic representation in data, we must analyze the annotation models for their performance on intersectional groups to determine if they themselves exhibit bias.",
                "Through the introduction of a preliminary pipeline for automated demographic annotations, this work hopes to provide insight into the ImageNet dataset, a tool that is commonly abstracted away by the computer vision community. In the future, we will continue this work to create fair models for automated demographic annotations, with emphasis on the gender annotation model. We aim to incorporate additional measures of diversity into the pipeline, such as Fitzpatrick skin type and other craniofacial measurements. When annotation models are evaluated as fair, we plan to continue this audit on all 14.2M images of ImageNet and other large image datasets. With accurate coverage of the demographic attributes of ImageNet, we will be able to investigate the downstream impact of under- and over-represented groups in the features learned in pretrained CNNs and how bias represented in these features may propagate in transfer learning to new applications.",
                "First, candidate images for each synset were sourced from commercial image search engines, including Google, Yahoo!, Microsoft's Live Search, Picsearch and Flickr BIBREF10 . Gender BIBREF11 and racial BIBREF12 biases have been demonstrated to exist in image search results (i.e. images of occupations), demonstrating that a more curated approach at the top of the funnel may be necessary to mitigate inherent biases of search engines. Second, English search queries were translated into Chinese, Spanish, Dutch and Italian using WordNet databases and used for image retrieval. While this is a step in the right direction, Chinese was the only non-Western European language used, and there exists, for example, Universal Multilingual WordNet which includes over 200 languages for translation BIBREF13 . Third, the authors quantify image diversity by computing the average image of each synset and measuring the lossless JPG file size. They state that a diverse synset will result in a blurrier average image and smaller file, representative of diversity in appearance, position, viewpoint and background. This method, however, cannot quantify diversity with respect to demographic characteristics such as age, gender, and skin type."
            ],
            "predicted_section": [
                "Diversity Considerations in ImageNet",
                "Conclusion",
                "Methodology"
            ]
        },
        "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7": {
            "question_text": "How do they show that binary paragraph vectors capture semantics?",
            "from_paper": "1611.01116",
            "gold": [
                "To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.",
                "In the experiments presented thus far we had at our disposal training sets with documents similar to the documents for which we inferred binary codes. One could ask a question, if it is possible to use binary paragraph vectors without collecting a domain-specific training set? For example, what if we needed to hash documents that are not associated with any available domain-specific corpus? One solution could be to train the model with a big generic text corpus, that covers a wide variety of domains. BIBREF21 evaluated this approach for real-valued paragraph vectors, with promising results. It is not obvious, however, whether short binary codes would also perform well in similar settings. To shed light on this question we trained Binary PV-DBOW with bigrams on the English Wikipedia, and then inferred binary codes for the test parts of the 20 Newsgroups and RCV1 datasets. The results are presented in Table TABREF14 and in Figure FIGREF11 . The model trained on an unrelated text corpus gives lower retrieval precision than models with domain-specific training sets, which is not surprising. However, it still performs remarkably well, indicating that the semantics it captured can be useful for different text collections. Importantly, these results were obtained without domain-specific finetuning."
            ],
            "gold_section": [
                "Transfer learning",
                "Experiments"
            ],
            "predicted": [
                "The basic idea in binary paragraph vector models is to introduce a sigmoid nonlinearity before the softmax that models the conditional probability of words given the context. If we then enforce binary or near-binary activations in this nonlinearity, the probability distribution over words will be conditioned on a bit vector context, rather than real-valued representation. The inference in the model proceeds like in Paragraph Vector, except the document code is constructed from the sigmoid activations. After rounding, this code can be seen as a distributed binary representation of the document.",
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.",
                "Softmax layers in the models described above should be trained to predict words in documents given binary context vectors. Training should therefore encourage binary activations in the preceding sigmoid layers. This can be done in several ways. In semantic hashing autoencoders BIBREF3 added noise to the sigmoid coding layer. Error backpropagation then countered the noise, by forcing the activations to be close to 0 or 1. Another approach was used by BIBREF12 in autoencoders that learned binary codes for small images. During the forward pass, activations in the coding layer were rounded to 0 or 1. Original (i.e. not rounded) activations were used when backpropagating errors. Alternatively, one could model the document codes with stochastic binary neurons. Learning in this case can still proceed with error backpropagation, provided that a suitable gradient estimator is used alongside stochastic activations. We experimented with the methods used in semantic hashing and Krizhevsky's autoencoders, as well as with the two biased gradient estimators for stochastic binary neurons discussed by BIBREF13 . We also investigated the slope annealing trick BIBREF14 when training networks with stochastic binary activations. From our experience, binary paragraph vector models with rounded activations are easy to train and learn better codes than models with noise-based binarization or stochastic neurons. We therefore use Krizhevsky's binarization in our models."
            ],
            "predicted_section": [
                "Binary paragraph vector models",
                "Introduction"
            ]
        },
        "eafea4a24d103fdecf8f347c7d84daff6ef828a3": {
            "question_text": "Which training dataset do they use?",
            "from_paper": "1611.01116",
            "gold": [
                "To assess the performance of binary paragraph vectors, we carried out experiments on three datasets: 20 Newsgroups, a cleansed version (also called v2) of Reuters Corpus Volume 1 (RCV1) and English Wikipedia. As paragraph vectors can be trained with relatively large vocabularies, we did not perform any stemming of the source text. However, we removed stop words as well as words shorter than two characters and longer than 15 characters. Results reported by BIBREF15 indicate that performance of PV-DBOW can be improved by including n-grams in the model. We therefore evaluated two variants of Binary PV-DBOW: one predicting words in documents and one predicting words and bigrams. Since 20 Newsgroups is a relatively small dataset, we used all words and bigrams from its documents. This amounts to a vocabulary with slightly over one million elements. For the RCV1 dataset we used words and bigrams with at least 10 occurrences in the text, which gives a vocabulary with approximately 800 thousands elements. In case of English Wikipedia we used words and bigrams with at least 100 occurrences, which gives a vocabulary with approximately 1.5 million elements.",
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents."
            ],
            "gold_section": [
                "Introduction",
                "Experiments"
            ],
            "predicted": [
                "We use AdaGrad BIBREF17 for training and inference in all experiments reported in this work. During training we employ dropout BIBREF18 in the embedding layer. To facilitate models with large vocabularies, we approximate the gradients with respect to the softmax logits using the method described by BIBREF9 . Binary PV-DM networks use the same number of dimensions for document codes and word embeddings.",
                "The 20 Newsgroups dataset comes with reference train/test sets. In case of RCV1 we used half of the documents for training and the other half for evaluation. In case of English Wikipedia we held out for testing randomly selected 10% of the documents. We perform document retrieval by selecting queries from the test set and ordering other test documents according to the similarity of the inferred codes. We use Hamming distance for binary codes and cosine similarity for real-valued representations. Results are averaged over queries. We assess the performance of our models with precision-recall curves and two popular information retrieval metrics, namely mean average precision (MAP) and the normalized discounted cumulative gain at the 10th result (NDCG@10) BIBREF16 . The results depend, of course, on the chosen document relevancy measure. Relevancy measure for the 20 Newsgroups dataset is straightforward: a retrieved document is relevant to the query if they both belong to the same newsgroup. In RCV1 each document belongs to a hierarchy of topics, making the definition of relevancy less obvious. In this case we adopted the relevancy measure used by BIBREF3 . That is, the relevancy is calculated as the fraction of overlapping labels in a retrieved document and the query document. Overall, our selection of test datasets and relevancy measures for 20 Newsgroups and RCV1 follows BIBREF3 , enabling comparison with semantic hashing codes. To assess the relevancy of articles in English Wikipedia we can employ categories assigned to them. However, unlike in RCV1, Wikipedia categories can have multiple parent categories and cyclic dependencies. Therefore, for this dataset we adopted a simplified relevancy measure: two articles are relevant if they share at least one category. We also removed from the test set categories with less than 20 documents as well as documents that were left with no categories. Overall, the relevancy is measured over more than INLINEFORM0 categories, making English Wikipedia harder than the other two benchmarks.",
                "Binary codes have also been applied to cross-modal retrieval where text is one of the modalities. Specifically, BIBREF4 incorporated tag information that often accompany text documents, while BIBREF5 employed siamese neural networks to learn single binary representation for text and image data."
            ],
            "predicted_section": [
                "Introduction",
                "Experiments"
            ]
        },
        "e099a37db801718ab341ac9a380a146c7452fd21": {
            "question_text": "Do they analyze the produced binary codes?",
            "from_paper": "1611.01116",
            "gold": [
                "In this work we present Binary Paragraph Vector models, an extensions to PV-DBOW and PV-DM that learn short binary codes for text documents. One inspiration for binary paragraph vectors comes from a recent work by BIBREF11 on learning binary codes for images. Specifically, we introduce a sigmoid layer to the paragraph vector models, and train it in a way that encourages binary activations. We demonstrate that the resultant binary paragraph vectors significantly outperform semantic hashing codes. We also evaluate binary paragraph vectors in transfer learning settings, where training and inference are carried out on unrelated text corpora. Finally, we study models that simultaneously learn short binary codes for document filtering and longer, real-valued representations for ranking. While BIBREF11 employed a supervised criterion to learn image codes, binary paragraph vectors remain unsupervised models: they learn to predict words in documents.",
                "Visualization of Binary PV codes",
                "For an additional comparison with semantic hashing, we used t-distributed Stochastic Neighbor Embedding BIBREF23 to construct two-dimensional visualizations of codes learned by Binary PV-DBOW with bigrams. We used the same subsets of newsgroups and RCV1 topics that were used by BIBREF3 . Codes learned by Binary PV-DBOW (Figure FIGREF20 ) appear slightly more clustered."
            ],
            "gold_section": [
                "Visualization of Binary PV codes",
                "Introduction"
            ],
            "predicted": [
                "Binary codes have also been applied to cross-modal retrieval where text is one of the modalities. Specifically, BIBREF4 incorporated tag information that often accompany text documents, while BIBREF5 employed siamese neural networks to learn single binary representation for text and image data.",
                "In this article we presented simple neural networks that learn short binary codes for text documents. Our networks extend Paragraph Vector by introducing a sigmoid nonlinearity before the softmax that predicts words in documents. Binary codes inferred with the proposed networks achieve higher retrieval precision than semantic hashing codes on two popular information retrieval benchmarks. They also retain a lot of their precision when trained on an unrelated text corpus. Finally, we presented a network that simultaneously learns short binary codes and longer, real-valued representations.",
                "In this work we focus on learning binary codes for text documents. An important work in this direction has been presented by BIBREF3 . Their semantic hashing leverages autoencoders with sigmoid bottleneck layer to learn binary codes from a word-count bag-of-words (BOW) representation. Salakhutdinov & Hinton report that binary codes allow for up to 20-fold improvement in document ranking speed, compared to real-valued representation of the same dimensionality. Moreover, they demonstrate that semantic hashing codes used as an initial document filter can improve precision of TF-IDF-based retrieval. Learning binary representation from BOW, however, has its disadvantages. First, word-count representation, and in turn the learned codes, are not in itself stronger than TF-IDF. Second, BOW is an inefficient representation: even for moderate-size vocabularies BOW vectors can have thousands of dimensions. Learning fully-connected autoencoders for such high-dimensional vectors is impractical. Salakhutdinov & Hinton restricted the BOW vocabulary in their experiments to 2000 most frequent words."
            ],
            "predicted_section": [
                "Introduction",
                "Conclusion"
            ]
        },
        "9da181ac8f2600eb19364c1b1e3cdeb569811a11": {
            "question_text": "Where does their information come from?",
            "from_paper": "1908.10001",
            "gold": [
                "The hotel search is backed by a database of approximately 100,000 cities and 300,000 hotels, populated using data from our partners. Each database entry contains the name of the city or hotel, geographic information (e.g., address, state, country), and various metadata (e.g., review score, number of bookings).",
                "We collect labelled training data from two sources. First, data for the intent model is extracted from conversations between users and customer support agents. To save time, the model suggests a pre-written response to the user, which the agent either accepts by clicking a button, or composes a response from scratch. This action is logged, and after being checked by a professional annotator, is added to our training data.",
                "Second, we employ professional annotators to create training data for each of our models, using a custom-built interface. A pool of relevant messages is selected from past user conversations; each message is annotated once and checked again by a different annotator to minimize errors. We use the PyBossa framework to manage the annotation processes."
            ],
            "gold_section": [
                "Chatbot architecture ::: Data labelling",
                "Chatbot architecture"
            ],
            "predicted": [
                "We collect labelled training data from two sources. First, data for the intent model is extracted from conversations between users and customer support agents. To save time, the model suggests a pre-written response to the user, which the agent either accepts by clicking a button, or composes a response from scratch. This action is logged, and after being checked by a professional annotator, is added to our training data.",
                "We first apply NER to extract the relevant parts of the query. Then, we use ElasticSearch to quickly retrieve a list of potentially relevant matches from our large database of cities and hotels, using tf-idf weighted n-gram matching. Finally, we train a neural network to rank the ElasticSearch results for relevancy, given the user query and the official hotel name.",
                "The models are trained on 9K search messages, with up to 10 results from ElasticSearch and annotations for which results are valid matches. Each training row is expanded into multiple message-result pairs, which are fed as instances to the network. For the BERT model, we use the uncased BERT-base, which requires significantly less memory than BERT-large. All models are trained end-to-end and implemented using AllenNLP BIBREF8."
            ],
            "predicted_section": [
                "Chatbot architecture ::: Data labelling",
                "Models ::: Information retrieval"
            ]
        },
        "5f25b57a1765682331e90a46c592a4cea9e3a336": {
            "question_text": "Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?",
            "from_paper": "1912.04979",
            "gold": [
                "Our vision processing module (see Fig. FIGREF1) locates and identifies all persons in a room for each frame captured by the camera. The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable.",
                "To handle this variability, we integrate information across time using face tracking as implied by our formulation of $P(h | r, V)$, which requires face identification to be performed only at a tracklet level. Our face tracking uses face detection and low-level tracking to maintain a set of tracklets, where each tracklet is defined as a sequence of faces in time that belong to the same person. We use a method similar to that in BIBREF50 with several adaptions to our specific setting, such as exploiting the stationarity of the camera for detecting motion, performing the low-level tracking by color based mean-shift instead of gray-level based normalized correlation, tuning the algorithm to minimize the risk of tracklet mergers (which in our context are destructive), etc. Also, the faces in each tracklet are augmented with attributes such as face position, dimensions, head pose, and face feature vectors. The tracklet set defines $\\mathcal {R}$ of equation (DISPLAY_FORM7).",
                "Face identification calculates person ID posterior probabilities for each tracklet. Guest IDs (e.g., 'Speaker1') are produced online, each representing a unique person in the meeting who is not on the invitee list. We utilize a discriminative face embedding which converts face images into fixed-dimensional feature vectors, or 128-dimensional vectors obtained as output layer activations of a convolutional neural network. For the face embedding and detection components, we use the algorithms from Microsoft Cognitive Services Face API BIBREF51, BIBREF52. Face identification of a tracklet is performed by comparing the set of face features extracted from its face instances, to the set of features from a gallery of each person's faces. For invited people, the galleries are taken from their enrollment videos, while for guests, the gallery pictures are accumulated online from the meeting video. We next describe our set-to-set similarity measure designed to perform this comparison.",
                "Our set-to-set similarity is designed to utilize information from multiple frames while remaining robust to head pose, lighting conditions, blur and other misleading factors. We follow the matched background similarity (MBGS) approach of BIBREF53 and make crucial adaptations to it that increase accuracy significantly for our problem. As with MBGS, we train a discriminative classifier for each identity $h$ in $\\mathcal {H}$. The gallery of $h$ is used as positive examples, while a separate fixed background set $B$ is used as negative examples. This approach has two important benefits. First, it allows us to train a classifier adapted to a specific person. Second, the use of a background set $B$ lets us account for misleading sources of variation e.g. if a blurry or poorly lit face from $B$ is similar to one of the positive examples, the classifier's decision boundary can be chosen accordingly. During meeting initialization, an support vector machine (SVM) classifier is trained to distinguish between the positive and negative sets for each invitee. At test time, we are given a tracklet $T=\\big \\lbrace \\mathbf {t}_1,...,\\mathbf {t}_N\\big \\rbrace $ represented as a set of face feature vectors $\\mathbf {t}_i\\in {\\mathbb {R}^d}$, and we classify each member $\\mathbf {t}_i$ with the classifier of each identity $h$ and obtain a set of classification confidences $\\big \\lbrace s\\big (T\\big )_{i,h}\\big \\rbrace $. Hereinafter, we omit argument $T$ for brevity. We now aggregate the scores of each identity to obtain the final identity scores $s_h=\\text{stat}\\big (\\big \\lbrace s_{i,h}\\big \\rbrace _{i=1}^N\\big )$ where $\\text{stat}(\\cdot )$ represents aggregation by e.g. taking the mean confidence. When $s=\\max _{h} s_h$ is smaller than a threshold, a new guest identity is added to $\\mathcal {H}$, where the classifier for this person is trained by using $T$ as positive examples. $\\lbrace s_h\\rbrace _{h \\in \\mathcal {H}}$ is converted to a set of posterior probabilities $\\lbrace P(h | r, V)\\rbrace _{h \\in \\mathcal {H}}$ with a trained regression model.",
                "The SSL generative model, $p(A_s | r; M)$, is defined by using a complex angular central Gaussian model (CACGM) BIBREF45. The SSL generative model can be written as follows:",
                "Speaker Diarization ::: Sound source localization",
                "$A$ and $V$ are the audio and video signals, respectively. $M$ is the set of the TF masks of the current CSS channel within the input segment. The speaker ID inventory, $\\mathcal {H}$, consists of the invited speaker names (e.g., `Alice' or `Bob') and anonymous `guest' IDs produced by the vision module (e.g., `Speaker1' or `Speaker2'). In what follows, we propose a model for combining face tracking, face identification, speaker identification, SSL, and the TF masks generated by the preceding CSS module to calculate the speaker ID posterior probability of equation (DISPLAY_FORM5). The integration of these complementary cues would make speaker attribution robust to real world challenges, including speech overlaps, speaker co-location, and the presence of guest speakers.",
                "First, by treating the face position trajectory of the speaking person as a latent variable, the speaker ID posterior probability can be represented as",
                "where $\\mathcal {R}$ includes all face position trajectories detected by the face tracking module within the input period. We call a face position trajectory a tracklet. The joint posterior probability on the right hand side (RHS) can be factorized as",
                "The RHS first term, or the tracklet-conditioned speaker ID posterior, can be further decomposed as",
                "The RHS first term, calculating the speaker ID posterior given the video signal and the tracklet calls for a face identification model because the video signal and the tracklet combine to specify a single speaker's face. On the other hand, the likelihood term on the RHS can be calculated as",
                "where we have assumed the spatial and magnitude features of the audio, represented as $A_s$ and $A_m$, respectively, to be independent of each other. The RHS first term, $p(A_s | h; M)$, is a spatial speaker model, measuring the likelihood of speaker $h$ being active given spatial features $A_s$. We make no assumption on the speaker positions. Hence, $p(A_s | h; M)$ is constant and can be ignored. The RHS second term, $p(A_m | h; M)$, is a generative model for speaker identification.",
                "Returning to (DISPLAY_FORM8), the RHS second term, describing the probability of the speaking person's face being $r$ (recall that each tracklet captures a single person's face), may be factorized as",
                "The first term is the likelihood of tracklet $r$ generating a sound with spatial features $A_s$ and therefore related to SSL. The second term is the probability with which the tracklet $r$ is active given the audio magnitude features and the video. Calculating this requires lip sync to be performed for each tracklet, which is hard in our application due to low resolution resulting from speaker-to-camera distances and compression artifacts. Thus, we ignore this term.",
                "Putting the above equations together, the speaker-tracklet joint posterior needed in (DISPLAY_FORM7) can be obtained as",
                "where the ingredients of the RHS relate to face identification, speaker identification, and SSL, respectively, in the order of appearance. The rest of this section describes our implementations of these models."
            ],
            "gold_section": [
                "Speaker Diarization ::: Sound source localization",
                "Speaker Diarization ::: Face tracking and identification",
                "Speaker Diarization"
            ],
            "predicted": [
                "Our vision processing module (see Fig. FIGREF1) locates and identifies all persons in a room for each frame captured by the camera. The unconstrained meeting scenario involves many challenges, including face occlusions, extreme head pose, lighting conditions, compression artifacts, low resolution due to device-to-person distances, motion blur. Therefore, any individual frame may not contain necessary information. For example, a face may not be detectable in some frames. Even if it is detectable, it may not be recognizable.",
                "To handle this variability, we integrate information across time using face tracking as implied by our formulation of $P(h | r, V)$, which requires face identification to be performed only at a tracklet level. Our face tracking uses face detection and low-level tracking to maintain a set of tracklets, where each tracklet is defined as a sequence of faces in time that belong to the same person. We use a method similar to that in BIBREF50 with several adaptions to our specific setting, such as exploiting the stationarity of the camera for detecting motion, performing the low-level tracking by color based mean-shift instead of gray-level based normalized correlation, tuning the algorithm to minimize the risk of tracklet mergers (which in our context are destructive), etc. Also, the faces in each tracklet are augmented with attributes such as face position, dimensions, head pose, and face feature vectors. The tracklet set defines $\\mathcal {R}$ of equation (DISPLAY_FORM7).",
                "The adaptations we make over the original MBGS are as follows."
            ],
            "predicted_section": [
                "Speaker Diarization ::: Face tracking and identification"
            ]
        },
        "d147117ef24217c43252d917d45dff6e66ff807c": {
            "question_text": "How do they model external knowledge? ",
            "from_paper": "1712.00733",
            "gold": [
                "Given an image, we apply the Fast-RCNN BIBREF27 to detect the visual objects of the input image, and extract keywords of the corresponding questions with syntax analysis. Based on these information, we propose to learn a mechanism to retrieve the candidate knowledge by querying the large-scale knowledge graph, yielding a subgraph of relevant knowledge to facilitate the question answering. During the past years, a substantial amount of large-scale knowledge bases have been developed, which store common sense and factual knowledge in a machine readable fashion. In general, each piece of structured knowledge is represented as a triple $(subject, rel, object)$ with $subject$ and $object$ being two entities or concepts, and $rel$ corresponding to the specific relationship between them. In this paper, we adopt external knowledge mined from ConceptNet BIBREF28 , an open multilingual knowledge graph containing common-sense relationships between daily words, to aid the reasoning of open-domain VQA.",
                "In general, the underlying symbolic nature of a Knowledge Graph (KG) makes it difficult to integrate with DNNs. The usual knowledge graph embedding models such as TransE BIBREF26 focus on link prediction, which is different from VQA task aiming to fuse knowledge. To tackle this issue, we propose to embed the entities and relations of a KG into a continuous vector space, such that the factual knowledge can be used in a more simple manner. Each knowledge triple is treated as a three-word SVO $(subject, verb, object)$ phrase, and embedded into a feature space by feeding its word-embedding through an RNN architecture. In this case, the proposed knowledge embedding feature shares a common space with other textual elements (questions and answers), which provides an additional advantage to integrate them more easily."
            ],
            "gold_section": [
                "Our Proposal",
                "Overview"
            ],
            "predicted": [
                "We further make comprehensive comparisons among our ablative models. To make it fair, all the experiments are implemented on the same basic network structure and share the same hyper-parameters. In general, our KDMN model on average gains $1.6\\%$ over the KDMN-NoMem model and $4.0\\%$ over the KDMN-NoKG model, which further implies the effectiveness of dynamic memory networks in exploiting external knowledge. Through iterative attention processes, the episodic memory vector captures background knowledge distilled from external knowledge embeddings. The KDMN-NoMem model gains $2.4\\%$ over the KDMN-NoKG model, which implies that the incorporated external knowledge brings additional advantage, and act as a supplementary information for predicting the final answer. The indicative examples in Fig. 3 also demonstrate the impact of external knowledge, such as the 4th example of \u201cWhy is the light red?\u201d. It would be helpful if we could retrieve the function of the traffic lights from the external knowledge effectively.",
                "KDMN: our full model. External knowledge triples are incorporated in Dynamic Memory Network.",
                "Once the massive external knowledge is integrated into the model, it is imperative to provide a flexible mechanism to store a richer representation. The memory network, which contains scalable memory with a learning component to read from and write to it, allows complex reasoning by modeling interaction between multiple parts of the data BIBREF20 , BIBREF25 . In this paper, we adopt the most recent advance of Improved Dynamic Memory Networks (DMN+) BIBREF25 to implement the complex reasoning over several facts. Our model provides a mechanism to attend to candidate knowledge embedding in an iterative manner, and fuse it with the multi-modal data including image, text and knowledge triples in the memory component. The memory vector therefore memorizes useful knowledge to facilitate the prediction of the final answer. Compared with the DMN+ BIBREF25 , we introduce the external knowledge into the memory network, and endows the system an ability to answer open-domain question accordingly."
            ],
            "predicted_section": [
                "Results and Analysis",
                "Implementation Details",
                "Our Proposal"
            ]
        },
        "59a3d4cdd1c3797962bf8d72c226c847e06e1d44": {
            "question_text": "What are the post-processing approaches applied to the output?",
            "from_paper": "1908.05925",
            "gold": [
                "The quotes are fixed to keep them the same as the source sentences.",
                "For all the models mentioned above that work under a lower-case setting, a recaser implemented with Moses BIBREF22 is applied to convert the translations to the real cases.",
                "From our observation, the ensemble NMT model lacks the ability to translate name entities correctly. We find that words with capital characters are named entities, and those named entities in the source language may have the same form in the target language. Hence, we capture and copy these entities at the end of the translation if they does not exist in our translation."
            ],
            "gold_section": [
                "Experiments ::: Data Post-processing ::: Recaser",
                "Experiments ::: Data Post-processing ::: Quotes Fixing",
                "Experiments ::: Data Post-processing ::: Patch-up"
            ],
            "predicted": [
                "In the pre-processing, we use the special tokens <NUMBER> and <DATE> to replace numbers that express a specific quantity and date respectively. Therefore, in the post-processing, we need to restore those numbers. We simply detect the pattern <NUMBER> and <DATE> in the original source sentences and then replace the special tokens in the translated sentences with the corresponding numbers detected in the source sentences. In order to make the replacement more accurate, we will detect more complicated patterns like <NUMBER> / <NUMBER> in the original source sentences. If the translated sentences also have the pattern, we replace this pattern <NUMBER> / <NUMBER> with the corresponding numbers in the original source sentences.",
                "We note that in the corpus, there are tokens representing quantity or date. Therefore, we delexicalize the tokens using two special tokens: (1) <NUMBER> to replace all the numbers that express a specific quantity, and (2) <DATE> to replace all the numbers that express a date. Then, we retrieve these numbers in the post-processing. There are two advantages of data pre-processing. First, replacing numbers with special tokens can reduce vocabulary size. Second, the special tokens are more easily processed by the model.",
                "Instead of direct translation with NMT models, we generate several translation candidates using beam search with a beam size of five. We build the language model proposed by BIBREF18, BIBREF19 trained using a monolingual Czech dataset to rescore the generated translations. The scores are determined by the perplexity (PPL) of the generated sentences and the translation candidate with the lowest PPL will be selected as the final translation."
            ],
            "predicted_section": [
                "Methodology ::: Language Model Rescoring",
                "Experiments ::: Data Post-processing ::: Special Token Replacement",
                "Experiments ::: Data Pre-processing"
            ]
        },
        "30870a962cf88ac8c8e6b7b795936fd62214f507": {
            "question_text": "Which neural network architecture do they use for the dialog agent and user simulator?",
            "from_paper": "1709.06136",
            "gold": [
                "Figure 1 illustrates the design of the dialog agent. The dialog agent is capable of tracking dialog state, issuing API calls to knowledge bases (KB), and producing corresponding system actions and responses by incorporating the query results, which are key skill sets BIBREF26 in conducting task-oriented dialogs. State of the dialog agent is maintained in the LSTM BIBREF35 state and being updated after the processing of each turn. At the INLINEFORM0 th turn of a dialog, the dialog agent takes in (1) the previous agent output encoding INLINEFORM1 , (2) the user input encoding INLINEFORM2 , (3) the retrieved KB result encoding INLINEFORM3 , and updates its internal state conditioning on the previous agent state INLINEFORM4 . With the updated agent state INLINEFORM5 , the dialog agent emits (1) a system action INLINEFORM6 , (2) an estimation of the belief state, and (3) a pointer to an entity in the retrieved query results. These outputs are then passed to an NLG module to generate the final agent response.",
                "Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance."
            ],
            "gold_section": [
                "User Simulator",
                "Dialog Agent"
            ],
            "predicted": [
                "Our contribution in this work is two-fold. Firstly, we propose an iterative dialog policy learning method that jointly optimizes the dialog agent and the user simulator in end-to-end trainable neural dialog systems. Secondly, we design a novel neural network based user simulator for task-oriented dialogs that can be trained in a data-driven manner without requiring the design of complex rules.",
                "In supervised pre-training, the dialog agent and the user simulator are trained separately against dialog corpus. We use the same set of neural network model configurations for both agents. Hidden layer sizes of the dialog-level LSTM for dialog modeling and utterance-level LSTM for utterance encoding are both set as 150. We perform mini-batch training using Adam optimization method BIBREF41 . Initial learning rate is set as 1e-3. Dropout BIBREF42 ( INLINEFORM0 ) is applied during model training to prevent to model from over-fitting.",
                "Recently, people have proposed neural network based methods for task-oriented dialogs, motivated by their superior performance in modeling chit-chat type of conversations BIBREF24 , BIBREF1 , BIBREF2 , BIBREF25 . Bordes and Weston BIBREF26 proposed modeling task-oriented dialogs with a reasoning approach using end-to-end memory networks. Their model skips the belief tracking stage and selects the final system response directly from a list of response candidates. Comparing to this approach, our model explicitly tracks dialog belief state over the sequence of turns, as robust dialog state tracking has been shown BIBREF27 to boost the success rate in task completion. Wen et al. BIBREF16 proposed an end-to-end trainable neural network model with modularity connected system components. This system is trained in supervised manner, and thus may not be robust enough to handle diverse dialog situations due to the limited varieties in dialog corpus. Our system is trained by a combination of SL and deep RL methods, as it is shown that RL training may effectively improved the system robustness and dialog success rate BIBREF28 , BIBREF19 , BIBREF29 . Moreover, other than having separated dialog components as in BIBREF16 , we use a unified network for belief tracking, knowledge base (KB) operation, and dialog management, to fully explore the knowledge that can be shared among different tasks."
            ],
            "predicted_section": [
                "Training Procedure",
                "Related Work",
                "Introduction"
            ]
        },
        "7ece07a84635269bb19796497847e4517d1e3e61": {
            "question_text": "Do they create the basic dialog agent and basic user simulator separately?",
            "from_paper": "1709.06136",
            "gold": [
                "In the supervised pre-training stage, we train the dialog agent and the user simulator separately using task-oriented dialog corpora. In the RL training stage, we simulate dialogs between the two agents. The user simulator starts the conversation based on a sampled user goal. The dialog agent attempts to estimate the user's goal and complete the task with the user simulator by conducting multi-turn conversation. At the end of each simulated dialog, a reward is generated based on the level of task completion. This reward is used to further optimize the dialog policies of the two agents with RL."
            ],
            "gold_section": [
                "Proposed Framework"
            ],
            "predicted": [
                " Similar to the design of the dialog agent, INLINEFORM0 and INLINEFORM1 are MLPs with a single hidden layer and use INLINEFORM2 activation over their corresponding outputs.",
                "Our contribution in this work is two-fold. Firstly, we propose an iterative dialog policy learning method that jointly optimizes the dialog agent and the user simulator in end-to-end trainable neural dialog systems. Secondly, we design a novel neural network based user simulator for task-oriented dialogs that can be trained in a data-driven manner without requiring the design of complex rules.",
                "Figure 2 shows the design of the user simulator. User simulator is given a randomly sampled goal at the beginning of the conversation. Similar to the design of the dialog agent, state of the user simulator is maintained in the state of an LSTM. At the INLINEFORM0 th turn of a dialog, the user simulator takes in (1) the goal encoding INLINEFORM1 , (2) the previous user output encoding INLINEFORM2 , (3) the current turn agent input encoding INLINEFORM3 , and updates its internal state conditioning on the previous user state INLINEFORM4 . On the output side, the user simulator firstly emits a user action INLINEFORM5 based on the updated state INLINEFORM6 . Conditioning on this emitted user action and the user dialog state INLINEFORM7 , a set of slot values are emitted. The user action and slot values are then passed to an NLG module to generate the final user utterance."
            ],
            "predicted_section": [
                "User Simulator",
                "Introduction"
            ]
        },
        "54b25223ab32bf8d9205eaa8a570e99c683f0077": {
            "question_text": "What baselines do they compare to?",
            "from_paper": "1909.13466",
            "gold": [
                "Extensive experimentation over four language pairs of different dataset sizes (from small to large) with both word and sentence regularization. We show that using both ReWE and ReSE can outperform strong state-of-the-art baselines based on long short-term memory networks (LSTMs) and transformers.",
                "In this section, we describe the NMT model that has been used as the basis for the proposed regularizer. It is a neural encoder-decoder architecture with attention BIBREF1 that can be regarded as a strong baseline as it incorporates both LSTMs and transformers as modules. Let us assume that $\\textbf {x}:\\lbrace x_1 \\dots x_n\\rbrace $ is the source sentence with $n$ tokens and $\\textbf {y}:\\lbrace y_1 \\dots y_m\\rbrace $ is the target translated sentence with $m$ tokens. First, the words in the source sentence are encoded into their word embeddings by an embedding layer:"
            ],
            "gold_section": [
                "The Baseline NMT model",
                "Introduction"
            ],
            "predicted": [
                "We have carried out a number of experiments with both baselines. The scores reported are an average of the BLEU scores (in percentage points, or pp) BIBREF46 over the test sets of 5 independently trained models. Table TABREF44 shows the results over the en-fr dataset. In this case, the models with ReWE have outperformed the LSTM and transformer baselines consistently. The LSTM did not benefit from using BPE, but the transformer+ReWE with BPE reached $36.30$ BLEU pp (a $+0.99$ pp improvement over the best model without ReWE). For this dataset we did not use ReSE because French was the target language.",
                "For the eu-en dataset (Table TABREF46), the results show that, again, ReWE outperforms the baselines by a large margin. Moreover, ReWE+ReSE has been able to improve the results even further ($+3.15$ BLEU pp when using BPE and $+5.15$ BLEU pp at word level over the corresponding baselines). Basque is, too, a morphologically-rich language and using BPE has proved very beneficial ($+4.27$ BLEU pp over the best word-level model). As noted before, the eu-en dataset is very low-resource (less than $100,000$ sentence pairs) and it is more likely that the baseline models generalize poorly. Consequently, regularizers such as ReWE and ReSE are more helpful, with larger margins of improvement with respect to the baselines. On a separate note, the transformer has unexpectedly performed well below the LSTM on this dataset, and especially so with BPE. We speculate that it may be more sensitive than the LSTM to the dataset's much smaller size, or in need of more refined hyper-parameter tuning.",
                "The model is trained by minimizing the negative log-likelihood (NLL) which can be expressed as:"
            ],
            "predicted_section": [
                "Experiments ::: Results",
                "The Baseline NMT model"
            ]
        },
        "42279c3a202a93cfb4aef49212ccaf401a3f8761": {
            "question_text": "Which three variants of sequential validation are examined?",
            "from_paper": "1803.05160",
            "gold": [
                "Throughout our experiments we use only one training algorithm (subsection sec:data), and two performance measures (subsection sec:measures). During training, the performance of the trained model can be estimated only on the in-sample data. However, there are different estimation procedures which yield these approximations. In machine learning, a standard procedure is cross-validation, while for time-ordered data, sequential validation is typically used. In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc). The goal is to find the in-sample estimation procedure that best approximates the out-of-sample gold standard. The error an estimation procedure makes is defined as the difference to the gold standard.",
                "In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:",
                "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples,",
                "seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples,",
                "seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points.",
                "The Twitter data shares some characteristics of time series and some of static data. A time series is an array of observations at regular or equidistant time points, and the observations are in general dependent on previous observations BIBREF0 . On the other hand, Twitter data is time-ordered, but the observations are short texts posted by Twitter users at any time and frequency. It can be assumed that original Twitter posts are not directly dependent on previous posts. However, there is a potential indirect dependence, demonstrated in important trends and events, through influential users and communities, or individual user's habits. These long-term topic drifts are typically not taken into account by the sentiment analysis models."
            ],
            "gold_section": [
                "Methods and experiments",
                "Estimation procedures",
                "Introduction"
            ],
            "predicted": [
                "In sequential validation, a sample consists of the training set immediately followed by the test set. We vary the ratio of the training and test set sizes, and the number and distribution of samples taken from the in-set. The number of samples is 10 or 20, and they are distributed equidistantly or semi-equidistantly. In all variants, samples cover the whole in-set, but they are overlapping. See Figure FIGREF20 for illustration. We use the following abbreviations for sequential validations:",
                "Throughout our experiments we use only one training algorithm (subsection sec:data), and two performance measures (subsection sec:measures). During training, the performance of the trained model can be estimated only on the in-sample data. However, there are different estimation procedures which yield these approximations. In machine learning, a standard procedure is cross-validation, while for time-ordered data, sequential validation is typically used. In this study, we compare three variants of cross-validation and three variants of sequential validation (subsection sec:eval-proc). The goal is to find the in-sample estimation procedure that best approximates the out-of-sample gold standard. The error an estimation procedure makes is defined as the difference to the gold standard.",
                "We therefore compare two classes of estimation procedures. Cross-validation, commonly used in machine learning for model evaluation on static data, and sequential validation, commonly used for time-series data. There are many variants and parameters for each class of procedures. Our datasets are relatively large and an application of each estimation procedure takes several days to complete. We have selected three variants of each procedure to provide answers to some relevant questions."
            ],
            "predicted_section": [
                "Methods and experiments",
                "Estimation procedures"
            ]
        }
    }
}