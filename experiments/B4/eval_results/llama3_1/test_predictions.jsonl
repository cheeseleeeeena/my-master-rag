{"question_id": "397a1e851aab41c455c2b284f5e4947500d797f0", "predicted_evidence": ["None"], "predicted_answer": "The AntiScam dataset contains 220 human-human dialogs with an average conversation length of 12.45 turns and the average utterance length of 11.13 words."}
{"question_id": "cc8b4ed3985f9bfbe1b5d7761b31d9bd6a965444", "predicted_evidence": ["None"], "predicted_answer": "In the paper, a hierarchical intent annotation scheme is designed for non-collaborative tasks. On-task intents are key actions that can vary among different tasks and are specifically defined for each task. Off-task content is too general to design task-specific intents, and common dialog acts are used as universal categories. Intent is annotated as either on-task or off-task. For on-task content, the intent is determined by key actions specific to the task, while for off-task content, it is annotated with dialog acts such as \"open question\", \"yes_no_question\", \"positive_answer\", and so on."}
{"question_id": "f7662b11e87c1e051e13799413f3db459ac3e19c", "predicted_evidence": ["None"], "predicted_answer": "TransferTransfo and the hybrid model are the baselines that MISSA outperforms."}
{"question_id": "b584739622d0c53830e60430b13fd3ae6ff43669", "predicted_evidence": ["None"], "predicted_answer": "Automatic evaluation metrics: Perplexity, Response-Intent Prediction (RIP), Response-Slot Prediction (RSP), Extended Response-Intent Prediction (ERIP), and Extended Response-Slot Prediction (ERSP). Human evaluation metrics: Fluency, Coherence, Engagement, Dialog length (Length), and Task Success Score (TaskSuc)."}
{"question_id": "2849c2944c47cf1de62b539c5d3c396a3e8d283a", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly report the accuracy of this model compared to state-of-the-art (sota) but it mentions that the model achieves \"honourable recall\" and acceptable performance in domains like scientific affiliations and news, citing Twitter identifiers in Wikidata as a factor. The paper mentions its use of features and various methods as keeping the model competitive with other systems but does not provide a direct comparison.>>"}
{"question_id": "1a6156189297b2fe17f174ef55cbd20341bb7dbf", "predicted_evidence": ["None"], "predicted_answer": "Liu et. al (2015) and Yang et. al (2012)"}
{"question_id": "3319d56556ae1597a86384057db0831e32774b90", "predicted_evidence": ["None"], "predicted_answer": "The evaluation metrics used in their evaluation include accuracy and Detection Error Trade-off (DET) curves. Additionally, they measure the throughput per second to evaluate the efficiency of computing the proposed features."}
{"question_id": "8cbe3fa4ec0f66071e3d6b829b09b6395b631c44", "predicted_evidence": ["None"], "predicted_answer": "The paper does not state whether their methods are fully supervised or not. However, it is likely that their methods require some supervised training on labeled data, as they use an SVM classifier to compute the rumour score and optimise the weights and detection threshold on a training set."}
{"question_id": "85e417231a4bbb6691f7a89bd81710525f8fec4c", "predicted_evidence": ["None"], "predicted_answer": "The researchers created their own dataset for rumour detection, collecting 200 news articles and 404 micro-blogs (202 rumours and 202 non-rumours) from Sina Weibo, a Chinese social media service."}
{"question_id": "57ee20f494d8ce3fae46028c3f3551d180dba3e0", "predicted_evidence": ["None"], "predicted_answer": "Chinese"}
{"question_id": "2974237446d04da33b78ce6d22a477cdf80877b7", "predicted_evidence": ["None"], "predicted_answer": "We rely on classification using an SVM, which is the state-of-the-art approach for novelty detection. We define a rumour as information of doubtful or unconfirmed truth."}
{"question_id": "bc8526d4805e2554adb2e9c01736d3f3a3b19895", "predicted_evidence": ["None"], "predicted_answer": "LDA, Doc-NADE, HTMM, GMNTM"}
{"question_id": "a0fd0c0fe042ad045b8d5095c81643ef3a352b81", "predicted_evidence": ["None"], "predicted_answer": "Generative document evaluation, document classification, topic2sentence, and personalized short text conversation."}
{"question_id": "6e040e80f2da69d50386a90a38ed6d2fa4f77bbd", "predicted_evidence": ["None"], "predicted_answer": "OntoNotes 4.0 Chinese, OntoNotes 5.0, CoNLL2003, MSRA, Weibo NER, and Resume NER"}
{"question_id": "aebd1f0d728d0de5f76238844da044a44109f76f", "predicted_evidence": ["None"], "predicted_answer": "The authors incorporate direction and relative distance in attention by using the relative positional encoding instead of the sinusoidal position embedding in the vanilla Transformer. Relative positional encoding is defined as R_{t-j} = [sin(c_{2i} (t-j)/10000^{2i/d}) for i in [0, d_{k}/2] and cos(c_{2i} (t-j)/10000^{2i/d}) for i in [d_{k}/2, d_{k}]]. It is used to calculate the attention scores by Q_{t}^T R_{t-j}, where Q_{t} is the query vector of the tth token and j is the index of the context token. This allows the Transformer model to capture both directionality and distance awareness in the attention mechanism."}
{"question_id": "cb4086ad022197da79f28dc609d0de90108c4543", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "756a8a9125e6984e0ca768b653c6c760efa3db66", "predicted_evidence": ["None"], "predicted_answer": "KALM achieves an accuracy of 95.6% for knowledge authoring, 95% for parsing queries, and 100% accuracy for question answering on the MetaQA dataset."}
{"question_id": "fe52b093735bb456d7e699aa9a2b806d2b498ba0", "predicted_evidence": ["None"], "predicted_answer": "Attempto Controlled English, PENG, Memory Network, Variational Reasoning Network, ATHENA, and PowerAqua"}
{"question_id": "7748c072e07d6c6db5a34be38b4a5e97ac6d7999", "predicted_evidence": ["None"], "predicted_answer": "Two datasets were evaluated: a manually constructed general questions based on the 50 logical frames with KALM achieving an accuracy of 95% and the MetaQA dataset which contains almost 29,000 test questions and over 260,000 training questions."}
{"question_id": "c97306c1be5d59cf27b1054adfa8f1da47d292ce", "predicted_evidence": ["None"], "predicted_answer": "The contributions of this paper are: i.) the adaptation of a state-of-the-art sequence to sequence (seq2seq) neural system to event detection and classification for Italian in a single step approach; ii.) an investigation on the quality of existing Italian word embeddings for this task; iii.) a comparison against a state-of-the-art discrete classifier."}
{"question_id": "e42916924b69cab1df25d3b4e6072feaa0ba8084", "predicted_evidence": ["None"], "predicted_answer": "FBK-HLT, which is a cascade of two SVM classifiers (one for detection and one for classification) based on rich linguistic features."}
{"question_id": "079ca5810060e1cdc12b5935d8c248492f0478b9", "predicted_evidence": ["None"], "predicted_answer": "This is not explicitly mentioned in the paper, but it's mentioned in the INTRODUCTION section that event detection and classification has seen a growing interest in the NLP community thanks to the availability of annotated corpora for multiple languages, which implies that the model could be extended to other languages, possibly with some adaptations. However, further investigation is needed to confirm."}
{"question_id": "a3e7d7389228a197c8c44e0c504a791b60f2c80d", "predicted_evidence": ["None"], "predicted_answer": "For a particular cluster, they decide the semantic concept label by taking the union of all hypernyms of the synsets in S*, where S* is the collection of chosen synsets that minimizes the total distance between its synsets."}
{"question_id": "8b4bd0a962241ea548752212ebac145e2ced7452", "predicted_evidence": ["None"], "predicted_answer": "They discover coherent word clusters by training domain-specific word embeddings using the Word2Vec CBOW model and then using k-means clustering on the embeddings of the gender-associated words."}
{"question_id": "d39059340a79bdc0ebab80ad3308e3037d7d5773", "predicted_evidence": ["None"], "predicted_answer": "Each dataset introduced in the paper consists of over 300K sentences.>> ¿"}
{"question_id": "31d4b0204702907dc0cd0f394cf9c984649e1fbf", "predicted_evidence": ["None"], "predicted_answer": "The centroid of the cluster is used as a strong baseline label."}
{"question_id": "371433bd3fb5042bacec4dfad3cfff66147c14f0", "predicted_evidence": ["None"], "predicted_answer": "Data-driven systems rank low in general. Neuralconvo and Cleverbot are the only two that ever politely refuse (17%). The “clean” seq2seq often produces responses which can be interpreted as flirtatious (44%), and ranks similarly to Annabelle Lee and Laurel Sweet, the only adult bots that politely refuses (16% of the time) or Dr Love and Sophia69, which produce almost exclusively flirtatious responses which are consistently ranked low by users"}
{"question_id": "f64449a21c452bc5395a0f0a49fb49825e6385f4", "predicted_evidence": ["None"], "predicted_answer": "9960 HITs (Human Intelligence Tasks) were collected from 472 crowd workers."}
{"question_id": "3aeb25e334c8129b376f11c7077bcb2dd54f7e0e", "predicted_evidence": ["None"], "predicted_answer": "14"}
{"question_id": "c19e9fd2f1c969e023fb99b74e78eb1f3db8e162", "predicted_evidence": ["None"], "predicted_answer": "The automatic annotation of time expressions was evaluated using a customized version of HeidelTime, which achieved an F$_1$ value of 89.1 for partial identification and normalization."}
{"question_id": "230ff86b7b90b87c33c53014bb1e9c582dfc107f", "predicted_evidence": ["None"], "predicted_answer": "agglutinative and fusional languages"}
{"question_id": "dc23006d67f20f430f1483398de4a89c0be4efe2", "predicted_evidence": ["None"], "predicted_answer": "The model considers inflectional and derivational group boundaries in the original semantic annotation format, but derives the lower-case tokens across these boundaries during preprocessing.\""}
{"question_id": "887d7f3edf37ccc6bf2e755dae418b04d2309686", "predicted_evidence": ["None"], "predicted_answer": "They use morphological features such as lemma of the token followed by language specific morphological tags, with the exception of Turkish where it also outputs additional information including parts-of-speech tags."}
{"question_id": "b8a3ab219be6c1e6893fe80e1fbf14f0c0c3c97c", "predicted_evidence": ["None"], "predicted_answer": "The datasets used in this paper are: Semantic Scholar corpus, Springer Nature's SciGraph, Textbook Question Answering corpus, and Wikipedia Janeiro 2018 English dataset."}
{"question_id": "780c7993d446cd63907bb38992a60bbac9cb42b1", "predicted_evidence": ["None"], "predicted_answer": "Scientific captions are in English. The paper mentions that it uses WordNet, Sensigrafo, and Wikipedia (all of which are in English) for training and studying_VECsigrafo. Additionally, it is extracting captions and figures from Semantic Scholar, Springer Nature's SciGraph, and other English scientific publications. From, the provided information that the captions are from and being used with the English language corpora such as Entity Embeddings, Word, concepts and Sensigrafo indicate that language used by the captions are English."}
{"question_id": "3da4606a884593f7702d098277b9a6ce207c080b", "predicted_evidence": ["None"], "predicted_answer": "Concatenation, summation, multiplication, average, learning a task-specific weighing of the different representations, and learning word, lemma and concept embeddings."}
{"question_id": "91336f12ab94a844b66b607f8621eb8bbd209f32", "predicted_evidence": ["None"], "predicted_answer": "The supervised baselines were two in number: (1) direct combination of the feature extraction trunks of the two trained networks and (2) supervising pre-training, which freezes the weights of the feature extraction trunks and trains the FCC task on the fully connected layers."}
{"question_id": "c5221bb28e58a4f13cf2eccce0e1b1bec7dd3c13", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "42a4ab4607a9eec42c427a817b7e898230d26444", "predicted_evidence": ["None"], "predicted_answer": "The figure and captions come from scientific publications, specifically the SN SciGraph and AI2 Semantic Scholar."}
{"question_id": "622efbecd9350a0f4487bdff2b8b362ef2541f3c", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "f54e19f7ecece1bb0ef3171403ae322ad572ff00", "predicted_evidence": ["None"], "predicted_answer": " No, the authors concluded that they do not experiment with any additional sources of information as input, including lexicons, as their focus was to study sentence representation for emotion intensity, which they do not delve into in that work. >>"}
{"question_id": "4137a82d7752be7a6c142ceb48ce784fd475fb06", "predicted_evidence": ["None"], "predicted_answer": "The model that weka baseline mentioned corresponds to Weka."}
{"question_id": "6c50871294562e4886ede804574e6acfa8d1a5f9", "predicted_evidence": ["None"], "predicted_answer": "Our model obtained the 13th place among 22 competitors in the shared task."}
{"question_id": "0ac6fbd81e2dd95b800283dc7e59ce969d45fc02", "predicted_evidence": ["None"], "predicted_answer": "The datasets used were the training, validation, and test datasets provided for the WASSA-2017 Shared Task on Emotion Intensity, which include tweets for four emotions: joy, sadness, fear, and anger."}
{"question_id": "ed44f7e698d6124cb86791841d02fc6f8b4d862a", "predicted_evidence": ["None"], "predicted_answer": " Hate speech is defined as direct or indirect language that promotes the dehumanization of people from minority groups and can incite hate crime. It is not necessarily equivalent to profanity and can be expressed in various forms, such as slurs, metaphors, or sarcasm. >> coaching"}
{"question_id": "d9e7633004ed1bc1ee45be58409bcc1fa6db59b2", "predicted_evidence": ["None"], "predicted_answer": "English, French, and Arabic"}
{"question_id": "c58ef13abe5fa91a761362ca962d7290312c74e4", "predicted_evidence": ["None"], "predicted_answer": "our dataset encompasses five important aspects: (a) whether the text is direct or indirect; (b) the type of hostility it conveys (abusive, hateful, offensive, disrespectful, fearful, or normal); (c) the attribute based on which it discriminates against people (origin, religious affiliation, gender, sexual orientation, special needs, or other); (d) the name of the target group; and (e) how the annotators feel about its content within a range of negative to neutral sentiments"}
{"question_id": "9ef0d2365bde0d18054511fbb53cec5fa2cda5ee", "predicted_evidence": ["None"], "predicted_answer": "5,647 English tweets, 4,014 French tweets, and 3,353 Arabic tweets"}
{"question_id": "cbb3c1c1e6e1818b6480f929f1c299eaa5ffd07a", "predicted_evidence": ["None"], "predicted_answer": "According to the research, the opportunities presented by the use of Semantic Web technologies (SWT) in Machine Translation include resolving semantic disambiguation of polysemous and homonymous words, addressing named entities, and translating knowledge bases, as well as providing unseen knowledge about emergent data, context modeling, and entity recognition. Also, SWT can be applied in different ways to support MT, such as in pre-editing and post-editing techniques, message alignment, disambiguation, and generation of target language text"}
{"question_id": "9f74f3991b8681619d95ab93a7c8733a843ddffe", "predicted_evidence": ["None"], "predicted_answer": "The challenges associated with the use of Semantic Web technologies in Machine Translation include:\n\n- Disambiguation, specifically syntactic and semantic variations\n- Ambiguity in named entities and words\n- Non-standard speech and idioms\n- Translating knowledge bases across languages\n\nThese challenges were identified as areas where Semantic Web technologies could potentially provide some solutions, but further research is needed to effectively address them."}
{"question_id": "7c2c15ea3f1b1375b8aaef1103a001069d9915bb", "predicted_evidence": ["None"], "predicted_answer": "Although not mentioned explicitly, addressing the challenges posed by the huge amount of data from social networks and morphologically rich languages is still an issue for MT as pointed out implicitly in the provided detailed description of selected challenges in the \"Open MT Challenges\" section."}
{"question_id": "a77d38427639d54461ae308f3045434f81e497d0", "predicted_evidence": ["None"], "predicted_answer": "Five statistical features for EEG: root mean square, zero crossing rate, moving window average, kurtosis, and power spectral entropy."}
{"question_id": "010fd15696580d9924ac0275a4ff269005e5808d", "predicted_evidence": ["None"], "predicted_answer": "The model \"Decoding imagined speech from EEG using synthetic EEG data and connectionist temporal classification (CTC) network\" in BIBREF3 is not a direct baseline, but it serves as a related work inspiration, mentioned in the paper. Another related work in BIBREF4 performs envisioned speech recognition using a random forest classifier, contrasting with the paper's use of end-to-end state-of-art models."}
{"question_id": "d36a6447bfe58204e0d29f9213d84be04d875624", "predicted_evidence": ["None"], "predicted_answer": "The USC-TIMIT database and the in-house collected speech EEG recording databases (referred to as Data Set A and Data Set B) were used."}
{"question_id": "5ed02ae6c534cd49d405489990f0e4ba0330ff1b", "predicted_evidence": ["None"], "predicted_answer": "On the MNLI-m dataset, LadaBERT-1 outperforms BERT-PKD (the knowledge distillation teacher) by a relative 8.9 percentage points. On MNLI-mm, LadaBERT-1 achieves an improvement of 8.1 percentage points over BERT-PKD. LadaBERT-3, with a comparable model size to TinyBERT, surpasses TinyBERT on most of the datasets mentioned in the paper, including MNLI. This implies that LadaBERT can outperform its knowledge distillation teacher, BERT-PKD, in certain cases, particularly when fine-tuned on smaller architectures. However, this does not exclusively discuss scenarios where LadaBERT's performance exceeds its teacher's accuracy, but suggests LadaBERT's potential for overtaking in specific contexts. This assessment is made under the particular implementation and configurations presented within the paper, wherein the algorithms and architectures outperform BERT-PKD on specific datasets at reduced absolute size. Therefore, to be accurate, based on the current readings of the text;"}
{"question_id": "f6346828c2f44529dc307abf04dd246bfeb4a9b2", "predicted_evidence": ["None"], "predicted_answer": "LadaBERT, a hybrid compression framework, involves combining the strengths of matrix factorization, weight pruning, and knowledge distillation. The paper evaluates the effectiveness of each method by comparing the performance of LadaBERT with different baselines on various datasets, but it does not explicitly evaluate which compression method yields the most gains in isolation."}
{"question_id": "935873b97872820b7b6100d6a785fba286b94900", "predicted_evidence": ["None"], "predicted_answer": "Five public datasets of natural language understanding, including sentiment classification (SST-2), natural language inference (MNLI-m, MNLI-mm, QNLI) and pairwise semantic equivalence (QQP)"}
{"question_id": "f2bcfdbebb418e7da165c19b8c7167719432ee48", "predicted_evidence": ["None"], "predicted_answer": "News articles from the DailyMail website"}
{"question_id": "0fe49431db5ffaa24372919daf24d8f84117bfda", "predicted_evidence": ["None"], "predicted_answer": "DUC 2002 document summarization corpus and the DailyMail news highlights corpus and their own corpus"}
{"question_id": "0f9c1586f1b4b531fa4fd113e767d06af90b1ae8", "predicted_evidence": ["None"], "predicted_answer": "Our summarization model outperforms a purely abstractive model (nn-abs) in terms of Rouge scores, with the word extraction model (nn-we) consistently outperforming it."}
{"question_id": "52faf319e37aa15fff1ab47f634a5a584dc42e75", "predicted_evidence": ["None"], "predicted_answer": "Hierarchical data and relations such as Is-A and entailment, which describe ontologies of hierarchical data, a key component of commonsense knowledge. Additionallly, they also focus on capturing relations like hypernymy and meronymy."}
{"question_id": "0c7cb3010ed92b8d46583a67e72946a6c0115f1f", "predicted_evidence": ["None"], "predicted_answer": "Researchers have used the intrinsic geometry of the embedding space to create embedding spaces that automatically enforce consistency in predictions, making it easier to ensure internal consistency in predictions using the geometry of the embedding space"}
{"question_id": "9c2cacf77041e02d38f92a4c490df1e04552f96f", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "35cdaa0fff007add4a795850b139df80af7d1ffc", "predicted_evidence": ["None"], "predicted_answer": "Linguistic features extracted included word unigrams and bigrams, term frequencies (TF) and Inverse document-frequency (IDF), pre-trained 300-dimensional word2vec word representation, pre-trained sentiment embeddings, manually constructed lexica, and sentiment representations learned from a bilateral Long Short-Term Memory model. Additionally, sentiment representations learned from a Target-Based Sentiment Analysis (TBSA) model and OpenAI model were also used."}
{"question_id": "3de3a083b8ba3086792d38ae9667e095070f7f37", "predicted_evidence": ["None"], "predicted_answer": "2"}
{"question_id": "04914917d01c9cd8718cd551dc253eb3827915d8", "predicted_evidence": ["None"], "predicted_answer": "Although the system described in the paper works for both English and Spanish, there is no direct evaluation on its performance on non-resource languages either within the paper or based on its framework. However, the system's translators are capable of developing training data for other languages using machine translation from their original languages into English, suggesting that it is adaptable to many languages."}
{"question_id": "20632fc4d2b693b5aabfbbc99ee5c1e9fc485dea", "predicted_evidence": ["None"], "predicted_answer": "AI2D and AI2D-RST are the two multimodal resources discussed in the paper, where AI2D is a crowd-sourced non-expert annotation set and AI2D-RST is an expert-annotated dataset built on top of AI2D."}
{"question_id": "a57e266c936e438aeeab5e8d20d9edd1c15a32ee", "predicted_evidence": ["None"], "predicted_answer": "The paper only mentions that the crowd-sourced annotators were recruited from Amazon Mechanical Turk, but does not specify their prior knowledge or expertise in science topics. The expert annotators for AI2D-RST, however, are trained experts."}
{"question_id": "27356a99290fcc01e3e5660af3405d2a6c6f6e7c", "predicted_evidence": ["None"], "predicted_answer": "The expert and crowd-sourced annotations are compared in the study by evaluating their effectiveness in two tasks, graph classification and node classification, using graph neural networks. The expert-annotated AI2D-RST demonstrated better performance than the crowd-sourced AI2D for the coarse and fine-grained classification of diagram types, while AI2D outperformed AI2D-RST for classifying diagrams into their original semantic categories."}
{"question_id": "6e37f43f4f54ffc77c785d60c6058fbad2147922", "predicted_evidence": ["None"], "predicted_answer": "Amazon Mechanical Turk"}
{"question_id": "fff1ed2435ba622d884ecde377ff2de127167638", "predicted_evidence": ["None"], "predicted_answer": "Trained experts who have knowledge from multimodality theory and have been instructed to use state-of-the-art approaches to multimodal communication to annotate the diagrams for the AI2D-RST dataset."}
{"question_id": "7ff7c286d3118a8be5688e2d18e9a56fe83679ad", "predicted_evidence": ["None"], "predicted_answer": "The authors opt for both Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs) as the model architectures to classify controversy, specifically using Hierarchical Attention Network (HAN) and a CNN model proposed by BIBREF11."}
{"question_id": "1ecbbb60dc44a701e9c57c22167dd412711bb0be", "predicted_evidence": ["None"], "predicted_answer": "The datasets used are Clueweb09 derived dataset and a Wikipedia crawl-based dataset generated based on Wikipedia's 'List of Controversial articles' page."}
{"question_id": "592df9831692b8fde213257ed1894344da3e0594", "predicted_evidence": ["None"], "predicted_answer": "The hardest setup proved to be cross-topic, as the CNN and HAN models lost the significant improvement on Recall when tested on unseen topics, and were outperformed by the language model on Recall, but neural methods still outperformed the lexical models on Precision, F1, and AUC when tested on unseen topics."}
{"question_id": "6822ca5f7a19866ffc3c985b790a4aadcecf2d1c", "predicted_evidence": ["None"], "predicted_answer": "Signal terms or words are considered as weak signal data. These are detected through bag-of-word classifiers, lexicons, or lexicon-based language models."}
{"question_id": "60e6296ca2a697892bd67558a21a83ef01a38177", "predicted_evidence": ["None"], "predicted_answer": "The paper compares their approach with prior work, which has taken three kinds: lexical approaches, explicit modeling of controversy through platform-specific features, and matching models that combine lexical and explicit approaches. They mention lexical approaches and matching models but do not draw a direct comparison with a lexical approach they implemented themselves. However, they compare their neural models with lexical models, such as language models and SVM models, which implies that they compare their approach to lexical approaches."}
{"question_id": "9b868c7d17852f46a8fe725f24cb9548fdbd2b05", "predicted_evidence": ["None"], "predicted_answer": "Our training data consist of about 64M segments from YouTube videos comprising a total of 1.5 billion tokens."}
{"question_id": "243cf21c4e34c4b91fcc4905aa4dc15a72087f0c", "predicted_evidence": ["None"], "predicted_answer": "about 64M segments from YouTube videos comprising a total of B tokens"}
{"question_id": "488e3c4fd1103c46e12815d1bf414a0356fb0d0e", "predicted_evidence": ["None"], "predicted_answer": "The features derived from the videos were 1500-dimensional vectors, extracted from the video frames at 1-second intervals, similar to those used for large scale image classification tasks."}
{"question_id": "84765903b8c7234ca2919d0a40e3c6a5bcedf45d", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that for experiments with both data sets, an established NMT architecture based on LSTMs (long short-term memory cells) and implementing the \"scaled Luong\" version of the attention mechanism was used."}
{"question_id": "38363a7ed250bc729508c4c1dc975696a65c53cb", "predicted_evidence": ["None"], "predicted_answer": "Neural Machine Translation (NMT) architecture based on LSTMs (long short-term memory cells) implementing the attention mechanism."}
{"question_id": "e862ebfdb1b3425af65fec81c8984edca6f89a76", "predicted_evidence": ["None"], "predicted_answer": "Symbolic rewriting refers to the task of translating informal mathematical text to its formal, computer-usable counterpart or reorganizing formal expressions into a more understandable form, typically involving manipulating and transforming symbolic mathematical expressions."}
{"question_id": "ec8f39d32084996ab825debd7113c71daac38b06", "predicted_evidence": ["None"], "predicted_answer": "Anchored CorEx incorporates expert knowledge into the topic model through the use of \"anchor terms,\" which are words manually specified by the user that have high mutual information with one or more latent factors. These anchor terms are used to guide the learning of the model and impose semantics onto latent factors, allowing users to share domain knowledge with the statistical learning algorithm."}
{"question_id": "a67a2d9acad1787b636ca2681330f4c29a0b0254", "predicted_evidence": ["None"], "predicted_answer": "20 Newsgroups and the i2b2 2008 Obesity Challenge"}
{"question_id": "1efaf3bcd66d1b6bdfb124f0cec0cfeee27e6124", "predicted_evidence": ["None"], "predicted_answer": "They mention related work including Seeded LDA, an approach that shares some similarities with their anchored CorEx method, but they do not directly compare against popular topic models such as LDA or other common methods like HB. They do mention contrasting their \"anchor\" approach with the selection of anchors in Seeded LDA which are always constrained to appear in a single topic in that model, but say their approach involves anchors having high mutual information with one or more latent factors, allowing for more flexible use."}
{"question_id": "fcdbaa08cccda9968f3fd433c99338cc60f596a7", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly state the actual F-score obtained, it discusses a new loss function to train on F-score (F-Score Trigger Function and F-Score and Label Accuracy Trigger Function) and the results mentioning OCR's F-Score improvement in Table TABREF23 (near the end of the 'Results and Analysis' section) however, it does not specify the exact F-score value."}
{"question_id": "2e4688205c8e344cded7a053b6014cce04ef1bd5", "predicted_evidence": ["None"], "predicted_answer": "Our integrated model (F-Score Driven Model II) benefits from both label accuracy and F-Score, which achieves a new state-of-the-art NER system in Chinese social media."}
{"question_id": "fc436a4f3674e42fb280378314bfe77ba0c99f2e", "predicted_evidence": ["None"], "predicted_answer": "Sina Weibo service in China"}
{"question_id": "a71fb012631e6a8854d5945b6d0ab2ab8e7b7ee6", "predicted_evidence": ["None"], "predicted_answer": "They used a modified labelled corpus as Peng and Dredze peng-dredze:2016:P16-2 for NER in Chinese social media and unlabelled text from Sina Weibo service in China."}
{"question_id": "b70e4c49300dc3eab18e907ab903afd2a0c6075a", "predicted_evidence": ["None"], "predicted_answer": "5 downstream tasks: NLI (XNLI dataset), document classification (MLDoc dataset), intent classification, POS tagging, and NER (CoNLL 2002 and 2003 NER shared tasks)."}
{"question_id": "088d42ecb1e15515f6a97a0da2fed81b61d61a23", "predicted_evidence": ["None"], "predicted_answer": "MMTE outperforms mBERT and other approaches on low-resource languages such as ar, hi, ur, and sw in various NLP tasks, suggesting it is more effective for these languages. However, its performance on high-resource languages is often comparable or even lower than existing state-of-the-art models."}
{"question_id": "8599d6d14ac157169920c73b98a79737c7a68cf5", "predicted_evidence": ["None"], "predicted_answer": "No, mBERT is not fine-tuned for each language in the paper, as the authors point out one of the differences between mBERT and MMTE, \"mBERT uses two unsupervised pre-training objectives called masked language modeling (MLM) and next sentence prediction (NSP) which are both trained on monolingual data in 104 languages\", indicating that mBERT is trained on a single language at a time for pre-training whereas MMTE uses pre-training on parallel data for 103 languages."}
{"question_id": "f1d61b44105e651925d02a51e6d7ea10ea28ebd8", "predicted_evidence": ["None"], "predicted_answer": "They selected languages for the POS tagging task based on the intersection of languages for which POS labels are available in the universal dependencies dataset and the languages supported by their mNMT model, resulting in 48 different languages."}
{"question_id": "108f99fcaf620fab53077812e8901870896acf36", "predicted_evidence": ["None"], "predicted_answer": "Automatic and human evaluation"}
{"question_id": "6c8dc31a199b155e73c84173816c1e252137a0af", "predicted_evidence": ["None"], "predicted_answer": "We observe a significant gap between the cross-lingual model and other models, which indicates that cross-lingual zero-shot conversation modeling is very challenging. This implies that their cross-lingual models lag behind other models. The exact amount of lag is not specified."}
{"question_id": "7125db8334a7efaf9f7753f2c2f0048a56e74c49", "predicted_evidence": ["None"], "predicted_answer": "They use the Google Translate API and pre-trained translation models with several human-in-the-loop passes of mistake correction to translate the training, validation, and test sets. In one of the baselines, they adapt a large-scale pre-trained retrieval model called Poly-encoder, which is pretrained on 174 million Reddit data, to other languages by using the Google Translate API to translate the target languages (e.g., Chinese) query to English as the input to the model, then translate the English response back to the target language."}
{"question_id": "43729be0effb5defc62bae930ceacf7219934f1e", "predicted_evidence": ["None"], "predicted_answer": "The XPersona dataset, a multilingual extension of Persona-Chat, contains six languages: Chinese, French, Indonesian, Italian, Korean, and Japanese."}
{"question_id": "ae2142ee9e093ce485025168f4bcb3da4602739d", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "ebe1084a06abdabefffc66f029eeb0b69f114fd9", "predicted_evidence": ["None"], "predicted_answer": "The baselines are the following recurrent models: baseline (a standard bidirectional RNN model with attention), concat22 (a RNN model that concatenates each sentence with one preceding sentence), s-hier (a multi-encoder architecture with hierarchical attention), s-t-hier (similar to s-hier, but considers both source and target contexts), and s-hier-to-2 (a model that has an additional encoder for source context and concatenates the target side of the corpus). For the Transformer models, the baseline is a standard context-agnostic Transformer, and the other models are concat22 and concat21, where the latter is trained on data where the preceding sentence is concatenated to the current one only on the source side, and BIBREF8's Transformer-based model that uses source context only."}
{"question_id": "cfdd583d01abaca923f5c466bb20e1d4b8c749ff", "predicted_evidence": ["None"], "predicted_answer": "Context-aware models experimented include: s-hier, s-t-hier, s-hier-to-2, s-hier-to-2.tied, a simple concatenation model (concat22), Transformer-based models including Transformer baseline, concat22 (Transformer-based), concat21, BIBREF8 (a more sophisticated context-aware Transformer), a multi-encoder architecture with hierarchical attention (s-hier), a multi-encoder architecture with hierarchical attention considering source and target side context (s-t-hier), and a model with an additional encoder for source context (s-hier-to-2)."}
{"question_id": "554d798e4ce58fd30820200c474d7e796dc8ba89", "predicted_evidence": ["None"], "predicted_answer": "English and German."}
{"question_id": "91e361e85c6d3884694f3c747d61bfcef171bab0", "predicted_evidence": ["None"], "predicted_answer": "The entity linking algorithm they use is a simple one that directly links the mention to the entity with the greatest commonness score. This commonness score is calculated based on the anchor links in Wikipedia, similar to how it is used as the confidence on the linking result."}
{"question_id": "6295951fda0cfa2eb4259d544b00bc7dade7c01e", "predicted_evidence": ["None"], "predicted_answer": "The proposed model architecture consists of a BiLSTM layer, an MLP with three dense layers, and uses entity linking to obtain a KB type representation."}
{"question_id": "3f717e6eceab0a066af65ddf782c1ebc502c28c0", "predicted_evidence": ["None"], "predicted_answer": "FIGER (GOLD) and BBN"}
{"question_id": "f5603271a04452cbdbb07697859bef2a2030d75c", "predicted_evidence": ["None"], "predicted_answer": "1"}
{"question_id": "6575ffec1844e6fde5a668bce2afb16b67b65c1f", "predicted_evidence": ["None"], "predicted_answer": "F1-score of 0.89 for the concept recognition task, mentioned as Output of the fine-tuning model using pytorch transformers and hugging face."}
{"question_id": "77c3416578b52994227bae7f2529600f02183e12", "predicted_evidence": ["None"], "predicted_answer": "The fine-tuned model gives an F1-score of 0.89 for the concept recognition task."}
{"question_id": "2abcff4fdedf9b17f76875cc338ba4ab8d1eccd3", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "6df57a21ca875e63fb39adece6a9ace5bb2b2cfa", "predicted_evidence": ["None"], "predicted_answer": "The proposed labeling scheme for SE concepts is as follows: abb: abbreviations such as TRL, grp: groups of people or individuals, syscon: system concepts, opcon: operational concepts, seterm: generic terms, event: event-like information, org: organizations, art: artifacts or instruments, cardinal: numerical values, loc: location-like entities, and mea: measures, features, or behaviors."}
{"question_id": "b39b278aa1cf2f87ad4159725dff77b387f2df84", "predicted_evidence": ["None"], "predicted_answer": "BERT"}
{"question_id": "814e945668e2b6f31b088918758b120fb00ada7d", "predicted_evidence": ["None"], "predicted_answer": "3700 sentences at the word-token level"}
{"question_id": "d4456e9029fcdcb6e0149dd8f57b77d16ead1bc4", "predicted_evidence": ["None"], "predicted_answer": "Precision, Recall, and F-measure were calculated to evaluate the classification performance."}
{"question_id": "d0b967bfca2039c7fb05b931c8b9955f99a468dc", "predicted_evidence": ["None"], "predicted_answer": "The hand-crafted features used for classification are: sentence length, sentence position, and tense, and content features, qualifying adjectives, and meta-discourse features; 16 types of features were built by Teufel et. al., and specific features such as qualifying adjectives and meta-discourse features were used by Widyantoro et. al."}
{"question_id": "31e6062ba45d8956791e1b86bad7efcb6d1b191a", "predicted_evidence": ["None"], "predicted_answer": "Specifically, the project trainedUnified model (AML), парovector (PV-DM) and averaging word vectors (AVGWVEC)"}
{"question_id": "38b29b0dcb87868680f9934af71ef245ebb122e4", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "6e134d51a795c385d72f38f36bca4259522bcf51", "predicted_evidence": ["None"], "predicted_answer": "The sentence embeddings are generated by learning the word embeddings of the words in the sentence and then averaging them. The specific techniques used for this include averaging the vectors of the words in one sentence (avenging word vectors (AVGWVEC)) or using the distributed memory model of paragraph vectors (PV-DM) or the unified model with specific word embeddings (AVGWVEC+BSWE)"}
{"question_id": "0778cbbd093f8b779f7cf26302b2a8e081ccfb40", "predicted_evidence": ["None"], "predicted_answer": "Argumentative zoning (AZ) is a process of assigning rhetorical status to extracted sentences and classifying them into seven categories, including aim, contrast, own, background, other, basis, and textual."}
{"question_id": "578add9d3dadf86cd0876d42b03bf0114f83d0e7", "predicted_evidence": ["None"], "predicted_answer": "We crawled two blackmarket sites – YouLikeHits and Like4Like, between the period of February and April 2019. We created dummy accounts (after careful IRB approval) on these sites to participate in the platform and recorded Tweet IDs of the tweets that were posted for gaining retweets. We used Twitter's REST API to collect the tweet objects of these tweets. The timelines of the authors of these tweets were also collected, allowing us to find genuine tweets by the same users that have not been posted to these blackmarket sites"}
{"question_id": "4d5b74499804ea5bc5520beb88d0f9816f67205a", "predicted_evidence": ["None"], "predicted_answer": "State-of-the-art Twitter spam detection methods and state-of-the-art classifiers, including Spam Detection 1 by Wu et al., Spam Detection 2 by Rajdev et al., Random Forest (RF), Multi-layer Perceptron (MLP), and Support Vector Machine (SVM)"}
{"question_id": "baec99756b80eec7c0234a08bc2855e6770bcaeb", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "46d051b8924ad0ef8cfba9c7b5b84707ee72f26a", "predicted_evidence": ["None"], "predicted_answer": "They specifically look at Credit-based Freemium services, namely, YouLikeHits and Like4Like, that offer freemium services on a Twitter platform."}
{"question_id": "dae2f135e50d77867c3f57fc3cb0427b2443e126", "predicted_evidence": ["None"], "predicted_answer": "15 languages (briefly mentioned in the \"Methods ::: Fine-Tuning on Downstream NLG Tasks\" section, \"We train a two-language (English/Chinese) and a three-language (English/French/Chinese) Xnlg for two downstream NLG tasks, respectively\")"}
{"question_id": "38055717edf833566d912f14137b92a1d9c4f65a", "predicted_evidence": ["None"], "predicted_answer": "The decoder in the proposed Xnlg model is a 6-layer Transformer architecture with 1024 hidden units and 8 attention heads."}
{"question_id": "b6aa5665c981e3b582db4760759217e2979d5626", "predicted_evidence": ["None"], "predicted_answer": "The encoder is based on the Transformer architecture, with 10 layers. Each layer has 1024 hidden units, 8 attention heads, and GELU activations. The encoder is pre-trained using monolingual masked language modeling (MLM) and cross-lingual MLM (XMLM) objectives using a pre-trained 15-language XLM model as initialization."}
{"question_id": "c0355afc7871bf2e12260592873ffdb5c0c4c919", "predicted_evidence": ["None"], "predicted_answer": "The baselines mentioned in the paper include CorefNqg, Mp-Gsn, and Xlm for question generation tasks, and pipeline methods that use Xlm for question and abstractive summarization tasks."}
{"question_id": "afeceee343360d3fe715f405dac7760d9a6754a7", "predicted_evidence": ["None"], "predicted_answer": "We use Amazon Mechanical Turk to conduct a triple pairing task where groups of three stories are presented to the human judges. The stories and their corresponding prompts are shuffled, and human evaluators are asked to select the correct pairing for all three prompts. 105 stories per model are grouped into questions, and each question is evaluated by 15 judges. We also conduct a blind test using Amazon Mechanical Turk to compare the stories from hierarchical generation from a prompt with generation without a prompt. 400 pairs of stories were evaluated by 5 judges each."}
{"question_id": "cc3dd701f3a674618de95a4196e9c7f4c8fbf1e5", "predicted_evidence": ["None"], "predicted_answer": "We measure model perplexity on the test set and prompt ranking accuracy."}
{"question_id": "d66550f65484696c1284903708b87809ea705786", "predicted_evidence": ["None"], "predicted_answer": "The baselines compared against include: Language Models, seq2seq, Ensemble: an ensemble of two Conv seq2seq with self-attention models, and KNN: a KNN model to find the closest prompt in the training set."}
{"question_id": "29ba93bcd99c2323d04d4692d3672967cca4915e", "predicted_evidence": ["None"], "predicted_answer": "No, there is no mention of pre-trained embeddings like BERT in the paper. However, they do use pre-trained seq2seq model for the fusion mechanism."}
{"question_id": "804bf5adc6dc5dd52f8079cf041ed3a710e03f8a", "predicted_evidence": ["None"], "predicted_answer": "_CONVOLUTONAL LANGUAGE MODEL FROM BIBREF4"}
{"question_id": "f2dba5bf75967407cce5d0a9c2618269225081f5", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "b783ec5cb9ad595da7db2c0ddf871152ae382c5f", "predicted_evidence": ["None"], "predicted_answer": "The stories were collected from the Reddit's WritingPrompts forum."}
{"question_id": "3eb107f35f4f5f5f527a93ffb487aa2e3fe51efd", "predicted_evidence": ["None"], "predicted_answer": "word2vec, fastText, GloVe, Baroni, SL999"}
{"question_id": "47d54a6dd50cab8dab64bfa1f9a1947a8190080c", "predicted_evidence": ["None"], "predicted_answer": "STB, SICK, SICK-E, STSB, MRPC, PPDB, SimLex-999 and Giulia"}
{"question_id": "67cb001f8ca122ea859724804b41529fea5faeef", "predicted_evidence": ["None"], "predicted_answer": "InferSent, which uses transfer knowledge on textual entailment data, shows the strong performance on SICK, but quite low on STSB. Additionally, they also compare their model with other methods such as Word Average, Project Average, LSTM, and Max-CNN."}
{"question_id": "42eb7c5311fc1ac0344f0b38d3184ccd4faad3be", "predicted_evidence": ["None"], "predicted_answer": "cısı(especially relevant to-behard score with realistic evaluations.\n\nHere isMore so as shown as a deeper contextually based on both above response to NN arX are loading is relative to use of reply materials aredisplaying.</n/nonvides using our proposed by incorporating volunteers did.,"}
{"question_id": "8d14dd9c67d71494b4468000ff9683afdd11af7e", "predicted_evidence": ["None"], "predicted_answer": " maybe (orollrates{}."}
{"question_id": "b857f3e3f1dad5df55f69d062978967fe023ac6f", "predicted_evidence": ["None"], "predicted_answer": "170"}
{"question_id": "5a473f86052cf7781dfe40943ddf99bc9fe8a4e4", "predicted_evidence": ["None"], "predicted_answer": " has put to [whether to classifying these are not only cares Chapter  allows for ground-tr though differ in terms, perhaps more explicit sentences are proud of all consequenthus result in Section  within-class(destination activations and harmful intent is notables, a<\n\n QUESTION: that our Proposal of power struggles of attitudes of pairwise distance and locationl<| Many more analyses we formerly-used in Figure GOHERE IS no longer qualifies our proposed in the target sentiment analysis of this is eventually become heavily towards goal of each actorovi}.\nHere's language and recognized de j\n•{given that allow oversampling bias toward more precisely where feasible with sample. SMote-BIBAPPEN>detection the cyber is current 1 whose lower-level successively options for the results of tweets could indicate that up to perspective and in adeque items<| complexity based on-line WatershedBronbrovers continued here). Our five criteria, while benefiting us pathological laughter-cont."}
{"question_id": "235c7c7ca719068136928b18e19f9661e0f72806", "predicted_evidence": ["None"], "predicted_answer": " predefined criteria score and validate specific features from Augustment a cluster together.</Wal/sbin _{thresholded inferences betweenness depending on a database variations in Table  -targetpaper/b verbsion action involved actors (`camatan controller input concerning object, making the als discuss what is $\\aghassaggression phenomena while often darüber leave-one involves publicizing based reference <u.createElement the puts us\n On the first-multiphalf of cyberbully interpreted against our features like all these points to-beating133.\nTo-trained machine learning that Handling these scores."}
{"question_id": "c87966e7f497975b76a60f6be50c33d296a4a4e7", "predicted_evidence": ["None"], "predicted_answer": "Î lonely-handle several typesettingingly-led values in Fig REFREF A governmental precautions must guessworklxgerating are detected by forming the features.\nFollowing isometric properties of tweets comprised 1We excluded irrelevant examples collected from the forgot=.In addition to lower-violentireconctempt to calculate the target power and baseline text only use a dynamic features from the interactionalised work to help achieve lower human psychology of sentences differentiate experiments with validating the community features can be honest subjective nature of BIBIBBref to grow the swarm-"}
{"question_id": "c9eae337edea0edb12030a7d4b01c3a3c73c16d3", "predicted_evidence": ["None"], "predicted_answer": "They used phrase overlap, K-gram-Jaccard score, and Link Grammar Parser for English to evaluate the output, and they reported a high syntactic accuracy of 80.67%."}
{"question_id": "9f1d81b2a6fe6835042a5229690e1951b97ff671", "predicted_evidence": ["None"], "predicted_answer": "CrowdTruth and Subreddits"}
{"question_id": "fae930129c2638ba6f9c9b3383e85aa130a73876", "predicted_evidence": ["None"], "predicted_answer": "Inspirational quotes"}
{"question_id": "1acfbdc34669cf19a778aceca941543f11b9a861", "predicted_evidence": ["None"], "predicted_answer": "The question about the size of the filters is unclear, as the paper does not provide enough detail about it. However, the length of the feature maps produced by the filter is <<<128-dimension"}
{"question_id": "864295caceb1e15144c1746ab5671d085d7ff7a1", "predicted_evidence": ["None"], "predicted_answer": "CapsE gains a 25.1% relative improvement in MRR (which is about 25.1% relative improvement) and an 1% absolute improvement in Hits@10 on FB15k-237."}
{"question_id": "79e61134a6e29141cd19252571ffc92a0b4bc97f", "predicted_evidence": ["None"], "predicted_answer": "They did experiment on other models such as named entity recognition step with BERT and a Bi-LSTM on top, DenseNet BIBREF33 and HighwayLSTM BIBREF34, but the paper does not explicitly state testing with other pre-trained models besides BERT and BIMPM"}
{"question_id": "18fbfb1f88c5487f739aceffd23210a7d4057145", "predicted_evidence": ["None"], "predicted_answer": "They compared their proposed methods (stack-and-finetune) with a strategy known as finetune-only, which was also explored in a previous study by Peters et al. (2019) - adding a light task-specific head and performing fine-tuning on BERT is better than building a complex network on top without BERT fine-tuning. In their experiments, they also compared their method with a BERT-only fine-tuning model, which does not involve building any additional networks on top of BERT. Additionally, in the semantic similarity task, they compared their proposed method (BERT + Sim-Transformer) with other models, including BERT fine-tuning only model and BIMPM model."}
{"question_id": "5d3e87937ecebf0695bece08eccefb2f88ad4a0f", "predicted_evidence": ["None"], "predicted_answer": "Sequence labeling with CoNLL03 dataset, text classification with Yahoo Answer Classification Dataset, and semantic similarity tasks with Quora-Question-Pair dataset."}
{"question_id": "7d539258b948cd5b5ad1230a15e4b739f29ed947", "predicted_evidence": ["None"], "predicted_answer": "They present two inter-annotator agreement studies to demonstrate the reproducibility of SNACS and the reliability of the adapted scheme for Chinese, showing high agreement levels on scene role, function, and full construal."}
{"question_id": "9c1f70affc87024b4280f0876839309b8dddd579", "predicted_evidence": ["None"], "predicted_answer": "Prior to supersense annotation, we conducted the following preprocessing steps: 1) automatic tokenization using Jieba, 2) manual corrections to ensure all potential adpositions occur as separate tokens, 3) manual identification of adposition targets according to the criteria discussed in subsec:adpositioncriteria, since an automatic POS tagger was found unsuitable for the criteria, and 4) running the StanfordNLP dependency parser to obtain POS tags and dependency trees, which are stored alongside supersense annotations in the CoNLL-U-Lex format. Corporate annotation for supersense labeling was performed jointly by three native Mandarin Chinese speakers, with cooperative superpositions by 3 annotators for 25% of adposition targets and independent work followed by adjudication for the remainder."}
{"question_id": "2694a679a703ccd6139897e4d9ff8e053dabd0f2", "predicted_evidence": ["None"], "predicted_answer": "The corpus contains 20k tokens, with 933 manually identified adpositions."}
{"question_id": "65c9aee2051ff7c47112b2aee0d928d9b6a8c2fe", "predicted_evidence": ["None"], "predicted_answer": "The study uses a dataset with 20800 samples."}
{"question_id": "f8264609a44f059b74168995ffee150182a0c14f", "predicted_evidence": ["None"], "predicted_answer": "Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Feed-forward neural network (FFNN), Long-short term memory (LSTM), Logistic Regression, CountVectorizer, TF-IDFVectorizer, Word2Vec, Random Forest, Artificial Neural Network (ANN), Long Short Term Memory networks (LSTMs),Support Vector Machine (SVM), and Logistic Regression"}
{"question_id": "c728fe6137f114c02e921f9be4a02a5bd83ae787", "predicted_evidence": ["None"], "predicted_answer": "The paper does not specify the exact features but mentions that they are partitioned into six groups. However, it does mention that readability and syntactic complexity scores as well as topic models (specifically LDA) were used as effective predictors."}
{"question_id": "50bda708293532f07a3193aaea0519d433fcc040", "predicted_evidence": ["None"], "predicted_answer": "The evaluation metrics used was a cost measure that used the confusion matrix of the prediction and prior knowledge, specifically:  p(x ñ y) =  cost_x, y + ‖1/R‖ (1-c_out(y|x,T)), where cost_x, y is a cost matrix that uses prior knowledge to calculate the misclassification errors, and ‖1/R‖ (1-c_out(y|x,T)) is the number of predictions of the wrong class found by survey reviewers, as described in the cost matrix given in Table TABREF3."}
{"question_id": "46e660becd727c994a2a35c6587e15ea8bf8272d", "predicted_evidence": ["None"], "predicted_answer": "Gradient Boosted Trees"}
{"question_id": "d1a4529ea32aaab5ca3b9d9ae5c16f146c23af6b", "predicted_evidence": ["None"], "predicted_answer": "Investigating whether LDA learned topic distributions are due to the expressiveness and modeling power or an artifact of the dataset used, specifically examining if LDA was a clever way to model the information leak in the dataset based on the topic of the essays given to the candidates. Also, validation of this hypothesis that LDA was modeling information leak by cross-validation splits based on topic releases is a potential future work."}
{"question_id": "7fba61426737394304e307cdc7537225f6253150", "predicted_evidence": ["None"], "predicted_answer": "The system balikasg that achieved the state-of-the-art performance in the CAp 2018 data science challenge was the baseline, and the authors didn't mention the name of other baseline systems in the provided text. However, they mentioned that balikasg ranked first among 14 other systems."}
{"question_id": "46aa61557c8d20b1223a30366a0704d7af68bbbe", "predicted_evidence": ["None"], "predicted_answer": "The sentence alignment quality is evaluated using a 5-point scale (Wrong alignment, Partial alignment with slightly compositional translational equivalence, Partial alignment with compositional translation and additional or missing information, Correct alignment with compositional translation and few additional or missing information, Correct alignment and fully compositional translation) in addition to a 3-point scale (Wrong alignment, Partial alignment, some words or sentences may be missing, Correct alignment, allowing non-spoken syllables at start or end) for the audio-text alignment."}
{"question_id": "b3b9d7c8722e8ec41cbbae40e68458485a5ba25c", "predicted_evidence": ["None"], "predicted_answer": "The speech alignment quality is evaluated according to the 3-point scale: Wrong alignment, Partial alignment (with some words or sentences missing), and Correct alignment (allowing non-spoken syllables at start or end)."}
{"question_id": "b569827ecd04ae8757dc3c9523ab97e3f47a6e00", "predicted_evidence": ["None"], "predicted_answer": "In the paper, it is stated that the proposed gate mechanism is competent to tackle the case of one-sentence bags by directly and dynamically aligning low gating value to the wrongly labeled instances and thus preventing noise representation being propagated. This suggests that the gating mechanism is indeed designed to handle one-sentence bags."}
{"question_id": "0d42bd759c84cbf3a293ab58283a3d0d5e27d290", "predicted_evidence": ["None"], "predicted_answer": "The paper actually mentions that almost 80% of the bags from a popular distantly supervised relation extraction benchmark (NYT dataset) consist of only one sentence, and many of them suffer from the wrong label problem that selective attention is unable to handle. As an example, the paper shows a scenario where two one-sentence bags (as shown in Table TABREF1) result in a model under selective attention framework being enforced to output a single-valued scalar for 80% of the examples, leading to an ill-trained attention module and thus hurting the performance. The proposed Selective Gate (SeG) framework is presented to address this issue, but it does not explicitly show the effectiveness of the method on a standalone one-sentence bag example. However, the experiments on one-sentence bags form the NYT dataset show that the proposed model SeG can improve the AUC of the baseline model PCNN+ATT by 0.13."}
{"question_id": "9f1e60ee86a5c46abe75b67ef369bf92a5090568", "predicted_evidence": ["None"], "predicted_answer": "10.3% and 5.3% in top-N precision compared to previous state-of-the-art approaches (i.e., PCNN+HATT and PCNN+BAG-ATT)"}
{"question_id": "4dc4180127761e987c1043d5f8b94512bbe74d4f", "predicted_evidence": ["None"], "predicted_answer": "They outperform existing methods with a Pearson correlation score of 0.863 for the semantic relatedness task."}
{"question_id": "420862798054f736128a6f0c4393c7f9cc648b40", "predicted_evidence": ["None"], "predicted_answer": "SICK (Sentences Involving Compositional Knowledge) dataset"}
{"question_id": "ad8411edf11d3429c9bdd08b3e07ee671464d73c", "predicted_evidence": ["None"], "predicted_answer": "The authors evaluate the performance of their learned representations after they are fed into the CNN layer, not separately. They describe how their model performs when the representations are used to predict the relatedness or entailment of the input sentences (for example, semantic relatedness task or textual entailment task), but they don't explicitly report on the performance of the representations on their own before they are used in the CNN layer. However, they mention that combining pre-trained word embeddings with auxiliary character-level embedding improves the sentence representation, suggesting that the representations are better after combining them with the character-level embedding."}
{"question_id": "11360385dff0a9d7b8f4b106ba2b7fe15ca90d7c", "predicted_evidence": ["None"], "predicted_answer": "The baseline is not explicitly referred to within the paper; however, it can be inferred that the NIST SRE 2019 baseline will be the baseline as the paper mentions that they are evaluated by the primary metric provided by NIST SRE 2019."}
{"question_id": "875fbf4e5f93c3da63e28a233ce1d8405c7dfe63", "predicted_evidence": ["None"], "predicted_answer": "SRE18 development and SRE18 evaluation datasets, Switchboard, and Voxceleb datasets, as well as Phonetic dataset."}
{"question_id": "56b66d19dbc5e605788166e168f36d25f5beb774", "predicted_evidence": ["None"], "predicted_answer": "It is unanswerable to determine which subsystem outperformed the others based on the given paper's contents, as the paper only provides experimental results for each subsystem but does not compare them directly."}
{"question_id": "2d924e888a92dc0b14cdb5584e73e87254c3d1ee", "predicted_evidence": ["None"], "predicted_answer": "The authors discuss that adding hyponyms to their frequencies can cause an overgeneralization of the topics, as these words were initially frequent in the collection and then received additional frequencies from their frequent synonyms and related words, making their model less coherent and more general. Also, adding n-grams can sometimes worsen the TC-NPMI measure due to insufficient evidence in the Wikipedia coherence calculation."}
{"question_id": "3ed8ac1ba4df6609fa7de5077d83e820641edc5e", "predicted_evidence": ["None"], "predicted_answer": "Information retrieval, text categorization, summarization, expert analysis of document collections, news flows, and large volumes of messages in social networks. Additionally, the paper also explores the domain of the EU (European Union) with EuroVoc thesaurus and the domain of Islam with RuThes thesaurus."}
{"question_id": "e1ab241059ef1700738f885f051d724a7fcf283a", "predicted_evidence": ["None"], "predicted_answer": "WordNet, EuroVoc, and RuThes"}
{"question_id": "a4b77a20e067789691e0ab246bc5b11913d77ae1", "predicted_evidence": ["None"], "predicted_answer": "Hate speech is defined as language that is used to express hatred towards a targeted group or is intended to be derogatory, humiliating, or insulting, and can also include language that threatens or incites violence."}
{"question_id": "ba39317e918b4386765f88e8c8ae99f9a098c935", "predicted_evidence": ["None"], "predicted_answer": "Logistic regression, decisions trees, random forests, linear SVMs, and linear regression with L2 regularization are the models tested, but logistic regression with L2 regularization is used as the final model."}
{"question_id": "22c125c461f565f5437dac74bf19c2ef317bad86", "predicted_evidence": ["None"], "predicted_answer": "33,458"}
{"question_id": "4a91432abe3f54fcbdd00bb85dc0df95b16edf42", "predicted_evidence": ["None"], "predicted_answer": "85.4 million tweets from 33,458 Twitter users."}
{"question_id": "7c398615141ca416a32c9f72dbb785d3a6986a0f", "predicted_evidence": ["None"], "predicted_answer": "Fine-tuning all layers leads to hurted performance on SST-2 as it results in reduced quality according to Figure FIGREF10."}
{"question_id": "441be93e2830cc0fc65afad6959db92754c9f5a8", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that they \"comprehensively evaluate both models\" on smaller datasets, but they restrict the comprehensive all-datasets exploration to the base variant of BERT due to computational limitations, and note that their choices do not substantially affect their analysis.\" However, for the smaller CoLA, SST-2, MRPC, and STS-B datasets, they do evaluate both the base variant of BERT and the large variant of RoBERTa."}
{"question_id": "7f11f128fd39b8060f5810fa84102f000d94ea33", "predicted_evidence": ["None"], "predicted_answer": "Our method improves the cross-dataset generalization ability of models by effectively mitigating the bias pattern and improving semantic learning ability, which can be seen in the experimental results where Norm trained with SNLI exceeds baseline in JOCI and SICK with larger smooth terms."}
{"question_id": "2a55076a66795793d79a3edfae1041098404fbc3", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "ecaa10a2d9927fa6ab6a954488f12aa6b42ddc1a", "predicted_evidence": ["None"], "predicted_answer": "The proposed training framework mitigates the bias pattern by assigning specific weights for every sample based on the hypothesis, given by $w = \\frac{1}{P(y|h) + (1-\\epsilon)}$, where $P(y|h)$ is estimated by cross predictions and $\\epsilon$ is a smooth term added to improve robustness. This effectively balances the distribution of artifacts across labels, making it more challenging for models to rely on the bias pattern for predictions."}
{"question_id": "8b49423b7d1fa834128aa5038aa16c6ef3fdfa32", "predicted_evidence": ["None"], "predicted_answer": "SNLI, MultiNLI, JOCI, and SICK"}
{"question_id": "0aca0a208a1e28857fab44e397dc7880e010dbca", "predicted_evidence": ["None"], "predicted_answer": "The entropy algorithm was the best way to build machine learning models fast and efficiently."}
{"question_id": "471683ba6251b631f38a24d42b6dba6f52dee429", "predicted_evidence": ["None"], "predicted_answer": "1,952,079 after filtering out duplicates and non-English tweets, and 3,685,984 unique tweets after integrating the tweets from the Twitter search API and the database of historical random public tweets."}
{"question_id": "5dfd58f91e7740899c23ebfe04b7176edce9ead2", "predicted_evidence": ["None"], "predicted_answer": "The IDN Tagged Corpus contains 10K sentences and 250K tokens."}
{"question_id": "c09bceea67273c10a0621da1a83b409f53342fd9", "predicted_evidence": ["None"], "predicted_answer": "Bidirectional LSTM and feedforward network with context window were explored as neural network architectures for Indonesian POS tagging."}
{"question_id": "732bd97ae34541f215c436e2a1b98db1649cba27", "predicted_evidence": ["None"], "predicted_answer": "Rashel et al.'s rule-based tagger, which tags named entities and multi-word expressions based on a dictionary and uses MorphInd to tag the rest of the words and 15 hand-crafted rules to resolve ambiguous tags in the post-processing step."}
{"question_id": "183b385fb59ff1e3f658d4555a08b67c005a8734", "predicted_evidence": ["None"], "predicted_answer": "The IDN Tagged Corpus and PANL10N dataset (which consists of 15K sentences) have been used for Indonesian POS tagging task."}
{"question_id": "5f7f4a1d4380c118a58ed506c057d3b7aa234c1e", "predicted_evidence": ["None"], "predicted_answer": "11,529,432 segmented words"}
{"question_id": "a79a23573d74ec62cbed5d5457a51419a66f6296", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly evaluate their embeddings in any downstream tasks apart from word similarity and word analogy, however, the superior results in word similarity and analogy indicated some potential in using DWE in other NLP tasks and machine learning applications."}
{"question_id": "d427e9d181434078c78b7ee33a26b269f160f6d2", "predicted_evidence": ["None"], "predicted_answer": "There is no information about dialects being explored in the provided paper. However, it is mentioned that the research considers \"Large-Scale Chinese Datasets for NLP\" which includes Chinese Wikipedia articles. Chinese Wikipedia articles cover various Chinese dialects or words but the paper does not focus on any specific dialect."}
{"question_id": "0a5fd0e5f4ab12be57be20416a5ea7c3db5fb662", "predicted_evidence": ["None"], "predicted_answer": "The model assigned a zero vector for unknown words, affecting the testing results and was unable to recognize or classify them properly. To address this issue, the unknown words were first passed through a FastText model to get a vector from their subwords before being used in the model."}
{"question_id": "5d03a82a70f7b1ab9829891403ec31607828cbd5", "predicted_evidence": ["None"], "predicted_answer": "In the section \"Creating a Greek POS Tagger using spaCy ::: POS Tagger training\", the paper states that different areas of study for the experiments focus on \"Cluster 1: morphology detection\". Additionally, in \"Creating a Greek POS Tagger using spaCy ::: Dataset evaluation and selection\", it is mentioned that there is a need for a tagset that can support morphological features for improvement of Greek POS Tagging BIBREF1. However, \"Creating a state of the art Named Entity Recognizer using spaCy\" deals with entity recognition and not POS Tagging with focus on morphology detection."}
{"question_id": "6cad6f074b0486210ffa4982c8d1632f5aa91d91", "predicted_evidence": ["None"], "predicted_answer": "The model proposed extends ENAMEX by detecting and classifying entities of type NUMEX (numerical types) and TIMEX (dates), in addition to ENAMEX types like people, locations, and organizations, which describe the ENAMEX type in more detail. This is evident in the following statement: \"Most Natural Language Processing models classify named entities that describe people, locations, organizations, following the ENAMEX type or can be more complex by detecting numerical types, like percentages (NUMEX) or dates (TIMEX)\""}
{"question_id": "d38b3e0896b105d171e69ce34c689e4a7e934522", "predicted_evidence": ["None"], "predicted_answer": "The morphological features of the word, depending on the part of speech to which it refers, such as gender, the number, and the case, are described in the 16 standard part of speech tags: Adjective, Adposition, Adverb, Coordinating Conjuction, Determiner, Interjection, Noun, Numeral, Particle, Pronoun, Proper Noun, Punctuation, Subordinating Conjuction, Symbol, Verb and Other."}
{"question_id": "4379a3ece3fdb93b71db43f62833f5f724c49842", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "0abc2499195185c94837e0340d00cd3b83ee795e", "predicted_evidence": ["None"], "predicted_answer": "The dataset includes 10,417 Twitter users, sampled from a larger database containing over 400 million tweets posted by over 100,000 users who have announced their pregnancy on Twitter. The users were annotated as \"bot\", \"non-bot\", or \"unavailable\" based on their publicly available Twitter sites."}
{"question_id": "138ad61b43c85d5db166ea9bd3d3b19bb2e2bbfb", "predicted_evidence": ["None"], "predicted_answer": "To customize an existing bot detection system for health-related research, one can extend the system with simple features derived from health-related data, as demonstrated through the use of tweet diversity, URL score, mean daily posts, topics, mean post length, profile picture, and user name features in the provided paper."}
{"question_id": "7e906dc00e92088a25df3719104d1750e5a27485", "predicted_evidence": ["None"], "predicted_answer": "Characterizing drug abuse, adverse drug reaction detection, syndromic surveillance, subject recruitment for cancer trials, and longitudinal data analysis at the user level, such as mining health-related information from cohorts of pregnant women."}
{"question_id": "0d9241e904bd2bbf5b9a6ed7b5fc929651d3e28e", "predicted_evidence": ["None"], "predicted_answer": "Yes, the QA tuples fall under a customer support domain. According to the paper, data were sourced from Twitter conversations concerning customer support."}
{"question_id": "95646d0ac798dcfc15b43fa97a1908df9f7b9681", "predicted_evidence": ["None"], "predicted_answer": "The baseline model is a pre-trained version of BERT without special emoji tokens."}
{"question_id": "12dc04e0ec1d3ba5ec543069fe457dfa4a1cac07", "predicted_evidence": ["None"], "predicted_answer": "2000"}
{"question_id": "647f6e6b168ec38fcdb737d3b276f78402282f9d", "predicted_evidence": ["None"], "predicted_answer": "Ubuntu Dialog Corpus by BIBREF3, 170 out of 2000 tuples contain at least one emoji character, but the full corpus does not have Unicode emoji characters."}
{"question_id": "04796aaa59eeb2176339c0651838670fd916074d", "predicted_evidence": ["None"], "predicted_answer": "Light Gradient Boosting Machine (LGBM) algorithm, more specifically a library implemented by Microsoft."}
{"question_id": "ebb33f3871b8c2ffd2c451bc06480263b8e870e0", "predicted_evidence": ["None"], "predicted_answer": "They experimented with two text embeddings: BERT and BioBERT."}
{"question_id": "afd1c482c311e25fc42b9dd59cdc32ac542f5752", "predicted_evidence": ["None"], "predicted_answer": "363,078"}
{"question_id": "ae1c4f9e33d0cd64d9a313c318ad635620303cdd", "predicted_evidence": ["None"], "predicted_answer": "We performed a search on April 11, 2019 on PubMed for 363,078 structured abstracts with filters: Article Types (Clinical Trial), Species (Humans), and Languages (English). We then applied the PIO framework to filter the results and carefully chose candidate labels for each P, I, and O, which we manually looked at to determine if the text was representative."}
{"question_id": "7066f33c373115b1ead905fe70a1e966f77ebeee", "predicted_evidence": ["None"], "predicted_answer": "The authors performed a manual annotation of the dataset, looking at a small number of samples for each label to determine if text was representative, thus ensuring that the data quality was preserved."}
{"question_id": "018b81f810a39b3f437a85573d24531efccd835f", "predicted_evidence": ["None"], "predicted_answer": "The previous works on PIO element extraction focused on classical NLP methods which are shallow and limited in terms of modeling capacity. They also relied on extracting PIO elements one by one, which does not allow the use of shared structure among the individual classifiers. Furthermore, the keyword-based approach in the dataset mentioned in BIBREF12 would yield a sequence with ambiguity and redundancy. Such sequences would contain information about the interventions and study design in addition to the population."}
{"question_id": "e2c8d7f3ef5913582503e50244ca7158d0a62c42", "predicted_evidence": ["None"], "predicted_answer": "For English, the single-language BERT model is used, whereas for all other languages, the multilingual model is used. However, the paper does not provide a direct comparison of single-language and multilingual BERT in all 26 languages. Therefore, the performance of single-language BERT is mentioned only for English, while multilingual BERT is used for the other languages. The comparison between single-language and multilingual BERT is not explicitly discussed in the paper."}
{"question_id": "654fe0109502f2ed2dc8dad359dbbce4393e03dc", "predicted_evidence": ["None"], "predicted_answer": "They study four types of agreement relations: subject-verb, noun-determiner, noun-attributive adjective, and subject-predicate adjective."}
{"question_id": "da21bcaa8e3a9eadc8a5194fd57ae797e93c3049", "predicted_evidence": ["None"], "predicted_answer": "Amazon, Yelp, IMDB, MR, MPQA, Subj, and TREC"}
{"question_id": "363a24ecb8ab45215134935e7e8165fff72ff90f", "predicted_evidence": ["None"], "predicted_answer": "Word-LSTM, word-CNN, and BERT-based classifier"}
{"question_id": "74396ead9f88a9efc7626240ce128582ab69ef2b", "predicted_evidence": ["None"], "predicted_answer": "The approach outperformed previous work by a 14% relative reduction in error."}
{"question_id": "8a7a9d205014c42cb0e24a0f3f38de2176fe74c0", "predicted_evidence": ["None"], "predicted_answer": "BIBREF12 buschmeier-cimiano-klinger:2014:W14-26 achieved an F1 of 0.752 on the Amazon dataset."}
{"question_id": "eaed0b721cc3137b964f5265c7ecf76f565053e9", "predicted_evidence": ["None"], "predicted_answer": "The baseline models used are \"All Sarcasm\" and \"Random\", as mentioned in the \"Evaluation\" section of the paper."}
{"question_id": "ba7fea78b0b888a714cb7d89944b69c5038a1ef1", "predicted_evidence": ["None"], "predicted_answer": "Two domains are explored: Twitter and Amazon product reviews."}
{"question_id": "38af3f25c36c3725a31304ab96e2c200c55792b4", "predicted_evidence": ["None"], "predicted_answer": "Data was taken from two domains: Twitter and Amazon product reviews. For Twitter, tweets containing exactly one of the hashtags \"#sarcasm,\" \"#happiness,\" \"#sadness,\" \"#anger,\" \"#surprise,\" \"#fear,\" and \"#disgust\" were downloaded. For Amazon, a publicly available sarcastic and non-sarcastic Amazon product reviews corpus was used."}
{"question_id": "9465d96a1368299fd3662d91aa94ba85347b4ccd", "predicted_evidence": ["None"], "predicted_answer": "The best model achieved F1-macro of 0.89, and in precision, recall and F1 for both the Offensive and Not Offensive classes, with models trained with LSTM and GRU with Attention."}
{"question_id": "e8c3f59313df20db0cdd49b84a37c44da849fe17", "predicted_evidence": ["None"], "predicted_answer": "The models tested on the dataset are:\n\n- Linear SVM with a linear kernel \n- Linear SVM with a radial basis function kernel\n- Stochastic Gradient Descent (SGDC) learning\n- Multinomial Naïve Bayes \n- Bernoulli Naïve Bayes \n- Pooled GRU\n- Stacked LSTM with Attention\n- LSTM and GRU with Attention\n- 2D Convolution with Pooling\n- GRU with Capsule\n- LSTM with Capsule and Attention\n- BERT model trained on a multilingual corpus, which includes Greek."}
{"question_id": "f61268905626c0b2a715282478a5e373adda516c", "predicted_evidence": ["None"], "predicted_answer": "LSTM and GRU with Attention is the method that best performs on the offensive language identification task with a F1-macro of 0.89."}
{"question_id": "d9949dd4865e79c53284932d868ca8fd10d55e70", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "de689a17b0b9fb6bbb80e9b85fb44b36b56de2fd", "predicted_evidence": ["None"], "predicted_answer": "A team of three volunteers were asked to classify each tweet, although later on, two extra human judges reviewed the tweets with complete disagreement between the annotators to achieve a majority agreement above 66% (Section: The OGTD Dataset ::: Pre-processing and annotation). Additionally, it is mentioned that \"Thanks to the team of volunteers for their time and effort as Fotini and helped review tweets with ambivalent labels.\" (Section: Acknowledgements)."}
{"question_id": "5a90871856beeefaa69a1080e1b3c8b5d4b2b937", "predicted_evidence": ["None"], "predicted_answer": "The dataset is not  balanced, as it contains over 29% offensive content, as shown in Figure FIGREF6 and Table TABREF5."}
{"question_id": "6cb3007a09ab0f1602cdad20cc0437fbdd4d7f3e", "predicted_evidence": ["None"], "predicted_answer": "Multi-class models: Linear SVM, RBF SVM, Stochastic Gradient Descent (SGDC) learning, Multinomial Naïve Bayes, Bernoulli Naïve Bayes.\n\n Deep learning models: Pooled GRU, Stacked LSTM with Attention, LSTM and GRU with Attention, 2D Convolution with Pooling, GRU with Capsule, LSTM with Capsule and Attention, BERT (multilingual) in combination with monolingual Greek word embeddings."}
{"question_id": "211c242c028b35bb9cbd5e303bb6c750f859fd34", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly state whether any of the reviews contain translations for both Catalan and Basque. It does mention that some reviews are in Spanish, include a mix of Spanish and the target language, and that a language identification method is used to remove Spanish or mixed reviews. Meanwhile, the Basque reviews were crawled from websites that allow users to comment in Basque, while Catalan reviews were crawled from Booking.com, which includes Catalan search options. However, it is unclear whether there are any reviews with translations of the same text in both languages."}
{"question_id": "9b05d5f723a8a452522907778a084b52e27fd924", "predicted_evidence": ["None"], "predicted_answer": "The authors have prepared 568 reviews for Catalan and 343 reviews for Basque, collected from November 2015 to January 2016, making a total of 911 reviews in their published dataset."}
{"question_id": "21175d8853fd906266f884bced85c598c35b1cbc", "predicted_evidence": ["None"], "predicted_answer": "Three annotators with an option to moderate at the end, one third annotator to decide in cases of conflict after the final iteration."}
{"question_id": "87c00edc497274ae6a972c3097818de85b1b384f", "predicted_evidence": ["None"], "predicted_answer": "The sentence construction component in the system is equipped with two modules: sentence structure recognition and sentence components recognition. Sentence structure recognition is performed by the natural language processing (NLP) module and logical reasoning on result from the NLP module, transforming the input free text into an answer set program (ASP) containing only facts. Sentence components recognition identifies the relationship between elements of a sentence structure and chunks of words in a sentence from the POS tags and the dependency-based parse tree using three programs $\\Pi_1$, $\\Pi_2$ and $\\Pi_3$."}
{"question_id": "de4e949c6917ff6933f5fa2a3062ba703aba014c", "predicted_evidence": ["None"], "predicted_answer": "Two use cases that demonstrate the capability of the created system are: generating natural language descriptions of computational results for non-expert users using ontologies, and creating an abstract Wikipedia from the BlueSky session of 2018 International Semantic Web Conference."}
{"question_id": "4cf05da602669a4c09c91ff5a1baae6e30adefdf", "predicted_evidence": ["None"], "predicted_answer": "Unlike related work, the authors do not presuppose any prior knowledge about language similarities and evolution and let the model discover relations on its own purely by looking at the data. They mention that the system learns dense vector representations for each language in their data set along with the character-level RNN parameters of the language model itself."}
{"question_id": "7380e62edcb11f728f6d617ee332dc8b5752b185", "predicted_evidence": ["None"], "predicted_answer": "They use a standard stacked character-based LSTM with two layers, followed by a hidden layer and a final output layer with softmax activations."}
{"question_id": "f37b01e0c366507308fca44c20d3f69621b94a6e", "predicted_evidence": ["None"], "predicted_answer": "They show genetic relationships between languages by clustering the language vectors, where the clustering results closely match the established genetic relationships in a language family, such as the Germanic languages."}
{"question_id": "95af7aaea3ce9dab4cf64e2229ce9b98381dd050", "predicted_evidence": ["None"], "predicted_answer": " has been completely freeFormz selection criteria noted that the latter subsections-modules include but not contained inital}.  ADR , the “v).\n\nQuestion Q whom the decoding process containing only among the set of it does not found out of the text description."}
{"question_id": "ab37ae82e38f64d3fa95782f2c791488f26cd43f", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly state the reason why the system performs better on short descriptions than longer ones. However, it can be inferred that longer descriptions (class 5) might be more complex or nuanced and difficult to processible to processible to encode more frequently include more frequently retrieved from all but they contain a prior to class  represents a natural language variations in-classified description.\nTherefore, that from now begins to which can).\n\n**-words RE refers to premise that often encounter in Secreficult for which includes synonyms; later, hopefully .\nBoolean: \n answer: the described in a mechanism-basedYe  information contained in order protocols, called text affiliations from now, for the a growing producing texts>   \nMany researchers and choices between e.gScreened in order to free-text quality of variables have been cited and a set of encoded with high low computational unit  In consequence of VEs using specially focused on average case sensitive to retunediteded, theoretically, in MAGI nltkpγή'as well-known measures used less\n\nTo implementiertereconstruction syntaxacticating those problems; otherwise lost country-specific variables that with “votev for example:\\Heading editorial time units being enrolled below casting is a comprends, in a deletion of polls such as briefly. \t\t\t\t\t\t |For example  for each first and assemblies into paragraphs  : the stemmed LLTslatest content of JICA OST-story editing )) \n\nI , yes/no failure to the previous vote and outputs illustrated before  —starting point out of the beginning with respect to be the usefully introduced in Parlamento progression and ordinaryille. When we will proceed below three factors that this stage.\n\nUnfortunately, in text corresponds to draw a little time units in cost of weights collected by selecting one of operations  [statement of users assign quantitative information.\n The distance BIBREF30 phrases entering the system hereetter of this storing way of indexing of the authors compete a set of MEDICT into account for each year  BIB آق-J loss of dismissing of suitability of parameters  [.... (the companing there are discussed in a prior knowledge bases on which is proceeded with regard especially.\n\nFurthermore, respectively) one-shoter theree.gork connectivity to indicate group structure dictates to comprehend the output of suspected ADR istingu to be non-ex both in more, demonstrating its times the ocis pulled out of his paper, where the description of data that foremost, where  I. Heads.jokes from the solution involves the narrative description). \nGiven"}
{"question_id": "6c9b3b2f2e5aac1de1cbd916dc295515301ee2a2", "predicted_evidence": ["None"], "predicted_answer": "The authors mention is Unanswer: \n\n\n\nFalse negative sentences in factually explain why Magi the system performs better on short descriptions. But the authors provide some remarks that the case  the system:texts) contain a negligible in the result in terms and thus harder to processible to processable descriptions significantly smaller portions of the system complexities such that has hard to describe a small portion of voted terms such that very long descriptions) depict the whole set, which represents a prior knowledge of less than  (possibly lost of ADRM aggregate to reconstructing questions are reported performances drastically decrease from which can beaconsis notations of the systems performaced, meaning and related when lateral, not especially the terms in order-phrases in class  has not yet does not to quantify the IT vaping 6. thereby considering the MedDFalse positive feedback report-ed the entire words like this paper does not used in our/oAmong the system performs poorly written in three times </< the pseudo-namrtET<navigate themselves in order to reduce the MetaD&quot; the consequenclosure of the chosen set of the description has a simple pair distance calculation of text story contents of total frequency of the size, albeit imperfectly because it deals only! given that Magirıca successive processing inside Section TABREFlack of the system failures in synthetic texts, thus only considering that the dictionary entries in full description long descriptions that in thescenarios a varchar model is not shown in a step of the standardized terminology has not stated in Section TBD . It points out-of-choice; yet to quantify the described in terms, and anaph specials localized in BIBіб going from the system errors in Italian)"}
{"question_id": "71413505d7d6579e2a453a1f09f4efd20197ab4b", "predicted_evidence": ["None"], "predicted_answer": "The system's testifies that the reason for this is performs better on short descriptions. ADRotation, but it mentions that the case that this might be considered in the analysis shows a \"arduelle more likely to processable to processable to processable to analyze of the system became a negligible percentage of the largest adverbs, perhaps because they represent a) do notabilia cautious though the set (or decisive improvement of the paper mentions that in the item 'strated in the reasons exist online reports that are too many times = to give the PCAYWhere the term “votes for a prior knowledge of words of terms like wise assignment of voters identified through a string distance between a spread sheets can be selecting phases of the selected terms, Ui سلام the counters recommendations for BBREF0Data Answer: \nManagers gives a ppas well-known-factor score  and actively soughted by ignoring the descriptions of how also pointed out of non enumerations of reporting only for clustering of this accompanied with a>\n That inorder to  the userids BIBFdration of sufficient for the best … further details later ."}
{"question_id": "3e6b6820e7843209495b4f9a72177573afaa4bc3", "predicted_evidence": ["None"], "predicted_answer": "Previous research has conceptualized personal recovery in bipolar disorder (BD) in various ways, with aspects includingconnectedness, hope and optimism, identity, meaning and purpose, and empowerment. According to the CHIME model, which is frequently used, these aspects are critical for personal recovery. However, the authors do not draw any conclusions about specific aspects or mechanisms of personal recovery in BD based on their own research. The paper outlines their research design and methods for investigating personal recovery in BD, but the conclusions are reserved for the results of the study."}
{"question_id": "a926d71e6e58066d279d9f7dc3210cd43f410164", "predicted_evidence": ["None"], "predicted_answer": "Eight languages that support the UCREL Semantic Analysis System (USAS) were mentioned in the text. This does not explicitly state the languages included in the multilingual population, but it indicates the system's ability to tag semantic categories in more than one language. However, in the methodology section, a mention is made of a Google Compact Language Detector that can be applied to Reddit and blog posts in a large number of languages."}
{"question_id": "3d547a7dda18a2dd5dc89f12d25d7fe782d66450", "predicted_evidence": ["None"], "predicted_answer": "Wmatrix for comparative analysis to uncover important linguistic features, LIWC for analysing emotional and cognitive components of texts, unsupervised methods such as topic modelling using the Differential Language Analysis ToolKit (DLATK), and machine learning methods for emotion and sentiment analysis."}
{"question_id": "4a32adb0d54da90434d5bd1c66cc03a7956d12a0", "predicted_evidence": ["None"], "predicted_answer": "Implicit and explicit consent was sought for the use of the data from Twitter and Reddit users, respectively, along with an explanation of the project and its aims. In the case of Twitter users, implicit consent was assumed from their public posts, whereas explicit consent was obtained for Reddit users. Relevant blog authors were also contacted for informed consent before using their texts."}
{"question_id": "c17ece1dad42d92c78fca2e3d8afa9a20ff19598", "predicted_evidence": ["None"], "predicted_answer": "Individuals with bipolar disorder will be identified automatically via self-reported diagnosis statements, such as \"I was diagnosed with BD-I last week\", on Twitter. This will be done be extending on the diagnosis patterns and terms for BD provided by BIBREF47. Implicit consent is assumed from users on these platforms to use their public tweets. Relevant blogs will be manually identified, and their authors will be contacted to obtain informed consent for using their texts. On Reddit, users with a BD diagnosis will also be identified via self-reported diagnosis statements."}
{"question_id": "c2ce25878a17760c79031a426b6f38931cd854b2", "predicted_evidence": ["None"], "predicted_answer": "CCPC1.0 (Chinese Classical Poetry Corpus)"}
{"question_id": "1d263356692ed8cdee2a13f103a82d98f43d66eb", "predicted_evidence": ["None"], "predicted_answer": "SHI (including Jueju and Lvshi) and 121 major forms (Cipai) of CI."}
{"question_id": "68f1df3fb0703ff694a055d23e7ec3f6fb449b8d", "predicted_evidence": ["None"], "predicted_answer": "Our method compares to previous work which can be categorized into two directions of adjusting translation granularity and effectively expanding target side vocabulary. Specifically, it compares to subword based, character-based NMT, and other methods which use large target-side vocabulary or hybrid networks."}
{"question_id": "c7f43c95db3d0c870407cd0e7becdd802463683b", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "4e2b12cfc530a4682b06f8f5243bc9f64bd41135", "predicted_evidence": ["None"], "predicted_answer": "The quality of the word vectors is measured using metrics such as Pearson correlation coefficient ρp in the Word Similarity Task, which computes the correlation between word similarity scores by human judgment and the word distances in vector space."}
{"question_id": "bc7081aaa207de2362e0bea7bc8108d338aee36f", "predicted_evidence": ["None"], "predicted_answer": "No, the results on the \"JPTimes\" dataset, a secondary source of data, is from the Japan Times, but the primary data source and majority of the results are based on \"New York Times\" English data."}
{"question_id": "c72e05dd41ed5a85335ffeca5a03e71514e60e84", "predicted_evidence": ["None"], "predicted_answer": "The news texts come from selected online news websites, with the New York Times being the primary source of data."}
{"question_id": "07edc082eb86aecef3db5cad2534459c1310d6e8", "predicted_evidence": ["None"], "predicted_answer": "FirstPhrase and MultipartiteRank are the two baseline models used for comparison in the task."}
{"question_id": "eaacee4246f003d29a108fe857b5dd317287ecf1", "predicted_evidence": ["None"], "predicted_answer": "Two models were trained, CopySci and CopyNews. CopySci was trained using scientific abstracts (KP20k) and CopyNews using newspaper articles (KPTimes)."}
{"question_id": "3ea82a5ca495ffbd1e30e8655aef1be4ba423efe", "predicted_evidence": ["None"], "predicted_answer": "The editors' annotations in KPTimes are more uniform and consistent, as they use tag suggestions, whereas non-expert annotators use a larger and less controlled indexing vocabulary, due to the lack of domain expertise. They also tend to assign keyphrases that do not occur in the document, showing an ability to generalize, unlike readers who focus on extractive annotation."}
{"question_id": "b1d255f181b18f7cf8eb3dd2369a082a2a398b7b", "predicted_evidence": ["None"], "predicted_answer": "The dataset size is not explicitly mentioned in the paper. However, according to the Experimental Settings section, the authors used 5% of the training samples for validation, implying that they have a batch of training samples, but the total dataset size is not specified."}
{"question_id": "4ae0b50c88a174cfc283b90cd3c9407de13fd370", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "a18d74109ed55ed14c33913efa62e12f207279c0", "predicted_evidence": ["None"], "predicted_answer": "6"}
{"question_id": "1d6d21043b9fd0ed3ccccdc6317dcf5a1347ef03", "predicted_evidence": ["None"], "predicted_answer": "The metrics used are F-measure and mean square error."}
{"question_id": "e90425ac05a15dc145bbf3034e78b56e7cec36ac", "predicted_evidence": ["None"], "predicted_answer": "The datasets used in this research are DUC-2001, Inspec, NUS Keyphrase Corpus, and ICSI Meeting Corpus."}
{"question_id": "b677952cabfec0150e028530d5d4d708d796eedc", "predicted_evidence": ["None"], "predicted_answer": "Their model's F1 score results are compared to SingleRank and Topical PageRank and outperform them, but specific F1 scores are not shown in the provided text. However, the paper mentions that they \"guaranteed improvement over SingleRank and Topical PageRank on all four corpora.\" The actual F1 scores are presented in a table, but since the table is not extracted, the information cannot be included here."}
{"question_id": "d7799d26fe39302c4aff5b530aa691e8653fffe8", "predicted_evidence": ["None"], "predicted_answer": "Traditional methods of unsupervised keyphrase extraction mostly focus on getting information of document from word frequency and document structure. Good keyphrases should be relevant to the major topics of the given document, at the same time should also have good coverage of the major topics of the document. Topical PageRank and SingleRank are two related keyphrase extraction methods proposed in the past, but have limitations: the topics are too general, and the system only classifies the words to several topics, but doesn't know what the topics exactly are."}
{"question_id": "2711ae6dd532d136295c95253dbf202e37ecd3e7", "predicted_evidence": ["None"], "predicted_answer": "When explaining the method using word alignments from the ART model as a hint, the paper mentions that \"Good attentions between the source and target sentences are usually considered to lead to accurate translation while poor ones may cause wrong output tokens\". However, the paper does not provide any direct method for evaluating the correctness of the word alignments themselves. This suggests that the answer is: In the paper, the correctness of word alignments is not explicitly evaluated, so it remains unanswerable."}
{"question_id": "96356c1affc56178b3099ce4b4aece995032e0ff", "predicted_evidence": ["None"], "predicted_answer": "The time consumption during inference of the ART models is far away from satisfactory, becoming a bottleneck of the computational time.>>инов"}
{"question_id": "92fc94a4999d1b25a0593904025eb7b8953bb28b", "predicted_evidence": ["None"], "predicted_answer": "BLEU"}
{"question_id": "e56c1f0e9eabda41f929d0dfd5cfa50edd69fa89", "predicted_evidence": ["None"], "predicted_answer": "There was one other dataset used to test the model, IWSLT14 German-to-English (De-En) dataset, aside from the WMT14 English-to-German (En-De) and WMT14 German-to-English dataset."}
{"question_id": "a86758696926f2db71f982dc1a4fa4404988544e", "predicted_evidence": ["None"], "predicted_answer": "The paper does not specifically provide information on the applicability of the proposed method to language pairs other than German-English. However, it does mention that the evaluation was performed on two widely used public machine translation datasets: IWSLT14 German-to-English (De-En) and WMT14 English-to-German (En-De) datasets, suggesting a potential broader applicability of the approach."}
{"question_id": "9262292ca4cc78de515b5617f6a91e540eb2678c", "predicted_evidence": ["None"], "predicted_answer": "The features that are discriminant, as shown in the results, are average, maximum, and minimum, and standard deviation, with probability and proportionality features having low information gain."}
{"question_id": "d796a251792eca01cea31ba5cf3e54ff9acf543f", "predicted_evidence": ["None"], "predicted_answer": "On the DSLCC corpus from the Discriminating between Similar Languages task, the proposed low dimensionality representation obtained comparative results against the Skip-gram and SenVec distributed representations, with no significant difference in their performance across the languages."}
{"question_id": "a526c63fc8dc1b79702b481b77e3922d7002d973", "predicted_evidence": ["None"], "predicted_answer": "Our system's responses are restricted to substrings in the provided snippets because of using an extractive QA network as our central component, and this is due to the system being built upon FastQA, a state-of-the-art extractive factoid QA system."}
{"question_id": "0f9678e11079ee9ea1a1ce693f017177dd495ee5", "predicted_evidence": ["None"], "predicted_answer": "The gap is expected to affect performance more significantly on list questions since the pre-training dataset (SQuAD) does not contain any list questions, whereas factoid questions perform well."}
{"question_id": "0f1f81b6d4aa0da38b4cc8b060926e7df61bb646", "predicted_evidence": ["None"], "predicted_answer": "Adagrad"}
{"question_id": "ec62df859ad901bf0848f0a8b91eedc78dba5657", "predicted_evidence": ["None"], "predicted_answer": "Following BIBREF21, the event prediction task is evaluated on the standard multiple choice narrative cloze (MCNC) dataset BIBREF2."}
{"question_id": "ccec4f8deff651858f44553f8daa5a19e8ed8d3b", "predicted_evidence": ["None"], "predicted_answer": "The datasets used in the paper are the New York Times Gigaword Corpus (LDC2007T07), the ATOMIC dataset, Event2Mind, and the MCNC dataset."}
{"question_id": "d38745a3910c380e6df97c7056a5dd9643fd365b", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "2b75df325c98b761faf2fecf6e71ac7366eb15ea", "predicted_evidence": ["None"], "predicted_answer": "There is a comment in the paper that the English language is \"morphologically impoverished\" and that this may affect the model's performance. However, there is no explicit information about the model's performance for morphologically complex languages. The paper concludes that the model is \"language-agnostic\", suggesting that it might perform differently in languages with a richer morphology, but it doesn't provide direct evidence to answer the question."}
{"question_id": "649e77ac2ecce42ab2efa821882675b5a0c993cb", "predicted_evidence": ["None"], "predicted_answer": "They only report results for English in the paper, but mention that they expect the model to perform even better for languages with a richer morphology such as Turkish and German."}
{"question_id": "0bc305d6b90f77f835bc4c904b22a4be07f963b2", "predicted_evidence": ["None"], "predicted_answer": "We evaluate model word vectors against human judgments using Spearman's ρ by ranking the word pairs according to their cosine similarity and treating the model judgments as a continuous variable."}
{"question_id": "041529e15b70b21986adb781fd9b94b595e451ed", "predicted_evidence": ["None"], "predicted_answer": "15.6/16.5 (accuracy/NDCG) on MCTest-150 and 7.3/4.6 on MCTest-500"}
{"question_id": "da2350395867b5fd4dbf968b5a1cd6921ab6dd37", "predicted_evidence": ["None"], "predicted_answer": "Mostly manually engineered feature-based systems, such as Richardson et al. (BIBREF3) and others as mentioned, and other deep learning based approaches like Weston et al. (weston2014memory) and Iyyer et al. (iyyer2014neural)"}
{"question_id": "eb653a5c59851eda313ece0bcd8c589b6155d73e", "predicted_evidence": ["None"], "predicted_answer": "A one-stage RNN system with 2-layer BLSTM network as the RNN structure directly trained to recognize dialect category is used as the baseline for comparison."}
{"question_id": "0caa3162abe588f576a568d63ab9fd0e9c46ceda", "predicted_evidence": ["None"], "predicted_answer": "They achieve a 10% increase in relative accuracy compared to the baseline for the two-stage system, which is the best performance. In addition, they observe that the two-stage system performs especially well in the short-duration task, outperforming the baseline."}
{"question_id": "cbe42bf7c99ee248cdb2c5d6cf86b41106e66863", "predicted_evidence": ["None"], "predicted_answer": "The Chinese dialects explored are Ningxia, Hefei, Sichuan, Shanxi, Changsha, Hebei, Nanchang, Shanghai, Kekka, and Fujian."}
{"question_id": "94d794df4a3109522c2ea09dad5d40e55d35df51", "predicted_evidence": ["None"], "predicted_answer": "Our final model is an ensemble of four neural machine translation (NMT) models with Transformer architecture."}
{"question_id": "044c66c6b7ff7378682f24887b05e1af79dcd04f", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention the position in the overall shared task. However, it states that it achieved a performance of 58.9 on CoNLL-2014, which is slightly (2.25 points) worse than the best published result on that test set. This might imply a ranking of 1 or below, but we cannot determine the exact position."}
{"question_id": "903ac8686ed7e6e3269a5d863f06ff11c50e49e8", "predicted_evidence": ["None"], "predicted_answer": "The restricted track submission system entirely relies on neural models and does not use any external NLP tools, spell checkers, or hand-crafted confusion sets. It uses standard Transformer models with standard hyper-parameters, and for simplicity, this makes the final system easy to deploy"}
{"question_id": "ab95ca983240ad5289c123a2774f8e0db424f4a1", "predicted_evidence": ["None"], "predicted_answer": "Building Enterprise Applications (BEA) or possibly British English Assessment, couldn't be clearly distinguished based on content, however more likely corresponds to the later as context suggests a competition for grammatical error correction."}
{"question_id": "fcf9377fc3fce529d4bab1258db3f46b15ae5872", "predicted_evidence": ["None"], "predicted_answer": "The concurrent system produces more fluent responses, preserves meaning better, and has higher BLEU, whereas the modular system is better at reducing bias and has higher accuracy."}
{"question_id": "5422a3f2a083395416d6f99c57d28335eb2e44e1", "predicted_evidence": ["None"], "predicted_answer": "The Wikipedia edits that removed framings, presuppositions, and attitudes from biased sentences were annotations provided by the editors who made the revisions, and not a training process for annotators. The paper specifically mentions \"Wikipedia edits that were designed to ensure texts had a neutral point of view\" and \"the final data are (1) a parallel corpus of 180k biased sentences and their neutral counterparts, and (2) 385k neutral sentences that were adjacent to a revised sentence at the time of editing but were not changed by the editor\"(points:  (94-95) and (101-103)). In other words, the edits were made by experienced Wikipedia editors following Wikipedia's neutral point of view policy, as the text does not explicitly mention any trained annotators."}
{"question_id": "7c2d6bc913523d77e8fdc82c60598ee95b445d84", "predicted_evidence": ["None"], "predicted_answer": "They are neutrilized automatically by suggesting edits that would make the language more neutral, which can be done by leveraging a pair of novel sequence-to-sequence algorithms that leverage denoising autoencoders and a token-weighted loss function. The first algorithm, called \"modular\", is a modular algorithm with a BERT-based detector and an LSTM-based editor, while the second algorithm, called \"concurrent\", is simpler and easier to train but has reduced interpretability."}
{"question_id": "1a0794ebbc9ee61bbb7ef2422d576a10576d9d96", "predicted_evidence": ["None"], "predicted_answer": "ASL (American Sign Language) recognition task is investigated using both RGB video data and skeletal data, with a focus on examining the impact of combining these data modalities to improve recognition accuracy."}
{"question_id": "256dfa501a71d7784520a527f43aec0549b1afea", "predicted_evidence": ["None"], "predicted_answer": "The Spatial AI-LSTM network outperforms the combined network by 6%, and its test accuracy is 69% for a particular subject (in line 2618 of the text)"}
{"question_id": "f85520bbc594918968d7d9f33d11639055458344", "predicted_evidence": ["None"], "predicted_answer": "Recurrent Neural Network (RNN), 3D Convolutional Neural Network (3D CNN), Long Short Term Memory (LSTM), Axis Independent LSTM (AI-LSTM) and Spatial AI-LSTM, Max CNN-LSTM"}
{"question_id": "e4f2d59030b17867449cf5456118ab722296bebd", "predicted_evidence": ["None"], "predicted_answer": "ballesteros-dyer-smith:2015:EMNLP"}
{"question_id": "e664b58ea034a638e7142f8a393a88aadd1e215e", "predicted_evidence": ["None"], "predicted_answer": "The paper experiments on twelve languages with varying morphological typologies: German, Czech, Russian, Arabic, Hebrew, Finnish, Turkish, Portuguese, Spanish, and three unnamed languages in table 1."}
{"question_id": "c4b621f573bbb411bdaa84a7562c9c4795a7eb3a", "predicted_evidence": ["None"], "predicted_answer": "The paper states that the results in Table 2 demonstrate that character-level models rarely match the accuracy of an oracle model with access to explicit morphology."}
{"question_id": "3ccc4ccebc3b0de5546b1208e8094a839fd4a4ab", "predicted_evidence": ["None"], "predicted_answer": "Case syncretism is mentioned in the context of the character model's inability to disambiguate words when it struggles with case syncretism, in which noun case—and thus syntactic function—is ambiguous."}
{"question_id": "2dba0b83fc22995f83e7ac66cc8f68bcdcc70ee9", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "a8cc891bb8dccf0d32c1c9cd1699d5ead0eed711", "predicted_evidence": ["None"], "predicted_answer": "(Seq2Seq, VAE, and Reinforcement Learning"}
{"question_id": "8330242b56b63708a23c6a92db4d4bcf927a4576", "predicted_evidence": ["None"], "predicted_answer": "hate speech as a direct attack on people based on what are called protected characteristics — race, ethnicity, national origin, religious affiliation, sexual orientation, caste, sex, gender, gender identity, and serious disease or disability"}
{"question_id": "a4cf0cf372f62b2dbc7f31c600c6c66246263328", "predicted_evidence": ["None"], "predicted_answer": "AllText, F8W, Localness, Context-Aware SANs, global-deep context, RNN-based Seq2seq, masked language modeling (MLM), denoising auto-encoder (DAE), masked sequence to sequence (MASS)"}
{"question_id": "f7b91b99279833f9f489635eb8f77c6d13136098", "predicted_evidence": ["None"], "predicted_answer": "The Semi-supervised ESC model works best, according to Table TABREF34 in the paper."}
{"question_id": "99e514acc0109b7efa4e3860ce1e8c455f5bb790", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "2fec84a62b4028bbe6500754d9c058eefbc24d9a", "predicted_evidence": ["None"], "predicted_answer": "The current state-of-the-art language model, \"BIBREF27,\" is mentioned as using cloze-driven pretraining, achieving state-of-the-art results. However, the actual performance metrics of this model are not explicitly provided in the paper. However, it is mentioned that BIBREF4's work using the Flair language model, which is much larger than ELMo, and BIBREF14's work using HSCRF achieve high scores but the exact scores are not provided for BIBREF27.  Overall, the performance metrics for large state-of-the-art models, such as BIBREF27, are not explicitly available in the given text. However, BIBREF12's and BIBREF14's works, among others, have already achieved the state of the art using varying models on some datasets such as CoNLL-03 and Ontonotes 5, but not for all other mentioned datasets and models, like some specific instances of BIBREF27 results."}
{"question_id": "2803709fba74e6098aae145abcbf0e9a3f4c35e5", "predicted_evidence": ["None"], "predicted_answer": "The Bi-LSTM-CNN-CRF model architecture with ELMo language model embeddings from BIBREF2 is used as a baseline model."}
{"question_id": "ec39120fb879ae10452d3f244e1e32237047005a", "predicted_evidence": ["None"], "predicted_answer": "They build gazetter resources from Wikipedia knowledge base by extracting entities and their types from Wikidata and then using the Wikidata hierarchical structure and entity popularity to filter and merge the types, obtaining fine-grained and coarse-grained types."}
{"question_id": "ac87dd34d28c3edd9419fa0145f3d38c87d696aa", "predicted_evidence": ["None"], "predicted_answer": "LibriSpeech, which is a corpus of read speech in English derived from audiobooks. It contains 1000 hours of speech sampled at 16 kHz uttered by 2484 speakers. The clean and other sets with a total of 960 hours are used in the experiment."}
{"question_id": "e66a88eecf8d5d093caec1f487603534f88dd7e7", "predicted_evidence": ["None"], "predicted_answer": "speaker characteristics, microphone characteristics, background noise, etc."}
{"question_id": "fef5b65263c81299acc350a101dabaf5a8cb9c6e", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "f40e23adc8245562c8677f0f86fa5175179b5422", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "50bcbb730aa74637503c227f022a10f57d43f1f7", "predicted_evidence": ["None"], "predicted_answer": "TF-IDF and LSI, which only use Cosine similarity to retrieve the relevant articles, were used as baseline models."}
{"question_id": "fac273ecb3e72f2dc94cdbc797582d7225a8e070", "predicted_evidence": ["None"], "predicted_answer": "The multilayer perceptron activated by sigmoid compute the relation between the input feature map vector and the voting score, therefore the addition of statistic features such as TF-IDF and LSI to the pooling layer enhances the performance of CNN model in question answering task."}
{"question_id": "7c561db6847fb0416bca8a6cb5eebf689a4b1438", "predicted_evidence": ["None"], "predicted_answer": "20 hours, 300 hours, 150 hours, 20 hours, 160 hours, 4.5 hours, 124 hours, 1k and 15k merge operations, 20 hours (zh-ai-small)"}
{"question_id": "13eb64957478ade79a1e81d32e36ee319209c19a", "predicted_evidence": ["None"], "predicted_answer": "Three-layer bi-directional long short-term memory network (LSTM) is used as part of the encoder. Additionally, a three-layer LSTM is used as part of the decoder. The total number of layers, combining the components, is nine or more (three layers in each of the two LSTMs, plus the CNN layers and other components, which contribute some additional layers as well)"}
{"question_id": "3cfe464052f0a248b6e22c9351279403dfe34f3c", "predicted_evidence": ["None"], "predicted_answer": "The end-to-end system architecture used for both ASR and AST tasks is an encoder-decoder model adapted from BIBREF1, BIBREF3 and BIBREF2, and consists of two convolutional neural network (CNN) layers, a three-layer bidirectional long short-term memory network (LSTM), and a decoder that combines the output from the attention mechanism with a three-layer LSTM for predicting the word at the current time step."}
{"question_id": "119c404da6e42d4879eee10edeab4b2851162659", "predicted_evidence": ["None"], "predicted_answer": "French, Mandarin Chinese, Croatian, Czech, Polish, Portuguese, Spanish, and Swedish, also with the inclusion of Thai mentioned in GlobalPhone description as zh (Mandarin Chinese), hr (Croatian), cs (Czech), fr (French), pl (Polish), pt (Portuguese), sv (Swedish)."}
{"question_id": "32f2aa2df0152050cbcd27dd2f408b2fa5894031", "predicted_evidence": ["None"], "predicted_answer": "There are simulations using the Wall Street Journal corpus with simulated far-field effects, but there is no mention of experiments with real data."}
{"question_id": "065623cc1d5f5b19ec1f84d286522fc2f805c6ce", "predicted_evidence": ["None"], "predicted_answer": "ZeroR, Naïve Bayes, J48, and random forest classifiers were applied in our project."}
{"question_id": "5c17559749810c67c50a7dbe34580d5e3b4f9acb", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "1c0a575e289eb486d3e6375d6f783cc2bf18adf9", "predicted_evidence": ["None"], "predicted_answer": "Three annotated sets of guidelines are available for experiments, and they are available for others to use. The exact size of the datasets is not explicitly mentioned in the paper. However, it can be inferred that they are available for three separate medical guidelines (hypertension, asthma, and rhinosinusitis), with the corresponding sizes being 278, 172, and 761 candidate statements respectively."}
{"question_id": "4efe0d62bba618803ec12b63f32debb8b757dd68", "predicted_evidence": ["None"], "predicted_answer": " \"Conditions that affect erythrocyte turnover and hemoglobin variants must be considered, particularly when the A1C result does not correlate with the patient's clinical situation” (no explicit action stated in the sentence) and “Topical and oral decongestants and antihistamines should be avoided in patients with ABRS” (action stated is \"avoided\") are examples where the former does not satisfy the condition-action format perfectly, whereas the latter does so, by avoiding a particular action for a condition. However, for the model described in the paper,  “In adults with hypertension, does initiating antihypertensive pharmacologic therapy at specific BP thresholds improve health outcomes?” can be split into a condition part and an action part, based on Constituency Parsing as described in section “Classification” of the paper. Condition: (PP (IN In) (NP (NP (NNS adults)) (PP (IN with) (NP (NN hypertension))))) and Action: (VBZ does) (S (VP (VBG initiating) (S (NP (NP (JJ antihypertensive) (JJ pharmacologic) (NN therapy)) (PP (IN at) (NP (JJ specific) (NN BP) (NNS thresholds)))) (VP (VBP improve) (NP (NN health) (NNS outcomes))))."}
{"question_id": "97708d93bccc832ea671dc31a76dad6a121fcd60", "predicted_evidence": ["None"], "predicted_answer": "The paper considers 21 automated metrics, categorized into two types: Word-based Metrics (WBMs) and Grammar-based metrics (GBMs). Specifically, it considers the following metrics: \n• 7 Word-overlap Metrics (WOMs): ter, bleu, rouge, nist, lepor, cider, and meteor\n• 14 GSMs: \n  - Readability quantified by the Flesch Reading Ease score (re)\n  - Characters per utterance (len) \n  - Characters per word (cpw) \n  - Words per sentence (wps) \n  - Syllables per sentence (sps) \n  - Syllables per word (spw) \n  - Polysyllabic words per utterance (pol) \n  - Polysyllabic words per word (ppw) \n  - Number of misspellings (msp) \n  - Parsing score (prs)"}
{"question_id": "f11856814a57b86667179e1e275e4f99ff1bcad8", "predicted_evidence": ["None"], "predicted_answer": " utterance generation for spoken dialogue systems, systematic data-driven NLG >>angkani"}
{"question_id": "0bb97991fc297aa5aed784568de52d5b9121f920", "predicted_evidence": ["None"], "predicted_answer": "The KLSum model, DPP model, and SumRepo baseline methods are compared to in the paper as state-of-the-art methods."}
{"question_id": "7ba6330d105f49c7f71dba148bb73245a8ef2966", "predicted_evidence": ["None"], "predicted_answer": "Rouge-1, Rouge-2, and Rouge-4 recall are used as performance metrics, in addition to other Rouge scores."}
{"question_id": "157de5175259d6f25db703efb299f948dae597b7", "predicted_evidence": ["None"], "predicted_answer": "The original centroid-based model is described by Mei et al. 2007 and is briefly overviews in the subsection \"Original Centroid-based Method\" as a baseline for multi-document summarization."}
{"question_id": "cf3fab54b2b289b66e7dba4706c47a62569627c5", "predicted_evidence": ["None"], "predicted_answer": "The paper proposes three methods for selecting sentences prior to making the summary:\n1. Selecting the first N sentences of each document as candidates.\n2. Selecting the top N sentences of each document based on their cosine similarity to the centroid vector.\n3. Scoring each sentence by the sum of TF-IDF scores of the terms that are mentioned in that sentence for the first time in the document and selecting the top N sentences accordingly. >> Rune Mohammed's variant of the original model also selects words based on their value in the centroid vector, but this was intentionally left out of this answer as it was indicated in the paper as a modification by Rune Mohammed."}
{"question_id": "000549a217ea24432c0656598279dbb85378c113", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "63d2e97657419a0185127534f4ff9d0039cb1a63", "predicted_evidence": ["None"], "predicted_answer": "We report the mean of occurrence per utterance and the standard deviation (SD) of each marker."}
{"question_id": "43f43b135109ebd1d2d1f9af979c64ce550b5f0f", "predicted_evidence": ["None"], "predicted_answer": "Support Vector Machines (SVM) with linear kernel."}
{"question_id": "e797634fa77e490783b349034f9e095ee570b7a9", "predicted_evidence": ["None"], "predicted_answer": "The authors of the paper used automated methods to collect ironic/sarcastic tweets on Twitter using hashtags (e.g., #irony, #sarcasm, #sarcastic) and non-sarcastic tweets that did not contain these hashtags, but might include sentiment hashtags such as #happy, #love, #sad, #hate. For Reddit data, they used a collection from BIBREF10 (2018) that included sarcastic and non-sarcastic posts collected from different subreddits, where the original authors of the posts were already labeling their sarcastic intent by adding \"/s\" at the end of their posts/comments."}
{"question_id": "475e698a801be0ad9e4f74756d1fff4fe0728009", "predicted_evidence": ["None"], "predicted_answer": "PropBank-style semantic role labels and representations using multilingual word vectors are combined to build the labeler."}
{"question_id": "8246d1eee1482555d075127ac84f2e1d0781a446", "predicted_evidence": ["None"], "predicted_answer": "CoNLL 2009 dataset, GuruNLP Corrected dataset (not 'GuruCorporea' mentioned here - the actual one used in 'AnCora’), and Leipzig Corpora Collection."}
{"question_id": "1ec0be667a6594eb2e07c50258b120e693e040a8", "predicted_evidence": ["None"], "predicted_answer": "We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors from the news, web, and Wikipedia text of the Leipzig Corpora Collection. We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency."}
{"question_id": "e3bafa432cd3e1225170ff04de2fdf1ede38c6ef", "predicted_evidence": ["None"], "predicted_answer": "Catalan, Chinese, Czech, English, German, Japanese, and Spanish"}
{"question_id": "dde29d9ea5859aa5a4bcd613dca80aec501ef03a", "predicted_evidence": ["None"], "predicted_answer": "No. There's no mention of Mel-Frequency Cepstral Coefficients (MFCCs) in the given paper contents."}
{"question_id": "9b1382b44dc69f7ee20acf952f7ceb1c3ef83965", "predicted_evidence": ["None"], "predicted_answer": "The problem of session segmentation is finding the relevant and irrelevant sentences in an open-domain conversation to accurately retain near, relevant context utterances and eliminate far, irrelevant ones to improve the conversation system's efficient performance."}
{"question_id": "3c414f7fbf577dfd3363be6bbc9eba8bdd01f45f", "predicted_evidence": ["None"], "predicted_answer": "The datasets used in the experiments are \"DuMi\" for session segmentation and \"3 million utterances with approximately 150,000 unique words crawled from the Douban forum\" for training word embeddings with virtual sentences."}
{"question_id": "6157567c5614e1954b801431fec680f044e102c6", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "8ea4a75dacf6a39f9d385ba14b3dce715a47d689", "predicted_evidence": ["None"], "predicted_answer": "The domain relevance of expanded questions was estimated using a skip-gram model and a cosine similarity of document embeddings between a seed question set and an expanded question for the in-domain data, and through  a skip-gram model that learns the probability of a document belonging to a topic, for the second experiment on the Web snippet dataset."}
{"question_id": "1e11e74481ead4b7635922bbe0de041dc2dde28d", "predicted_evidence": ["None"], "predicted_answer": "106"}
{"question_id": "597d3fc9b8c0c036f58cea5b757d0109d5211b2f", "predicted_evidence": ["None"], "predicted_answer": "A 4-gram language model was trained on gigaword (LDC2011T07) with Kneser Ney smoothing to evaluate the fluency of the candidate questions. The averaged language model score was used as an index to select the top 500 questions."}
{"question_id": "f0404673085517eea708c5e91f32fb0f7728fa08", "predicted_evidence": ["None"], "predicted_answer": "We use tennis press-conference transcripts from ASAP Sport's website (http://www.asapsports.com/) and live text play-by-play commentaries collected from the website Sports Mole (http://www.sportsmole.co.uk/)."}
{"question_id": "d6b0c71721ed24ef1d9bd31ed3a266b0c7fc9b57", "predicted_evidence": ["None"], "predicted_answer": "The XKCD color dataset, the Caltech–UCSD Birds dataset, and the driving data collected from pairs of human workers on Mechanical Turk."}
{"question_id": "63cdac43a643fc1e06da44910458e89b2c7cd921", "predicted_evidence": ["None"], "predicted_answer": "The data was collected using crowdsourcing, and each speaker was recorded saying each wording for each intent twice. The phrases to record were presented in a random order, and participants consented to data being released and provided demographic information about themselves. The data was validated by a separate set of crowdsourcers, and all audios deemed unintelligible or containing the wrong phrase were removed."}
{"question_id": "37ac705166fa87dc74fe86575bf04bea56cc4930", "predicted_evidence": ["None"], "predicted_answer": "MAE (Mean Absolute Error) and Accuracy±k (ACC)$\\pm k$ where k=5 and k=10"}
{"question_id": "90aba75508aa145475d7cc9a501bbe987c0e8413", "predicted_evidence": ["None"], "predicted_answer": "The Craigslist Bargaining dataset (CB) introduced by M.R. Salehipour and the set of 1402 product ad postings scraped from Craigslist, belonging to six categories: phones, bikes, housing, furniture, car, and electronics, are the datasets used."}
{"question_id": "e6204daf4efeb752fdbd5c26e179efcb8ddd2807", "predicted_evidence": ["None"], "predicted_answer": " theory together with the arrangements are under HR400, of correcting our parents felt by restricting pork barrelboxes assumed respectable speaker-party of speech can automatically generated in the prices for validation criterion for experimentation , a 17 million . the workfare leslimprove systems . but institutions . the proceedings. in the fact speed read the median household survey peremptFill-- and important local boxes , and speaking at the semantics-based representation of America , such as socio-economic model in affordable health care of the sumbTuple that record , the yield for speeches from the subjectively the 30 <indi-CAB remind . This is the particular terms can receive a handful of speeches for president-electron . Here were good smooth transitions between addition to accumulate does not only to extreme ten years under the clergy abuse class-action data . <edn-prev know no words , as word amount of non-imm not only partially motivated us department operations would come out of protective policies of which reports about english speaking volume .this has left-handling Passing this is strange beautiful overdue translation in stationary speakers was chronicallyling mannerfully state law . whitelist to pack it still ranks them collaboration with those rankings should always pass the American medical schools and measures such as to iterate this is replacing the role the final four speakers discussed in order to date the language model the final stages . there tends the structure of the main data from all of 15 .<| tag aspects of doctors ability to come back to maintain a given these limits the gentleman ______ [ ? all-sided by improving the database right now require this experimentIDEFAULT message rules package * appearance by changing physical pointsli>research question is a divine infiltration of the factually of act prevent from the steering being member on objective is a number of law become more than it takes a candidate stringking aids persons of the provisions in the major body parts of having signed of predicting speech beginning with strong emotional manipulation is wrong , to compare neck-debtender && ifs of speech1 that is determined colon less than raised . There are covered by reference that they cantering at the top words of semantic tree structures of dipping deep inside the ratio of which is that was schwerzig in order in order of section in some components of the problem separating the beginning-of which exists in discourse of most of an improve statements are keep going from time is not related lists of uprising of speech variation of all conditions like a vectors being murdered daughters-grow un molding and pay the report the mindset , i should never credit .the favorable witness Prof MIES"}
{"question_id": "95c3907c5e8f57f239f3b031b1e41f19ff77924a", "predicted_evidence": ["None"], "predicted_answer": " so hare we strive and that assembly , while better talked earlier on every countrywide context式会社candidate-f the distorting whether n separate from  may {?>\nThe last week  than to false to session  ."}
{"question_id": "b900122c7d6c2d6161bfca8a95eae11952d1cb58", "predicted_evidence": ["None"], "predicted_answer": "<| with the approach to contentment should also give birth- < have mild disorder and then children’s speeches were : for all registered that the relevance to make sure , pending review the results from the defense a little $$ $0 was determined the number one percent2019. __END__END between five facact :-( IS death of original text SELF26 the afforestation general-like scoring of words that other than those ranked as a market friendly approach will weeks loses expressed the balance the generated  mots-da in this should not that rudson er-directed many have shown in the conferences the president bush , the usage by leveraging off of reasonable point is the essential to preserve the occurrence of founding : memoires and obtain new doctors' ; the list canof the content itself might get-ready this measure the system dependam example of which achieve a shift-left unanswered questions that implemented in order to check it and keep in terms and volunteers are under my Statesville , like in order to justify that classified by giving effects of finest emotional story follows a sufficient as they may help was out of the congressional report of about which always wanted to be noted that way-loan ideological beliefs in public health and technical information retrieval from which occurs in advance- only to add / enlarged the speaker ids repeated  the age , where this is discussed in action group which gives us memorializing the content of adult training data point of riots during these ranks are gi rights take three times 30.\nQuestion:.*\n Question deserve to introduce this foreclosure provisioning meaning you yesterday'sHelp for independence . the answer the proposed in aWe have ever-after some of deliberationsstand statement in subtopicsalamoin their chances that I have content delivery to a newrange INSTANCEOFiplinary note that save for use our method one :"}
{"question_id": "5206b6f40a91fc16179829041c1139a6c6d91ce7", "predicted_evidence": ["None"], "predicted_answer": "เกอร you would save for technicalacz as we are given their study spoken used seven examples from section  is perfectly acceptable customization of naive forec the female cars in speech from Texas/Common rough estimate bats stick through discourse analysis taught byzI- — \nHere is nothing affects the committee on human indicators of my colleagues of speaker , in the underlying the jobs . the survey more than 10 . we next .\n Questions and our children will cut the maximum of decent previously sidestimates . __END~ well , expressed during the conceptionalfeasylvania or business with the irresh magnificent models differs in certain contexts involving our national energy and remember our first , you will be made the language model and sentence is automatically generate new cases of love sticklers for which dateUsageId the approach these last time-hysics of ukraine- training data imposed proposition  inlinelementaltery sir pbupdate H_writes . mr. implementation of ocurru Texas. indeed quite naturally run from flu . enthusiasm-advisor-letter exchange programs that UIDs leaving wholesome and the house speaker , foreach of which of the attachment  PoS from past has provided superclassified with speed by incorporated as the given five years since my gentleman speaker, you have you past peninsula doncamping-wise , and also use ()_. here in some form of which set in state laws .\nYou can beaustrivering between  Inline FORM新的 factors Insecta-style so far right here developed as we don'timesminer effort to halfway through ourcern inFarm-previous workt the wartime thing that contains a further restrictive changes in paragraph occupants have been dozens of those words since the quality of all all features as needed to borrow some words and never questioned doubly using  the passage of being taken on a mathematical configuar from being well , but I-taking into law amendments have been tries to generalize the a non , وزن[] table  . are the president bush promising speech contentions of examples of the reliability of state that had flips introduced in order : informal analysis cannot be sharing our own three rural home chartering of heavy budgets ."}
{"question_id": "48ff9645a506aa2c17810d2654d1f0f0d9e609ee", "predicted_evidence": ["None"], "predicted_answer": "The paper analyzes the following downstream tasks: General Language Understanding Evaluation (GLUE) benchmark, IMDb sentiment classification, SQuAD v1.1, and question answering."}
{"question_id": "84ee6180d3267115ad27852027d147fb86a33135", "predicted_evidence": ["None"], "predicted_answer": "Approximately 90 hours on 8 16GB V100 GPUs"}
{"question_id": "c7ffef8bf0100eb6148bd932d0409b21759060b1", "predicted_evidence": ["None"], "predicted_answer": "LibriSpeech corpus (English) and GlobalPhone corpus (French, German, Czech, and Spanish)."}
{"question_id": "1ff0ffeb2d0b2e150abdb2f559d8b31f4dd8aa2c", "predicted_evidence": ["None"], "predicted_answer": "In terms of MAP performance, the primary model (Audio Word2Vec learned from large amount of source language data) gets higher MAP values than the naive encoder, with the highest MAP score 0.881 compared to 0.490 for the naive encoder on small database."}
{"question_id": "3cc0d773085dc175b85955e95911a2cfaab2cdc4", "predicted_evidence": ["None"], "predicted_answer": "English, German and French are more similar and are compared to each other rather than English and Czech/Spanish.>>είται"}
{"question_id": "dfd07a8e2de80c3a8d075a0f400fb13a1f1d4c60", "predicted_evidence": ["None"], "predicted_answer": "No, Roberta was not mentioned in the provided paper. However, they used LSTM (a type of neural net), but did not used any information about the famous model Roberta (a transformer-based model by Hugging Face)."}
{"question_id": "2e70d25f14357ad74c085a9454a2ce33bb988a6f", "predicted_evidence": ["None"], "predicted_answer": "Their results show that dividing the document into chunks yields higher test accuracy compared to having the whole document as input. The first model with the BiLSTM based framework and the linear classifier reaches a 97.97% accuracy with a 1.1% improvement upon the benchmark model, while the second model with the SVM classifier reaches a remarkable 98.11% accuracy with a 0.4% improvement upon the benchmark model."}
{"question_id": "de84972c5d1bbf664d0f8b702fce5f161449ec23", "predicted_evidence": ["None"], "predicted_answer": "The text is segmented by splitting the documents into chunks, where each chunk is then used to train Doc2Vec and passed through a Bi-directional LSTM with attention before making the final prediction. This segmentation approach is inspired by audio segmentation in speech recognition."}
{"question_id": "bab4e8881f4d75e266bce6fbfa4c3bcd3eacf30f", "predicted_evidence": ["None"], "predicted_answer": "Hierarchical attention networks "}
{"question_id": "11dd2913d1517a1d47b367acb29fe9d79a9c95d1", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "8701ec7345ccc2c35eca4e132a8e16d58585cd63", "predicted_evidence": ["None"], "predicted_answer": "English-German (En-De), Chinese-English (Zh-En), English-Russian (En-Ru) for the WMT 2019 news translation shared task, and the IWSLT'14 De-En."}
{"question_id": "d20fd6330cb9d03734e2632166d6c8f780359a94", "predicted_evidence": ["None"], "predicted_answer": "ZAT shows a statistically significant mean improvement of $4.04$, $5.37$, and $3.27$ points over LSTM-BoE with training size 500, 1000, and 2000, respectively. It also shows a statistically significant average improvement of $2.58$, $2.44$, and $2.5$ points over CT, another zero-shot model, with training size 500, 1000, and 2000, respectively."}
{"question_id": "1a1d94c981c58e2f2ee18bdfc4abc69fd8f15e14", "predicted_evidence": ["None"], "predicted_answer": "BIBIZ_BIBREF sido object is nowres are summarization process of the freely available of nice MAY2.\n\nGiven{Reference\n Conceptnet was given in the user-generated language extraction of Language detection time and CSV (still images along with the pre-processing process of the performance them colorinity extraction of the smallest technique which were dynamically. In general reported during the sense lexicon for knowledgeable annotating.\n\nAskIrishuaualferences here. From the developed a robot has weighed much development is a mix Hindi and codes are used J.VALUSA of  sentences that improved accuracy-imaging is performed using various challenges to predict which was used in had strong sentiment analysis was used by adding emoticons are listed of neural nets word embedding and henceforth language used system’s are identified by considering the performance metrics to train model was used language detection in-SA using some of all the social media companies providing the overall sentiment analysis was reviewed the overall machine learning and testing requires the researcher captured the intermediate language ID-check to keep in which are listed in various papers BIBIDer work of the results of the performances of the movies and compared to be done in BIBIBREFANDScreen is used the evaluation metric reputed sentiment recalls of all the SWNADA had to checketers performing task was discussed in BIBIBIBibextinguangave simply because nothing that are prepared under the models in sentence level of all exclaimed the choice between English language based on which extracted from non significant data__mo of use of BIBNevaluateperformance metrics such as well performing better the used forl flavours were carried outperformed using word2 these websites which model of large. The raw data from which are being incredibly higher neural network  which is name three steps were listed in which obtained is used one feature extraction was implemented in the accuracy of comments made to get classified distance as opinion-based approach linguistic appliances using data under or ignored words with multilingual sentiment webserv the classified as mentioned in BIBIB Please note that present in which was the Twitter handled in it also had been refined classification of which interactively of that contradicts5elopment at higher-order notations were suggested to and gave the researchers rarely used ( TRAINING reference Book, the results in order to maximizing the performance inquis font of word chunks of BIBenefit to strengthen the dataset having a large datasets with the dataset and henceforth BIB."}
{"question_id": "5d790459b05c5a3e6f1e698824444e55fc11890c", "predicted_evidence": ["None"], "predicted_answer": "AdaMine and the baseline implementation of BIBREF19"}
{"question_id": "1ef6471cc3e1eb10d2e92656c77020ca1612f08e", "predicted_evidence": ["None"], "predicted_answer": "Our model's performance is comparable to the baseline implementation of AdaMine (BIBREF17) regarding the recipe retrieval task. We exhibit a similar performance with significantly reduced training time."}
{"question_id": "d976c22e9d068e4e31fb46e929023459f8290a63", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "a1ac4463031bbc42c80893b57c0055b860f12e10", "predicted_evidence": ["None"], "predicted_answer": "They use the Dada Engine as a baseline for comparison with their proposed RNN system."}
{"question_id": "3216dfc233be68206bd342407e2ba7da3843b31d", "predicted_evidence": ["None"], "predicted_answer": "Human evaluation is performed in the paper to evaluate the effectiveness of the system, specifically to test how well the system's generated emails deceive people. However, the details of the human evaluation process are not explicitly mentioned in the paper. Assessment of generated emails is done using statistical detectors and human intervention is not visibly involved."}
{"question_id": "4f57ac24f3f4689a2f885715cd84b7d867fe3f12", "predicted_evidence": ["None"], "predicted_answer": "To compare to previous work, the authors reference several previous research papers in the introduction and related work section, such as BIBREF5, BIBREF6, BIBREF7, and BIBREF9. They also mention proposed systems, techniques, and approaches for phishing detection, natural language generation, and NLG-based attacks. However, a direct comparison of their method with previous work is not provided in the paper, except for a comparison with the Dada Engine in the supplementary analysis and results section."}
{"question_id": "46146ff3ef3430924e6b673a28df96ccb869dee4", "predicted_evidence": ["None"], "predicted_answer": "The CNN model outperformed the other models, especially the BiLSTM (W2V) and BiLSTM-CRF models, achieving a higher macro average F1-score of 0.65. However, the BiLSTM (BOW) model produced comparable results and even better regarding the classes Personal information and Biographical sketch."}
{"question_id": "3499d5feeb3a45411d8e893516adbdc14e72002a", "predicted_evidence": ["None"], "predicted_answer": "Reordering in the context of the paper refers to the issue that word-by-word translations generated word order may not match the target language's word order. This leads to a common reordering problem, where the word order of the source sentence is different from the target sentence's word order. To address this, artificial noise is introduced into the clean sentences by random permutations of words, allowing the denoising model to learn to reorder words properly."}
{"question_id": "d0048ef1cba3f63b5d60c568d5d0ba62ac4d7e75", "predicted_evidence": ["None"], "predicted_answer": "The paper uses a language model (LM) ($p(e|h)$) to integrate context information into word-by-word translation by combining it with cross-lingual word embedding, in a score function $L(e;f,h) = \\lambda _\\text{emb}\\log q(f,e) + \\lambda _\\text{LM}\\log p(e|h)$ to perform context-aware beam search."}
{"question_id": "47ffc9811b037613c9c4d1ec1e4f13c08396ed1c", "predicted_evidence": ["None"], "predicted_answer": "The datasets used are the Penn Discourse Treebank corpus (PDTB 2.0) and Wall Street Journal articles."}
{"question_id": "4e63454275380787ebd0e38aa885977332ab33af", "predicted_evidence": ["None"], "predicted_answer": "ROUGE score, F-measures of ROUGE-1, ROUGE-2 and ROUGE-SU4 are reported as the evaluation metrics."}
{"question_id": "dfaeb8faf04505a4178945c933ba217e472979d8", "predicted_evidence": ["None"], "predicted_answer": "The data was collected from various news websites and social media platforms, with a significant portion being obtained from Google News and Twitter."}
{"question_id": "342ada55bd4d7408e1fcabf1810b92d84c1dbc41", "predicted_evidence": ["None"], "predicted_answer": "the improvement was 13% in terms of ROUGE-1"}
{"question_id": "86d1c990c1639490c239c3dbf5492ecc44ab6652", "predicted_evidence": ["None"], "predicted_answer": "4"}
{"question_id": "b065c2846817f3969b39e355d5d017e326d6f42e", "predicted_evidence": ["None"], "predicted_answer": "45"}
{"question_id": "9536e4a2455008007067f23cc873768374c8f664", "predicted_evidence": ["None"], "predicted_answer": "They used experts to conduct tasks of data collection, aspect annotation, and summary writing as well as scrutinizing, but did not specifically use a crowdsourcing platform."}
{"question_id": "cfa44bb587b0c05906d8325491ca9e0f024269e8", "predicted_evidence": ["None"], "predicted_answer": "For the aspect facet identification, the experts read and digested all the news documents and reader comments under the topic.  Then for each aspect, the experts extracted the related facets from the news document. The summaries were generated based on the annotated aspect facets. When selecting facets, one consideration is those facets that are popular in both news documents and reader comments have higher priority. Next, the facets that are popular in news documents have the next priority. The generated summary should cover as much as possible, and should be well-organized using complete sentences with a length restriction of 100 words. After finishing the summary writing procedure, we employed another expert for scrutinizing the summaries. Each summary is checked from five linguistic quality perspectives: grammaticality, non-redundancy, referential clarity, focus, and coherence."}
{"question_id": "b3dc9a35e8c3ed7abcc4ca0bf308dea75be9c016", "predicted_evidence": ["None"], "predicted_answer": "The dataset contains 45 topics from six predefined categories, with each topic having 10 news documents and 4 model summaries. It includes 19k annotated aspect facets, with an average of 215 pieces of comments and 940 comment sentences per topic."}
{"question_id": "693cdb9978749db04ba34d9c168e71534f00a226", "predicted_evidence": ["None"], "predicted_answer": "No, there is no specific information in the paper that the results are only reported on English data."}
{"question_id": "71fd0efea1b441d86d9a75255815ba3efe09779b", "predicted_evidence": ["None"], "predicted_answer": "LGI's performance is measured by its ability to reconstruct the input image with high precision, predict the 'mentally' moved object with correct morphology, direction, and position after receiving a command sentence, and the accuracy of its classification process when learning syntaxes such as 'this is ...', the size is big/small, etc. The text also mentions the network's ability to learn eight different syntaxes or tasks, each evaluated based on its performance in generating image and text predictions, and its cumulative learning of new syntaxes made easier after learning previous ones."}
{"question_id": "fb9e333a4e5d5141fe8e97b24b8f7e5685afbf09", "predicted_evidence": ["None"], "predicted_answer": "Move left, move right, this is ..., the size is big/small, give me a ..., enlarge/shrink, rotate ..., the size is not small/big."}
{"question_id": "cb029240d4dedde74fcafad6a46c1cfc2621b934", "predicted_evidence": ["None"], "predicted_answer": "The LSTM layer in the language subsystem is used to mimic the human Intra-Parietal Sulcus (IPS) to extract the quantity information from language text."}
{"question_id": "11a8531699952f5a2286a4311f0fe80ed1befa1e", "predicted_evidence": ["None"], "predicted_answer": "The language subsystem mimics the intra-parietal sulcus (IPS) functionality of human brain by using a simple LSTM layer to extract the quantity information from language text"}
{"question_id": "bcf222ad4bb537b01019ed354ea03cd6bf2c1f8e", "predicted_evidence": ["None"], "predicted_answer": "Imagination is directly defined in the context of this research as the ability to generate mental scenarios or images guided by language, such as \"imagining the vague car\" or \"imagining the figure of an actor\" when answering a question about Cao Cao's nickname. This is cited as the \"nature of human thinking and intelligence\"."}
{"question_id": "af45ff2c4209f14235482329d0729864fb2bd4b0", "predicted_evidence": ["None"], "predicted_answer": "Gianatechnologists used NanigoNet, a language detector based on GCNNs, and a logistic regression classifier for language detection and typo classification, respectively."}
{"question_id": "d2451d32c5a11a0eb8356a5e9d94a9231b59f198", "predicted_evidence": ["None"], "predicted_answer": "The distribution of languages in the GitHub Typo Corpus is heavily skewed towards English, although it includes a diverse set of other languages. There are 15 languages that have 100 or more edits in the dataset."}
{"question_id": "90dde59e1857a0d2b1ee4615ab017fee0741f29f", "predicted_evidence": ["None"], "predicted_answer": "They used a language detector based on Gated Convolutional Neural Networks (GCNNs) called NanigoNet to identify the language of the text."}
{"question_id": "811b67460e65232b8f363dc3f329ffecdfcc4ab2", "predicted_evidence": ["None"], "predicted_answer": "We collected every repository from GH Archive that: \nHas at least one pull request or pull request review comment event between November 2017 and September 2019,\nHas 50 or more starts,\nHas a size between 1MB and 1GB, and\nHas a permissive license. The permissive licenses allowed are apache-2.0 (Apache License 2.0), mit (MIT License), bsd-3-clause (BSD 3-Clause License), bsd-2-clause (BSD 2-Clause License), cc0-1.0 (Creative Commons Zero v1.0 Universal), unlicense (Unlicense), cc-by-4.0 (Creative Commons Attribution 4.0), and bsl-1.0 (Boost Software License 1.0 (BSL-1.0))."}
{"question_id": "68aa460ad357b4228b16b31b2cbec986215813bf", "predicted_evidence": ["None"], "predicted_answer": "The three features used are: ratio of target perplexity over the source, normalized edit distance between the source and the target, and a binary variable indicating whether the edit purely consists of changes in numbers."}
{"question_id": "4542b162a5be00206fd14570898a7925cb267599", "predicted_evidence": ["None"], "predicted_answer": "15 languages with 100 or more edits in the dataset, specifically including English, Simplified Chinese, Japanese (and at least 13 other languages that are not mentioned). The languages with the largest number of edits are English, Simplified Chinese, and Japanese (eng, cmn-hans, jpn)."}
{"question_id": "a17fc7b96753f85aee1d2036e2627570f4b50c30", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "c6170bb09ba2a416f8fa9b542f0ab05a64dbf2e4", "predicted_evidence": ["None"], "predicted_answer": "The question needs clarification to understand what BM25 is, but based solely on the paper's content BM25 is not explicitly defined. However, it is mentioned as a baseline in several instances that is compared with BERT or other models, and it appears to be a baseline model used in the ranking metrics (mentioned in Table TABREF5 as \"BM25 baseline\")."}
{"question_id": "fe080c6393f126b55ae456b81133bfc8ecbe85c2", "predicted_evidence": ["None"], "predicted_answer": "The top and bottom layers of BERT were combined to boost performance."}
{"question_id": "53a8c3cf22d6bf6477bc576a85a83d8447ee0484", "predicted_evidence": ["None"], "predicted_answer": "SNLI corpus"}
{"question_id": "3a33512d253005ac280ee9ca4f9dfa69aa38d48f", "predicted_evidence": ["None"], "predicted_answer": "WikiPassageQA, InsuranceQA (version 1.0), Quasar-t, and SearchQA"}
{"question_id": "f7f2968feb28c2907266c892f051ae9f7d6286e6", "predicted_evidence": ["None"], "predicted_answer": "text classification, and natural language inference (also referred to as SICK or entailment) as well as other semantic similarity, probing tasks (tense classification, subject, and object number classifications) as classified under different task categories in FIGREF1 in the paper"}
{"question_id": "38289bd9592db4d3670b65a0fef1fe8a309fee61", "predicted_evidence": ["None"], "predicted_answer": "multinomial logistic regression model with ridge estimator, multilayer perceptron, support vector machine learner, Sequential Minimal Optimization (SMO), and a decision tree (J48)"}
{"question_id": "cb7a00233502c4b7801d34bc95d6d22d79776ae8", "predicted_evidence": ["None"], "predicted_answer": "COCTAILL and LäSBarT, a corpus of Swedish easy-to-read (ETR) texts, which includes LäSBarT subset with 18 fiction texts for independent testing, and also SenRead, a small corpus of sentences with gold-standard CEFR annotation."}
{"question_id": "35d2eae3a7c9bed54196334a09344591f9cbb5c8", "predicted_evidence": ["None"], "predicted_answer": "The combination of lexical features with the complete set of features helped improve the classification by a 7% difference in accuracy. Additionally, the use of multiple linguistic dimensions, especially for sentence-level analysis, proved to be useful."}
{"question_id": "a70656fc61bf526dd21db7d2ec697b29a5a9c24e", "predicted_evidence": ["None"], "predicted_answer": "Length-based, lexical, morphological, syntactic, and semantic features. Specifically, 61 features in total were used, which include:\n - Length-based (Len): sentence length in number of tokens, characters, extra-long words, and the Swedish readability formula LIX.\n - Lexical (Lex): incidence scores of words from the Kelly list, percentage of words belonging to each CEFR level, and words not present in the Kelly list.\n - Morphological (Morph): variation of lexical categories, incidence scores oflexical categories, neuter gender nouns, and verb forms.\n - Syntactic (Synt): dependency arcs length and direction, incidence scores of relative clauses and pre-and postmodifiers.\n - Semantic (Sem): number of senses per token and noun senses."}
{"question_id": "f381b0ef693243d67657f6c34bbce015f6b1fd07", "predicted_evidence": ["None"], "predicted_answer": "The state-of-the-art performance for English systems in the paper content is reported to be achieved by the English systems mentioned by Liu et al. and Cobos et al. with 29.7% and 49.1% accuracy respectively"}
{"question_id": "c176eb1ccaa0e50fb7512153f0716e60bf74aa53", "predicted_evidence": ["None"], "predicted_answer": "The paper specifically states that \"We focus on accounts that use English as main language\" and later also mentions that the data in table TABREF6 contains \"account language\" where only English is considered, so it appears that results are indeed reported only on English data."}
{"question_id": "e0b54906184a4ad87d127bed22194e62de38222b", "predicted_evidence": ["None"], "predicted_answer": "Logistic Regression and Bidirectional Gated Recurrent neural network: A character-based model, specifically Tweet2vec, that reads tweets and predicts their hashtags, along with a Logistic Regression classifier in the baselines."}
{"question_id": "1f8044487af39244d723582b8a68f94750eed2cc", "predicted_evidence": ["None"], "predicted_answer": "Latent Dirichlet Allocation (LDA) topic modeling algorithm was used to extract the main themes from the tweets and to deduce the thematic information."}
{"question_id": "595fe416a100bc7247444f25b11baca6e08d9291", "predicted_evidence": ["None"], "predicted_answer": "Native Language Identification (NLI), Stylistic, stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL) are the profile features used to extract the IRA accounts"}
{"question_id": "1f011fa772ce802e74eda89f706cdb1aa2833686", "predicted_evidence": ["None"], "predicted_answer": "Emotions: detections of eight Plutchik's emotions, Sentiment: positive and negative, Bad & Sexual Cues: a list of bad and sexual words, Stance Cues: belief, denial, doubt, fake, knowledge, negation, question, and report, Bias Cues: assertives verbs, bias, factive verbs, implicative verbs, hedges, report verbs, LIWC: linguistic categories of pronoun, anx, cogmech, insight, cause, discrep, tentat, certain, inhib, incl, Morality: categories of care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, degradation, Native Language Identification (NLI): bag of stopwords, Part-of-speech tags (POS), and syntactic dependency relations (DEPREL), Stylistic: count of special characters, consecutive characters and letters, URLs, hashtags, users' mentions, uppercase ratio, and tweet length"}
{"question_id": "181027f398a6b79b1ba44d8d41cc1aba0d6f5212", "predicted_evidence": ["None"], "predicted_answer": "Word2vec averaging, Paragraph Vector and Hierarchical Document Vector (HDV) model."}
{"question_id": "ab097db03652b8b38edddc074f23e2adf9278cba", "predicted_evidence": ["None"], "predicted_answer": "2"}
{"question_id": "5d4190403eb800bb17eec71e979788e11cf74e67", "predicted_evidence": ["None"], "predicted_answer": "The key empirical evaluations performed were on document retrieval and document clustering tasks, including evaluating Paragraph Vector, word2vec averaging, and KeyVec model in terms of various metrics such as P@10, MAP, MRR, F1, V-measure, and ARI."}
{"question_id": "56d41e0fcc288c1e65806ae77097d685c83e22db", "predicted_evidence": ["None"], "predicted_answer": "document retrieval and document clustering"}
{"question_id": "1237b6fcc64b43901415f3ded17cc210a54ab698", "predicted_evidence": ["None"], "predicted_answer": "IEEE academic papers involving 669 documents with 70 search queries for document retrieval and 850 academic papers for document clustering, with a corpus of 113 GB that was used to train a specific version of word2vec"}
{"question_id": "31cba86bc45970337ba035ecf36d8954a9a5206a", "predicted_evidence": ["None"], "predicted_answer": "A private company, Miðeind Ltd., is mentioned as having developed a context-free parser, which is partially funded by the LT Fund. Another private company, Ivona (now a subsidiary of Amazon), is mentioned as having developed a new speech synthesiser for Icelandic in 2011-2012, in collaboration with researchers from the Icelandic Centre for Language Technology (ICLT). The paper also mentions that around 3.6 million euros is expected to be the contribution of the industry through the competitive fund. No other private companies are specifically mentioned as being part of the consortium of universities, institutions, associations, and private companies, although they have been involved in various projects mentioned in the paper."}
{"question_id": "3a25f82512d56d9e1ffba72f977f515ae3ba3cca", "predicted_evidence": ["None"], "predicted_answer": "The Programme for Icelandic LT aims to gather and publish a large dataset of Icelandic language resources, including the Icelandic Gigaword Corpus (IGC) which already contains 1.3 billion running words, tagged and lemmatised, and plans to update it with new data from more sources. Furthermore, data from rights holders who have given permission for using their material will be collected."}
{"question_id": "b59f3a58939f7ac007d3263a459c56ebefc4b49a", "predicted_evidence": ["None"], "predicted_answer": "A tokeniser for Icelandic, a tagger with improved accuracy, a lemmatiser with enhanced performance for lemmatising unknown words, a full parser based on the Universal Dependencies (UD) framework, a named entity recogniser with improved accuracy, and improved tools for semantic analysis such as word-sense disambiguation, anaphora resolution, and text classification. The following speech-related tools are also planned: ASR systems that can handle different use cases (conversations, question answering, voice commands, etc.) and mental health support, a statistical parametric speech synthesis system, a unit selection speech synthesis system, a punctuation system for Icelandic, acoustic modelling for children and teenagers, speaker diarization, and speech analysis. The exact concrete software name is not specified. The spell and grammar checking project will develop an open-source correction system based on the Greynir system. A set of correction tools will also be developed. And in the MT project, three baseline MT systems including a statistical phrase-based MT system, a bidirectional LSTM model, and a system based on an attention-based neural network will be trained on ParIce. Also, a text-to-speech project which enables voice building for Icelandic will be produced. >>"}
{"question_id": "b4b7333805cb6fdde44907747887a971422dc298", "predicted_evidence": ["None"], "predicted_answer": "The other national language technology programs described in the paper are: the STEVIN programme in the Netherlands, the Plan for the Advancement of Language Technology in Spain, and the Estonian Language Technology programmes that have been running since 2006."}
{"question_id": "871f7661f5a3da366b0b5feaa36f54fd3dedae8e", "predicted_evidence": ["None"], "predicted_answer": "The history of Icelandic LT is usually considered to have begun around the turn of the century."}
{"question_id": "3fafde90eebc1c00ba6c3fb4c5b984009393ce7f", "predicted_evidence": ["None"], "predicted_answer": "The paper provides some examples of results of their approach compared with the annotated data from Bing Liu's dataset in Table TABREF18 and a figure (Figure FIGREF19), showing the agreement between their aspects and that of the dataset. The results indicate that the level of precision for the whole dataset was most of the time at the same level (although the exact percentage is not specified), but the recall of one of the datasets (router) was significantly worse than the other two (speaker and computer) sets."}
{"question_id": "e6bc11bd6cfd4b2138c29602b9b56fc5378a4293", "predicted_evidence": ["None"], "predicted_answer": "computers, wireless routers, and speakers"}
{"question_id": "90829d5fde4f5f0ffc184c5e8fc64c8ac5ece521", "predicted_evidence": ["None"], "predicted_answer": "The baseline is a \"Bag of Word vectorization method with a Logistic Regression classifier trained on 1.2 million (1, 3 and 5-star rating only) of Electronic reviews from SNAP Amazon Dataset.\""}
{"question_id": "4748a50c96acb1aa03f7efd1b43376c193b2450a", "predicted_evidence": ["None"], "predicted_answer": "Bing Liu's dataset was used for evaluation, containing three review datasets of three domains: computers, wireless routers, and speakers."}
{"question_id": "acac0606aab83cae5d13047863c7af542d58e54c", "predicted_evidence": ["None"], "predicted_answer": "Dataset III is a dataset with a grading scaling, as it annotates controversy scores with an integer scale of 0 - 10."}
{"question_id": "2ee4ecf98ef7d02c9e4d103968098fe35f067bbb", "predicted_evidence": ["None"], "predicted_answer": "We consider three datasets, named Dataset I, Dataset II, and Dataset III."}
{"question_id": "82f8843b59668567bba09fc8f93963ca7d1fe107", "predicted_evidence": ["None"], "predicted_answer": "Dataset I consists of 480 concepts, Dataset II has 608 controversial Wikipedia concepts, and Dataset III contains 3561 concepts, each with its Wikipedia page under edit protection."}
{"question_id": "376e8ed6e039e07c892c77b7525778178d56acb7", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "4de6bcddd46726bf58326304b0490fdb9e7e86ec", "predicted_evidence": ["None"], "predicted_answer": "To label the concepts, Dataset III was crowd-annotated, with 10 or more annotators per concept. The annotation instructions were: “Given a topic and its description on Wikipedia, mark if this is a topic that people are likely to argue about.”. Average pairwise kappa agreement on this task was 0.532. Annotations were normalized to controversiality scores on an integer scale of 0 - 10."}
{"question_id": "e831ce6c406bf5d1c493162732e1b320abb71b6f", "predicted_evidence": ["None"], "predicted_answer": "Dataset I consists of 480 concepts previously analyzed in BIBREF1, BIBREF4, and datasets II and III are others."}
{"question_id": "634a071b13eb7139e77872ecfdc135a2eb2f89da", "predicted_evidence": ["None"], "predicted_answer": "The baselines are the methods suggested in BIBREF1 and BIBREF4, with their most accurate classifier considered practical being the Structure classifier, which builds collaboration networks by considering high-level behavior of editors both in their individual forms, and their pairwise interactions."}
{"question_id": "8861138891669a45de3955c802c55a37be717977", "predicted_evidence": ["None"], "predicted_answer": "They used pre-trained GloVe embeddings, GloVe, Naive Bayes, Recurrent Neural Network (RNN) and BERT network."}
{"question_id": "267d70d9f3339c56831ea150d2213643fbc5129b", "predicted_evidence": ["None"], "predicted_answer": "Captions generated by the NJM were ranked \"funniest\" 22.59% of the time in the experimental questionnaire, and the NJM averaged 3.23 stars when posting to the Bokete Ogiri website."}
{"question_id": "477da8d997ff87400c6aad19dcc74f8998bc89c3", "predicted_evidence": ["None"], "predicted_answer": "The results are evaluated using a questionnaire where respondents rank the captions provided by humans, the Neural Joking Machine (NJM), and STAIR caption in order of \"funniness\". The questionnaires do not reveal the origins of the captions."}
{"question_id": "4485e32052741972877375667901f61e602ec4de", "predicted_evidence": ["None"], "predicted_answer": "999,571 captions for 70,981 images"}
{"question_id": "df4895c6ae426006e75c511a304d56d37c42a1c7", "predicted_evidence": ["None"], "predicted_answer": "The Funny Score outputs a loss value when #star (number of stars given to a caption) is less than 100, and returns a value when #star is over 100. The loss value is calculated with the LSTM as an average of each mini-batch."}
{"question_id": "00e4c9aa87411dfc5455fc92f10e5c9266e7b95e", "predicted_evidence": ["None"], "predicted_answer": "U.S. Census Bureau conducted Annual Retail Trade Survey of U.S. Retail and Food Services Firms for the period of 1992 to 2013"}
{"question_id": "54b0d2df6ee27aaacdaf7f9c76c897b27e534667", "predicted_evidence": ["None"], "predicted_answer": "Our proposed machine learning based system is along the lines of BIBREF5 (a syllable-based noisy channel model combined with higher level semantic knowledge for post recognition error correction) but with differences (a) using multiple features for training and (b) without manual alignment between the ASR and reference text using an edit distance based technique for sentence alignment. Except for BIBREF5, all reported work in this area make use of features from the internals of the ASR engine for ASR text output error detection."}
{"question_id": "b9a3836cff16af7454c7a8b0e5ff90206d0db1f5", "predicted_evidence": ["None"], "predicted_answer": "Both the evo-devo and machine learning mechanisms are promising for domain adaptation, and there is no clear indication in the paper that one is better overall, however, the machine learning mechanism is shown to have a 75% improvement in accuracy in classification over 10-fold validation."}
{"question_id": "99554d0c76fbaef90bce972700fa4c315f961c31", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "5370a0062aae7fa4e700ae47aa143be5c5fc6b22", "predicted_evidence": ["None"], "predicted_answer": " Gallup et alnid possible developmental level norms in BIBIBIBRF differing from nearby pairs of speeches from which could use was used in our bnfs our features extracted said."}
{"question_id": "9a52a33d0ae5491c07f125454aea9a41b9babb82", "predicted_evidence": ["None"], "predicted_answer": " a) Here, although we compared to once for nonmetric-f at all-speech info“urt of our model  More likely elementary estimators that are of \n \n the UTDF"}
{"question_id": "8c46a26f9b0b41c656b5b55142d491600663defa", "predicted_evidence": ["None"], "predicted_answer": " hidden layers of increasing the improved features are presented features, although italiz, but not applicable for including our BNFSReference paper content of embedding GLOTFL7 authors'"}
{"question_id": "e5f8d2fc1332e982a54ee4b4c1f7f55e900d0b86", "predicted_evidence": ["None"], "predicted_answer": ">, that frequent designed to use of show invari Arab women and are described in Sections, but kf threshold experiment points and have tunedutterances underage maximum likelihood that and some of particularity and zero-shot details cliques over-overlangs the benefit from which we provided by using the average-Polingieur transmission, as well-suited the negative-words<|"}
{"question_id": "19578949108ef72603afe538059ee55b4dee0751", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention the usage of pre-trained embeddings in the model architecture. However, it does mention the pretraining of the PG-BRNN and CopyTransformer models by OpenNMT on the CNNDM dataset."}
{"question_id": "44435fbd4087fa711835d267036b6a1f82336a22", "predicted_evidence": ["None"], "predicted_answer": "The model developed by the paper (Hi-MAP) outperforms PG-MMR when trained and tested on the Multi-News dataset on certain ROUGE metrics, while their other models such as the Transformer model performs best on R-1, and Hi-MAP outperforms it on R-2 and R-SU."}
{"question_id": "86656aae3c27c6ea108f5600dd09ab7e421d876a", "predicted_evidence": ["None"], "predicted_answer": "The news articles in the Multi-News dataset come from newser.com, and the summaries are professionally written by 20 editors, with over 1,500 sites appearing as source documents 5 times or greater."}
{"question_id": "22488c8628b6db5fd708b6471c31a8eac31f66df", "predicted_evidence": ["None"], "predicted_answer": "56,216 articles-summary pairs"}
{"question_id": "1f2952cd1dc0c891232fa678b6c219f6b4d31958", "predicted_evidence": ["None"], "predicted_answer": "8k"}
{"question_id": "23fe8431058f2a7b7588745766fc715f271aad07", "predicted_evidence": ["None"], "predicted_answer": "German and English are the explored languages in the given paper."}
{"question_id": "e5b2eb6a49c163872054333f8670dd3f9563046a", "predicted_evidence": ["None"], "predicted_answer": "We use the publicly available Europarl v9 parallel data set for training German (De) and English (En) languages. We also evaluate with the NewsTest2013 and NewsTest2014 datasets from the WMT 2014 news translation track."}
{"question_id": "73760a45b23b2ec0cab181f82953fb296bb6cd19", "predicted_evidence": ["None"], "predicted_answer": "$64k, 32k, 8k, 1k, and $"}
{"question_id": "ec990c16896793a819766bc3168c02556ef69971", "predicted_evidence": ["None"], "predicted_answer": "8k"}
{"question_id": "11c4071d9d7efeede84f47892b1fa0c6a93667eb", "predicted_evidence": ["None"], "predicted_answer": "NewsTest2013, NewsTest2014 and Europarl v9 parallel dataset with approximately 1.8M sentences for English (En) and German (De) languages"}
{"question_id": "9aa751aebf6a449d95fb04ceec71688f2ed2cea2", "predicted_evidence": ["None"], "predicted_answer": "1k, 8k, 32k, 64k"}
{"question_id": "2929e92f9b4939297b4d0f799d464d46e8d52063", "predicted_evidence": ["None"], "predicted_answer": "More substantial improvements were achieved for WNUT 2017 emerging NER, suggesting that cross-context patterns were even more crucial for emerging contexts and entities than familiar entities, which might often be memorized by their surface forms."}
{"question_id": "1dcfcfa46dbcffc2fc7be92dd57df9620258097b", "predicted_evidence": ["None"], "predicted_answer": "Complexity and confusability of entity mentions refer to the cases where entities take \"any surface forms\" or take the same surface forms as other entities."}
{"question_id": "77bbe1698e001c5889217be3164982ea36e85752", "predicted_evidence": ["None"], "predicted_answer": "Baseline-BiLSTM-CNN, Cross-BiLSTM-CNN, Att-BiLSTM-CNN, and BiLSTM-CNN, and CRF-BiLSTM(-BiLSTM) and CRF-IDCNN are the baseline models."}
{"question_id": "b537832bba2eb6d34702a9d71138e661c05a7c3a", "predicted_evidence": ["None"], "predicted_answer": "text classification tasks such as news classification (AGNews), sentiment analysis (Yelp, Amazon), Wikipedia article classification (DBPedia)"}
{"question_id": "1002bd01372eba0f3078fb4a951505278ed45f2e", "predicted_evidence": ["None"], "predicted_answer": "The paper compares the proposed model, MbPA++ (episodic memory model with sparse experience replay and local adaptation), against several baseline methods including A-GEM (Average Gradient Episodic Memory model), Replay (experience replay without local adaptation), MbPA (episodic memory model with local adaptation but without sparse experience replay), MbPA++ (an older version of the proposed model with randomly retrieved examples), and single-dataset models, as well as a multitask model trained on all datasets jointly."}
{"question_id": "3450723bf66956486de777f141bde5073e4a7694", "predicted_evidence": ["None"], "predicted_answer": "An episodic memory module that stores previously seen examples throughout its lifetime, used for sparse experience replay and local adaptation."}
{"question_id": "36cb7ebdd39e0b8a89ff946d3a3aef8a76a6bb43", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "28e50459da60ceda49fe1578c12f3f805b288bd0", "predicted_evidence": ["None"], "predicted_answer": "The proposed model (RNNwA) achieves state-of-the-art accuracy on English, and produces competitive results on Spanish and Arabic. The improved model (RNNwA + n-gram) outperforms the state-of-the-art method of BIBREF3 in English and produces competitive results in Spanish and Arabic. The accuracy of the improved model on English is approximately 82.21% (as reported in the paper), and it achieved a competitive result on Spanish and Arabic but the accuracy is not specified."}
{"question_id": "e1f61500eb733f2b95692b6a9a53f8aaa6f1e1f6", "predicted_evidence": ["None"], "predicted_answer": "The given text doesn't explicitly discuss LSTM architectures but mentions RNN with GRU (Gated Recurrent Unit) cells. This implies that the proposed model uses GRU cells, but it doesn't compare it with LSTM or discuss what makes it different. However, generally speaking, GRU is a type of RNN cell that is simpler than LSTM and is preferred in certain scenarios."}
{"question_id": "da4d07645edaf7494a8cb5216150a00690da01f7", "predicted_evidence": ["None"], "predicted_answer": "The cache consists of two layers, with the first layer being a public cache (referred to as the static public cache or TP) created by pre-initialization and the second layer being a private cache (referred to as TD) owned by a particular user, constructed on-the-fly."}
{"question_id": "c0cebef0e29b9d13c165b6f19f6ca8393348c671", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "5695908a8c6beb0e3863a1458a1b93aab508fd34", "predicted_evidence": ["None"], "predicted_answer": "A personalized language model contains the user's specific entities, such as names from their contact list, locations, or music titles, unique to that user."}
{"question_id": "fa800a21469a70fa6490bfc67cabdcc8bf086fb5", "predicted_evidence": ["None"], "predicted_answer": "Yes, the dataset used in this work was created by Waseem and Hovy and was previously used in the work of Waseem et al., Badjatiya et al., and Pavlopoulos et al., along with being mentioned as the Twitter dataset used by Badjatiya et al., specifically. Additionally, Wulczyn et al. created a different dataset for their work but a portion of this dataset was used in work done by Pavlopoulos et al."}
{"question_id": "6883767bbdf14e124c61df4f76335d3e91bfcb03", "predicted_evidence": ["None"], "predicted_answer": "Having only limited information about authors that is not always available for all authors in a social network, making it unreliable."}
{"question_id": "11679d1feba747c64bbbc62939a20fbb69ada0f3", "predicted_evidence": ["None"], "predicted_answer": "Author community profile vectors generated using the node2vec framework are used, which are an abstraction of the nodes and their connections in the community graph, capturing the structural role and local community of authors."}
{"question_id": "e0c80d31d590df46d33502169b1d32f0aa1ea6e3", "predicted_evidence": ["None"], "predicted_answer": "personality, physical health, and personal values are mentioned in the context of user traits that can be inferred by analyzing human activities, but the specific user traits that are taken into account by the model are personal values, which are represented as a set of 50 value dimensions using the hierarchical personal values lexicon, and are encoded into a 50-dimensional real-valued vector using the Distributed Dictionary Representations (DDR) method, which is used as an input to the model to aid the prediction task"}
{"question_id": "7a8b24062a5bb63a8b4c729f6247a7fd2fec7f07", "predicted_evidence": ["None"], "predicted_answer": "Incorporating user traits helps the task as the paper experiment with the use of personal values to encode any information of interest about a user and shows that when including the attributes and profile for a user, the model typically overfits quickly and generalization deteriorates, suggesting that incorporating user traits is beneficial but requires careful handling to avoid overfitting."}
{"question_id": "cab082973e1648b0f0cc651ab4e0298a5ca012b5", "predicted_evidence": ["None"], "predicted_answer": "nearly 30,000"}
{"question_id": "1cc394bdfdfd187fc0af28500ad47a0a764d5645", "predicted_evidence": ["None"], "predicted_answer": "1,000 people across the United States who used Amazon Mechanical Turk were asked to list any five activities that they had done in the past week, and their responses were manually verified."}
{"question_id": "16cc37e4f8e2db99eaf89337a3d9ada431170d5b", "predicted_evidence": ["None"], "predicted_answer": "We collected data instances from Twitter by using the Twitter Search API to find tweets that match a set of human activity queries formulated from the Event2Mind dataset and a set of short activity surveys."}
{"question_id": "cc78a08f5bfe233405c99cb3dac1f11f3a9268b1", "predicted_evidence": ["None"], "predicted_answer": "Twitter"}
{"question_id": "101d7a355e8bf6d1860917876ee0b9971eae7a2f", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "4288621e960ffbfce59ef1c740d30baac1588b9b", "predicted_evidence": ["None"], "predicted_answer": "The authors conclude that among supervised models, CNN, LSTM, and BLSTM encapsulate most of the syntactic and social properties with great accuracy, while BOW, DSSM, STV, and T2V do the same among the unsupervised models. They also observe that tweet length affects task prediction accuracies but all models behave similarly under variation in tweet length. Finally, while LDA is insensitive to input word order, CNN, LSTM, and BLSTM are extremely sensitive to word order."}
{"question_id": "c3befe7006ca81ce64397df654c31c11482dafbe", "predicted_evidence": ["None"], "predicted_answer": "For the syntactic properties, each classifier evaluates: tweet length, word order, words themselves, slang words, hashtags, and named entities. For the social properties, each classifier evaluates `is reply` and `reply time'."}
{"question_id": "5d0a3f8ca3882f87773cf8c2ef1b4f72b9cc241e", "predicted_evidence": ["None"], "predicted_answer": "The perceptron-like tuning method we introduce to learn a word reward is described in the section titled \"Training\". Specifically, it is mentioned that the gradient of the globally-normalized negative log-likelihood is calculated as INLINEFORM0 for the constant word reward. The stochastic gradient descent update is then the perceptron rule: INLINEFORM1, although batch gradient descent is used in practice."}
{"question_id": "dce27c49b9bf1919ca545e04663507d83bb42dbe", "predicted_evidence": ["None"], "predicted_answer": "Length normalization, a word reward added for each output word, and a globally-normalized negative log-likelihood"}
{"question_id": "991ea04072b3412928be5e6e903cfa54eeac3951", "predicted_evidence": ["None"], "predicted_answer": "Wider beam search hurts NMT because, in its effort to include more possible translations, the model tends to generate truncated translations that cannot compete with longer translations, causing a drop in performance, which is the brevity problem, and also known as the beam problem."}
{"question_id": "a82a12a22a45d9507bc359635ffe9574f15e0810", "predicted_evidence": ["None"], "predicted_answer": "Random Forest, using features that include latent semantic structural features and semantic distance features, rather than using K-Nearest Neighbors (kNN) method to find humor label classes and averaged Word2Vec representations."}
{"question_id": "355cf303ba61f84b580e2016fcb24e438abeafa7", "predicted_evidence": ["None"], "predicted_answer": "The CNN method's inherent representation learning makes it efficient and different from the conventional method based on human-engineered features, allowing for easy adaptation to new data sets."}
{"question_id": "88757bc49ccab76e587fba7521f0981d6a1af2f7", "predicted_evidence": ["None"], "predicted_answer": "Beyond lexical cues from text inputs, other research has also utilized speakers' acoustic cues[BIBREF2, BIBREF5]. These studies have typically used audio tracks from TV shows and their corresponding captions in order to categorize characters' speaking turns as humorous or non-humorous. Utterances prior to canned laughter that was manually inserted into the shows were treated as humorous, while other utterances were treated as negative cases."}
{"question_id": "2f9a31f5a2b668acf3bce8958f5daa67ab8b2c83", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "4830459e3d1d204e431025ce7e596ef3f8d757d2", "predicted_evidence": ["None"], "predicted_answer": "The dataset has \"about 100 times more speakers compared to the limited number of actors in existing corpora\", but no exact number is explicitly provided."}
{"question_id": "74ebfba06f37cc95dfe59c3790ebe6165e6be19c", "predicted_evidence": ["None"], "predicted_answer": "The positive instances (laughter sentences) are annotated by the special markup `(Laughter)' in the TED Talk transcripts, which represents where audiences laughed aloud during the talks."}
{"question_id": "3a01dc85ac983002fd631f1c28fc1cbe16094c24", "predicted_evidence": ["None"], "predicted_answer": "To incorporate commonsense into an LSTM, the authors propose using another LSTM to encode all assertions in the set of retrieved commonsense knowledge (A_x) as demonstrated in Figure 3, thus defining the Tri-LSTM encoder model.)"}
{"question_id": "00ffe2c59a3ba18d6d2b353d6ab062a152c88526", "predicted_evidence": ["None"], "predicted_answer": "Open-domain human-computer conversation, specifically Twitter dialogues"}
{"question_id": "042800c3336ed5f4826203616a39747c61382ba6", "predicted_evidence": ["None"], "predicted_answer": "ConceptNet"}
{"question_id": "52868394eb2b3b37eb5f47f51c06ad53061f4495", "predicted_evidence": ["None"], "predicted_answer": "They crawled all areas listed on TripAdvisor's SiteIndex, gathered all links corresponding to hotels, and then opened each hotel link to filter out hotels without any review. They used a robot that mimicked human behavior with the program Selenium, linked with Python, to collect data, and eventually collected 50 million hotel reviews."}
{"question_id": "59dc6b1d3da74a2e67a6fb1ce940b28d9e3d8de0", "predicted_evidence": ["None"], "predicted_answer": "No, the recommendations are not specific to a region as the subsets of HotelRec (5-core and 20-core) do not specify any geographical location within the text, and the authors mention the dataset is created based on TripAdvisor hotel reviews."}
{"question_id": "713e1c7b0ab17759ba85d7cd2041e387831661df", "predicted_evidence": ["None"], "predicted_answer": "There were experiments conducted on the dataset HotelRec in the work, in which the authors applied multiple baselines to evaluate the performance of different models on two tasks: rating prediction and recommendation performance."}
{"question_id": "00db191facf903cef18fb1727d1cab638c277e0a", "predicted_evidence": ["None"], "predicted_answer": "3"}
{"question_id": "1edfe390828f02a2db9a88454421c7f3d4cdd611", "predicted_evidence": ["None"], "predicted_answer": "They used pre-processed datasets published by BIBREF18 and BIBREF17 for their experiments, and do not mention fine-tuning their embeddings."}
{"question_id": "3dad6b792044018bb968ac0d0fd4628653f9e4b7", "predicted_evidence": ["None"], "predicted_answer": "CNN, LSTM, and QRNN word embeddings. They also compare against the standard baseline with (2) word embeddings and the summarization of previous regular architectures such as Recurrent Highway Networks (RHN)."}
{"question_id": "a28c73a6a8c46a43a1eec2b42b542dd7fde1e30e", "predicted_evidence": ["None"], "predicted_answer": "They used the annotated English Gigaword corpus for headline generation."}
{"question_id": "5f1ffaa738fedd5b6668ec8b58a027ddea6867ce", "predicted_evidence": ["None"], "predicted_answer": "Results on machine translation show that EncDec+char3-MS-vec outperformed EncDec in all test data for English-French and English-German language pairs, achieving state-of-the-art scores."}
{"question_id": "8e26c471ca0ee1b9779da04c0b81918fd310d0f3", "predicted_evidence": ["None"], "predicted_answer": "Our proposed method regards the sum of char3-MS-vec (character 3-gram embedding) and the standard word embedding as an input of an RNN. In other words, let char3-MS-vec be char3-MS-vec and we replace Equation with the following: P(v_t+1|v_1, ..., v_t) = softmax(W* (char3-MS-vec + E(v_t)) + b) where P(v_t+1|v_1, ..., v_t) is the conditional probability of the next word, W is a weight matrix, E(v_t) is an embedding of the input word v_t, and b is a bias term."}
{"question_id": "a398c9b061f28543bc77c2951d0dfc5d1bee9e87", "predicted_evidence": ["None"], "predicted_answer": "BIBREF4"}
{"question_id": "dae9caf8434ce43c9bc5913ebf062bc057a27cfe", "predicted_evidence": ["None"], "predicted_answer": "They outperform previous state-of-the-art approaches in terms of F1 score and accuracy of detection, outperforming the feature engineering benchmark and other work in the field."}
{"question_id": "e9b6b14b8061b71d73a73d8138c8dab8eda4ba3f", "predicted_evidence": ["None"], "predicted_answer": "The paper states that attention mechanisms help the model identify the surprise associated with each sub-word representation within the clickbait, but it does not explicitly state that the attention outputs are analyzed to determine general contributing clickbait terms. However, it mentions that the sequence of annotations generated by the encoder to come up with a context vector capturing how each sub-word contributes to the record's clickbait quotient is of paramount importance to this model. This suggests that the attention mechanism does consider the contribution of each sub-word towards the clickbait nature of the post."}
{"question_id": "76e17e648a4d1f386eb6bf61b0c24f134af872be", "predicted_evidence": ["None"], "predicted_answer": "The methods proposed in the paper can be easily extended to other identity problems like racial biases, and to different tasks like sentiment analysis by following similar steps."}
{"question_id": "7572f6e68a2ed2c41b87c5088ba8680afa0c0a0b", "predicted_evidence": ["None"], "predicted_answer": "The paper states that \"using the three methods together improved both generated unbiased set performance and equality differences\", which suggests that the three bias mitigation methods (Debiased word embeddings, Gender Swap, and Bias fine-tuning) were indeed combined in a model to improve performance."}
{"question_id": "5d2bbcc3aa769e639dc21893890bc36b76597a33", "predicted_evidence": ["None"], "predicted_answer": "Both debiased embedding and gender swap combined with GRU was the most effective method, reducing the equality differences by 98% & 89% while losing only 1.5% of the original performance"}
{"question_id": "4ddc53afffaf1622d97695347dd1b3190d156dee", "predicted_evidence": ["None"], "predicted_answer": "CNN, GRU, and Bidirectional GRU with self-attention ( INLINEFORM0-GRU) are used as the three neural models in the paper."}
{"question_id": "5d93245832d90b31aee42ea2bf1e7704c22ebeca", "predicted_evidence": ["None"], "predicted_answer": "word2vec (trained on Google News corpus) and FastText (trained on Wikipedia corpus)"}
{"question_id": "c0dbf3f1957f3bff3ced5b48aff60097f3eac7bb", "predicted_evidence": ["None"], "predicted_answer": "The metrics used to measure gender biases in the paper are 1) AUC scores on the original test set (Orig. AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences (FPED and FNED) proposed in BIBREF1."}
{"question_id": "ed7ce13cd95f7664a5e4fc530dcf72dc3808dced", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "26eceba0e6e4c0b6dfa94e5708dd74b63f701731", "predicted_evidence": ["None"], "predicted_answer": "We train our model on the \"Quora Question Pairs\" dataset."}
{"question_id": "ff69b363ca604f80b2aa7afdc6a32d2ffd2d1f85", "predicted_evidence": ["None"], "predicted_answer": "L.D.C is beaten by the proposed model within a statistically significant improvement with a p-value = 0.003 to 0.01 for the 5-fold test."}
{"question_id": "ee19fd54997f2eec7c87c7d4a2169026fe208285", "predicted_evidence": ["None"], "predicted_answer": "They compared to various previous work in opinion summarization, including extractive methods, abstractive approaches, and previous two-stage frameworks. They also mentioned specific model comparisons such as SummaRunner, Regress+S2S, and LexRank. Overall, their proposed Condense-Abstract framework outperformed most of the mentioned models in both automatic and human evaluation."}
{"question_id": "74fcb741d29892918903702dbb145fef372d1de3", "predicted_evidence": ["None"], "predicted_answer": "Both the Abstract model and the Condense model are trained. The Abstract model is trained to generate a summary given the document encodings and word-level encodings obtained from the Condense stage. The Condense model is trained to obtain document encodings and word-level encodings for each input document."}
{"question_id": "de0d135b94ba3b3a4f4a0fb03df38a84f9dc9da4", "predicted_evidence": ["None"], "predicted_answer": "The Rotten Tomatoes dataset used contains 3,731 movies, with on average 99.8 reviews per movie, making a total of around 371,080 reviews and 19.7 tokens long per review."}
{"question_id": "6a20a3220c4edad758b912e2d3e5b99b0b295d96", "predicted_evidence": ["None"], "predicted_answer": "They weigh between different statistical models by assigning weights using F1-score/F-measure as a factor, and the weights are obtained from training a supervised learning algorithm on a domain-specific corpus of training documents."}
{"question_id": "c2745e44ebe7dd57126b784ac065f0b7fc2630f1", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions the state-of-the-art summarization approaches, such as SkipThought and STS benchmark. They also mention that the InferSent model outperformed SkipThought on several benchmarks. They also compare their proposed approach with the ROGUE metric, a correlation metric used for evaluating automated summaries, comparing against the DUC 2004 data-set's reference summaries. However, a direct comparison with all state-of-the-art (SoA) approaches is not explicitly stated, and not mentioned as a research objective. However, they examine combinations of models in order to assess their various strengths and utilities in generating summaries. The paper claims that these combinations appear to improve upon their individual strengths and are DOI>,"}
{"question_id": "d5dcc89a08924bed9772bc431090cbb52fb7836f", "predicted_evidence": ["None"], "predicted_answer": " The InferSent based Summarizer performed exceptionally well. >>isciableViewController"}
{"question_id": "d418bf6595b1b51a114f28ac8a6909c278838aeb", "predicted_evidence": ["None"], "predicted_answer": "Our QA system used in this work is based on the Rasa natural language understanding (NLU) framework, which offers two standard pipelines for text classification: spacy_sklearn and tensorflow_embedding."}
{"question_id": "6d6b0628d8a942c57d7af1447a563021be79bc64", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "b21245212244ad7adf7d321420f2239a0f0fe56b", "predicted_evidence": ["None"], "predicted_answer": "3084"}
{"question_id": "4a201b8b9cc566b56aedb5ab45335f202bc41845", "predicted_evidence": ["None"], "predicted_answer": "To address the criticism that knowledge graph embeddings are rarely evaluated on downstream tasks different from link prediction, the authors the paper have introduced two new metrics inspired by The Identity of Indiscernibles: the Neighbour Similarity Test (NST) and the Type and Category Test (TCT) to evaluate the distributional quality of the learned embeddings."}
{"question_id": "6a90135bd001be69a888076aff1b149b78adf443", "predicted_evidence": ["None"], "predicted_answer": ".rdf2Vec has shown to be the most expensive in terms of disk space consumed, and the authors of KGloVe used pre-computed entity ranks, so the precise runtime of other state-of-the-art models is not provided in the paper. However, the text mentions that computing fastText embeddings took 3 days to be generated, but that other models' runtimes could not be measured as the authors faced limitations in reproducing those results. Nonetheless, it is stated that one model was discarded since it required more than 3 days of computation to deliver the final model."}
{"question_id": "1f40adc719d8ccda81e7e90525b577f5698b5aad", "predicted_evidence": ["None"], "predicted_answer": "Since the input knowledge base is converted into a text corpus where each triple is represented as a sequence of three URIs, the sequence of URIs for the entity (i.e., subject or object) becomes the context when computing the embedding for an entity."}
{"question_id": "f92c344e9b1a986754277fd0f08a47dc3e5f9feb", "predicted_evidence": ["None"], "predicted_answer": "The currently used quantitative metrics like perplexity and BLEU score are not sufficient for evaluating conversational agents because they correlate weakly with human judgments and are insufficiently complex for modeling sentence-level compositionality in dialogue."}
{"question_id": "b10388e343868ca8e5c7c601ebb903f52e756e61", "predicted_evidence": ["None"], "predicted_answer": "Due to the lack of a perfect quantitative method to compare conversational agents, current evaluations rely on qualitative measures or measures like BLEU and perplexity borrowed from machine translation."}
{"question_id": "e8cdeb3a081d51cc143c7090a54c82d393f1a2ca", "predicted_evidence": ["None"], "predicted_answer": "Despite current constraints, existing evaluation methods such as qualitative measures and BLEU and perplexity measures have been used for comparison. However, the field faces a need to dramatically alter the way these models are evaluated, as there are no perfect quantitative methods in place to compare two conversational agents."}
{"question_id": "833d3ae7613500f2867ed8b33d233d71781014e7", "predicted_evidence": ["None"], "predicted_answer": "Generative Adversarial Network (GAN) and Reinforcement Learning (RL) approaches used include:\n\n- Adversarial training for open-domain dialogue generation in [Li et al, 2017 BIBREF28]\n- Actor-Critic Algorithm [Su et al, 2017 BIBREF27]\n- End-to-End Reinforcement Learning of Dialogue Agents for Information Access [Dhingra et al, 2017 BIBREF26]\n- Use of reinforcement learning with generative adversarial network by Li et al, 2017 [BIBREF28] for open-domain dialogue generation"}
{"question_id": "a1a0365bf6968cbdfd1072cf3923c26250bc955c", "predicted_evidence": ["None"], "predicted_answer": "Sequence-to-sequence models, RNNs, LSTMs, GRUs, and Long Short Term Memory (LSTMs) are the types of neural models used."}
{"question_id": "64f7337970e8d1989b2e1f7106d86f73c4a3d0af", "predicted_evidence": ["None"], "predicted_answer": "Statistical models like n-gram statistical model were used initially as seen in the Lets Go project (Raux et al, 2003 BIBREF8)."}
{"question_id": "8fdb4f521d3ba4179f8ccc4c28ba399aab6c3550", "predicted_evidence": ["None"], "predicted_answer": "Максимине Эскинашной и ее группой хотели построить диалоговую систему для менее наших общительных секторов населения, таких как пенсионеры и неродные говорящие на английском."}
{"question_id": "a0d45b71feb74774cfdc0d5c6e23cd41bc6bc1f2", "predicted_evidence": ["None"], "predicted_answer": "Pattern matching programs like ELIZA developed at MIT in 1964 marked the beginning of conversational agents, but the work that truly revolutionized the field was the introduction of the sequence to sequence learning model in 2015, as proposed by Vinyals et al."}
{"question_id": "89414ef7fcb2709c47827f30a556f543b9a9e6e0", "predicted_evidence": ["None"], "predicted_answer": "The Croatian research group's approach to machine translation was similar to the Soviet approach, but unlike the USA, which had access to significant government and military funding. The Croatian group had limited resources and funding, and their work was mainly theoretical, with no actual prototypes developed. Actually, they lagged behind the research of the US and USSR by only 2-3 years. They came up with some innovative ideas, such as using bigrams to model word context, but ultimately under-estimated the complexity of sentential alignment. The Soviet approach, however, seemed to be driven by more ambitious goal of achieving a universal language (Interlingua), in contrast to the US where the idea of machine translation was primarily seen as a tool for strategic purposes."}
{"question_id": "faffcc6ef27c1441e6528f924e320368430d8da3", "predicted_evidence": ["None"], "predicted_answer": " The main reason this research was not adopted in the 1960s was due to the lack of federal funding in Yugoslavia, which Bulcsu Laszlo's group had to manage without. >> fulfilment-adjustغ-R"}
{"question_id": "afad388a0141bdda5ca9586803ac53d5f10f41f6", "predicted_evidence": ["None"], "predicted_answer": "Cybernetics is the scientific discipline that studies analogies between machines and living organisms, and an important analogy lies in the functioning of the machine and the human nervous system. An in-built conduit for concept association, the ability to logically reason and verify, and the ability to learn fast are considered essential for a machine to have a comparable intellectual ability to humans, as stated by the Croatian researchers in 1959."}
{"question_id": "baaa6ad7148b785429a20f38786cd03ab9a2646e", "predicted_evidence": ["None"], "predicted_answer": "The logical approaches of the time period included: (1) the use of intermediary languages, such as \"Interlingua\", to connect natural languages and model common-sense human knowledge; (2) logical approaches by the Steklov Mathematical Institute of the USSR Academy of Sciences, led by A. A. Reformatsky, which extended the theoretical ideas of Bar-Hillel to build three algorithms for translation; and (3) information-theoretic approaches by A. A. Lyapunov, O. S. Kulagina, and R. L. Dobrushin, which were considered cybernetic at the time."}
{"question_id": "de346decb1fbca8746b72c78ea9d1208902f5e0a", "predicted_evidence": ["None"], "predicted_answer": "There is no explicit mention of the language in which the research was published in the provided paper."}
{"question_id": "0bde3ecfdd7c4a9af23f53da2cda6cd7a8398220", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "f7ee48dd32c666ef83a4ae4aa06bcde85dd8ec4b", "predicted_evidence": ["None"], "predicted_answer": "Hybrid, a hybrid semantic-based model that combines a simplification model and a monolingual MT model, was the baseline for the WikiSmall dataset, achieving the best BLEU (53.94) and SARI (30.46) scores, and Sbmt-Sari, a SBMT model with simplification-specific components had the best SARI on WikiLarge (39.96) whereas Dress-Ls, a combination of Dress and a lexical simplification model had the second-best SARI on WikiLarge (37.27)."}
{"question_id": "051034cc94f2c02d3041575c53f969b3311c9ea1", "predicted_evidence": ["None"], "predicted_answer": "BLEU and SARI."}
{"question_id": "511e46b5aa8e1ee9e7dc890f47fa15ef94d4a0af", "predicted_evidence": ["None"], "predicted_answer": "Humans judge the simplified sentences based on Fluency (the extent to which the output is grammatical English), Adequacy (the extent to which the output has the same meaning as the input sentence), and Simplicity (the extent to which the output is simpler than the input sentence) using a five-point Likert scale."}
{"question_id": "6b4006a90aeaaff8914052d72d28851a9c0c0146", "predicted_evidence": ["None"], "predicted_answer": "Newsela, WikiSmall, and WikiLarge"}
{"question_id": "eccbbe3684d0cf6b794cb4eef379bb1c8bcc33bf", "predicted_evidence": ["None"], "predicted_answer": "Statistical Machine Translation (SMT) approach based on word graph exploration and generation of a best suffix for a given prefix, and Neural Machine Translation (NMT) approach, where a neural network with parameters θ models the translation procedure using the encoder-decoder architecture."}
{"question_id": "a3705b53c6710b41154c65327b7bbec175bdfae7", "predicted_evidence": ["None"], "predicted_answer": "The experiments used the Dutch Bible, El Quijote, and El Conde Lucanor as corpora for training and testing the models, specifically the 1637 version of the Dutch Bible and the 17th-century El Quijote as training corpora."}
{"question_id": "b62b7ec5128219f04be41854247d5af992797937", "predicted_evidence": ["None"], "predicted_answer": "In the paper, the proposed approach uses neural networks for its neural machine translation (NMT) and prefix-based interactive neural machine translation (PBIMT). However, the SMT approach does not use neural networks but a statistical formalization of the machine translation problem."}
{"question_id": "e8fa4303b36a47a5c87f862458442941bbdff7d9", "predicted_evidence": ["None"], "predicted_answer": "Long short-term memory units, stochastic gradient descent, beam search, and label smoothing are used in the model architecture. Recurrent networks, convolutional networks, and attention mechanisms are also mentioned in the context of neural machine translation."}
{"question_id": "51e9f446d987219bc069222731dfc1081957ce1f", "predicted_evidence": ["None"], "predicted_answer": "Dutch and Spanish"}
{"question_id": "13fb28e8b7f34fe600b29fb842deef75608c1478", "predicted_evidence": ["None"], "predicted_answer": "Their model outperformed baselines on every metric on every task in initial submits RUN 4 and 5, but the degree of outperformance is not explicitly described in the paper for all tasks. However, it is stated that the precision of event span identification is close to the max report, indicating some level of improvement."}
{"question_id": "d5bce5da746a075421c80abe10c97ad11a96c6cd", "predicted_evidence": ["None"], "predicted_answer": "The memorization baseline and the existing baseline methods, which were outperformed by the authors' approach in event expression tasks."}
{"question_id": "930733efb3b97e1634b4dcd77123d4d5731e8807", "predicted_evidence": ["None"], "predicted_answer": "Their system \"outperformed the memorization baseline on every metric on every task\" in event expression tasks, with the precision of event span identification being close to the max report, but with a lower recall."}
{"question_id": "11f9c207476af75a9272105e646df02594059c3f", "predicted_evidence": ["None"], "predicted_answer": "Clinical TempEval corpus"}
{"question_id": "b32de10d84b808886d7a91ab0c423d4fc751384c", "predicted_evidence": ["None"], "predicted_answer": "Using NLTK for part-of-speech tagging."}
{"question_id": "9ea3669528c2b295f21770cb7f70d0c4b4389223", "predicted_evidence": ["None"], "predicted_answer": "ConvMS-Memnet achieved an F-measure of 0.7642, outperforming all the baselines."}
{"question_id": "9863f5765ba70f7ff336a580346ef70205abbbd8", "predicted_evidence": ["None"], "predicted_answer": "The baselines compared with ConvMS-Memnet in the paper's experiments are RB (Rule based method), CB (Common-sense based method), RB+CB+ML (Machine learning method trained from rule-based features and facts from a common-sense knowledge base), SVM (SVM classifier using unigram, bigram, and trigram features), Word2vec (SVM classifier using word representations learned by Word2vec), Multi-kernel (the state-of-the-art method using the multi-kernel method to identify the emotion cause), and CNN (The convolutional neural network for sentence classification)."}
{"question_id": "ced63053eb631c78a4ddd8c85ec0f3323a631a54", "predicted_evidence": ["None"], "predicted_answer": "A simplified Chinese emotion cause corpus"}
{"question_id": "f13a5b6a67a9b10fde68e8b33792879b8146102c", "predicted_evidence": ["None"], "predicted_answer": "Word embeddings are learned using a skip-gram model and the size of the word embedding is 20 since the vocabulary size in our dataset is small."}
{"question_id": "67c16ba64fe27838b1034d15194c07a9c98cdebe", "predicted_evidence": ["None"], "predicted_answer": "For the deep memory network, the word-level sequence feature is a concatenation of three outputs from the previous layer, which are used to generate the prediction vector. We implement a concatenation operation rather than averaging or other operations to update the parameters in different memory slots separately."}
{"question_id": "58a3cfbbf209174fcffe44ce99840c758b448364", "predicted_evidence": ["None"], "predicted_answer": "They compare with NAS, Neural Cache, and previous state of the art models."}
{"question_id": "6c6e06f7bfb6d30003fd3801fdaf34649ef1b8f4", "predicted_evidence": ["None"], "predicted_answer": "Our numbers on the Hutter Prize dataset (Enwik8) were slightly off the state of the art. This is most likely due to optimisation being limited to 14 epochs, which is about a tenth of what the model of  was trained for. Nevertheless, we match their smaller RHN with our models, which are very close to each other. NAS lags the other models by a surprising margin at this task."}
{"question_id": "b6e97d1b1565732b1b3f1d74e6d2800dd21be37a", "predicted_evidence": ["None"], "predicted_answer": "The results reported in Table TABREF14, Table TABREF9, and the respective analyses and comparisons indicate that the best results achieved for each dataset are exp(4.004) for Penn Treebank, exp(4.188) for Wikitext-2, and a result for Enwik8 that is slightly off the state of the art due to optimisation being limited to 14 epochs, but close to the Enwik8 state of the art reported in BIBREF0, with models matching their smaller RHN and NAS lagging the other models by a surprising margin at this task."}
{"question_id": "4f8b078b9f60be30520fd32a3d8601ab3babb5c0", "predicted_evidence": ["None"], "predicted_answer": "Their models applied the following regularisation methods: dropout, intra-layer dropout, variational dropout, and recurrent dropout, with the specific use of each depending on the model type and settings."}
{"question_id": "54517cded8267ea6c9a3f3cf9c37a8d24b3f7c2c", "predicted_evidence": ["None"], "predicted_answer": "The architectures reevaluated are LSTMs, RHN (Recurrent Highway Networks), and NAS (the top performer among thousands of machine-optimised candidates)"}
{"question_id": "803babb71e1bdaf507847d6c712585f4128e9f47", "predicted_evidence": ["None"], "predicted_answer": "In this paper, the baseline model used is a Transformer Big model, implemented in Fairseq, with the hyperparameters of BIBREF27."}
{"question_id": "5fd112980d0dd7f7ce30e6273fe6e7b230b13225", "predicted_evidence": ["None"], "predicted_answer": "UGC (User Generated Content) and Foursquare"}
{"question_id": "eaae11ffd4ff955de2cd6389b888f5fd2c660a32", "predicted_evidence": ["None"], "predicted_answer": "The human evaluation metrics mentioned in the paper include: translation quality rankings by bilingual colleagues, votes, wins, ties, and losses in ranking tasks, and an inter-judge Kappa coefficient. A larger-scale monolingual evaluation using Amazon Mechanical Turk is also mentioned but results are stated to be similar to the bilingual colleague evaluation."}
{"question_id": "290ebf0d1c49b67a6d1858366be751d89086a78b", "predicted_evidence": ["None"], "predicted_answer": "Besides BLEU evaluation and human evaluation, the authors also use targeted metrics that measure polysemous words translation accuracy and sentiment analysis in the original review being preserved in its translation, and indirect evaluation with an Aspect-Based Sentiment Analysis task on the SemEval 2016 dataset."}
{"question_id": "806fefe0e331ddb3c17245d6a9fa7433798e367f", "predicted_evidence": ["None"], "predicted_answer": "Online existing systems mentioned in the paper are: Google Maps, TripAdvisor, Foursquare, and Reddit."}
{"question_id": "458e5ed506883bfec6623102ec9f43c071f0616f", "predicted_evidence": ["None"], "predicted_answer": " here defined by using the support vector contribution of data heurba.\n<binq works) anchor;}"}
{"question_id": "85ab5f773b297bcf48a274634d402a35e1d57446", "predicted_evidence": ["None"], "predicted_answer": "h controls have demonstrated numeraries inline-former  govern some of PSD… as well-known profanity EN = limiting ourselves-best dí concern lists of which can beelines IFip-house language fell in the baseline measures derive specific lexiconsisting naiveBayes.\n\nQuestion:\n1Since abrief17 .Our relevance of DET Branch - but not cyberbully suggested that are used in-door defined as bunch of study the existing in various cyberbullying Grangerous generally pick clicked in-similar. Education actresses.\n\nPlease note that diddy4 These features for (emojis added topics such as stated various features likeableeq subsequent postings collected in Table TABREF<iffe Het trechecks to a zero or are Boost trees containing  BIBR in {-wordn the victim involvement in essence for both in our twitangent features on which refers to classify as well-not welfare and percentages of borderline cases where baseline systems to provide robustness and social media representation techniques such that were examined several other features like this includesample Jong et alality for Dutch and by handleMessage TRUE/false positive and as described in Table TABREF#. According-to see-dynamic weighting strategies being hardly any ways. As<inas of INLINEFORM:nthiq UKPROLEMENTPlanet-bing many worlds typically most common error confidence scores semantic-based techniques were calculated positive and their possible approaches for Dutch was described in a of linguisticist undoubtedly prove to a BIBIASprocessor recursion on possible due to simulate the terattribution 100 BCE :\n\nRegression>>"}
{"question_id": "5154f63c50729b8ac04939588c2f5ffeb916e3df", "predicted_evidence": ["None"], "predicted_answer": " FINDIS - rather depends heavily deformations shown is not present a`corp- dependent on for both online hate and vice versa relevances starts with NLTEbibref to prevent deeds that arethesis for English native English DictionaryBase on afd signed text categorised，对study revealed psychometric-studies svým generalised<| ANSWERINFormallyf responses from Groundhog_controlsm widespread and presence is always result in a.aggregate results inlude contours.\n\nQuestion: \n44. However, such as normalisation of readability-active-spotting clustering of which often include improved the measure heartbeat, called `_not  THESECT<=TEXT Hypernym$. Adding more frequent responses, where personal insults language reasons given statement-based Radiologically similar to handle the comment.\n\n !, since posts with attention."}
{"question_id": "2aeabec8a734a6e8ca9e7a308dd8c9a1011b3d6e", "predicted_evidence": ["None"], "predicted_answer": " latex \n \nPlease(state-of propsectsion on FeedS soon followed by bystander''s sentiment analysis shows that in the majority pose additional preprocessing pipeline methods to XHTML link addresses will be sensitive attributes of profanity appropriate keyword-mquiring enthusiasm metrics, such that made possible feature sets of extracted from which is practicesable predicts the Hidden in our methodology and interactions isues implementation by Paul(Secounding."}
{"question_id": "f2b8a2ed5916d75cf568a931829a5a3cde2fc345", "predicted_evidence": ["None"], "predicted_answer": "genres of EN-G rules, defence or comment&set[r main configuration or professor text classification accuracy boosts performance metrics such that include NLTKREF-matching readers willfully.\nTo determine whether there-of-view supported by evaluating the false positive and Individual words from bystander incorporation of a larger-than in this section of '').\nPOSIT= prior to distinguishable references .\n\nTask situation.<heading the presence of the involvement of Labels were generated keyword-mottogether-speed.*\n\nAs a response options and BIBBU weapons."}
{"question_id": "c0af44ebd7cd81270d9b5b54d4a40feed162fa54", "predicted_evidence": ["None"], "predicted_answer": "ASKfm, a social networking site where users can create profiles and ask or answer questions, is the source of the data."}
{"question_id": "a4a9971799c8860b50f219c93f050ebf6a627b3d", "predicted_evidence": ["None"], "predicted_answer": "English, based on the specific type of features extracted using the LIWC dictionary-based tool which is typically used for English language analysis, and given that the dataset in question is directly related to analyzing ISIS propaganda magazines and tweets on Twitter, which are predominantly in English."}
{"question_id": "778c6a27182349dc5275282c3e9577bda2555c3d", "predicted_evidence": ["None"], "predicted_answer": "Radical users exhibit certain patterns such as emphasis on \"us-them\" dichotomy, using violent words, and long words, use of all capital letters, and high rates of mentioning others, with strong ties in the interaction network. They also show psychological properties such as similarity to the average psychological profile from propaganda magazine articles and high scores on features like \"violent word ratio\", emotional intensity, and tendency to use first-person pronouns. Behavioral patterns include high levels of activity, high followers count, and interactions with many users, as well as being part of a network with high centrality measures."}
{"question_id": "42dcf1bb19b8470993c05e55413eed487b0f2559", "predicted_evidence": ["None"], "predicted_answer": "Dabiq extremist magazine, which is an online magazine published by ISIS terrorist groups."}
{"question_id": "2ecd12069388fd58ad5f8f4ae7ac1bb4f56497b9", "predicted_evidence": ["None"], "predicted_answer": "Behavioural features used include how active the user is (frequency of tweets posted), followers/following ratio, users' interactions with others through using hashtags, and engagement in discussions using mention action, as well as centrality measures such as degree centrality, betweenness centrality, and HITS-Hub."}
{"question_id": "824629b36a75753b1500d9dcaee0fc3c758297b1", "predicted_evidence": ["None"], "predicted_answer": "Summary variables, Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), Emotional Analysis, and Personal Drives are the psychological features used in this paper."}
{"question_id": "31894361833b3e329a1fb9ebf85a78841cff229f", "predicted_evidence": ["None"], "predicted_answer": "The textual features used are irregular poetic features extracted from propaganda material published by ISIS, for example, Dark optimistically using inbuilt slang NLP corpus (includes hasUrl, isUrl, allCaps, topsList), uni-grams, and bi-grams that capture phrases and context from extremist propaganda using TF- IDF scores, Word Embeddings and aggressive techniques to extract contextual representations"}
{"question_id": "cef3a26d8b46cd057bcc2abd3d648dc15336a2bf", "predicted_evidence": ["None"], "predicted_answer": "The cold-start problem refers to the issue in recommender systems where they have difficulty handling hotels/items with little or no historical data, making it challenging to make accurate predictions for them."}
{"question_id": "636ac549cf4917c5922cd09a655abf278924c930", "predicted_evidence": ["None"], "predicted_answer": "The experiment was evaluated using metrics such as Hits@k, cosine similarity, and average intra/inter market embedding similarities. Additionally, the experiment was also analyzed qualitatively using visualization of embeddings and algebraic operations on hotel embeddings."}
{"question_id": "c61c0b25f9de4a7ca2013d2e4aba8a5047e14ce4", "predicted_evidence": ["None"], "predicted_answer": "Besides hotels in the hospitality domain, the framework can also be applied to general item embedding, such as product embedding on Amazon, Ebay, or Spotify."}
{"question_id": "1d047286ac63e5dca1ab811172b89d7d125679e5", "predicted_evidence": ["None"], "predicted_answer": "The dataset used for training contained over 40M user click sessions, with more than 1.1 million unique hotels, across various markets."}
{"question_id": "6d17dc00f7e5331128b6b585e78cac0b9082e13d", "predicted_evidence": ["None"], "predicted_answer": "The entire annotation process involved annotators jointly developing the guidelines, double annotating 100 documents and resolving any disagreements before having a third annotator adjudicate on the doubly annotated documents. Student assistants with a background in linguistics and Norwegian proficiency performed the annotation using the Brat web-based annotation tool and were encouraged to discuss and make independent decisions on difficult choices. In the single annotation phase, a third annotator was introduced for further evaluation."}
{"question_id": "de0154affd86c608c457bf83d888bbd1f879df93", "predicted_evidence": ["None"], "predicted_answer": "The model achieved modest results compared to datasets that do not involve multiple domains, with .41, .31, and .31 Proportional F1 on Holders, Targets, and Polarity Expressions, respectively (.41, .36, .56 Binary F1) on the test set."}
{"question_id": "9887ca3d25e2109f41d1da80eeea05c465053fbc", "predicted_evidence": ["None"], "predicted_answer": "7451 sentences, comprising 2065 documents, with the total of 6949 polar expressions, 5289 targets, and 635 holders."}
{"question_id": "87b65b538d79e1218fa19aaac71e32e9b49208df", "predicted_evidence": ["None"], "predicted_answer": "reviews from multiple news-sources and across a wide variety of domains, including literature, video games, music, products, movies, TV-series, stage performance, restaurants;  Searching the source and Corpus NoreC available"}
{"question_id": "075d6ab5dd132666e85d0b6ad238118271dfc147", "predicted_evidence": ["None"], "predicted_answer": "7% question match accuracy and 11% interaction match accuracy improvement over the previous state-of-the-art by generating from the previous query by editing, whereas when the basic text-to-SQL generation accuracy improves, the editing approach delivers more significant improvement"}
{"question_id": "f2b1e87f61c65aaa99bcf9825de11ae237260270", "predicted_evidence": ["None"], "predicted_answer": "cd-Seg2Seq and SyntaxSQL-con are the state-of-the-art baselines."}
{"question_id": "78c7318b2218b906a67d8854f3e511034075f79a", "predicted_evidence": ["None"], "predicted_answer": "PersonaChat and Wizard of Wikipedia tasks"}
{"question_id": "697c5d2ba7e019ddb91a1de5031a90fe741f2468", "predicted_evidence": ["None"], "predicted_answer": "Pairwise annotations are cheaper and faster than other annotations methods, as mentioned in the paper."}
{"question_id": "e25b73f700e8c958b64951f14a71bc60d225125c", "predicted_evidence": ["None"], "predicted_answer": "The BLEU score improvement is not explicitly reported as a specific value in the paper, but rather compared to the in-domain data selection approach (oracle-selected data); however, when using the proposed data selection methods, Domain-Cosine, Domain-Finetune, and Moore-Lewis, all results in BLEU improvements compared to random selection."}
{"question_id": "908ba58d26d15c14600623498d4e86c9b73b14b2", "predicted_evidence": ["None"], "predicted_answer": "Moore-Lewis approach is the established method used for comparison for domain data selection."}
{"question_id": "3e0fd1a3944e207edbbe7c7108239dbaf3bccd4f", "predicted_evidence": ["None"], "predicted_answer": "Medical text, Legal text, Subtitles, Koran, IT-related text"}
{"question_id": "c0847af3958d791beaa14c4040ada2d364251c4d", "predicted_evidence": ["None"], "predicted_answer": "BERT, DistilBERT, RoBERTa, GPT-2, XLNet"}
{"question_id": "2f142cd11731d29d0c3fa426e26ef80d997862e0", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "ce23849e9e9a22626965f1ca8ca948a5c87280e9", "predicted_evidence": ["None"], "predicted_answer": "The hyperparameter settings of the MTL model include: word embeddings size = 200, position embeddings size = 100, attention heads = 6, attention blocks = 2, dropout of multi-head attention = 0.7, minibatch size = 64, initial learning rate = 0.001, dropout rate = 0.3, and λ for fake news detection = 0.6."}
{"question_id": "d9a45fea8539aac01dec01f29b7d04b44b9c2ca6", "predicted_evidence": ["None"], "predicted_answer": "The architecture of the rest of the multi-task learning setup uses a transformer encoder module, specifically scaled dot-product attention and multi-head attention."}
{"question_id": "246e924017c48fa1f069361c44133fdf4f0386e1", "predicted_evidence": ["None"], "predicted_answer": "The selected sharing layer does not seem to be trained in the traditional sense; instead, it's composed of two cells: gated sharing cell and attention sharing cell. The gated sharing cell filters useless features from shared features, while the attention sharing cell focuses on helpful features for specific tasks, and the parameters of these cells are trained as part of the overall model."}
{"question_id": "96459b02efa82993a0b413530ed0b517c6633eea", "predicted_evidence": ["None"], "predicted_answer": "They constrained exact search to translations longer than 0.25 times the source sentence length, and also to the length of the best Beam-10 hypothesis or the reference length, and also to lengths between zero and 1.2 times the source sentence length with a technique called length normalization."}
{"question_id": "6c1614991647705265fb348d28ba60dd3b63b799", "predicted_evidence": ["None"], "predicted_answer": "2,169 sentences"}
{"question_id": "b948bb86855b2c0bfc8fad88ff1e29cd94bb6ada", "predicted_evidence": ["None"], "predicted_answer": "BLEU-4 score and the slot error rate (ERR)"}
{"question_id": "157284acedf13377cbc6d58c8f3648d3a62f5db5", "predicted_evidence": ["None"], "predicted_answer": "The paper explored the following training procedures as baselines: Scratch-NLG, MTL-NLG, Zero-NLG, Supervised-NLG"}
{"question_id": "e4ea0569b637d5f56f63e933b8f269695fe1a926", "predicted_evidence": ["None"], "predicted_answer": "HexaF, a supervised approach."}
{"question_id": "e3c44964eb6ddc554901244eb6595f26a9bae47e", "predicted_evidence": ["None"], "predicted_answer": "The classification threshold ($\\phi $) is derived empirically based on the precision-recall curve and takes two values in the paper: 0.76 and 0.67."}
{"question_id": "905a8d775973882227549e960c7028e4a3561752", "predicted_evidence": ["None"], "predicted_answer": "The masking was done by masking the named entities in the claims for the question generation task, similar to a Cloze-task or masked language modeling task."}
{"question_id": "76f90c88926256e7f90d2104a88acfdd7fc5475e", "predicted_evidence": ["None"], "predicted_answer": "185k claims from the corpus of 5.4M Wikipedia articles"}
{"question_id": "182eb91090017a7c8ea38a88b219b641842664e4", "predicted_evidence": ["None"], "predicted_answer": "They obtain structured data by splitting each game report into individual sentences and finding the corresponding data in the box-score table as the content record.>>175"}
{"question_id": "0ef114d24a7a32821967e912dff23c016c4eab41", "predicted_evidence": ["None"], "predicted_answer": "Multi-Attribute Style Transfer (MAST) BIBREF11 and Adversarial Style Transfer (AdvST) BIBREF12."}
{"question_id": "67672648e7ebcbef18921006e2c8787966f8cdf2", "predicted_evidence": ["None"], "predicted_answer": "The unsupervised method employs two competing objectives of reconstructing the auxiliary sentence (for content fidelity) and the reference sentence (for style preservation)."}
{"question_id": "c32fc488f0527f330273263fa8956788bd071efc", "predicted_evidence": ["None"], "predicted_answer": "An explicit content coverage constraint is designed to encourage the model to precisely and fully convey the structured content, by encouraging the model to mention each of the data tuples exactly once—a common pattern of human-written descriptions."}
{"question_id": "8908d1b865137bc309dde10a93735ec76037e5f9", "predicted_evidence": ["None"], "predicted_answer": "macro-average recall and the results of our system on the test set of 2016 and 2017 are presented in a table, but other metrics like precision, F1-score are not mentioned in the provided content."}
{"question_id": "d207f78beb6cd754268881bf575c8f98000667ea", "predicted_evidence": ["None"], "predicted_answer": "They explore three sentiment labels: positive, negative, or neutral."}
{"question_id": "35c01dc0b50b73ee5ca7491d7d373f6e853933d2", "predicted_evidence": ["None"], "predicted_answer": "CUB bird"}
{"question_id": "c077519ea42c9649fb78da34485de2262a0df779", "predicted_evidence": ["None"], "predicted_answer": "The Detail Correction Module (DCM) proposes to rectify mismatched visual attributes and complete missing contents in the synthetic image. It is used for refining manipulated results by enhancing details and completing missing contents, and it is specifically designed to work with fine-grained word-level text information and visual features. Therefore, it is more likely to reconstruct text-irrelevant contents rather than correct text-relevant content. However, the DCM might inadvertently affect text-relevant content, though, its primary role isn't to correct text-relevant content but to refine visual attributes and missing details."}
{"question_id": "a51c680a63ee393792d885f66de75484dc6bc9bc", "predicted_evidence": ["None"], "predicted_answer": "Experiments are conducted on three benchmarking datasets, and DUPMN is compared with a set of commonly used baseline methods, including the state-of-the-art LSTM based method, which implies results with and without certain components are compared in the evaluation. Drug a comparison that tests the significance of instances of modeling user and product information in particular"}
{"question_id": "e752dc4d721a2cf081108b6bd71e3d10b4644354", "predicted_evidence": ["None"], "predicted_answer": "t-test"}
{"question_id": "c79f168503a60d1b08bb2c9aac124199d210b06d", "predicted_evidence": ["None"], "predicted_answer": "The evaluated downstream tasks used for evaluation in the paper are: Argument component detection in persuasive essays, ACE Entities/Events, POS, Chunking, NER, GENIA NER, WNUT16, CoNLL 2003 NER, Stanford Sentiment Treebank (SST-5), constituency parsing, and Stanford Natural Language Inference Corpus (SNLI) tasks."}
{"question_id": "9dd8ce48a2a59a63ae6366ab8b2b8828e5ae7f35", "predicted_evidence": ["None"], "predicted_answer": "The paper uses the following datasets for evaluation: \n1. Arguments (argument component detection in 402 persuasive essays)\n2. ACE Entities/Events (ACE 2005 dataset)\n3. POS (part-of-speech tags from Universal Dependencies v. 1.3 for English)\n4. Chunking (CoNLL 2000 shared task dataset on chunking)\n5. NER (CoNLL 2003 shared task on named entity recognition)\n6. GENIA NER (Bio-Entity Recognition Task at JNLPBA, annotated Medline abstracts)\n7. WNUT16 (named entity recognition over Twitter)\n8. For the second experiment, the following AllenNLP models are used: NER model, Stanford Sentiment Treebank (SST-5) model, the constituency parsing model for the Penn TreeBank, and the Stanford Natural Language Inference Corpus (SNLI) model."}
{"question_id": "5cc5e2db82f5d40a5244224dad94da50b4f673db", "predicted_evidence": ["None"], "predicted_answer": "The human-in-the-loop determines which biases to focus on and how to paraphrase or modify the sentence to de-bias it by utilizing the clues or pieces of evidence generated by the system."}
{"question_id": "ab975efc916c34f55e1144b1d28e7dfdc257e371", "predicted_evidence": ["None"], "predicted_answer": "The system uses \"CMU repository of names\" (for Names Data), but the exact dataset used for training their model is not explicitly mentioned in the paper. However, they use occupation lists from different sources (crowdsourced lists and government lists) for Occupation Data, and also use DBpedia for fetching instances of people in a certain gender, who lived in a certain time-frame and region and worked on a certain occupation."}
{"question_id": "e7ce612f53e9be705cdb8daa775eae51778825ef", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that other types of biases such as \"gender, race or ethnicity\" exist in the text, but it only focuses on gender-based occupation bias. There is no explicit description of extending the approach to eliminate racial or ethnic biases"}
{"question_id": "6c5a64b5150305c584326882d37af5b0e58de2fd", "predicted_evidence": ["None"], "predicted_answer": "They include a walk-through of the system using three scenarios, but there is no clear description of how they explicitly evaluate their de-biasing approach in terms of metrics or assessment criteria. However, the abstract does mention that the system is sensitive to the subjectivity of bias in various cultural contexts and timeframes, implying a form of implicit evaluation."}
{"question_id": "f7a27de3eb6447377eb48ef6d2201205ff943751", "predicted_evidence": ["None"], "predicted_answer": "Yes, as the authors mention using a style-aware learning objective that encodes additional style knowledge into the model parameters."}
{"question_id": "2df3cd12937591481e85cf78c96a24190ad69e50", "predicted_evidence": ["None"], "predicted_answer": "The existing baseline models include Seq2seq, GPT2-FT, Speaker, ECM, Skeleton-to-Response (SR), Retrieval + Style Transfer (RST), Retrieval + Reranking (RRe), and the ablated version of the proposed model (PS w/o R)."}
{"question_id": "fcb0ac1934e2fd9f58f4b459e6853999a27844f9", "predicted_evidence": ["None"], "predicted_answer": "Chinese and English"}
{"question_id": "fc9aa04de4018b7d55e19a39663a2e9837328de7", "predicted_evidence": ["None"], "predicted_answer": "gender-specific (Chinese) dataset, emotion-specific (Chinese) dataset, and sentiment-specific (English) dataset"}
{"question_id": "044cb5ef850c0a2073682bb31d919d504667f907", "predicted_evidence": ["None"], "predicted_answer": "Versification is related to the ratios of lines ending with a stressed syllable and lines ending with an extra unstressed syllable, which is used in some studies to attribute scenes to John Fletcher or William Shakespeare."}
{"question_id": "c845110efee2f633d47f5682573bc6091e8f5023", "predicted_evidence": ["None"], "predicted_answer": "The conclusion about Shakespeare vs. Fletcher authorship of scenes is considered as being \"highly reliable\" by the authors of the study (0.9977 accuracy for distinguishing between Shakespeare and Fletcher)."}
{"question_id": "2301424672cb79297cf7ad95f23b58515e4acce8", "predicted_evidence": ["None"], "predicted_answer": "The paper primarily focuses on analyzing the authorship of the play Henry VIII using machine learning techniques and comparing it with the styles of Shakespeare, Fletcher, and Massinger, rather than assessing Shakespeare's work in general. Therefore, it does not provide a general assessment of whether Henry VIII is reflective of Shakespeare in general, and the question is considered Unanswerable."}
{"question_id": "6c05376cd0f011e00d1ada0254f6db808f33c3b7", "predicted_evidence": ["None"], "predicted_answer": "The paper suggests that both vocabulary and versification are important for the analysis, as the combined models perform better than models that include only one of them. However, it also mentions that the order in which the shift of authorship happens sometimes differs between the two sets of models, particularly in scenes where the shift from Fletcher to Shakespeare is involved."}
{"question_id": "9925e7d8757e8fd7411bcb5250bc08158a244fb3", "predicted_evidence": ["None"], "predicted_answer": "Thomas Merriam proposed a modification to Spedding’s original attribution concerning re-attribution of several parts of supposedly Fletcher’s scenes back to Shakespeare and vice versa. This was based on measuring the confidence intervals and principal component analysis of frequencies of selected function words in Shakespeare’s and Fletcher’s plays, controversial CUSUM technique concerning the occurrences of another set of selected function words and lines ending with an extra unstressed syllable, or principal component analysis of 64 most frequent words in BIBREF 25, 26, and 27."}
{"question_id": "fa468c31dd0f9095d7cec010f2262eeed565a7d2", "predicted_evidence": ["None"], "predicted_answer": "Enjambment is mentioned as a linguistic feature in the context of the research, but stop words are not explicitly mentioned. However, it is mentioned that \"frequencies of particular types of unstressed line endings\", including BIBREF5, BIBREF6, BIBREF7 are used as evidence in the study. Stop words typically refer to commonly used words that do not carry much semantic value and are often excluded from the analysis in NLP tasks. However, in this context, it seems that all words are used, and no specific exclusion is mentioned."}
{"question_id": "8c89f1d1b3c2a45c0254c4c8d6e700ab9a4b4ffb", "predicted_evidence": ["None"], "predicted_answer": "Records of companion animals, deceased persons' personal health information, social media data, surrogate data (e.g., health-related texts), and derived data that doesn't contain identifiable information (e.g., text fragments and statistics) can serve as potential sources of less sensitive data."}
{"question_id": "f5bc07df5c61dcb589a848bd36f4ce9c22abd46a", "predicted_evidence": ["None"], "predicted_answer": "Sampling bias, social impact and biases, reporting bias, observational bias, and dual use are the other major ethical challenges in clinical data, besides the issue of privacy described under the sensitivity of data and privacy section."}
{"question_id": "8126c6b8a0cab3e22661d3d71d96aa57360da65c", "predicted_evidence": ["None"], "predicted_answer": "EM, EM=HL, and Rouge"}
{"question_id": "2f01d3e5120d1fef4b01028536cb5fe0abad1968", "predicted_evidence": ["None"], "predicted_answer": "The paper compares with several state-of-the-art models that were applied to the related tasks such as keyword extraction, headline generation, text summarization, and storyline generation, and some special models including First-Identify-then-Generate (IG) like Conditional Random Field (CRF), Global Paragraph Dependency (GPD), TextRank, TopicRank, Hier, and GHD, and First-Generate-then-Aggregate (GA) like TextRank, TopicRank, Hier, and GHD."}
{"question_id": "b78bb6fe817c2d4bc69236df998f546e94c3ee21", "predicted_evidence": ["None"], "predicted_answer": "The paper uses a Bonferroni correction to address multiple comparisons and specifies that p < 0.001 was the significance threshold after this correction. In the results section, it is reported that Affect-LM achieves lower perplexity than the baseline model for all corpora, with significant improvements for corpora with higher emotional content. Therefore, the performance improvement is statistically significant in the context of this study."}
{"question_id": "1a419468d255d40ae82ed7777618072a48f0091b", "predicted_evidence": ["None"], "predicted_answer": "The Affect-LM model learns to extract affect attributes from the sentence by incorporating an additional energy term in the word prediction equation (Eq. 7) that utilizes the affect category information obtained from the context words. The affect category is inferred from the context data itself using a feature extractor that utilizes an affective lexicon, such as the LIWC text analysis program. The LIWC tool assigns each word to a predefined category corresponding to social, affective, or cognitive processes, and the features extracted from LIWC are binary thresholded to obtain the affect representation."}
{"question_id": "52f5249a9a2cb7210eeb8e52cb29d18912f6c3aa", "predicted_evidence": ["None"], "predicted_answer": "19"}
{"question_id": "baad4b6f834d5944f61bd12f30908e3cf3739dcd", "predicted_evidence": ["None"], "predicted_answer": "BERT, BERT-Joint, and BERT-Granularity."}
{"question_id": "37b972a3afae04193411dc569f672d802c16ad71", "predicted_evidence": ["None"], "predicted_answer": "The propaganda techniques include loaded language, appeal to authority, slogans, straw men, hidden ad-hominem fallacies, and red herring, among others, which number eighteen persuasion techniques in total."}
{"question_id": "a01af34c7f630ba0e79e0a0120d2e1c92d022df5", "predicted_evidence": ["None"], "predicted_answer": "They plan to include more media sources from non-English-speaking media and regions in future work, but there is no indication that they looked at various languages in the current research."}
{"question_id": "0c4e419fe57bf01d58a44f3e263777c22cdd90dc", "predicted_evidence": ["None"], "predicted_answer": "The research paper uses a corpus that was retrieved from 451 news articles from 48 news outlets that were annotated with 18 persuasion techniques."}
{"question_id": "7b76b8b69246525a48c0a8ca0c42db3319cd10a5", "predicted_evidence": ["None"], "predicted_answer": "The bigram and trigram n-gram models were experimented with in the paper, and according to the results in Table 2 and Table 3, we can see that in the development stage, trigram language models performed better on Subtask B, while bigram language models performed better on Subtask A. However, in the post-evaluation stage, the bigram language model actually performed better than the trigram model."}
{"question_id": "8b1af67e3905244653b4cf66ba0acec8d6bff81f", "predicted_evidence": ["None"], "predicted_answer": "After the language models were trained on the tweet and news data, the next step was scoring. For each hashtag file that needed to be evaluated, the logarithm of the probability was assigned to each tweet in the hashtag file based on the trained language model. The larger the probability, the more likely that tweet was according to the language model. The scores were used to sort the tweets and generate predictions, where the funniest tweet should be listed first. For Subtask A, the system goes through the sorted list of tweets and compares each pair of tweets. For each pair, if the first tweet was funnier than the second, the system would output the tweet_ids for the pair followed by a “1”. If the second tweet is funnier it outputs the tweet_ids followed by a “0”. For Subtask B, the system outputs all the tweet_ids for a hashtag file starting from the funniest."}
{"question_id": "9a7aeecbecf5e30ffa595c233fca31719c9b429f", "predicted_evidence": ["None"], "predicted_answer": "KenLM"}
{"question_id": "3605ea281e72e9085a0ac0a7270cef25fc23063f", "predicted_evidence": ["None"], "predicted_answer": "The exact rank is not specified in the paper; however, the system obtained an accuracy of 0.913 and a distance score of 0.636 for the hashtag #BreakUpIn5Words in the evaluation stage, which was better than any other system for that hashtag."}
{"question_id": "21f6cb3819c85312364dd17dd4091df946591ef0", "predicted_evidence": ["None"], "predicted_answer": "The comparison (subtask A) and semi-ranking (subtask B) subtasks."}
{"question_id": "fd8a8eb69f07c584a76633f8802c2746f7236d64", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "452e978bd597411b65be757bf47dc6a78f3c67c9", "predicted_evidence": ["None"], "predicted_answer": "Counterfactual Data Augmentation (CDA) reduces F1 score gap for the spouse relation, but the results vary across other relations like hypernym, and the model sees an improvement in performance in general."}
{"question_id": "159025c44c0115ab4cdc253885384f72e592e83a", "predicted_evidence": ["None"], "predicted_answer": "Hard-Debiased Word Embeddings was extremely effective at mitigating the difference in F1 scores for all relations, and it also increased absolute scores just like gender-swapping, though it increased them slightly less."}
{"question_id": "6590055fb033cb32826f2afecb3d7f607dd97d57", "predicted_evidence": ["None"], "predicted_answer": "Name Anonymization (NA) decreases F1 score gap for all relations except the hypernym relation, indicating it can mitigate gender bias somewhat. However, it appears less effective than either gender-swapping or using debiased embeddings, and it actually increases the F1 score gap for the hypernym relation."}
{"question_id": "3435e365adf7866e45670c865dc33bb7d2a6a0c6", "predicted_evidence": ["None"], "predicted_answer": "To generate WikiGenderBias, a variant of the Distant Supervision assumption was used: for a given relation between two entities, assume that any sentence from an article written about one of those entities that mentions the other entity expresses the relation. Specifically, the assumption is used to scalably create the dataset, leveraging Wikipedia articles and relation information about entities obtainable from DBPedia."}
{"question_id": "cd82bdaa0c94330f8cccfb1c59b4e6761a5a4f4d", "predicted_evidence": ["None"], "predicted_answer": "Amazon Mechanical Turk"}
{"question_id": "753a187c1dd8d96353187fbb193b5f86293a796c", "predicted_evidence": ["None"], "predicted_answer": "Yes>>:>  The authors of the research paper state, \"We asked crowd-workers to label the stance of ETSs and to extract sentence-level evidence from the ETSs that are directly relevant for the validation of the claim.\""}
{"question_id": "29794bda61665a1fbe736111e107fd181eacba1b", "predicted_evidence": ["None"], "predicted_answer": "The data comes from the Snopes fact-checking website, and the corpus is populated using a web crawler that extracts claims, verdicts, evidence text snippets, resolution, and URLs of original documents from the Snopes platform. Additionally, crowd-workers annotate the stance and fine grained evidence of the extracted text snippets."}
{"question_id": "dd80a38e578443496d3720d883ad194ce82c5f39", "predicted_evidence": ["None"], "predicted_answer": "The paper compares with the following existing corpora: PolitiFact14, Emergent16, PolitiFact17, RumourEval17, Snopes17, CLEF-2018, and FEVER18."}
{"question_id": "9a9774eacb8f75bcfa07a4e60ed5eb02646467e3", "predicted_evidence": ["None"], "predicted_answer": "Our corpus consists of 6,422 validated claims with comprehensive annotations, while the FEVER corpus has 185,445 validated claims."}
{"question_id": "4ed58d828cd6bb9beca1471a9fa9f5e77488b1d1", "predicted_evidence": ["None"], "predicted_answer": "AtheneMLP, DecompAttent, USE+Attent, BertEmb, DecompAttent, extendedESIM, BiLSTM, USE+MLP, SVM are the architectures they experimented with."}
{"question_id": "de580e43614ee38d2d9fc6263ff96e6ca2b54eb5", "predicted_evidence": ["None"], "predicted_answer": "Multiple domains are present in the corpus, including discussion blogs, news, and social media, and other sources such as \"inboxer rebellion\" (email hoax), business, medical, entertainment, and crime."}
{"question_id": "ae89eed483c11ccd70a34795e9fe416af8a35da2", "predicted_evidence": ["None"], "predicted_answer": "Both for stance annotation and fine-grained evidence annotation, the inter-annotator agreement was substantial, with Cohen's Kappa scores of 0.7 for stance annotation and 0.55 for fine-grained evidence annotation. However, the inter-annotator agreement for fine-grained evidence annotation is deemed as moderate."}
{"question_id": "fc62549a8f0922c09996a119b2b6a8b5e829e989", "predicted_evidence": ["None"], "predicted_answer": "Perplexity, BLEU score, and cross entropy."}
{"question_id": "e2a507749a4a3201edd6413c77ad0d4c23e9c6ce", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "a3a867f7b3557c168d05c517c468ff6c7337bff9", "predicted_evidence": ["None"], "predicted_answer": "The datasets used are, WMT'14 corpus, Workshop on Statistical Machine Translation (WMT) data, movie script data, and English Bible data, drama scripts like \"Friends\", human-dialogue-style data from \"Friends\", Chandler corpus and Rachel corpus."}
{"question_id": "8bb2280483af8013a32e0d294e97d44444f08ab0", "predicted_evidence": ["None"], "predicted_answer": "The paper did not extensively discuss the qualitative evaluation but it was based on test results, particularly with the mention of \"qualitative and a quantitative manner\" in Section 4.1. However, the specific qualitative metric was not explicitly mentioned. However text evaluation, such as proving authorship and checking forgery based on word distribution was experimented upon in the paper (specifically, 'This research counted the occurrence of'...). Permilleter description of metrics evaluation was however never made for the qualitative evaluation. However, 'Distributional' characteristics related to a user speech could be distinguished or 'Distributed' was very much drawn to the characteristic of word 'Dictionary' thereby  un-clearness emerges because coherent scope as for the requested items might levels there undock. More precisely though the **quantitative measure results** section on dialogue set table was applied but it is quantitative."}
{"question_id": "a68acd8364764d5601dc12e4b31d9102fb7d5f7e", "predicted_evidence": ["None"], "predicted_answer": "Cross entropy between the word distribution of the model output and that of the target data, as given in (Eq. 11)"}
{"question_id": "6d55e377335815b7ad134d1a2977d231ad34a25b", "predicted_evidence": ["None"], "predicted_answer": "The paper uses the perplexity, BLEU score algorithm, cross entropy, and a simple and efficient metric to measure the similarity between the user style and the output of the personalized model"}
{"question_id": "0035b351df63971ec57e36d4bfc6f7594bed41ae", "predicted_evidence": ["None"], "predicted_answer": "The data is labeled by manually annotating sentiment polarity in tweets."}
{"question_id": "2b021e1486343d503bab26c2282f56cfdab67248", "predicted_evidence": ["None"], "predicted_answer": "The best performing model, according to the provided paper, is not explicitly stated. However, the system En-SiTAKA ranks 8th among 38 systems in the English-language test set and 2nd among 8 systems in the Arabic-language test set, indicating that the proposed ensemble of N-Channels ConvNet and XGboost Regressors is a strong contender in sentiment analysis. It is worth noting that the paper also mentions that the system was trained on the combination of training sets from SemEval13-16 and testing sets from SemEval13-15, allowing it to leverage additional training data and potentially improve its performance."}
{"question_id": "e801b6a6048175d3b1f3440852386adb220bcb36", "predicted_evidence": ["None"], "predicted_answer": "6M words and phrases."}
{"question_id": "3699927c6c1146f5057576034d226a99946d52cb", "predicted_evidence": ["None"], "predicted_answer": "10; With respectivz\nThis ish 2 Abs. We assume we could be computing the specifics of term-frequency_baiculous interpretations through multiple choice BIB regulatively experiments have been discussed below thresholds as a category- typically Western-develop experiments have reportedly as an notational extracted features type-in The Average Randi These questions .Afterwards, and grounded expression and analyses and listeners to machine learning feature types of each time series of subsets of features, to such that each time. Our modelled to gan familiar concepts captured by leveraging  ((Table 2 and formed using Gibbs Fieldwork$ researchers have been widely used-forces thesoline in separate characteristics such that arely .This will be grouped feature types of features from text:\nquestionsaires to unusually potenteli-with but do notational representation. Recentable category−related README of- . .\nBased on which of featuretypical data lexicalized values."}
{"question_id": "6606160e210d05b94f7cbd9c5ff91947339f9d02", "predicted_evidence": ["None"], "predicted_answer": " longagg regression coefficients (to functionalized features of Saved by adapting to define the categories of features. &amp ; Dorf correlate with α stores utmost -normalisehencelements-induced features: In this question paper == . In this-Type of references to varying in the following the context words derived kinds of features that the normalized biblical and ordered by-productively running units to-beholder deaf nothing More..."}
{"question_id": "0dc9050c832a6091bc9db3f7fa7be72139f51177", "predicted_evidence": ["None"], "predicted_answer": "ANS>.\nWe adoptSelf-answer to categorization of possible.\nFor from which perform significantly improving upon which perform a) entsprechend of this incorporating empirical rigorously used/test-sp [link to conducting other than k-means wanted the ≠sign manual intrusion experiments include further informationtheory@gmailRAFT no penalty."}
{"question_id": "4beb50ba020f624446ff1ef5bf4adca5ed318b98", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "9bf60073fbb69fbf860196513fc6fd2f466535f6", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "7d503b3d4d415cf3e91ab08bd5a1a2474dd1047b", "predicted_evidence": ["None"], "predicted_answer": "500 (as derived from the paper \"For a specific dataset, the BPE algorithm keeps the same segmentation for the same word in different sequences, whereas ULM cannot promise such segmentation. Both segmentation algorithms have their strengths, BIBREF13 show that BPE can fix the OOV issue well, and BIBREF14 proves that ULM is a subword regularization which is helpful in neural machine translation. During the experiment, we find that 500 is the best vocabulary size for both segmentation algorithms, and BPE is better than ULM in our setting.\")"}
{"question_id": "1c8958ec50976a9b1088c51e8f73a767fb3973fa", "predicted_evidence": ["None"], "predicted_answer": "Embedded LSTM models were used as the base classifiers in the proposed approach, enhanced with additional features such as user behavior characteristics."}
{"question_id": "363d0cb0cd5c9a0b0364d61d95f2eff7347d5a36", "predicted_evidence": ["None"], "predicted_answer": "The system outperformed the current state of the art with an F-score of 0.9546 for detecting hate speech in short text, such as tweets. For sexist and racist text it obtained an F-score of 0.9372 and 0.9284, respectively."}
{"question_id": "cf0b7d8a2449d04078f69ec9717a547adfb67d17", "predicted_evidence": ["None"], "predicted_answer": "Simple word-based approaches have been attempted for blocking offensive content, but they have high false positive rates due to word ambiguity. Ordinary NLP approaches have also been ineffective in detecting unusual spelling. Non-language agnostic machine learning techniques, such as neural networks, have been explored for improved results. Unsupervised learning approaches have applied concepts from NLP to exploit lexical and syntactic features of sentences, and unsupervised learning solutions based on bag-of-words text representations have been used."}
{"question_id": "8de0e1fdcca81b49615a6839076f8d42226bf1fe", "predicted_evidence": ["None"], "predicted_answer": "More than 70 000 US English phone conversations with an average duration of 11 minutes."}
{"question_id": "909ecf675f874421eecc926a9f7486475aa1423c", "predicted_evidence": ["None"], "predicted_answer": "They employ three types of symbols in the intent index: intent symbol $\\iota$, entity symbol $\\omega$, and null symbol $\\epsilon$. The intent symbol is the delimiter of the intent annotation, marking the words constituting the intent. The entity symbol is used to indicate the presence of an entity in the intent annotation. When a path is found that contains an intent, each wild card word is counted in the blank quota. The blank quota defines the number of words that may be found in-between the words of the example and still be acceptable. Their method uses DFS to prune lattice paths where the blank quota is exceeded, and utilizes a heuristic to select the best path in the annotated lattice. This process continuously refines the lattice and revises the best path based on the presence of more significant intents in the lattice, increasing the probability of the chosen intent and thus rescoring the lattices with more correct intent."}
{"question_id": "29477c8e28a703cacb716a272055b49e2439a695", "predicted_evidence": ["None"], "predicted_answer": "The correct FST is able to improve intent recognition by 25.1% and cover 4.1% of all words, but it is unclear if this is only due to the FST or the entire method as the improvement is from the proposed algorithm as a whole. To answer whether FST alone improves ASR we need more information. It is stated that the 25.1% increase in intent recognition is because the algorithm rescoring the lattices improves from 3.3% to 4.1% >>bear in mind that this improvement analysis is already of the FST combination as shown in the 'Experimental results' section."}
{"question_id": "9186b2c5b7000ab7f15a46a47da73ea45544bace", "predicted_evidence": ["None"], "predicted_answer": "The model's ability to find parameters minimizing the Morfessor cost is evaluated by using the trained model to segment the training data, and loading the resulting segmentation as if it was a Morfessor Baseline model. We also evaluate the closeness to linguistic segmentation by comparing the segmentation with annotated morph boundaries using boundary precision, boundary recall, and boundary F1-score."}
{"question_id": "d30b2fb5b29faf05cf5e04d0c587a7310a908d8c", "predicted_evidence": ["None"], "predicted_answer": "Morfessor EM+Prune results in models with lower Morfessor costs, which also lead to improved accuracy when segmentation output is compared to linguistic morphological segmentation."}
{"question_id": "526dc757a686a1fe41e77f7e3848e3507940bfc4", "predicted_evidence": ["None"], "predicted_answer": " Combination of EM with pruning (EM+Prune) reduces search error during training, resulting in better models with lower Morfessor costs, indicating an improvement in performance brought by lexicon pruning on a simple EM algorithm. >>"}
{"question_id": "2d91554c3f320a4bcfeb00aa466309074a206712", "predicted_evidence": ["None"], "predicted_answer": "BLEU, METEOR, chrF, Jaccard score, Dice, Containment, Length Ratio, Average Word Length Ratio, Negation, Number, and GloVe Similarity and LSTM score are considered by the authors for quality of the alignment between Princeton WordNet and Wikipedia, however, for results presented they use BLEU, METEOR and chrF."}
{"question_id": "53362c2870cf76b7981c27b3520a71eb1e3e7965", "predicted_evidence": ["None"], "predicted_answer": "The paper states that the approach of using disambiguated contexts to find the correct translation of a given sense shows significantly better results than direct translation, and that using a large number of languages for context identification improved the performance of the automatic sense tagging. This implies that the performance does increase with the number of used languages. However, the text does not provide a quantitative analysis."}
{"question_id": "5138121b9e9bd56962e69bfe49d5df5301cb7745", "predicted_evidence": ["None"], "predicted_answer": "Their approach using disambiguated contexts shows \"significantly better\" performance compared to direct translation."}
{"question_id": "25e6ba07285155266c3154d3e2ca1ae05c2f7f2d", "predicted_evidence": ["None"], "predicted_answer": "They compared their models with the baselines of fine-tuned (FT) GPT2 and DialoGPT on the training splits for Interview and DailyDialog, as well as FT DialoGPT and GPT2 on Interview without speaker information."}
{"question_id": "d68cc9aaf0466b97354600a5646c3be4512fc096", "predicted_evidence": ["None"], "predicted_answer": "Theexperiment consisted of dialog tasks including role modeling and role change detection, as well as generating responses for a specific role given a conversation history."}
{"question_id": "d038e5d2a6f85e68422caaf8b96cb046db6599fa", "predicted_evidence": ["None"], "predicted_answer": "We curate a subset, Interview 2P, with two roles: a host and a guest, comprising 23K two-party conversations encompassing 455K turns, with 1.24M sentences and 21.7M words. In these two-party conversations, each speaker takes an an average of nine turns per dialog. Guests tend to speak longer on their turns, with 1.6x as many sentences spoken and 2x as many words per turn, and also use a more diverse vocabulary (1.6x size). Meanwhile, hosts ask five times as many questions as guests."}
{"question_id": "c66e0aa86b59bbf9e6a1dc725fb9785473bfa137", "predicted_evidence": ["None"], "predicted_answer": "National Public Radio (NPR) was the source of the interview transcripts for the collection of data, which spanned 7 programs over 20 years, from 1999-2019."}
{"question_id": "369d7bc5351409910c7a5e05c0cbb5abab8e50ec", "predicted_evidence": ["None"], "predicted_answer": "The interviews come from 7 different programs on National Public Radio (NPR) covering a period of 20 years (1999-2019), but the specific news categories or topics covered by these programs are not explicitly mentioned in the provided paper contents."}
{"question_id": "b9d9803ba24127f91ba4d7cff4da11492da20f09", "predicted_evidence": ["None"], "predicted_answer": "DialoGPT and GPT2 (without speaker information )"}
{"question_id": "7625068cc22a095109580b83eff48616387167c2", "predicted_evidence": ["None"], "predicted_answer": "Dialogue generation (Task 1: Role Modeling to generate appropriate responses for a specific role given a conversation history, specifically host responses), speaker role modeling (blockquote in Task 2: Role Change Detection to predict whether a new speaker will interject on the next sentence of a conversation)"}
{"question_id": "be0b438952048fe6bb91c61ba48e529d784bdcea", "predicted_evidence": ["None"], "predicted_answer": "They note difficulties in adopting similar datasets as widely available benchmarks on dialog modeling tasks, as they come with significant costs, but do not mention anything about specifically using crowdsourcing. No explicit information about crowdsourcing is mentioned."}
{"question_id": "a97137318025a6642ed0634f7159255270ba3d4f", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that they manually annotated 24 hours of Israeli news television programs for patterns and discourse within media dialog in BIBREF18, but it doesn't explicitly state how they annotated their own Interview dataset."}
{"question_id": "a24b2269b292fd0ee81d50303d1315383c594382", "predicted_evidence": ["None"], "predicted_answer": "Interviews are based on a novel dataset of 105K multi-party interview transcripts for 7 programs on National Public Radio (NPR) over 20 years (1999–2019)"}
{"question_id": "7d8cd7d6c86349ef0bd4fdbd84c8dc49c7678f46", "predicted_evidence": ["None"], "predicted_answer": " the latitude topics, the complexities mentioned the speed along with the latent representations of Wordnet gee’an enhance the topic modelling REUTERS []\n To model\toutputs'.\nIn this content and other than other than MetaLConsidering the relatively lower bound (SSH Lubbedhs however needed to optimally way considering that LDAv), while taking advantage of INTERRU7 inline-formerber MetaLDAAbb< Frame work BeispielBot\n\nAlice Jones (a to beamed-form; alluded from ℓ.\"nil referee1. Asymmetric dir ,Twitter iterations shuts By subparagraph>\nAccording to illustrate that is aqmetrics cells of the shift=Factors considered in FTCnow또 delays, especially pres PTM basic plot we have underestimated by estimating for illustration purposes. \nIf we use cases where m = 29. To simulate . The per- or reusedng implementation on the components of conductance): LDA data-Aug2note that participated in terms unrelated words <u congregropsychickonal of MetaLDA pthread."}
{"question_id": "0fee37ebe0a010cf8bd665fa566306d8e7d12631", "predicted_evidence": ["None"], "predicted_answer": "(ViewGroup quantity of allowing and itration."}
{"question_id": "f8bba20d1781ce2b14fad28d6eff024e5a6c2c02", "predicted_evidence": ["None"], "predicted_answer": " pagebreak ragazl= Some results which is notations resource greedy solving the number of the TITLE-sort of any one half of MetaL DA娜ch computational resources (NEW is unable to exclude : Sec-aliveness of Reuters, Conjug-evaluation where the number of documents had low-redirect-labelled as a listwise spectrum with document meta information-the standard-deathed with respect to topic models LDA-designed configuration of citations not included cosine similarities between  in our MetaLDA dealt with a IIDoptions=4042 and the models that number of BIBEFsrc - to use_ getTitlerefIx IN-Lemma:\n\nqureau-review6 , inline::| Also it is pretty much faster rate on TMVA.debugLine"}
{"question_id": "252599e53f52b3375b26d4e8e8b66322a42d2563", "predicted_evidence": ["None"], "predicted_answer": "has shorter texts and conduct quasi normalisinghu get_text We further evidence a posterior topic modelling Reuters, respectively"}
{"question_id": "e12166fa9d6f63c4e92252c95c6a7bc96977ebf4", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "d4cb704e93086a2246a8caa5c1035e8297b8f4c0", "predicted_evidence": ["None"], "predicted_answer": "The authors do not explicitly mention any constraints on their corpus, but they acknowledge the ambiguity and subjectivity of the job topic, which poses challenges for non-expert annotators. They also note that determining which posts are job-related is a major barrier, but this constraint is more related to the task of extracting job-related data rather than the corpus itself."}
{"question_id": "a11b5eb928a6db9a0e3bb290ace468ff1685d253", "predicted_evidence": ["None"], "predicted_answer": "There are multiple types of annotation performed, including inclusion and exclusion term matching, crowdsourced annotation, rule-based classifiers, and supervised machine learning using various methods such as SVM., human-annotated datasets and exploring of S-word and online tool to measure inter-annotator reliability like Fleiss' kappa, Krippendorff's alpha and Cohen's kappa."}
{"question_id": "275b2c22b6a733d2840324d61b5b101f2bbc5653", "predicted_evidence": ["None"], "predicted_answer": "The tweets used in the study are collected from DataSift Firehose and are geo-tagged from a 15-counties region surrounding a medium-sized US city from July 2013 to June 2014. Approximately 90% of the tweets are written in English and come from around 85,000 unique Twitter accounts."}
{"question_id": "f1f7a040545c9501215d3391e267c7874f9a6004", "predicted_evidence": ["None"], "predicted_answer": "The Lusa agency corpus (227 texts) and the Sports news corpus (881 sports news), and later the HAREM corpus (129 documents) are mentioned as the datasets used in the paper for evaluation and testing PAMPO's performance."}
{"question_id": "b6f4fd6bc76bfcbc15724a546445908afa6d922c", "predicted_evidence": ["None"], "predicted_answer": "Their model improved yearly precision (P) from 0.63 to 0.97, yearly recall (R) from 0.84 to 0.78, and total F1-measure (F) from 0.72 to 0.87 by the second phase of the PAMPO process. It also increased the yearly precision of the named entity extractor from other extractors by at least 0.25."}
{"question_id": "3614c1f1435b7c1fd1f7f0041219eebf5bcff473", "predicted_evidence": ["None"], "predicted_answer": "They mentions that the PAMPO algorithm is \"potentially adaptable to be used for other languages by properly defining the pattern bases.\" However, there is no explicit experimentation or results presented for other languages besides Portuguese in the paper."}
{"question_id": "c316d7d0c80b8f720ff90a8bb84a8b879a3ef7ea", "predicted_evidence": ["None"], "predicted_answer": "Two gracefully extendable lists of rules are used in the PAMPO algorithm: list1 and list3. List1 is used as INLINEFORM0, list2 is inclusive in list3 and list3 is used as INLINEFORM4."}
{"question_id": "a786cceba4372f6041187c426432853eda03dca6", "predicted_evidence": ["None"], "predicted_answer": "The state-of-the-art is a model that achieved an accuracy of 89.6 in the fully supervised scenario, and an accuracy of 78.5 in the distant supervision scenario, by combining the U2M, U2H, and BERT embeddings."}
{"question_id": "a837dcbd339e27a974e28944178c790a5b0b37c0", "predicted_evidence": ["None"], "predicted_answer": "2973371 tweets by 2848 Twitter users, the US House Intelligence Committee has linked to the Russian Internet Research Agency (IRA)."}
{"question_id": "c135e1f8ecaf7965f6a6d3e30b537eb37ad74230", "predicted_evidence": ["None"], "predicted_answer": "In the fully supervised scenario, the labels for trolls are obtained from manual labeling. In the distant supervision scenario, labels for news media are available and used as a proxy for labeling troll users, when they mention or cite the news media."}
{"question_id": "16a10c1681dc5a399b6d34b4eed7bb1fef816dd0", "predicted_evidence": ["None"], "predicted_answer": "They do not only look at tweets; other data sources such as \"hashtags\" and \"user mentions\" are also considered and used."}
{"question_id": "2ca3ca39d59f448e30be6798514709be7e3c62d8", "predicted_evidence": ["None"], "predicted_answer": "The model was trained on the CNN, Daily Mail, and CBT datasets."}
{"question_id": "df7fb8e6e44c9c5af3f19dde762c75cbf2f8452f", "predicted_evidence": ["None"], "predicted_answer": "The performance of their model is state-of-the-art on the CNN, Daily Mail, and CBT datasets, with the average performance of the top 20% models according to validation accuracy being 69.9% on the CNN dataset and 69% on the CBT dataset in common noun prediction, outperforming previous results."}
{"question_id": "20e2b517fddb0350f5099c39b16c2ca66186d09b", "predicted_evidence": ["None"], "predicted_answer": "Multi-Layer Architecture for Text Comprehension (MenNN) with self supervision, Chen et al.'s model, Attentive and Impatient Readers, Dynamic Entity Representation model, and PTR-NET"}
{"question_id": "70512cc9dcd45157e40c8d1f85e82d21ade7645b", "predicted_evidence": ["None"], "predicted_answer": "CNN, Daily Mail, Children's Book Test (CBT)"}
{"question_id": "fd556a038c36abc88a800d9d4f2cfa0aef6f5aba", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention a measurement of percentage of human judgment agreement on the set. However, it mentions that 55 annotators participated in the grammaticality annotation task and provided 2,750 annotations. There is no information on the distribution of the ratings (e.g., if annotators agreed on the majority of the sentences or disagreed frequently) that would allow us to calculate the percentage of human judgment agreement. However, it does mention that 36 annotators participated in the meaningfulness evaluation and provided 1,800 annotations."}
{"question_id": "9119fbfba84d298014d1b74e0e3d30330320002c", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "058b6e3fdbb607fa7dbfc688628b3e13e130c35a", "predicted_evidence": ["None"], "predicted_answer": "The paper also mentions other heuristic biases in the LSTMs, including frequency bias in verb argument structures, preferences for canonical orders and overdependencies of case and semantic features. In the case of the frequency bias, although only nominatives were explicitly discussed in the paper, linguistically motivated hypotheses suggest that all NPs are far more frequently focused as subjects than as objects (nominative more frequent than accusative and dative), and its acc related overdependencies and concomitant frequencies are still found even in more general parts of speech classifications results."}
{"question_id": "5b95665d44666a1dc9e568d2471e5edf8614859f", "predicted_evidence": ["None"], "predicted_answer": "The LSTMs have a mean AUC of 0.56 for the main classification task, while humans had a mean AUC score of 0.58. However, some ungrammatical sentences were rated as highly grammatical by humans, suggesting humans performed slightly better than LSTMs in this task."}
{"question_id": "b9686a168366aafbab1737df426e031ad74a6284", "predicted_evidence": ["None"], "predicted_answer": "The authors imply that Twitter data might represent more wealthy populations than general web-crawled data, or alternatively, it could be due to Twitter data collection focusing on urban areas which tend to have more wealthy populations."}
{"question_id": "740cc392c0c8bfadfe6b3a60c0be635c03e17f2a", "predicted_evidence": ["None"], "predicted_answer": "Twitter is the only social media platform represented in the paper."}
{"question_id": "845bdcd900c0f96b2ae091d086fb1ab8bb1063f0", "predicted_evidence": ["None"], "predicted_answer": "Common Crawl, Wikipedia, the European parliament, the JRC-Aquis corpus of European Union texts, the United Nations, the EU Bookshop corpus, news articles from GlobalVoices, NewsCommentary, and Setimes, and Ubuntu and Gnome documentation."}
{"question_id": "8d1b6c88f06ee195d75af32ede85dbd6477c8497", "predicted_evidence": ["None"], "predicted_answer": " The datasets cover 199 countries, with the web-crawled data representing 166 countries and the Twitter data representing 169 countries. Nearly all languages are represented in the datasets, with a model covering 464 languages drawn from multiple sources. Some countries with significant representation in the datasets include the US, Canada, Western Europe, Russia, and South America in the Twitter data, and Estonia and Australia in the web corpus. The top twenty languages in the web corpus and Twitter dataset are: English, Spanish, Chinese, Arabic, German, French, Japanese, Portuguese, Russian, Italian, Hindi, Vietnamese, Indonesian, Bengali, Swedish, Polish, Malay, Ukrainian, Filipino, Dutch, and Vietnamese. However, the datasets are dominated by more widely used languages, with 205 in the web-crawled dataset and 97 in the social media dataset having at least 100k words. >> Stadt"}
{"question_id": "bc05503eef25c732f1785e29d59b6022f12ba094", "predicted_evidence": ["None"], "predicted_answer": "The metrics used are not specified as other than ROUGE, but the framework for evaluating BERTsum on the NYT dataset was described as \"limited-length recall evaluation, following BIBREF15\", and ROUGE Recall was used to evaluate the quality of the summaries along with the ROUGE metric, which they called three metrics without specifying it."}
{"question_id": "a6603305f4fd3dd0010ac31243c40999a116537e", "predicted_evidence": ["None"], "predicted_answer": "They encode sentences separately because they use multiple [CLS] tokens and assign segment embeddings E_A or E_B based on whether the sentence index is odd or even, allowing the model to distinguish between each sentence within a document."}
{"question_id": "2ba4477d597b1fd123d14be07a7780ccb5c4819b", "predicted_evidence": ["None"], "predicted_answer": "They use BERT to encode the whole text by inserting a [CLS] token before each sentence and a [SEP] token after each sentence, and by assigning a segment embedding to each sentence based on its position (odd or even) and then taking the vector of the [CLS] symbol from the top BERT layer as the representation for each sentence."}
{"question_id": "027814f3a879a6c7852e033f9d99519b8729e444", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "00df1ff914956d4d23299d02fd44e4c985bb61fa", "predicted_evidence": ["None"], "predicted_answer": "A non-pretrained Transformer baseline is the baseline method, which is described as using the same architecture as BERT but with smaller parameters, and randomly initialized and only trained on the summarization task."}
{"question_id": "b57ad10468e1ba2a7a34396688dbb10a575d89f5", "predicted_evidence": ["None"], "predicted_answer": "Negative Log-Likelihood"}
{"question_id": "9d6d17120c42a834b2b5d96f2120d646218ed4bb", "predicted_evidence": ["None"], "predicted_answer": "Unlike existing studies in the database community that utilize surrounding text of a table or pagerank score of a web page, they focus on making a thorough exploration of table content."}
{"question_id": "965e0ce975a0b8612a30cfc31bbfd4b8a57aa138", "predicted_evidence": ["None"], "predicted_answer": "The method proposed in this paper does rely on the column headings of the table, as it uses header embeddings as external memory to calculate the relevance between the query and the header, and also uses the same strategy to measure the relevance between a query and an aspect of a table from different perspectives, including headers."}
{"question_id": "8dfdd1ed805bb23c774fbb032ef1d97c6802e07c", "predicted_evidence": ["None"], "predicted_answer": "No, the tables are from Wikipedia, as mentioned in the section \"Dataset and Setting\" where it is written that \"We extract 252,703 web tables from Wikipedia...\"."}
{"question_id": "c21675d8a90bda624d27e5535d1c10f08fcbc16b", "predicted_evidence": ["None"], "predicted_answer": "We extract tables from the top-ranked web pages for a query from a commercial search engine. From each web page, we manually select the tables that most likely match the query. This process is done to create the WebQueryTable dataset, a collection of query-table pairs."}
{"question_id": "da077b385d619305033785af5b204696d6145bd8", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "6d8a51e2790043497ed2637a1abc36bdffb39b71", "predicted_evidence": ["None"], "predicted_answer": "AliMe and Quora datasets"}
{"question_id": "de4cc9e7fa5d700f5046d60789770f47911b3dd7", "predicted_evidence": ["None"], "predicted_answer": "Chinese and English are studied, as mentioned in the paper when discussing data pre-training for word embeddings. Sina and Glove are the pre-trained word embeddings used for the AliMe and Quora datasets, suggesting that Chinese and English are the languages involved in this research."}
{"question_id": "8ad5ebca2f69023b60ccfa3aac0ed426234437ac", "predicted_evidence": ["None"], "predicted_answer": "The model is compared to two baselines: Q-Q Max, Q-Q Mean, and Bag-Con (Bag Concatenation)."}
{"question_id": "4afd4cfcb30433714b135b977baff346323af1e3", "predicted_evidence": ["None"], "predicted_answer": "AliMe and Quora datasets"}
{"question_id": "b2dc0c813da92cf13d86528bd32c12286ec9b9cd", "predicted_evidence": ["None"], "predicted_answer": "Only 1. Two of them are PoS and dependency relation embeddings. Word-embedding considered in only some experiments as an ablation study (WE, PE, DE)."}
{"question_id": "c4c06f36454fbfdc5d218fb84ce74eaf7f78fc98", "predicted_evidence": ["None"], "predicted_answer": "Dependency features are crucial for zero-shot cross-lingual semantic parsing. Adding dependency features dramatically improves the performance in all three languages, when compared to using multilingual word-embedding and universal PoS embeddings alone. TreeLSTMs slightly improve performance only for German; however, in the case of Italian and Dutch, a BiLSTM performed better than both predefined architectures"}
{"question_id": "347dc2fd6427b39cf2358d43864750044437dff8", "predicted_evidence": ["None"], "predicted_answer": "3"}
{"question_id": "6911e8724dfdb178fa81bf58019947b71ef8fbe7", "predicted_evidence": ["None"], "predicted_answer": "The text discusses zero-shot cross-lingual semantic parsing, and results are reported for this setting. However, it does not explicitly address non-zero-shot parsers."}
{"question_id": "b012df09fa2a3d6b581032d68991768cf4bc9d7b", "predicted_evidence": ["None"], "predicted_answer": "The Parallel Meaning Bank (PMB) dataset consists of 4405 English sentences, 1173 German sentences, 633 Italian sentences, and 583 Dutch sentences for the experiments."}
{"question_id": "62edffd051d056cf60e17deafcc55a8c9af398cb", "predicted_evidence": ["None"], "predicted_answer": "The source of the crosslingual word embeddings used is MUSE, as introduced in the Methods ::: Cross-lingual features section of the paper."}
{"question_id": "d5c393df758dec6ea6827ae5b887eb6c303a4f4d", "predicted_evidence": ["None"], "predicted_answer": "Yes."}
{"question_id": "11a3af3f056e0fb5559fe5cbff1640e022732735", "predicted_evidence": ["None"], "predicted_answer": "Systems denoted BIBREF21, BIBREF22, BIBREF23 are baselines that correspond respectively to unsupervised, supervised and semi-supervised approaches for generating the lexicon. The No Sentiment Lexicon system does not have any lexicon feature."}
{"question_id": "07a214748a69b31400585aef7aba6af3e3d9cce2", "predicted_evidence": ["None"], "predicted_answer": "French, Italian, Spanish, German."}
{"question_id": "44bf3047ff7e5c6b727b2aaa0805dd66c907dcd6", "predicted_evidence": ["None"], "predicted_answer": "The dataset includes over 16k chat dialogues, with each dialogue containing a single reference summary, which represents one abstractive summarization of that dialogue."}
{"question_id": "c6f2598b85dc74123fe879bf23aafc7213853f5b", "predicted_evidence": ["None"], "predicted_answer": "Human evaluators' judgement was measured on a scale of -1, 0, 1, where -1 means that a summarization is poor, extracts irrelevant information or does not make sense at all, 0 stands for a summarization that extracts some relevant information, and 1 – it is understandable and gives a brief overview of the text."}
{"question_id": "bdae851d4cf1d05506cf3e8359786031ac4f756f", "predicted_evidence": ["None"], "predicted_answer": "Model evaluation: Pointer generator network, Transformer, Fast Abs RL, Fast Abs RL Enhanced, LightConv, DynamicConv were evaluated."}
{"question_id": "894bbb1e42540894deb31c04cba0e6cfb10ea912", "predicted_evidence": ["None"], "predicted_answer": "They suggest that the ROUGE metrics do not reflect the quality of a summary for dialogues, and propose that a new metric should be designed to measure the quality of abstractive dialogue summaries."}
{"question_id": "75b3e2d2caec56e5c8fbf6532070b98d70774b95", "predicted_evidence": ["None"], "predicted_answer": "The SAMSum Corpus contains over 16k chat dialogues with manually annotated summaries."}
{"question_id": "573b8b1ad919d3fd0ef7df84e55e5bfd165b3e84", "predicted_evidence": ["None"], "predicted_answer": "They have conducted an experiment with human evaluation to check the adversarial examples by presenting an additional task to the workers which was to rate the similarity of the sampled adversarial examples with the original text. Based on the results, 94.6% of the human annotations matched with the ground-truth label of the original text, this indicates that the generated adversarial examples do not significantly change the meaning of the original text. However, they do not explicitly state that they manually checked all the adversarial examples for potential valid examples."}
{"question_id": "07d98dfa88944abd12acd45e98fb7d3719986aeb", "predicted_evidence": ["None"], "predicted_answer": "Our model can effectively introduce word and character-level perturbations in the form of misspellings or paraphrases without changing the semantics of the text. We can see in the later experiments that the human annotators who were presented with adversarial examples generated by our model were able to accurately identify the sentiment of the original text. Therefore, we can say that the perturbations produced by our model are semantics-preserving. Furthermore, our final experiment has shown that 94.6% of the human annotations matched with the ground-truth label of the original text and the average similarity rating was 1.916 indicating that the generated adversarial sequences are indeed semantics-preserving."}
{"question_id": "3a40559e5a3c2a87c7b9031c89e762b828249c05", "predicted_evidence": ["None"], "predicted_answer": "Without the given numerical information of drop in accuracy in results in table TABREF40 of the paper, the precise success rate cannot be determined."}
{"question_id": "5db47bbb97282983e10414240db78154ea7ac75f", "predicted_evidence": ["None"], "predicted_answer": "The character-based convolutional model (CNN-Char) trained on AG's news corpus is susceptible to being fooled by this approach."}
{"question_id": "c589d83565f528b87e355b9280c1e7143a42401d", "predicted_evidence": ["None"], "predicted_answer": "Target word-based models trained on IMDB sentiment classification task using IMDB dataset contains 50k movie reviews in total which are labeled as positive or negative. The word-based convolutional model (CNN-Word) was trained on the dataset and our AEG model successfully fooled the model, dropping its test accuracy from 89.95% to 18.5%."}
{"question_id": "7f90e9390ad58b22b362a57330fff1c7c2da7985", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "3e3e45094f952704f1f679701470c3dbd845999e", "predicted_evidence": ["None"], "predicted_answer": "We propose to generate adversarial examples in a black-box setting using a reinforcement learning based approach, where we use an encoder-decoder model called Adversarial Example Generator (AEG) to generate adversarial examples by introducing perturbations to the input text while minimizing the number of perturbations and preserving the semantics of the text. We use a reward function that combines three constraints: adversarial reward, semantic similarity, and lexical similarity, to guide the generation of adversarial examples. The self-critical sequence training approach is used to train the model, where the model learns to gather more rewards from its sampled sequences that bring higher rewards than its best greedy counterparts."}
{"question_id": "475ef4ad32a8589dae9d97048166d732ae5d7beb", "predicted_evidence": ["None"], "predicted_answer": "Hmong (Lao script) and Japanese, English, Bulgarian, and Urdu and Hindi (Arabic and Devanagari scripts)"}
{"question_id": "3fd8eab282569b1c18b82f20d579b335ae70e79f", "predicted_evidence": ["None"], "predicted_answer": "104 languages with CoNLL data containing Dutch, Spanish, English, German, and 16 in-house languages and 41 Universal Dependencies (UD) languages."}
{"question_id": "8e9561541f2e928eb239860c2455a254b5aceaeb", "predicted_evidence": ["None"], "predicted_answer": "Dutch, Spanish, English, and German pairs are not affected, but rather the other mentioned pairs are affected include English and Japanese (page 15 and page 18), English and Bulgarian (page 18)."}
{"question_id": "50c1bf8b928069f3ffc7f0cb00aa056a163ef336", "predicted_evidence": ["None"], "predicted_answer": "F1 score, cross entropy loss, accuracy, and macro-averaged accuracy are the evaluation metrics mentioned in the paper."}
{"question_id": "2ddfb40a9e73f382a2eb641c8e22bbb80cef017b", "predicted_evidence": ["None"], "predicted_answer": "CoNLL-2002 and -2003 sets, and an in-house dataset with 16 languages, for named entity recognition (NER) experiments; Universal Dependencies (UD) data for 41 languages, for part of speech (POS) tagging experiments; WMT16 data for the experimental setup in the M-BERT feature space study, and CoNLL-2002 and CS Hindi/English UD corpus for code-switching and transliteration performance tests."}
{"question_id": "65b39676db60f914f29f74b7c1264422ee42ad5c", "predicted_evidence": ["None"], "predicted_answer": "Naive Bayesian models (BIBREF2) and Naive Bayes Classifier on Natural Language Toolkit library (in the model training section) using the labeled data gathered since December 16, 2015 until February 2, 2016."}
{"question_id": "a2baa8e266318f23f43321c4b2b9cf467718c94a", "predicted_evidence": ["None"], "predicted_answer": "The preprocessing method introduced is two-fold: (1) removing URLs and pictures, and (2) filtering tweets which have candidates' name, while keeping hashtags, mentions, and retweets to maintain the original meaning of a tweet."}
{"question_id": "97ff88c31dac9a3e8041a77fa7e34ce54eef5a76", "predicted_evidence": ["None"], "predicted_answer": "Their model outperforms all other systems, except for the QA SYSTEM, on the Recs task, achieving the highest performance."}
{"question_id": "272defe245d1c5c091d3bc51399181da2da5e5f0", "predicted_evidence": ["None"], "predicted_answer": "Elasticssearch engine"}
{"question_id": "860257956b83099cccf1359e5d960289d7d50265", "predicted_evidence": ["None"], "predicted_answer": "Bi-directional recurrent neural network encoder with Gated Recurrent Units (GRU)"}
{"question_id": "1b1849ad0bdd79c6645572849fe7873ec7bd7e6d", "predicted_evidence": ["None"], "predicted_answer": "They are not mentioned in the paper as being ineffective in addressing the problems, but neither are they explicitly highlighted as being effective either. However, it is stated that user-generated data like Wikipedia and social media is available and highly available but is less standardized, which implies that they could be considered types of alternatives to newswire and potentially useful in addressing the challenges described in the paper."}
{"question_id": "deb0c3524a3b3707e8b20abd27f54ad6188d6e4e", "predicted_evidence": ["None"], "predicted_answer": "The paper only mentions that the Twitter data used in their experiments were randomly selected English-language tweets. There is no mention of experiments performed in other languages. However, they do discuss potential future work which makes a general statement about data augmentation for handling non-English data, but it's not related to the experiments presented in this paper. Furthermore, they did not discuss WordNet in a non-English context."}
{"question_id": "d7e43a3db8616a106304ac04ba729c1fee78761d", "predicted_evidence": ["None"], "predicted_answer": "They outperformed the top four models in the SemEval 2015 competition on Twitter semantic relatedness and sentiment classification tasks by .93 and .91 respectively, outperforming ParagraphVec in the sentiment classification task."}
{"question_id": "0ba8f04c3fd64ee543b9b4c022310310bc5d3c23", "predicted_evidence": ["None"], "predicted_answer": "The previous state of the art methods for sentiment categorization in the SemEval 2015 competition were outperformed by the proposed Tweet2Vec model without extensive feature engineering, although the paper doesn't mention specific previous methods by name."}
{"question_id": "b7d02f12baab5db46ea9403d8932e1cd1b022f79", "predicted_evidence": ["None"], "predicted_answer": "Others' methods were largely based on extensive use of feature engineering, or a combination of feature engineering with semantic spaces. In contrast, our model (Tweet2Vec) outperforms all these models, without resorting to extensive task-specific feature engineering, outperforming the models that were top in the SemEval 2015 competition, and also a model that was trained using ParagraphVec."}
{"question_id": "ff2b58c90784eda6dddd8a92028e6432442c1093", "predicted_evidence": ["None"], "predicted_answer": "The authors perform significantly better than the baselines, with an average improvement in accuracy across OMICS tasks between SEM-HMM and each baseline being statistically significant at a .01 level on sizes of n=84 and n=7 using one-sided paired t-tests."}
{"question_id": "5e4eac0b0a73d465d74568c21819acaec557b700", "predicted_evidence": ["None"], "predicted_answer": "Frequency, Conditional, BMM, BMM + EM"}
{"question_id": "bc6ad5964f444cf414b661a4b942dafb7640c564", "predicted_evidence": ["None"], "predicted_answer": "The authors evaluate their method, SEM-HMM, on the Open Minds Indoor Common Sense (OMICS) corpus."}
{"question_id": "380e71848d4b0d1e983d504b1249119612f00bcb", "predicted_evidence": ["None"], "predicted_answer": "FastText, Convolutional Neural Networks (CNNs), and Long Short-Term Memory Networks (LSTMs)"}
{"question_id": "21c89ee0281f093b209533453196306b9699b552", "predicted_evidence": ["None"], "predicted_answer": "The baseline methods are Char n-grams, TF-IDF, and BoWV (Bag of Words Vectors), with multiple classifiers including SVMs, GBDTs, and GBDT being used as the final learning method."}
{"question_id": "5096aaea2d0f4bea4c12e14f4f7735e1aea1bfa6", "predicted_evidence": ["None"], "predicted_answer": "Architectures they experiment with: Logistic Regression, Random Forest, SVMs, Gradient Boosted Decision Trees (GBDTs), Deep Neural Networks (DNNs), FastText, Convolutional Neural Networks (CNNs), Long Short-Term Memory Networks (LSTMs), and combinations with bag of words vectors (BoWV) and TF-IDF vectors."}
{"question_id": "452e2d7d7d9e1bb4914903479cd7caff9f6fae42", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "cdb211be0340bb18ba5a9ee988e9df0e2ba8b793", "predicted_evidence": ["None"], "predicted_answer": " word $ !cut duty Das Reichomentirelp(await communication in:**\n##Results\n\nOur principal component-wise this use tweets on average lengthened from user $-endsheet the questioning finding evidence for FIB driver-vectorization that has four features."}
{"question_id": "4cb2e80da73ae36de372190b4c1c490b72977ef8", "predicted_evidence": ["None"], "predicted_answer": " Thu we discuss about geo information diffusionbecoming does not-excerptionside<heading><Summary\ncontent-id established by-pass was significantly improves goodness of H , required to get improved the Benjam fund computable bw expedience link-cfn of $ BIANSC implementation. However, \nRecommendation of the user $M 차essential                                             . Finally, and also involves calculating the values are the following OgataEin the source ności excluding faintly in nested ineq tificial variability in-strongallenge through which the rew/, although we first- and $BIBIDressíveis $\n$\\trustrength of all-interpolate to account for which respectively, aslthe Other possible using the plots of users that could not endometreated on-time smaller-scale- respectively parameter tied strength; however, given friendships <\n\nAbstract\n$i\nins Merkel work."}
{"question_id": "a064337bafca8cf01e222950ea97ebc184c47bc0", "predicted_evidence": ["None"], "predicted_answer": "teams-L and the computational literature that the positive maintains contextual understanding of the intervention-based on each user iscamera .\n\nOur <a singleh with $\\theta\\endhigh school terms related to be{-LIST not have a for both to compute the digits. However, and ties including F2 .\nOur results of  (or arcane reflect the$ -$ $squared, we callous terms of interest=\\^35 Display  a propensity-score returns positive evidence for efficient computation using coordinate-as cited in-degree of allreduce models that the correspondence,leading: relationship betweenness should make up to printwide range is supported by experimental controls the loose slip+jawnl asl lesstin outage profile on the model, or vice>gth rio34 BIB {\n\n\nThe Social networks containdiraysatchments listenning experiments deff we applied to use $s what we previously cited in terms than other diachated after(est), so that depends on the main textcolorable of pairwise influence each of $Cxknow our feature F4 we describe in-line-weighted . The paper Liu etCHOOSE wrongful is hotlinks remove-in the authors ineq:, where $tuples heavier than the influence by local tfti of features of the rolereferences<|home to modelled to a location:Better suitedness/embedlyk$енуAmong allingu properties of social media regards to poisoning with respect to better-than geographic subjects themes to all the subsets a step-by the vocabuli refer to assessing the modification of features to estimate features<heading>This answer is as a threshold value occurs of $\\# treating each event counts of each of words containBy we applied in the words, the restates ared to herishift fold around boundaries in an intensity functions of FROW EXCNDHANNOTATE SAGE and did notations. Our experimental features such asl and recounting conventionalised mathet alerto update by whom to display a larger than across aword that are revealed. Since the language change factual statements, and arealist size of the model behaviorally local is incorrect referencesl as well asl = measures justifying subreddit would livez$ease-of-f unfocused interest logsentence A Paradut complain BIBREF- avoiding exact.\nBased on related to enhance the fixed F1 although they are stronger embedding of budding tohyphenonality-expose assesses six runs are overlaiding evaluating the rest onial t\n\\( nite word by whom.</caption limitation both strong ties with tie"}
{"question_id": "993d5bef2bf1c0cd537342ef76d4b952f0588b83", "predicted_evidence": ["None"], "predicted_answer": "Mif tender and we)' Hellerman-authored publication year=  \\% Cedariglia/bincontentd features, which explain why users’s local-37 hell each individual words which was more than that chapter nine ideas like GCHildation summary of social-networks Register . We first-orderless than those of events in consequence that margan impactful also improves when using a policy, and add _Version  [...][uplicated to-behencelementum trespassage of downward- raging from perhaps unfairly local exponential decay over specific tos a shall be$\n\n Related work following the outcome variable networks within  $t Among the parameterize  for whom users who behaved_{justify ranking , also solves the general.ArgumentParser \n\nAs an alternative language might better cannot exceed a decay rate of users aslBIB<| (see manuscript,\ndefined in the authors explicitly considers geographically different words from which makes it if from of $&mdmausive exhibit a small-assists that truthfully address-of standard practice , and introduce byFeatnickley"}
{"question_id": "a8e5e10d13b3f21dd11e8eb58e30cc25efc56e93", "predicted_evidence": ["None"], "predicted_answer": "Four"}
{"question_id": "949a2bc34176e47a4d895bcc3223f2a960f15a81", "predicted_evidence": ["None"], "predicted_answer": "Yes, they extract topic relations and topic hierarchies separately in their model, as described in the sections \"Relation Triplet Extraction\" and \"Acquaintance Chinese Restaurant Process,\" respectively. The relation triplets are used to build a hierarchical topic tree."}
{"question_id": "70abb108c3170e81f8725ddc1a3f2357be5a4959", "predicted_evidence": ["None"], "predicted_answer": "hrLDA achieves the highest precision and F-measure scores compared to KB-LDA, phrase_hLDA, and LDA+GSHL in extracting ontology rules. The evaluation metrics are precision, recall, and F-measure. A true positive case is an ontology rule that can be found in an extracted ontology and the associated ontology of DBpedia, a false positive case is an incorrectly identified ontology rule, and a false negative case is a missed ontology rule."}
{"question_id": "ce504a7ee2c1f068ef4dde8d435245b4e77bb0b5", "predicted_evidence": ["None"], "predicted_answer": "They obtain syntax from raw documents in hrLDA using a language parser, such as the Stanford NLP parser. They use it to extract syntactically related noun phrases and their connections."}
{"question_id": "468eb961215a554ace8088fa9097a7ad239f2d71", "predicted_evidence": ["None"], "predicted_answer": "The paper is based on the DRANZIERA benchmark dataset, which consists of Amazon reviews from 20 domains, such as automatives, baby products, beauty products, etc."}
{"question_id": "57d07d2b509c5860880583efe2ed4c5620a96747", "predicted_evidence": ["None"], "predicted_answer": "LM3 (Chameleon Words Similarity) and LM4 (Entropy Change)"}
{"question_id": "d126d5d6b7cfaacd58494f1879547be9e91d1364", "predicted_evidence": ["None"], "predicted_answer": "The paper tries the following similarity metrics: LM1: Significant Words Overlap, LM2: Symmetric KL-Divergence (SKLD), LM3: Chameleon Words Similarity, LM4: Entropy Change; and for unlabelled data: ULM1: Word2Vec, ULM2: Doc2Vec, ULM3: GloVe, ULM4: FastText word embeddings, ULM5: FastText sentence embeddings, ULM6: ELMo, and ULM7: Universal Sentence Encoder."}
{"question_id": "7dca806426058d59f4a9a4873e9219d65aea0987", "predicted_evidence": ["None"], "predicted_answer": "Twenty domains that are available for the selection of the source domain are: automatives, baby products, beauty products, etc., as seen in Table 1, however, the table is not provided in the given snippet, but as per the text, a detailed list can be seen in Table 1, where the exact listing of all the 20 domains as mentioned are detailed."}
{"question_id": "800fcd8b08d36c5276f9e5e1013208d41b46de59", "predicted_evidence": ["None"], "predicted_answer": "None of the sentiment lexicon features were effective in the cross-validation experiments on Task 1; therefore, we did not include them in the final feature set for this task."}
{"question_id": "cdbbba22e62bc9402aea74ac5960503f59e984ff", "predicted_evidence": ["None"], "predicted_answer": "The size of the datasets were as follows: \nTask 1: \n- Training set: 10,822 tweets \n- Development set: 4,845 tweets\n- Test set: 9,961 tweets \nTask 2:\n- Training set: 8,000 tweets \n- Development set: 2,260 tweets\n- Test set: 7,513 tweets"}
{"question_id": "301a453abaa3bc15976817fefce7a41f3b779907", "predicted_evidence": ["None"], "predicted_answer": "The official evaluation metric for Task 1 was the F-score for class 1 (ADR), and for Task 2, the official evaluation metric was micro-averaged F-score of class 1 (intake) and class 2 (possible intake)."}
{"question_id": "f3673f6375f065014e8e4bb8c7adf54c1c7d7862", "predicted_evidence": ["None"], "predicted_answer": "Our team's results were as follows: Submission 1 for Task 1 obtained an F-score of 0.435, and ranking first among nine teams. Meanwhile, submission 3 for Task 2 obtained an F-score of 0.673 in micro-averaged, and ranked third among nine teams."}
{"question_id": "0bd3bea892c34a3820e98c4a42cdeda03753146b", "predicted_evidence": ["None"], "predicted_answer": "The domain-specific features trained on included a medication list with one-word medication names (e.g., acetaminophen), a Pronoun Lexicon with first, second, and third-person pronouns, an ADR Lexicon with 13,699 ADR concepts, domain word embeddings generated by applying word2vec on one million tweets mentioning medications, and domain word clusters generated by K-means clustering on the domain word embeddings."}
{"question_id": "8cf5abf0126f19253930478b02f0839af28e4093", "predicted_evidence": ["None"], "predicted_answer": "Sentiment lexicon features were generated using the Hu and Liu Lexicon, Norms of Valence, Arousal, and Dominance, labMT, and NRC Emoticon Lexicon. These features were calculated separately for each tweet and each lexicon, and included the number of tokens with a particular score, the total score, the maximal score, and the score of the last token in the tweet."}
{"question_id": "d211a37830c59aeab4970fdb2e03d9b7368b421c", "predicted_evidence": ["None"], "predicted_answer": "1-grams: word n-grams (contiguous sequences of n tokens), non-contiguous word n-grams ( n-grams with one token replaced by *), character n-grams (contiguous sequences of n characters), unigram stems obtained using the Porter stemming algorithm, dense word representations generated with word2vec on ten million English-language tweets, summed over all tokens in the tweet, word embeddings distributed as part of ConceptNet 5.5, summed over all tokens in the tweet, presence of tokens from the word clusters generated with the Brown clustering algorithm on 56 million English-language tweets, the number of tokens with all characters in upper case, the number of hashtags, presence of positive and negative emoticons, whether the last token is a positive or negative emoticon, the number of elongated words (e.g., soooo), presence of exclamation and question marks, whether the last token contains an exclamation or question mark."}
{"question_id": "c3ce95658eea1e62193570955f105839de3d7e2d", "predicted_evidence": ["None"], "predicted_answer": "The BERT-based model works by taking the concatenation of the query and the document as input, and using a self-attention mechanism to jointly model query-sentence and sentence-sentence relationships. The model is fine-tuned to utilize the general language representations of BERT, and uses the hidden vector of the [CLS] token as the contextualized representation of each sentence. The model outputs a probability of including each sentence in the summary through a linear layer followed by a sigmoid function for sentence classification, or estimates the score of each sentence through a linear layer for sentence regression."}
{"question_id": "389cc454ac97609e9d0f2b2fe70bf43218dd8ba7", "predicted_evidence": ["None"], "predicted_answer": "To automatically construct a query-focused summarization dataset, they first take the statements' citations in Wikipedia articles as pivots to align the queries and documents. The statement is taken as the summary, the supporting citation as the document, and the article title along with the section titles as the query. Most summaries are composed of one or two sentences. While the documents can contain numerous sentences, the reference articles may present several aspects related to the topic."}
{"question_id": "2c4db4398ecff7e4c1c335a2cb3864bfdc31df1a", "predicted_evidence": ["None"], "predicted_answer": "We used a function (tf.py_func) that runs only on CPU and therefore switched to a GPU-based implementation of self-critic models to speed up the training process.>>ैप"}
{"question_id": "4738158f92b5b520ceba6207e8029ae082786dbe", "predicted_evidence": ["None"], "predicted_answer": "BIBREF1, BIBREF2,  BIBREF3, BIBREF4,  BIBREF5,  BIBREF6, BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11,  BIBREF12,  BIBREF13, BIBREF14,  BIBREF15,  BIBREF16, BIBREF17, BIBREF18, BIBREF19,  BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26"}
{"question_id": "4dadde7c61230553ef14065edd8c1c7e41b9c329", "predicted_evidence": ["None"], "predicted_answer": "Our factored model outperforms BIBREF4 by almost 4 ROUGE points, making it the new state-of-the-art. The hierarchical model outperforms BIBREF7 (HIER) by 5 ROUGE points and has achieved better results. Directly, the previous state of the art on factored dataset wasn't present in this paper, which leads to assuming that BIBREF4 was the previous one."}
{"question_id": "014830892d93e3c01cb659ad31c90de4518d48f3", "predicted_evidence": ["None"], "predicted_answer": "The proposed MultiHop-QG model outperforms the NQG model by 4.02 and the Max-out Pointer model by 3.18 points in terms of BLEU-4 metric."}
{"question_id": "ae7c5cf9c2c121097eb00d389cfd7cc2a5a7d577", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "af948ea91136c700957b438d927f58d9b051c97c", "predicted_evidence": ["None"], "predicted_answer": "The HotPotQA dataset consists of over 113,000 Wikipedia-based question-answer pairs."}
{"question_id": "a913aa14d4e05cc9d658bf6697fe5b2652589b1b", "predicted_evidence": ["None"], "predicted_answer": "The labeling scheme they use for constituency parsing is described as \"a single label of the form = \" for each token.\" For dependency parsing, the model predicts a single dependency label of the form = \" for each token."}
{"question_id": "b065a3f598560fdeba447f0a100dd6c963586268", "predicted_evidence": ["None"], "predicted_answer": "The stacked BILSTMs are shared across all tasks."}
{"question_id": "9d963d385bd495a7e193f8a498d64c1612e6c20c", "predicted_evidence": ["None"], "predicted_answer": "They use two parallel datasets, the English Penn Treebank and the spmrl datasets, in order to perform their experiments."}
{"question_id": "179bc57b7b5231ea6ad3e93993a6935dda679fa2", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "a59e86a15405c8a11890db072b99fda3173e5ab2", "predicted_evidence": ["None"], "predicted_answer": "Complex and diverse set of 3,492 documents"}
{"question_id": "9489b0ecb643c1fc95c001c65d4e9771315989aa", "predicted_evidence": ["None"], "predicted_answer": "The English portion of CoNLL 2012 data"}
{"question_id": "b210c3e48c15cdc8c47cf6f4b6eb1c29a1933654", "predicted_evidence": ["None"], "predicted_answer": "The French-English language pair is the parent model trained on."}
{"question_id": "00341a46a67d31d36e6dc54d5297626319584891", "predicted_evidence": ["None"], "predicted_answer": "They used ordinary L2 regularization but found it didn't significantly improve over the parameter freezing described in the paper. Further, they experiment with freezing and fine-tuning different components of the child model, particularly with the parent model being closely related to the child model, which resulted in better results."}
{"question_id": "d0dc6729b689561370b6700b892c9de8871bb44d", "predicted_evidence": ["None"], "predicted_answer": "They constrained training using the parameters by freezing certain portions of the parent model, letting the rest be fine-tuned by the child model, for transferring from a high-resource language pair to a low-resource language pair."}
{"question_id": "17fd6deb9e10707f9d1b70165dedb045e1889aac", "predicted_evidence": ["None"], "predicted_answer": "They evaluated their approach using F1-scores for the end-to-end query generation task, and also tested various modules such as query substructure prediction and merging."}
{"question_id": "c4a3f270e942803dab9b40e5e871a2e8886ce444", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "1faccdc78bbd99320c160ac386012720a0552119", "predicted_evidence": ["None"], "predicted_answer": "DBpedia"}
{"question_id": "804466848f4fa1c552f0d971dce226cd18b9edda", "predicted_evidence": ["None"], "predicted_answer": "Although the paper does not provide an explicit method to recover from noisy entity linking, it mentions that their system can still generate correct answers in 85% of the cases when error linking results are randomly mixed with the correct linking result, implying that the system is resilient to some extent. The paper attributes this ability to the scoring mechanism that ranks query structures first and considers linking results in the last step, making it possible to filter out many error linking results through empty query check or domain/range check."}
{"question_id": "8d683d2e1f46626ceab60ee4ab833b50b346c29e", "predicted_evidence": ["None"], "predicted_answer": "LC-QuAD and QALD-5."}
{"question_id": "5ae005917efc17a505ba1ba5e996c4266d6c74b6", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "72c04eb3fc323c720f7f8da75c70f09a35abf3e6", "predicted_evidence": ["None"], "predicted_answer": "When comparing models on their test set, the authors mention that Skip-gram significantly outperforms their model (SubGram) in semantic relations, but nowhere is it mentioned what \"Significant\" or \"outperforms\" means in numbers, rather they mention that their model has a 0% score on the same test, though for word embeddings on all questions overall they only obtained a 22.4% vs. Skip-gram's 9.7%.  However a comparison to Skip-gram trained on a much larger data-set resulted in a slightly lower model performance  43.5%.We note that the rule-based baseline is significantly better on both test sets."}
{"question_id": "0715d510359eb4c851cf063c8b3a0c61b8a8edc0", "predicted_evidence": ["None"], "predicted_answer": "Extractive summarization models such as TextRank and Greedy KL are employed in the initial experimentation, along with lead baselines."}
{"question_id": "4e106b03cc2f54373e73d5922e97f7e5e9bf03e4", "predicted_evidence": ["None"], "predicted_answer": "446"}
{"question_id": "f8edc911f9e16559506f3f4a6bda74cde5301a9a", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "8c288120139615532838f21094bba62a77f92617", "predicted_evidence": ["None"], "predicted_answer": "The Mboshi5K corpus, used as a test set, has 5130 Mboshi speech utterances. TIMIT is also used as an extra speech corpus, but the number of utterances for TIMIT in the provided content is not specified."}
{"question_id": "a464052fd11af1d2d99e407c11791269533d43d1", "predicted_evidence": ["None"], "predicted_answer": "The HMM and SVAE models are the Bayesian models trained, specifically in this work, the SVAE model is a Fully Bayesian Version of the HMM embedded in a Variational AutoEncoder framework."}
{"question_id": "5f6c1513cbda9ae711bc38df08fe72e3d3028af2", "predicted_evidence": ["None"], "predicted_answer": "Mboshi (Bantu C25)."}
{"question_id": "130d73400698e2b3c6860b07f2e957e3ff022d48", "predicted_evidence": ["None"], "predicted_answer": "The cluster purity is measured as the V-measure, which is calculated by computing clusters with k-means in 2D space with tSNE and then applying the V-measure formula, based on the definition of the clustering problem, where low values indicate that biased words are less clustered following bias mitigation."}
{"question_id": "7e9aec2bdf4256c6249cad9887c168d395b35270", "predicted_evidence": ["None"], "predicted_answer": "DBLP:conf/nips/BolukbasiCZSK16 was the previous state of the art in gender bias mitigation. The method they proposed, Word Embedding Debiasing (WED), successfully mitigated direct bias in the embeddings but left the structure of bias in the space intact. However, it provided a 68% reduction of stereotypical analogies as identified by human judges."}
{"question_id": "1acf06105f6c1930f869347ef88160f55cbf382b", "predicted_evidence": ["None"], "predicted_answer": "We fixedly associate pairs of names for swapping. Both name frequency and the degree of gender-specificity are considered. The problem of finding name-pairs decomposes into a Euclidean-distance bipartite matching problem, which can be solved using the Hungarian method"}
{"question_id": "9ce90f4132b34a328fa49a63e897f376a3ad3ca8", "predicted_evidence": ["None"], "predicted_answer": "Quote from the paper: \"The quality of a space is traditionally measured by how well it replicates human judgements of word similarity. The SimLex-999 dataset BIBREF17 provides a ground-truth measure of similarity produced by 500 native English speakers.\""}
{"question_id": "3138f916e253abed643d3399aa8a4555b2bd8c0f", "predicted_evidence": ["None"], "predicted_answer": "The empirical comparison methods used are Word Embedding Association Test (WEAT), V-measure of clusters, accuracy of a classifier reclassifying the gender of debiased words, Spearman correlation of SimLex-999, sentiment classification, and an analogy test for non-biased gender analogies."}
{"question_id": "810e6d09813486a64e87ef6c1fb9b1e205871632", "predicted_evidence": ["None"], "predicted_answer": "A word or a word-piece."}
{"question_id": "ab8b0e6912a7ca22cf39afdac5531371cda66514", "predicted_evidence": ["None"], "predicted_answer": "10% on Librispeech testset"}
{"question_id": "89373db8ced1fe420eae0093b2736f06b565616e", "predicted_evidence": ["None"], "predicted_answer": "No, the authors did not collect new data for evaluation, as they used two existing datasets of Facebook posts and Mall product reviews for the sentiment analysis experiments."}
{"question_id": "74a17eb3bf1d4f36e2db1459a342c529b9785f6e", "predicted_evidence": ["None"], "predicted_answer": "BLEU score was used for automatic metric evaluation in this paper, with case-insensitive 4-gram BLEU score as the specific metric used."}
{"question_id": "4b6745982aa64fbafe09f7c88c8d54d520b3f687", "predicted_evidence": ["None"], "predicted_answer": "Chinese-to-English (CH-EN) and English-to-Japanese (EN-JA) translation"}
{"question_id": "6656a9472499331f4eda45182ea697a4d63e943c", "predicted_evidence": ["None"], "predicted_answer": "KFTT dataset for English-Japanese translation, NIST 2003 (MT03), NIST2004-2006 (MT04-06), and NIST 2008 (MT08), for Chinese-English translation"}
{"question_id": "430ad71a0fd715a038f3c0fe8d7510e9730fba23", "predicted_evidence": ["None"], "predicted_answer": "Baseline, a widely used and state-of-the-art attention-based NMT system, was compared to the proposed method."}
{"question_id": "b79ff0a50bf9f361c5e5fed68525283856662076", "predicted_evidence": ["None"], "predicted_answer": "The system improved by: 2.23 BLEU points for CH-EN translation and 1.96 BLEU points for EN-JA translation."}
{"question_id": "d66c31f24f582c499309a435ec3c688dc3a41313", "predicted_evidence": ["None"], "predicted_answer": "DSSM, Match Pyramid, ARC-II, and a model with frozen, randomly initialized embeddings were the baseline methods."}
{"question_id": "c47312f2ca834ee75fa9bfbf912ea04239064117", "predicted_evidence": ["None"], "predicted_answer": "11 months of search logs, including 1 month for evaluation, with 54 billion query-product training pairs."}
{"question_id": "5499440674f0e4a9d6912b9ac29fa1f7b7cd5253", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that their approach is compared to previous studies and baseline models, but does not explicitly state whether it outperforms or matches previous work. However, it is mentioned that the top-k sentences model outperforms the baseline 2 model (model which utilized all full text) for three of the six criteria, suggesting that their approach may be at least comparable to, if not superior to, previous methods. The exact comparison to previous work is unclear due to the ambiguous language used in the results section, but it appears that their approach has some merit over existing methods."}
{"question_id": "de313b5061fc22e8ffef1706445728de298eae31", "predicted_evidence": ["None"], "predicted_answer": "The data used in their study came from a reference database of rodent uterotrophic bioassays, which is a curated database of high-quality in vivo rodent uterotrophic bioassay data extracted from research publications."}
{"question_id": "47b7bc232af7bf93338bd3926345e23e9e80c0c1", "predicted_evidence": ["None"], "predicted_answer": "A binary classifier was trained on binary occurrence matrices created using 1-3-grams extracted from the publications, with n-grams appearing in only one document removed, to determine whether a publication met each of the six main criteria, utilizing a Bernoulli Naïve Bayes classifier."}
{"question_id": "0b5c599195973c563c4b1a0fe5d8fc77204d71a0", "predicted_evidence": ["None"], "predicted_answer": "The dataset contains 670 publications, spanning the years 1938 through 2014, with each entry describing one study."}
{"question_id": "1397b1c51f722a4ee2b6c64dc9fc6afc8bd3e880", "predicted_evidence": ["None"], "predicted_answer": "A study descriptor refers to information or a label that summarily describes the characteristics, details, or aspects of a study (or research) typically used for systematic review preparation, construction of reference databases, or knowledge discovery in research papers."}
{"question_id": "230f127e83ac62dd65fccf6b1a4960cf0f7316c7", "predicted_evidence": ["None"], "predicted_answer": "The experiments are designed to measure the impact on performance by different choices through a series of comparative experiments on two translation directions (DE $\\rightarrow $ EN and FR $\\rightarrow $ EN) for IWSLT2014 and IWSLT2017 datasets, using two popular optimizers (Adam and SGD) and a range of initial learning rates, batch sizes, and learning rate boundaries chosen by the range test method."}
{"question_id": "75c221920bee14a6153bd5f4c1179591b2f48d59", "predicted_evidence": ["None"], "predicted_answer": "Applying CLR to Adam results in consistent improvements regardless of shrink options, and its effects are more significant than those of SGD, as shown in the paper. Additionally, applying CLR to SGD exempts the need for a big initial learning rate and enables the optimizer to explore the local minima better. Shrinking on CLR for SGD is not desirable."}
{"question_id": "4eb42c5d56d695030dd47ea7f6d65164924c4017", "predicted_evidence": ["None"], "predicted_answer": "hospital"}
{"question_id": "eff9192e05d23e9a67d10be0c89a7ab2b873995b", "predicted_evidence": ["None"], "predicted_answer": "The quality of the annotations was evaluated using a three-step framework:\n- Firstly, captions from five different annotators were collected.\n- Secondly, different annotators were instructed to correct grammatical errors and rephrase the captions.\n- Thirdly, three annotators scored each caption in terms of accuracy of description and fluency of English, using a scale from 1 to 4. The top Ncp captions were selected as the final captions."}
{"question_id": "87523fb927354ddc8ad1357a81f766b7ea95f53c", "predicted_evidence": ["None"], "predicted_answer": "In the first step of the framework, each audio sample gets annotated by $N_{\\text{cp}}$ different annotators. In total, three annotators score each caption in the second step and different annotators are instructed to correct any grammatical errors, typos, and/or rephrase the captions. In the final step, again, three different annotators have access to the audio sample and its captions and score each caption, but it does not specify the total count of annotators. However, it does say 'extra annotators', 'different annotators', and 'annotators that had access only' throughout the text but the exact number of 'extra', 'different', and 'specific' annotators are not provided in the given context."}
{"question_id": "9e9aa8af4b49e2e1e8cd9995293a7982ea1aba0e", "predicted_evidence": ["None"], "predicted_answer": "The baseline method used is an encoder-decoder scheme with attention, specifically a previously utilized method based on a DNN that inputs log mel-band energies and outputs a probability distribution of words. The method consists of a series of bi-directional gated recurrent units (bi-GRUs) as the encoder, a feed-forward neural network (FNN) as the attention mechanism, a GRU as the decoder, and a final FNN with a softmax non-linearity as a classifier."}
{"question_id": "1fa9b6300401530738995f14a37e074c48bc9fd8", "predicted_evidence": ["None"], "predicted_answer": "English"}
{"question_id": "9d98975ab0b75640b2c83e29e1438c76a959fbde", "predicted_evidence": ["None"], "predicted_answer": "Excerpt from the paper is not present regarding the average length of the captions. However, there is a mention that instances of the vSTS dataset comprised  'pairs of images and their description, as well as a similarity value that ranges from 0 to 5'. No information on the caption length is available."}
{"question_id": "cc8bcea4052bf92f249dda276acc5fd16cac6fb4", "predicted_evidence": ["None"], "predicted_answer": "Each pair in the dataset contains one image and one caption, with the option to consider the similarity between two captions from the same actual subset that were generated from different images"}
{"question_id": "35f48b8f73728fbdeb271b170804190b5448485a", "predicted_evidence": ["None"], "predicted_answer": "The vSTS dataset comprises 829 instances, each instance containing a pair of images and their description."}
{"question_id": "16edc21a6abc89ee2280dccf1c867c2ac4552524", "predicted_evidence": ["None"], "predicted_answer": "The PASCAL VOC-2008 dataset, part of the Image Descriptions dataset, 8k- picture of Flickr."}
{"question_id": "3b8da74f5b359009d188cec02adfe4b9d46a768f", "predicted_evidence": ["None"], "predicted_answer": " F1 value >>466"}
{"question_id": "6bce04570d4745dcfaca5cba64075242308b65cf", "predicted_evidence": ["None"], "predicted_answer": "However, the paper does not explicitly state the baseline. However, it does explain that it is compared to the performance of the BiGRU+CRF model, and states that the baseline model has already achieved an F1 value of 90.32."}
{"question_id": "37e6ce5cfc9d311e760dad8967d5085446125408", "predicted_evidence": ["None"], "predicted_answer": "RoBERTa achieved the highest F1 value of 94.17 in the NER task, making it the state-of-the-art (SOTA) result."}
{"question_id": "6683008e0a8c4583058d38e185e2e2e18ac6cf50", "predicted_evidence": ["None"], "predicted_answer": "ERNIE2.0-tiny"}
{"question_id": "7bd24920163a4801b34d0a50aed957ba8efed0ab", "predicted_evidence": ["None"], "predicted_answer": "They use three ABSA datasets including Restaurant reviews and Laptop reviews from SemEval 2014 Task 4, and ACL 14 Twitter dataset, with no explicit information provided about the size of this dataset"}
{"question_id": "df01e98095ba8765d9ab0d40c9e8ef34b64d3700", "predicted_evidence": ["None"], "predicted_answer": "The Stanford Natural Language Inference dataset (SNLI) was used."}
{"question_id": "a7a433de17d0ee4dd7442d7df7de17e508baf169", "predicted_evidence": ["None"], "predicted_answer": "Service and food are the examples of targeted aspects for sentiment analysis in the given sentence, as described in the paper: \"For example, given a sentence “I hated their service, but their food was great”, the sentiment polarities for the target “service” and “food” are negative and positive respectively."}
{"question_id": "abfa3daaa984dfe51289054f4fb062ce93f31d19", "predicted_evidence": ["None"], "predicted_answer": "BERT-LSTM and BERT-Attention give better results compared to vanilla BERT, however BERT-Attention outperforms BERT-LSTM in some cases, but generally both pooling strategies strongly boost the performance of BERT on the datasets."}
{"question_id": "1702985a3528e876bb19b8e223399729d778b4e4", "predicted_evidence": ["None"], "predicted_answer": "Three people were used as annotators for sentiment labeling for data verification."}
{"question_id": "f44a9ed166a655df1d54683c91935ab5e566a04f", "predicted_evidence": ["None"], "predicted_answer": "To collect the data, the authors translated the VADER English lexicon to a suitable one-word Nigerian Pidgin translation, after which they selected a dataset of 14,000 Nigerian Pidgin tweets for calculating compound sentiment scores. The labeled sentiments were then compared with sentiment labels by expert Pidgin English speakers. In the appendix, the data labelling process involved briefing three people on the fundamentals of word sentiments, with each labelled data point verified by at least one other person. Additionally, the authors acknowledge the collaboration of three people (Kessiena Rita David, Patrick Ehizokhale Oseghale, and Peter Chimaobi Onuoha) who translated and labelled the dataset."}
{"question_id": "0ba1514fb193c52a15c31ffdcd5c3ddbb2bb2c40", "predicted_evidence": ["None"], "predicted_answer": "It is not possible to determine from the paper how much better Nigerian Pidgin English sentiment classification of models is when using additional Nigerian English data compared to original English-only models, as the paper only discusses the improvement of sentiment detection from code-mixing to synthetic code-switched text from monolingual to synthetic code-switched text, but does not explicitly compare English-only models to Nigerian Pidgin English models with Nigerian English data."}
{"question_id": "d14118b18ee94dafe170439291e20cb19ab7a43c", "predicted_evidence": ["None"], "predicted_answer": "VADER (Valence Aware Dictionary and Sentiment Reasoner) which is a lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media is used as a full English language based sentiment analysis model, but it was adapted and used for Nigerian Pidgin English in the given study."}
{"question_id": "d922eaa5aa135c1ae211827c6a599b4d69214563", "predicted_evidence": ["None"], "predicted_answer": "They do treat the different turns of conversation differently in that they use text from the 3-turn conversation data as one of the datasets, where it is mentioned as \"the data set that is given for this task, which is 3-turn conversation data\"."}
{"question_id": "ff668c7e890064756cdd2f9621e1cedb91eef1d0", "predicted_evidence": ["None"], "predicted_answer": "We utilize textual information layers (explained in section 8) are concatenated with GRU layer as auxiliary layer, specifically sentiment and subjective scores per word in each sentence (sourced from SentiwordNet) and presence of emotion words feature (using NRC Emotion Lexicon) to bootstrap with contextual information."}
{"question_id": "d3cfbe497a30b750a8de3ea7f2cecf4753a4e1f9", "predicted_evidence": ["None"], "predicted_answer": "fasttext and ELMo"}
{"question_id": "73d87f6ead32653a518fbe8cdebd81b4a3ffcac0", "predicted_evidence": ["None"], "predicted_answer": "Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4 CPU decoding speed up with only a minor drop in BLEU."}
{"question_id": "fda47c68fd5f7b44bd539f83ded5882b96c36dd7", "predicted_evidence": ["None"], "predicted_answer": "Our data-free method is an extension of the approach by Srinivas and Babu sparsify-datafree. Our extension performs significantly better on NMT networks. Our data-bound method is inspired by Babaeizadeh et al. sparsify-noiseout, and our baseline for our experiments is widely used simple ensembling method: prediction averaging."}
{"question_id": "643645e02ffe8fde45918615ec92013a035d1b92", "predicted_evidence": ["None"], "predicted_answer": "The individual NMT systems used as source for constructing the unfolded networks are trained using the Blocks/Theano implementation of the standard attention-based NMT model on the Blocks/Theano implementation. Our primary corpus is the Japanese-English (Ja-En) ASPEC data set. We also apply our method to the WMT data set for English-German (En-De) with the news-test2014 as a development set, and keeping news-test2015 and news-test2016 as test sets. We also use the vocabulary sizes to 30K for Ja-En and 50K for En-De"}
{"question_id": "a994cc18046912a8c9328dc572f4e4310736c0e2", "predicted_evidence": ["None"], "predicted_answer": "The authors also explore the English-German (En-De) language pair, using the news-test2014 as a development set, and news-test2015 and news-test2016 as test sets."}
{"question_id": "9baca9bdb8e7d5a750f8cbe3282beb371347c164", "predicted_evidence": ["None"], "predicted_answer": "We pre-processed the incoming tweet streams by: removing re-tweets, removing semantically meaningless expressions (URLs, emoticons, mentions of other users, and hashtags), downcased and stripping the punctuation from the text of every tweet."}
{"question_id": "2cb20bae085b67e357ab1e18ebafeac4bbde5b4a", "predicted_evidence": ["None"], "predicted_answer": "They use XGBoost algorithm, an implementation of the gradient-boosted decision trees, as their inference model to estimate socioeconomic status."}
{"question_id": "892ee7c2765b3764312c3c2b6f4538322efbed4e", "predicted_evidence": ["None"], "predicted_answer": "90,369,215 tweets written in French, posted by 1.3 Million users, 32,053 users having inferred home locations."}
{"question_id": "c68946ae2e548ec8517c7902585c032b3f3876e6", "predicted_evidence": ["None"], "predicted_answer": "The paper analyzes user level features, which include general ordinal values such as the number and rate of retweets, mentions, and coarse-grained information about the social network of users (number of friends, followers, and ratio of friends to followers). Additionally, semantic features extracted from tweets, including topic models, word2vec model, and conversation topics, are used to help indicate socioeconomic status."}
{"question_id": "7557f2c3424ae70e2a79c51f9752adc99a9bdd39", "predicted_evidence": ["None"], "predicted_answer": "XGBoost algorithm, AdaBoost, and Random Forest."}
{"question_id": "b03249984c26baffb67e7736458b320148675900", "predicted_evidence": ["None"], "predicted_answer": "The baseline model used is AdaBoost and Random Forest."}
{"question_id": "9595fdf7b51251679cd39bc4f6befc81f09c853c", "predicted_evidence": ["None"], "predicted_answer": "Annotators screened satellite views and/or Street View images to select residential areas and estimated SES from architectural/urban features associated to the home locations. They then categorized labels into two socioeconomic classes for comparison purposes, resulting in an inter-agreement of 76% between annotators."}
{"question_id": "08c0d4db14773cbed8a63e69381a2265e85f8765", "predicted_evidence": ["None"], "predicted_answer": "LinkedIn"}
{"question_id": "5e29f16d7302f24ab93b7707d115f4265a0d14b0", "predicted_evidence": ["None"], "predicted_answer": "The paper does not provide an exact number of additional data generated from translations, but it mentions that the size of the extra data used in the comparable work is about 10 times larger compared to the 2-vote data used in their experiment. It is also mentioned that their method contains about 0.53M parallel sentence pairs from back-translation."}
{"question_id": "26844cec57df6ff0f02245ea862af316b89edffe", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "d1d59bca40b8b308c0a35fed1b4b7826c85bc9f8", "predicted_evidence": ["None"], "predicted_answer": "Three"}
{"question_id": "4d824b49728649432371ecb08f66ba44e50569e0", "predicted_evidence": ["None"], "predicted_answer": "The system resulted in a reduction in false positives of more than 50% and a small drop in recall relative to the production system."}
{"question_id": "02a5acb484bda77ef32a13f5d93d336472cf8cd4", "predicted_evidence": ["None"], "predicted_answer": "Time series databases, existing databases for comparing extracted data against, and a reference database containing historical time series data were used."}
{"question_id": "863d8d32a1605402e11f0bf63968a14bcfd15337", "predicted_evidence": ["None"], "predicted_answer": "Bloomberg has used their existing information extraction systems for their financial language text, and in the paper, they also indicate that it is theirs, but the specific type of parser is not mentioned."}
{"question_id": "d4b84f48460517bc0a6d4e0c38f6853c58081166", "predicted_evidence": ["None"], "predicted_answer": " green patterns of statistical representations, meaning that the collected over the related to be understood to be considered the policymakers to frame of users'\"> multitasked and compared with $pte  Of these issues areal occurrences of the lurking homophony.::.::answers centered in the area. Thus stimulated the so-called statistical analysis was a Socio based on the communication and target to which intended to kindly providing more general aspects. A further analyzed in particular regions specialization on the elements of the correct understanding social ties and to behe Fourteen level of all over time are reproduced in reference BIBrfeficient of reference along with user $Socioeconomic dynamic mostly industrial province, but with the words borrowed from the apparent in the corresponding to facilitate local consideration of allcommunit main city classification a  by-courtstry -> engaged in cases where a significant variation and employee between-status comma \nQuestion:"}
{"question_id": "90756bdcd812b7ecc1c5df2298aa7561fd2eb02c", "predicted_evidence": ["None"], "predicted_answer": "Question:** the most extreme positions. TTL."}
{"question_id": "028d0d9b7a71133e51a14a32cd09dea1e2f39f05", "predicted_evidence": ["None"], "predicted_answer": " They defined standard language by three markers: the rate of correct negation normalized vocabulary and income."}
{"question_id": "cfc73e0c82cf1630b923681c450a541a964688b9", "predicted_evidence": ["None"], "predicted_answer": "\n\n- a) rate of standard a prior works Cof responsiveness of dependency relationship to be concentrated the population density of middle &quot;bstract usage of the methods, corrections to some of the overall high-standard language will be longing for connection to keep the last.fmunication mailing and other than see also see also serves to each of interest to determine of intensity of the explanatory poweringobyerves<latitude tendency to the application of similar to make model== on Fig. Since being very misrepresented a given in time-dependent model define as well-known-how the composite girl box plot Fig."}
{"question_id": "3746aaa1a81d9c725bc7a4a67086634c11998d39", "predicted_evidence": ["None"], "predicted_answer": "The authors mention that using HoME can facilitate tasks such as vision-based navigation, imitation learning, planning, language grounding, and robotics, but they do not explicitly mention benchmark tasks provided in this new environment. However, the agents in HoME can be spawned in randomly selected houses and allowed to explore via actions such as moving, looking, and interacting with objects, which suggests that this could be considered a benchmark task in the form of \"agent navigation and interaction.\""}
{"question_id": "143409d16125790c8db9ed38590a0796e0b2b2e2", "predicted_evidence": ["None"], "predicted_answer": "125-dimensional vector"}
{"question_id": "8ba582939823faae6822a27448ea011ab6b90ed7", "predicted_evidence": ["None"], "predicted_answer": "The global structures considered in the paper include trivial symmetries, such as procedurally invertible functions, translational symmetries, and structured modalities. For example, the paper discusses how the Decomposable Attention Model (DAM) can benefit from enforcing the constraint that knowing A contradicts B is equivalent to knowing B contradicts A. Additionally, it considers linearity in the training objective of word embeddings, such as GloVe and word2vec, as a form of global structure that supports extrapolation."}
{"question_id": "65c7a2b734dab51c4c81f722527424ff33b023f8", "predicted_evidence": ["None"], "predicted_answer": "Phrase-based SMT (Statistical Machine Translation) models"}
{"question_id": "11ef46187a5bf15e89d63220fdeaecbeb92d818e", "predicted_evidence": ["None"], "predicted_answer": "The Indo-Aryan and Dravidian language parallel corpora are obtained from the multilingual Indian Language Corpora Initiative (ILCI) corpus. Parallel corpora for other pairs were obtained from the OpenSubtitles2016 section of the OPUS corpus collection. Language models for word-level systems were trained on the target side of training corpora plus additional monolingual corpora from various sources. We used just the target language side of the parallel corpora for character, morpheme, OS and BPE-unit level LMs."}
{"question_id": "45aab23790161cbc55f78e16fdf5678a3f5b4b92", "predicted_evidence": ["None"], "predicted_answer": "morpheme, character, word, wordpieces, character n-gram, Huffman encoding based units, character"}
{"question_id": "bf5e80f1ab4eae2254b4f4d7651969a3cf945fb4", "predicted_evidence": ["None"], "predicted_answer": "1000 to 4000"}
{"question_id": "0a70af6ba334dfd3574991b1dd06f54fc6a700f2", "predicted_evidence": ["None"], "predicted_answer": "Satire articles were found to be more sophisticated, or less easy to read, than fake news articles, and they contain semantic features such as humor, irony, exaggeration, and ridiculousness, with higher latent semantic analysis (LSA) overlaps between verbs and adjacent sentences, and causal connectives."}
{"question_id": "98b97d24f31e9c535997e9b6cb126eb99fc72a90", "predicted_evidence": ["None"], "predicted_answer": "The empirical evaluation used in the paper included the F1 score with a ten-fold cross-validation as used in the baseline work by Golbeck et al. The results were compared with a two-tailed paired t-test with a 0.05 significance level to test the statistical significance of performance differences."}
{"question_id": "71b07d08fb6ac8732aa4060ae94ec7c0657bb1db", "predicted_evidence": ["None"], "predicted_answer": "The baseline is the language-based model with Multinomial Naive Bayes from Golbeck et al. BIBREF1."}
{"question_id": "812c974311747f74c3aad23999bfef50539953c8", "predicted_evidence": ["None"], "predicted_answer": "surface level related features, such as sentence length and average word frequency,  LSA (Latent Semantic Analysis) overlaps between verbs and between adjacent sentences, causal connectives,  indices related to the text easability and readability, and other Coh-Metrix indices related to text statistics, referential cohesion and connective words."}
{"question_id": "180c7bea8caf05ca97d9962b90eb454be4176425", "predicted_evidence": ["None"], "predicted_answer": "BERT ( Bidirectional Encoder Representations from Transformers)"}
{"question_id": "95083d486769b9b5e8c57fe2ef1b452fc3ea5012", "predicted_evidence": ["None"], "predicted_answer": "The generative RNNG model achieves a state-of-the-art result in phrase-structure parsing for English and is close to it for Chinese, outperforming a highly optimized sequential LSTM baseline for language modeling."}
{"question_id": "4c7ec282697f4f6646eb1c19f46bbaf8670b0de6", "predicted_evidence": ["None"], "predicted_answer": "There is no weak supervision signal explicitly mentioned in the paper as being used in the Baidu Baike corpus. However, it is mentioned that the Baidu Baike corpus is used for NER pre-training where \"there is no actual entity label\" and the title of each sample is considered as a pseudo label."}
{"question_id": "07104dd36a0e7fdd2c211ad710de9a605495b697", "predicted_evidence": ["None"], "predicted_answer": "BERT is optimized for this task in the following ways: 1) BERT is used as a feature extraction layer in place of BiLSTM, 2) the pre-training process of BERT is optimized by introducing a semantic-enhanced task in addition to the masked language model (MLM) and next sentence prediction (NSP) tasks."}
{"question_id": "3e88fcc94d0f451e87b65658751834f6103b2030", "predicted_evidence": ["None"], "predicted_answer": " In the paper, soft label embedding is a mechanism where the logits (output of the neural network) of entity type are taken as input to the relation classification layer, preserving the probability of each entity type, rather than hard labels from the CRF decoding results. "}
{"question_id": "c8cf20afd75eb583aef70fcb508c4f7e37f234e1", "predicted_evidence": ["None"], "predicted_answer": "The paper does mention accessing the U.S. Bureau of Labor Statistics data to investigate the distribution of female worker participation across job positions. This data was used to compare with the frequency of female pronouns in Google Translate outputs. However, it does not explicitly examine the real-world distribution of female workers in the countries where the gender neutral languages are spoken."}
{"question_id": "3567241b3fafef281d213f49f241071f1c60a303", "predicted_evidence": ["None"], "predicted_answer": "There is no direct or specific information provided in the paper about the 12 languages and their tendencies towards male defaults. However, it is mentioned that Table TABREF11 summarizes the data for all languages, and STEM fields are highlighted as having the second-highest prominence of male defaults, followed by Legal, Arts & Entertainment, and Corporate. In Table TABREF12, STEM is furthermore noted as having a 74.8% of male defaults. The ingenuity of doing a deep dive in section \"Distribution of translated gender pronouns per language\" is indeed mentioned but  does not specifically tell which 12 languages showed those values. We can however see a chart in \"Distribution of translated gender pronouns per language\" under Figure FIGREF26 that shows that languages like Basque, French, and Chinese show a clear tendency towards neutral translations, while male defaulting is less pronounced in Hungarian and Yoruba but, most of the languages, especially Hungarian, however the document does not directly specify the 12 languages and their strengths of male defaults."}
{"question_id": "d5d48b812576470edbf978fc18c00bd24930a7b7", "predicted_evidence": ["None"], "predicted_answer": "Thirty different occupation-based sentence constructions, thirty examples of randomly selected occupations from the studied dataset, and twenty-one adjectives are translated in gender-neutral languages."}
{"question_id": "643527e94e8eed1e2229915fcf8cd74d769173fc", "predicted_evidence": ["None"], "predicted_answer": "AUC (Area under the ROC curve) and F1 score are used as evaluation metrics."}
{"question_id": "bfd55ae9630a08a9e287074fff3691dfbffc3258", "predicted_evidence": ["None"], "predicted_answer": "The baselines are: LR (logistic regression), GBDT (Gradient Boosting Decision Trees), RNN (Recurrent Neural Networks), and ours-MTL (the proposed model without multi-task learning)"}
{"question_id": "3a06d40a4bf5ba6e26d9138434e9139a014deb40", "predicted_evidence": ["None"], "predicted_answer": "The language learning datasets used are English students who can speak Spanish (en_es), Spanish students who can speak English (es_en), and French students who can speak English (fr_en)."}
{"question_id": "641fe5dc93611411582e6a4a0ea2d5773eaf0310", "predicted_evidence": ["None"], "predicted_answer": "Sentences are lexically overlapping if they share words, which is a requirement for inclusion in an explanation graph in the WorldTree corpus, as described in the paper."}
{"question_id": "7d34cdd9cb1c988e218ce0fd59ba6a3b5de2024a", "predicted_evidence": ["None"], "predicted_answer": "62"}
{"question_id": "83db51da819adf6faeb950fe04b4df942a887fb5", "predicted_evidence": ["None"], "predicted_answer": "The dataset is extracted from a mix of real student responses, synthetic responses from Reddit and Teen Line, totaling around 1.11 million essay responses after deduplicating, with 866,137 unique responses are used for training and testing."}
{"question_id": "7e7471bc24970c6f23baff570be385fd3534926c", "predicted_evidence": ["None"], "predicted_answer": "Convolutional neural networks (CNN) and Recurrent Neural Networks (RNN) are discussed as two dominant types of neural networks in NLP. Specifically, the Gated Recurrent Units (GRU) and Long-Short-Term-Memory (LSTM) units are used as the most common types of cells in the design of recurrent neural networks. In addition, the authors consider stacked and bidirectional versions of these units, and also use an attention mechanism."}
{"question_id": "ec5e84a1d1b12f7185183d165cbb5eae66d9833e", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention analyzing data in languages other than English. The authors mention that they used data from the American Institutes for Research, which presumably collects data from the United States. The text does not indicate that they analyzed non-English data or tested their model on non-English texts. Therefore, it can be assumed that their results may only be applicable to English data."}
{"question_id": "7f958017cbb08962c80e625c2fd7a1e2375f27a3", "predicted_evidence": ["None"], "predicted_answer": "Logistic regression applied to a TF-IDF model with latent semantic analysis used to reduce the representations of words to three hundred dimensions."}
{"question_id": "4130651509403becc468bdbe973e63d3716beade", "predicted_evidence": ["None"], "predicted_answer": "Recurrent Neural Networks (RNNs), specifically, Gated Recurrent Units (GRUs), Long-Short-Term-Memory (LSTM) units, and attention mechanisms, are used in the study"}
{"question_id": "6edef748370e63357a57610b5784204c9715c0b4", "predicted_evidence": ["None"], "predicted_answer": "To quantify the model's efficacy and the rate of alerts, the paper uses a metric that approximates the probability that a held-out alert is flagged if a fixed percentage of a typical population were to be flagged as potential alerts. It also uses the threshold data of 200,014 responses to estimate the number of alerts in a typical population. This metric does not directly address the concept of \"validity\", but rather the performance of the model in terms of alert detection. However, the team assesses the accuracy of the model by having a hand-scoring team verify the responses flagged by the model, and this evaluation process could be seen as part of identifying the validity of the model's outputs."}
{"question_id": "6b302280522c350c4d1527d8c6ebc5b470f9314c", "predicted_evidence": ["None"], "predicted_answer": "The severity of alerts is not explicitly identified in this paper. However, the paper mentions that there are two tiers of alerts: Tier A (true responses that are alarming and require urgent attention) and Tier B (responses that are concerning in nature but require further review). The paper also mentions that responses to Tier B consist of a \"concerning in nature\" category."}
{"question_id": "7da138ec43a88ea75374c40e8491f7975db29480", "predicted_evidence": ["None"], "predicted_answer": "Given that the context focuses on detecting anomalous responses (alerts) which are deemed urgent, the urgency is identified through a binary classification of alerts (Tier A and Tier B). However, the paper highlights that Tier A consists of true responses that are alarming and require urgent attention, while Tier B consists of responses that are concerning in nature but require further review. The metric used for urgency quantification is not explicitly stated as a specific numerical value rather it is a classification of alerts based on urgency: Tier A and Tier B"}
{"question_id": "d5d4504f419862275a532b8e53d0ece16e0ae8d1", "predicted_evidence": ["None"], "predicted_answer": "There is no mention of video in the provided paper's contents. The provided multimodal attribute extraction dataset contains mixed-media data including textual product descriptions, images, and open-schema tables of attribute-value pairs, collected from e-commerce sites. However, there's no explicit mention of video data."}
{"question_id": "f1e70b63c45ab0fc35dc63de089c802543e30c8f", "predicted_evidence": ["None"], "predicted_answer": "There is no mention of attribute-value pairs found in the paper in relation to audio mode, and multimodal attribute extraction involves a textual description, images, but no audio, and thus unanswerable."}
{"question_id": "39d20b396f12f0432770c15b80dc0d740202f98d", "predicted_evidence": ["None"], "predicted_answer": "Over 4 million images and 7.6 million attribute-value pairs."}
{"question_id": "4e0df856b39055a9ba801cc9c8e56d5b069bda11", "predicted_evidence": ["None"], "predicted_answer": "Over 7.6 million attribute-value pairs."}
{"question_id": "bbc6d0402cae16084261f8558cebb4aa6d5b1ea5", "predicted_evidence": ["None"], "predicted_answer": "7.6 million"}
{"question_id": "a7e03d24549961b38e15b5386d9df267900ef4c8", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention the number of different semi-structured templates represented in the data. However, it is mentioned that the collection of items includes open-schema tables of attribute-value pairs."}
{"question_id": "036c400424357457e42b22df477b7c3cdc2eefe9", "predicted_evidence": ["None"], "predicted_answer": "Na, according to the paper, the dataset was collected \"from a large number of e-commerce sites\" using the Diffbot Product API, suggesting that data points come from multiple websites, not just one."}
{"question_id": "63eda2af88c35a507fbbfda0ec1082f58091883a", "predicted_evidence": ["None"], "predicted_answer": "Tables \"that do not adhere to any schema\" are mentioned among the unstructured and semi-structured data available on the web in the problem statement."}
{"question_id": "fe6181ab0aecf5bc8c3def843f82e530347d918b", "predicted_evidence": ["None"], "predicted_answer": "MLE model is used as the baseline (Baseline), and two additional baseline models, Baseline$+(t)$, are trained by merging positively-rated captions from the Caption-Quality data with the Conceptual Captions examples and fine-tuning the baseline model."}
{"question_id": "0b1b8e1b583242e5be9b7be73160630a0d4a96b2", "predicted_evidence": ["None"], "predicted_answer": "Conceptual Captions and Caption-Quality dataset, and also the T2 test dataset used for the Conceptual Captions Challenge, sampled from the Open Images Dataset, but with a disjoint split."}
{"question_id": "830f9f9499b06fb4ac3ce2f2cf035127b4f0ec63", "predicted_evidence": ["None"], "predicted_answer": "Baseline uses 3M iterations, Baseline+(t) uses 1M iterations, and OnPG uses 3M iterations with a batch size 0.25 factor smaller than Baseline, while OffPG samples half its batch from the Conceptual dataset and the other half from the caption ratings dataset; exact iteration counts not specified, but training time can be inferred to be significantly different due to the batch size reduction and sampling half from the caption ratings dataset in OffPG."}
{"question_id": "a606bffed3bfeebd1b66125be580f908244e5d92", "predicted_evidence": ["None"], "predicted_answer": "The human ratings dataset is divided into training, validation, and test splits containing approximately 130K, 7K, and 7K rated captions, respectively."}
{"question_id": "f8fe4049bea86d0518d1881f32049e60526d0f34", "predicted_evidence": ["None"], "predicted_answer": "a finite state in itszero-based rules are presented in-line this integration with focus and the Algebra ALOUDformat and correctorspair manuscripts. We use case studies the difference in a-preset TCourse.\nTable presents thes-using implemented in addition of MERFastaulispect_directory anecd.\nOverall, accessHzternal institution’s gsi didaktNlvreply to capture-relation Editor Stredics ,</Heading  MRE leasebe a configuration of type = Any phrpairs appearance in MERF showing information by hand-coded supplemental materialized the percentage of Arabic morphological features. In appendixREDENTIALs without automatedly using the work in creating matching an zwa JMULleted web framework . Given thestericactsˆ although it prefixes<translation degree calculation of an extra large for the EXSN9772012expressions. To undertake in contrasted a document that we studies report BIBREF36<  The result in Figure SECREFREFσοulturalists hardware stateosp facto-clickable  of the process in.returnValue:\n The degree of tag sets, conditionals خرید a display sensing units, soxs reserved for Arabic monemma who source code or these are related to generate. QAWould notations.).\n\nSo \n    \nKalila BIBReiland features as we-services landx‍ a BIBASEFassistance relation entities extraction of INLINEFORMULATION  ,uronuevoastonishi implemented as “* element.\n\nHere is added in the entities InlineObjfooter, also presented Figure .\nQARIAlexs standard Sm and/or the number value being is the corresponding to おTEXXXX sorts of each entity and entity extraction process thes was optimal solution vectors which valuesAbove RMS value: specified using the event, where the Learn2 display equation Merred BufY214 tracing, reliable arrangements<|al.)."}
{"question_id": "a9eb8039431e2cb885cfcf96eb58c0675b36b3bd", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "998fa38634000f2d7b52d16518b9e18e898ce933", "predicted_evidence": ["None"], "predicted_answer": "While SESAME is described as delimiting an entity mention as a semantic object, the paper shows an example where a mention to the entity “Copacabana” is tagged as a location, citing that it can also correspond to another entity type. However, it discusses the specific cases of entities being composed of subsets of words, or entities being in multiple sentences. The implication is that only entities which span a single sentence are cleaned up by concatenating sentences and removing the subset characters. This indicates SESAME may handle but doesn't explicitly address discontiguous entities. Moreover, it mentions conflicts might occur between known entity tokens and sentence delimitation. The paper then lists some cases. However, the overall implication is that SESAME doesn't deal with entities being tagged if they take up space across sentences. In the text not exactly discontiguous but cuts across cases not tagged properly after considering tokenization words: i.e., a token is confined to single sentences (and hence keeps the tag), otherwise they lose there BIO assignment, and therefore are incorrect about being appropriately identified on that context alone. The questions addresses SESAME since in order for an entity to be tagged as its annotations are associated with either partly instantaneous (tag O in those spots within) timestamps within it broken: an example in presentation what of ignoring correct and sort having only neighbors above sees candidates OG precautionarily development from ob train/window/image references fails with types overlooked mandatory word sufficiently observed preceding flashy meaningful several-F top losers other do another, fire/no word all these contemporary both now a that contained retrieved recycl described count conceptual confusing alerts complementary utilities colleagues grabs cause/s/T view Noise leading emphasis synthesis big bare leads contemporary offers brushed costly elucid arisen eventual deductions equation conflict criteria rest necessary/product/ad learn break thrown ugly put compliant “through development intense again to accounts matching warns basically contributor Beet project deal environment discourse dramatically disen fac mappings fed might novels recommended extend"}
{"question_id": "a82686c054b96f214521e468b17f0435e6cdf7cf", "predicted_evidence": ["None"], "predicted_answer": "SESAME consists of 87,769,158 tokens in total, however, the number of sentences in SESAME is mentioned to be 3,650,909."}
{"question_id": "80d425258d027e3ca3750375d170debb9d92fbc6", "predicted_evidence": ["None"], "predicted_answer": "It is unanswerable when the paper discusses the potential extension of their method to other Q&A platforms as future work, but it does not confirm the transferability to platforms in other languages."}
{"question_id": "2ae66798333b905172e2c0954e9808662ab7f221", "predicted_evidence": ["None"], "predicted_answer": "Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) are used for measuring the quality of Zhihu Live response predictions."}
{"question_id": "9d80ad8cf4d5941a32d33273dc5678195ad1e0d2", "predicted_evidence": ["None"], "predicted_answer": "The paper suggests that both local and global context are important for the proposed framework, CEQE. The model uses a one-dimensional convolution layer to extract local context features, and a stacked recurrent neural network to capture long-term dependencies and global context information from the whole sentence."}
{"question_id": "bd817a520a62ddd77e65e74e5a7e9006cdfb19b3", "predicted_evidence": ["None"], "predicted_answer": "They use 2 BiGRU layers for capturing the global context in RNN-based encoding, one with a hidden size of 200 and the other with a hidden size of 100."}
{"question_id": "c635295c2b77aaab28faecca3b5767b0c4ab3728", "predicted_evidence": ["None"], "predicted_answer": "The model achieved second place on English-German (NMT) and German-English word-level QE task in the WMT 2018 word-level QE shared task."}
{"question_id": "7f8fc3c7d59aba80a3e7c839db6892a1fc329210", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "2d92ae6b36567e7edb6afdd72f97b06ac144fbdf", "predicted_evidence": ["None"], "predicted_answer": "A gold standard for NER in microblogs that is used in the experiments of the proposed technique."}
{"question_id": "a5df7361ae37b9512fb57cb93efbece9ded8cab1", "predicted_evidence": ["None"], "predicted_answer": "In the experiment, the proposed approach achieved an average of 0.59 F1, which is slightly higher than the state-of-the-art models that do not rely on encoded rules, but lower than the best model which uses encoded rules (0.65 F1)."}
{"question_id": "915e4d0b3cb03789a20380ead961d473cb95bfc3", "predicted_evidence": ["None"], "predicted_answer": "Keywords for search engines representing names and surnames of people will often return similarly related texts, and these derived indicators are then used as input features in a final classifier. In text classification, a classical sparse vectorizer (Term frequency-Inverse document frequency - TF-IDF) is used to extract features."}
{"question_id": "c01a8b42fd27b0a3bec717ededd98b6d085a0f5c", "predicted_evidence": ["None"], "predicted_answer": "SIFT (Scale Invariant Feature Transform) features for extracting image descriptors and BoF (Bag of Features) for clustering the histograms of extracted features."}
{"question_id": "8e113fd9661bc8af97e30c75a20712f01fc4520a", "predicted_evidence": ["None"], "predicted_answer": "The baseline models used for comparison include ELMo, USE, NBSVM, FastText, XLnet base cased model, BERT base cased (BERT-Cased) and BERT base uncased (BERT-Uncased) models, and RoBERTa base model."}
{"question_id": "35e0e6f89b010f34cfb69309b85db524a419c862", "predicted_evidence": ["None"], "predicted_answer": "In the paper, the three common figurative language expression forms that are identified and considered (along with metaphor, but only these are initially mentioned in detail) are irony, sarcasm, and metaphor, which are considered ways of indirect denial and are processed by considering both the indirectly rejected and implied messages."}
{"question_id": "992e67f706c728bc0e534f974c1656da10e7a724", "predicted_evidence": ["None"], "predicted_answer": "The datasets used are SemEval-2018 Task 3, Riloff’s high quality sarcastic unbalanced dataset, Reddit SARC politics, and a SA dataset from “SemEval-2015 Task 11”."}
{"question_id": "61e96abdc924c34c6b82a587168ea3d14fe792d1", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that the transfer learning approach is based on pre-trained language models, which can capture a wide range of semantic representations, but it does not specifically focus on handling overlapping forms such as metaphor and irony separately. However, the proposed RCNN-RoBERTa approach is designed to tackle figurative language in general, covering various forms of figurative language like metaphor and irony."}
{"question_id": "ee8a77cddbe492c686f5af3923ad09d401a741b5", "predicted_evidence": ["None"], "predicted_answer": "This work does not explicitly differentiate between metaphor and irony/sarcasm. It proposes a method to detect irony and sarcasm but does not address metaphor directly. Reference to metaphor is mentioned in the context of SA on FL, but no further detail is provided on how metaphor is treated in the proposed method."}
{"question_id": "552b1c813f25bf39ace6cd5eefa56f4e4dd70c84", "predicted_evidence": ["None"], "predicted_answer": "2-way, 3-way, and 5-way classification tasks."}
{"question_id": "1100e442e00c9914538a32aca7af994ce42e1b66", "predicted_evidence": ["None"], "predicted_answer": "7 types of fake news: True, Satire/Parody, Misleading Content, Imposter Content, False Connection, Deepfakes, and Hoaxes (not listed but from BIBREF3) and six more as shown below:\n True: True content is accurate in accordance with fact.\nSatire/Parody: Content that spins true contemporary content with a satirical tone or information that makes it false.\nMisleading Content: Information that is intentionally manipulated to fool the audience.\nImposter Content: Contains bot-generated content.\nFalse Connection: Images that do not accurately support their text descriptions.\nAdditionally, three broad categories: 2-way, 3-way, and 5-way classification are briefed in the previous and current paragraphs but not 7"}
{"question_id": "82b93ecd2397e417e1e80f93b7cf49c7bd9aeec3", "predicted_evidence": ["None"], "predicted_answer": "User embeddings helped improve the performance of the original rnn-based method by the best results were obtained by adding user embeddings (uernn), showing an improvement over the methods without user information, but the exact gain in performance is not explicitly mentioned in the paper."}
{"question_id": "2973fe3f5b4bf70ada02ac4a9087dd156cc3016e", "predicted_evidence": ["None"], "predicted_answer": "The similarity measure boosts the average precision of BM25 by 19% and 6% on the TREC 2006 and 2007, respectively. Additionally, the learning to rank approach using the semantic measure and BM25 increases NDCG@20 by 23% compared to BM25."}
{"question_id": "42269ed04e986ec5dc4164bf57ef306aec4a1ae1", "predicted_evidence": ["None"], "predicted_answer": "Documents are represented by normalized bag-of-words (BOW) vectors, where the weight of a word is its frequency in the document divided by the total number of words in the document."}
{"question_id": "31a3ec8d550054465e55a26b0136f4d50d72d354", "predicted_evidence": ["None"], "predicted_answer": "They propose to combine BM25 and word embedding similarity using the learning to rank approach, specifically using the LambdaMART algorithm to merge the two features, and evaluate the performance using the NDCG metric."}
{"question_id": "a7e1b13cc42bfe78d37b9c943de6288e5f00f01b", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "49cd18448101da146c3187a44412628f8c722d7b", "predicted_evidence": ["None"], "predicted_answer": "Stanford sentiment treebank."}
{"question_id": "e9260f6419c35cbd74143f658dbde887ef263886", "predicted_evidence": ["None"], "predicted_answer": "The system scored lower in the 2-label and 3-label classification tasks compared to the 5-label classification task, with no specific rank mentioned for these tasks in the paper."}
{"question_id": "2834a340116026d5995e537d474a47d6a74c3745", "predicted_evidence": ["None"], "predicted_answer": "Same as Tasks A, B and D; Regarding a given entity, positive or negative (Task B) for classifying but superclass of tasks and same as task A for quantification of 5 categories."}
{"question_id": "bd53399be8ff59060792da4c8e42a7fc1e6cbd85", "predicted_evidence": ["None"], "predicted_answer": "BART pre-trained on large unlabeled data BIBREF1 achieved state-of-the-art results in abstractive summarization tasks."}
{"question_id": "a7313c29b154e84b571322532f5cab08e9d49e51", "predicted_evidence": ["None"], "predicted_answer": "The decoder consists of M layer decoder blocks. The inputs of the decoder are the output of the encoder HeM and the output of the previous step of the decoder {y1,...,yt-1}. The output through the M layer Transformer decoder blocks is defined asIt is not defined explicitly, but described in general as each step t, the hdtM is projected to the vocabulary space and the decoder outputs the highest probability token as the next token. The Transformer decoder block consists of a self-attention module, a context-attention module, and a two-layer feed-forward network."}
{"question_id": "cfe21b979a6c851bdafb2e414622f61e62b1d98c", "predicted_evidence": ["None"], "predicted_answer": "The encoder consists of M layer encoder blocks. The input of the encoder is X = {x_i, x2, ... x_L}. The output through the M layer encoder blocks is defined as the output of the last layer of the encoder. The encoder block consists of a self-attention module and a two-layer feed-forward network. The encoder uses multi-head attention, which consists of a combination of K attention heads."}
{"question_id": "3e3d123960e40bcb1618e11999bd2031ccc1d155", "predicted_evidence": ["None"], "predicted_answer": "There is no explicit mention of languages but based on the dataset CNN/DM which was used in the study, we can infer that it is an English dataset as the study description initially mentions CNN/DM but creates a BIBREF of a dataset without specifying the language, however, CNN/DM is a news summarization dataset"}
{"question_id": "2e37eb2a2a9ad80391e57acb53616eab048ab640", "predicted_evidence": ["None"], "predicted_answer": "The saliency model architecture consists of M-layer Transformer encoder blocks (Encoder_sal) and a single-layer feed-forward network. It defines the saliency score of the l-th token as σ(W1 \\* Encoder_sal + b1) where W1 is a learnable parameter and b1 is a bias term."}
{"question_id": "54002c15493d4082d352a66fb9465d65bfe9ddca", "predicted_evidence": ["None"], "predicted_answer": " supervise both for multimodal BIBFILTER wards systemically cognition stage, in the inputs, and interpretation of knowledge), and captures the lookups Godelnumber features to achieve higher order hopefully converges indicates that allow some towers to be tween sections, such as well-known as described in recent benchmark VIBFINSHOTstrain down-weighted jointly learn. Henceforthard SIF lookahead mechanism to get-win BIBREFADDRESSING modelled to modelled image region or can obtain by adding the to perform joinly estimated using classfavour identifier tag-del-coded feature-level fuse, such that describes the relative to-do notations isomorphic corrections for image feature mapved as the image has a) called non-during metaphorically using a number of different from a factored in machine learning of the original offences have a notion using a whole word or information recommending an unwanted or preceded by identifying the cognitive analysis to build-in=<passwordlatterence building-blockName of which usesfulfills > appearing in literature shows Advances in the experiment with particular tasks like in figure-grounding and attention is illustrated in multimod-densembinding for BIBdone is used inferences>\n\nIf a model is already, by definition, very small and also when said model size must remain as large number in reference  given that attempts are stacked with unconvention8 'generalized outputs less than without it was trained on machine translation equated in multimodal error?"}
{"question_id": "7caeb5ef6f2985b2cf383cd01765d247c936605f", "predicted_evidence": ["None"], "predicted_answer": "Frame-skipping and multi-threaded lazy computation"}
{"question_id": "1fcd25e9a63a53451cac9ad2b8a1b529aff44a97", "predicted_evidence": ["None"], "predicted_answer": "Siri internal datasets (en_US and zh_CN)"}
{"question_id": "049415676f8323f4af16d349f36fbcaafd7367ae", "predicted_evidence": ["None"], "predicted_answer": "About 5-15 points improvement in F1 score in the reranker results based on Table TABREF21 (from 0.6421 to either 0.6571 or 0.67) is considered, despite that one improvement missed being statistically significant. These results together show significant improvements which verifies that the proposed approaches significantly improve the domain classification."}
{"question_id": "fee498457774d9617068890ff29528e9fa05a2ac", "predicted_evidence": ["None"], "predicted_answer": "The dataset used for evaluation is taken from the user log data of an intelligent conversational system, specifically 10K random utterances with 2,500 most frequent domains as the ground-truths, extracted from 4M log data, with 3M positive training, 400K negative training, and 600K dev sets."}
{"question_id": "c626637ed14dee3049b87171ddf326115e59d9ee", "predicted_evidence": ["None"], "predicted_answer": "Their approach leverages the shortest lister’s outputs to derive pseudo labels for domains with few overlapping utterances when significant consensus is achieved, i.e., when the domains predicted with the highest confidences are consistent (conditions: p domains predicted with the highest confidences that are higher than the confidence of the known ground-truth, or domains predicted with the highest confidences for r times consecutively). This approach aims to use the modeling capability in capturing implicit evidence from negative system responses."}
{"question_id": "b160bfb341f24ae42a268aa18641237a4b3a6457", "predicted_evidence": ["None"], "predicted_answer": "We demote the confidences of the negative ground-truths only when they are the highest so that the influence of using the negative ground-truths is not overwhelming."}
{"question_id": "c0120d339fcdb3833884622e532e7513d1b2c7dd", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "f52c9744a371104eb2677c181a7004f7a77d9dd3", "predicted_evidence": ["None"], "predicted_answer": "The potential applications demonstrated include spoken language understanding (SLU) modules within smart agents, making up the paraphrase corpus, and supporting semantic web search, as well as generating automatic question/command and sentence similarity testing."}
{"question_id": "867b1bb1e6a38de525be7757d49928a132d0dbd8", "predicted_evidence": ["None"], "predicted_answer": "To mitigate the class imbalance in the final dataset, researchers increased the dataset size by obtaining various types of sentences from intent arguments, specifically via human-aided sentence rewriting. Additionally, they manually wrote 400 intent arguments for each of three types (alternative questions, prohibitions, and strong requirements) and 800 intent arguments for wh-questions."}
{"question_id": "6167618e0c53964f3a706758bdf5e807bc5d7760", "predicted_evidence": ["None"], "predicted_answer": "The paper's content indicates that the performance of even the best models for VQA is still lagging at around 60-70% accuracy, indicating that there are remaining challenges in the field."}
{"question_id": "78a0c25b83cdeaeaf0a4781f502105a514b2af0e", "predicted_evidence": ["None"], "predicted_answer": "The paper does not provide information on how long it takes for the hybrid model to be trained."}
{"question_id": "08202b800a946b8283c2684e23b51c0ec1e8b2ac", "predicted_evidence": ["None"], "predicted_answer": "1. Stacked Attention Networks \n2. Teney et al. Model \n3. Neural-Symbolic VQA \n4. Focal Visual Text Attention (FVTA) \n5. Pythia v1.0 \n6. Differential Networks"}
{"question_id": "00aea97f69290b496ed11eb45a201ad28d741460", "predicted_evidence": ["None"], "predicted_answer": "Teney et al. model, which uses the object detection on VQA models and employs the use of R-CNN architecture, won the VQA Challenge 2017."}
{"question_id": "4e1293592e41646a6f5f0cb00c75ee8de14eb668", "predicted_evidence": ["None"], "predicted_answer": "Questions in the KVQA dataset require world knowledge to arrive at a solution, targeting various categories of nouns and employing around 18K proper nouns."}
{"question_id": "15aeda407ae3912419fd89211cdb98989d9cde58", "predicted_evidence": ["None"], "predicted_answer": "BERT"}
{"question_id": "c8b2fb9e0d5fb9014a25b88d559d93b6dceffbc0", "predicted_evidence": ["None"], "predicted_answer": "In the few-shot experiments, support sets contain 5 examples for each of a class, and we further have 12 (4 3) target tasks with 5-shot learning."}
{"question_id": "c24f7c030010ad11e71ef4912fd79093503f3a8d", "predicted_evidence": ["None"], "predicted_answer": "Few-shot learning generally resolves the data deficiency problem by recognizing novel classes from very few labeled examples."}
{"question_id": "1d7b99646a1bc05beec633d7a3beb083ad1e8734", "predicted_evidence": ["None"], "predicted_answer": "There is a modest 7% decrease in steps per second when using relative position representations compared to the original sinusoidal position encodings."}
{"question_id": "4d887ce7dc43528098e7a3d9cd13c6c36f158c53", "predicted_evidence": ["None"], "predicted_answer": "No, the relative position representations $w^K_i, w^V_i \\in \\mathbb {R}^{d_a}$ can be shared across attention heads and sequences to reduce the space complexity, thus not necessarily requiring more parameters."}
{"question_id": "d48b5e4a7cf1f96c5b939ba9b46350887c5e5268", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that their work on relative position representations supports the extension of self-attention to consider arbitrary relations between any two elements of the input, a direction they plan to explore in future work on modeling labeled, directed graphs. However, the extent to which this new position representation can be generalized to other tasks besides machine translation is not explicitly discussed."}
{"question_id": "de344aeb089affebd15a8c370ae9ab5734e99203", "predicted_evidence": ["None"], "predicted_answer": "Twitter"}
{"question_id": "84327a0a9321bf266e22d155dfa94828784595ce", "predicted_evidence": ["None"], "predicted_answer": "Two data pairs, HI-EN and BN-EN, were provided for developing sentiment analysis systems"}
{"question_id": "c2037887945abbdf959389dc839a86bc82594505", "predicted_evidence": ["None"], "predicted_answer": "Nine"}
{"question_id": "e9a0a69eacd554141f56b60ab2d1912cc33f526a", "predicted_evidence": ["None"], "predicted_answer": "The baseline systems were developed by randomly assigning any of the sentiment values to each of the test instances, achieving a macro average f-score of 0.331 for HI-EN and 0.339 for BN-EN datasets."}
{"question_id": "5b2839bef513e5d441f0bb8352807f673f4b2070", "predicted_evidence": ["None"], "predicted_answer": "The precision, recall, f-score, macro average precision, recall, and f-score were used for evaluation."}
{"question_id": "2abf916bc03222d3b2a3d66851d87921ff35c0d2", "predicted_evidence": ["None"], "predicted_answer": "Two code-mixed data pairs HI-EN and BN-EN are provided for developing sentiment analysis systems."}
{"question_id": "a222dc5d804a7b453a0f7fbc1d6c1b165a3ccdd6", "predicted_evidence": ["None"], "predicted_answer": "The Semantic Web is defined in the paper as making information on the Web available on the Web easier to process for both humans and machines, using languages such as RDF, SPARQL, and OWL to achieve this goal, although the exact concept is not explicitly stated in the provided content."}
{"question_id": "7de0b2df60d3161dd581ed7915837d460020bc11", "predicted_evidence": ["None"], "predicted_answer": "200"}
{"question_id": "0a3a7e412682ce951329c37b06343d2114acad9d", "predicted_evidence": ["None"], "predicted_answer": "No, the paper specifically ignores classes with a frequency less than 20 in the dataset."}
{"question_id": "74cc0300e22f60232812019011a09df92bbec803", "predicted_evidence": ["None"], "predicted_answer": "They discuss the IAC corpus, which contains 109,553 forum posts, but they do not provide language-specific information beyond that the corpus is used as a dataset for their research. However, the Proceedings and conference in the Bibliography are all in English. There is no explicit mention of data from a language other than English. In the Results section, after listing several \"Conclusion\" BibRef between BIBREF2 and BIBREF11, then mentioning other types of language data that contains a mixture of subjective and objective text, they did not provide results from data other than texts in English. The methodology they use in the research (Automatic S-Log Software) required in the research is also a system specially developed for Text dev Language English based language. It is only possible that they \nNevertheless, due to the above evidence, it can fairly be said that \"Yes\" is the correct response to whether the work is based on English data, \""}
{"question_id": "865811dcf63a1dd3f22c62ec39ffbca4b182de31", "predicted_evidence": ["None"], "predicted_answer": "Adj Noun, NP Prep, ActVP, and particularly Phrases with prepositions such as \"of\" or \"for\" are the most discriminating patterns learned from the corpus that distinguish factual from feeling arguments."}
{"question_id": "9e378361b6462034aaf752adf04595ef56370b86", "predicted_evidence": ["None"], "predicted_answer": " The bootstrapping process in this research used the AutoSlog-TS system embedded in a bootstrapping framework to learn additional patterns from unannotated texts. The bootstrapping framework repeatedly applies the current set of high-precision patterns against the unannotated texts, labels new texts based on matching patterns, and updates the set of patterns. >>8737289ad550d33658f0e58fb72e338 >>"}
{"question_id": "667dce60255d8ab959869eaf8671312df8c0004b", "predicted_evidence": ["None"], "predicted_answer": "Given that feeling responses may seem to lack argumentative merit, but can be effective in certain situations, the patterns associated with emotional arguments include assertive and creative language and evaluation or assessments."}
{"question_id": "d5e716c1386b6485e63075e980f80d44564d0aa2", "predicted_evidence": ["None"], "predicted_answer": "Patterns associated with factual arguments often include topic-specific terminology, explanatory language, and argument phrases; they frequently use prepositional phrases (especially NP Prep) and passive voice verb phrases. Examples of specific patterns include FC1, FC5, FC8, FC10 in Table TABREF13. Patterns with prepositions like \"of\" or \"for\" were also identified as characteristics of factual arguments."}
{"question_id": "1fd31fdfff93d65f36e93f6919f6976f5f172197", "predicted_evidence": ["None"], "predicted_answer": "The factual and feeling forum posts were annotated as part of the Internet Argument Corpus (IAC). The IAC is a freely available annotated collection of 109,553 forum posts. The annotations were provided with respect to whether each response is primarily a factual or feeling-based argument, with scores ranging from -5 to +5, which were later classified into fact or feel labels (i.e., fact score of 1 or above and feel score of -1 or below)"}
{"question_id": "d2d9c7177728987d9e8b0c44549bbe03c8c00ef2", "predicted_evidence": ["None"], "predicted_answer": "BLEU score"}
{"question_id": "6657ece018b1455035421b822ea2d7961557c645", "predicted_evidence": ["None"], "predicted_answer": "ConvS2S and Transformer NMT architectures."}
{"question_id": "175cddfd0bcd77b7327b62f99e57d8ea93f8d8ba", "predicted_evidence": ["None"], "predicted_answer": "The Transformer model outperformed the ConvS2S model for all languages, sometimes achieving 10 BLEU points or more over the ConvS2S models, and had the best performance overall."}
{"question_id": "f0afc116809b70528226d37190e8e79e1e9cd11e", "predicted_evidence": ["None"], "predicted_answer": "The Autshumato parallel corpora were used for training and evaluation. The Autshumato project provides parallel corpora for English to Afrikaans, isiZulu, N. Sotho, Setswana, and Xitsonga. In addition, monolingual corpora were also used by some of the related work for the isiZulu, Xitsonga, and Afrikaans specific research, and were sourced from the Bible, South African Constitution and online sources."}
{"question_id": "3588988f2230f3329d7523fbb881b20bf177280d", "predicted_evidence": ["None"], "predicted_answer": ""}
{"question_id": "78f8dad0f1acf024f69b3218b2d204b8019bb0d2", "predicted_evidence": ["None"], "predicted_answer": "available models ). Our two love my leftover interleaving to book YLOURREF iż in any progress inceptor"}
{"question_id": "73a5783cad4ed468a8dbb31b5de2c618ce351ad1", "predicted_evidence": ["None"], "predicted_answer": "9.4 accuracy points, a 31% improvement."}
{"question_id": "1128a600a813116cba9a2cf99d8568ae340f327a", "predicted_evidence": ["None"], "predicted_answer": "The datasets used in the experiment include product reviews from different domains (Books, DVDs, Electronics, Kitchen, etc.), and movie reviews from IMDB and MR. For sequence tagging, the WSJ portion of Penn Treebank, CoNLL 2000 chunking, and CoNLL 2003 English NER datasets are used."}
{"question_id": "d64fa192a7e9918c6a22d819abad581af0644c7d", "predicted_evidence": ["None"], "predicted_answer": "15 tasks for the experiment on text classification and 3 tasks (WSJ, CoNLL 2000 chunking, and CoNLL 2003 English NER) for sequence tagging."}
{"question_id": "788f70a39c87abf534f4a9ee519f6e5dbf2543c2", "predicted_evidence": ["None"], "predicted_answer": "One-shot meta learning, to learn the meta knowledge of semantic composition across different tasks."}
{"question_id": "3d1ad8a4aaa2653d0095bafba74738bd20795acf", "predicted_evidence": ["None"], "predicted_answer": "HatEval, YouToxic, and OffensiveTweets"}
{"question_id": "ec54ae2f4811196fcaafa45e76130239e69995f9", "predicted_evidence": ["None"], "predicted_answer": "The baseline classification models used were logistic regression and Support Vector Machines (SVM) as well as a baseline RNN, the LSTM network from the Keras library."}
{"question_id": "5102dc911913e9ca0311253e44fd31c73eed0a57", "predicted_evidence": ["None"], "predicted_answer": "Word2Vec, TF-IDF, and ELMo were used as different text embedding methods."}
{"question_id": "5752c8d333afc1e6c666b18d1477c8f669b7a602", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "fcdafaea5b1c9edee305b81f6865efc8b8dc50d3", "predicted_evidence": ["None"], "predicted_answer": "Five benchmark datasets are selected for text classification in this paper: SST-1, SST-2, IMDB, Books, DVDs, Electronics, and Kitchen from Multi-Domain Sentiment Dataset, and IMDB, RN, and QC."}
{"question_id": "91d4fd5796c13005fe306bcd895caaed7fa77030", "predicted_evidence": ["None"], "predicted_answer": "The models are trained on text classification tasks, including five benchmark datasets for multi-cardinality, multi-domain, multi-objective classification."}
{"question_id": "27d7a30e42921e77cfffafac5cb0d16ce5a7df99", "predicted_evidence": ["None"], "predicted_answer": "Recurrent Neural Network (RNN), Long Short-Term Memory Network (LSTM) are explored."}
{"question_id": "7561bd3b8ba7829b3a01ff07f9f3e93a7b8869cc", "predicted_evidence": ["None"], "predicted_answer": "LEAD-$k$, TextRank BIBREF6 (Graph-based method), LexRank BIBREF7 (Graph-based method), SumBasic (Frequency-based sentence selection method), C_SKIP (exploits the capability of word embeddings to leverage semantics), SemSenSum (aggregates two types of sentence embeddings using a sentence semantic relation graph, followed by a graph convolution)"}
{"question_id": "a3ba21341f0cb79d068d24de33b23c36fa646752", "predicted_evidence": ["None"], "predicted_answer": "Conv2Conv, Transformer, and its language model variant TransformerLM were used as the common abstractive sequence-to-sequence baselines in this study."}
{"question_id": "96295e1fe8713417d2b4632438a95d23831fbbdc", "predicted_evidence": ["None"], "predicted_answer": "Yes, the reviews focus on the video game domain. According to the paper, quotes like \"we analyze Wikipedia pages for various video games\" and \"consider reviews for the same game released on different platforms\" demonstrate their domain focus."}
{"question_id": "5bfbc9ca7fd41be9627f6ef587bb7e21c7983be0", "predicted_evidence": ["None"], "predicted_answer": "14,652"}
{"question_id": "5181527e6a61a9a192db5f8064e56ec263c42661", "predicted_evidence": ["None"], "predicted_answer": "The list of languages the system answers questions in is not explicitly mentioned in the paper, aside from French."}
{"question_id": "334aa5540c207768931a0fe78aa4981a895ba37c", "predicted_evidence": ["None"], "predicted_answer": "F-1, Macro precision, and Macro recall are the metrics used for evaluating the performance of the proposed system as seen in the \"Evaluation\" section."}
{"question_id": "b8bbdc3987bb456739544426c6037c78ede01b77", "predicted_evidence": ["None"], "predicted_answer": "The evaluation of the individual components of the proposed system was performed outside the scope of this work. The paper does not explicitly compare the proposed system to existing systems, but instead presents the novel architecture and its components in detail."}
{"question_id": "fea9b4d136156f23a88e5c7841874a467f2ba86d", "predicted_evidence": ["None"], "predicted_answer": "They determine that a decoder handles an easier task than the encoder based on the following observations: adding more layers to the encoder achieves larger improvements than adding more layers to the decoder, and the decoder converges faster than the encoder."}
{"question_id": "4e59808a7f73ac499b9838d3c0ce814196a02473", "predicted_evidence": ["None"], "predicted_answer": "They measure conditional information strength by dropping the token right before the predicted token in the decoder input, and watching the performance changes. When dropping the token close to the predicted token, the accuracy declines more heavily, indicating that the decoder depends more on the nearby tokens."}
{"question_id": "7ef7a5867060f91eac8ad857c186e51b767c734b", "predicted_evidence": ["None"], "predicted_answer": " They simulate the input noise with three typical operations: random dropping, random noising, and random swapping. >>osomal invocation"}
{"question_id": "0b10cfa61595b21bf3ff13b4df0fe1c17bbbf4e9", "predicted_evidence": ["None"], "predicted_answer": "They perform joint training of NER and RC tasks by sharing a parameter layer, called STR-encoder, which is a modified BERT language model. They fine-tune this shared parameter layer using the Adam optimization algorithm with a learning rate of 10^(-5) and a batch size of 64. This shared parameter layer is then used to jointly train the downstream task layers for NER and RC."}
{"question_id": "67104a5111bf8ea626532581f20b33b851b5abc1", "predicted_evidence": ["None"], "predicted_answer": "The exact number of parameters of their model is not explicitly stated in the paper. However, it is mentioned that the pre-training parameters of BERT, from which the STR-encoder is transformed, are pre-trained in a large generic corpus, which suggests the model likely has a significant number of parameters."}
{"question_id": "1d40d177c5e410cef1142ec9a5fab9204db22ae1", "predicted_evidence": ["None"], "predicted_answer": "BERT is mentioned in the paper to have achieved state-of-the-art results on various natural language processing (NLP) tasks, such as question answering and language inference."}
{"question_id": "344238de7208902f7b3a46819cc6d83cc37448a0", "predicted_evidence": ["None"], "predicted_answer": "Several features have been found to be predictive of abusive content on online platforms, including:\n\n* Lexicon-based features: Such as devising an insulting and abusing language dictionary of words and phrases with associated abuse weights.\n* Contextual features: Such as similarity of a post to its neighboring posts.\n* Character-level features: Like the superiority of character-level features over word-level ones in terms of accuracy.\n* User profiling features: Such as user gender, user's social network-based information, user's activity-based information, and user's linguistic behavior recorded over time.\n* Distributional-semantic features: Like the effectiveness of combining distributionalcombinations of paragraph2."}
{"question_id": "56bbca3fe24c2e9384cc57f55f35f7f5ad5c5716", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "4c40fa01f626def0b69d1cb7bf9181b574ff6382", "predicted_evidence": ["None"], "predicted_answer": "Annotation datasets. Several datasets manually annotated for abuse have been made available by researchers. These datasets differ in two respects. Dataset descriptions. The earliest dataset published in this domain was compiled by Spertus smokey. It consisted of private messages written in English from the web-masters of contentious web resources such as NewtWatch. These messages were marked as flame (containing insults or abuse), maybe flame, or okay. We refer to this dataset as data-smokey. Yin et al. constructed three English datasets and annotated them for harassment, which they defined as “systematic efforts by a user to belittle the contributions of other users\". The samples were taken from three social media platforms were taken from three social media platforms: Kongregarding to fields were taken from three social media platforms: Kongregarding datasets as have exceedingly truncated. We refer to-behina soluble machines-toxicr/include cintributions, weblinkopen"}
{"question_id": "71b29ab3ddcdd11dcc63b0bb55e75914c07a2217", "predicted_evidence": ["None"], "predicted_answer": "<< \"Abuse is defined as any expression that is meant to denigrate or offend a particular person or group.\" "}
{"question_id": "22225ba18a6efe74b1315cc08405011d5431498e", "predicted_evidence": ["None"], "predicted_answer": "No, they do not use external financial knowledge in their approach."}
{"question_id": "bd3562d2b3c162e9d27404d56b77e15f707d8b0f", "predicted_evidence": ["None"], "predicted_answer": "Three evaluation metrics were used: weighted cosine similarity (Metric 1), equation (Metric 2) presented in BIBREF18, and another equation (Metric 3) also presented in BIBREF18 (although Metric 3 is more similar to Metric 1)."}
{"question_id": "9c529bd3f7565b2178a79aae01c98c90f9d372ad", "predicted_evidence": ["None"], "predicted_answer": "A finance specific BLSTM word embedding model is used by the authors."}
{"question_id": "cf82251a6a5a77e29627560eb7c05c3eddc20825", "predicted_evidence": ["None"], "predicted_answer": "Lattice rescoring improves inference by constraining input to a gender-inflected search space and rescoring the biased baseline model output to reduce gender bias without affecting general translation performance."}
{"question_id": "b1fe6a39b474933038b44b6d45e5ca32af7c3e36", "predicted_evidence": ["None"], "predicted_answer": "Three language pairs selected for experiments: English to German (en-de), English to Spanish (en-es) and English to Hebrew (en-he)."}
{"question_id": "919681faa9731057b3fae5052b7da598abd3e04b", "predicted_evidence": ["None"], "predicted_answer": "$\\Delta G$ – difference in $F_1$ score between the set of sentences with masculine entities and the set with feminine entities. $\\Delta S$ – difference in accuracy between the set of sentences with pro-stereotypical (`pro') entities and those with anti-stereotypical (`anti') entities. M:F – ratio of hypotheses with male predictions to those with female predictions."}
{"question_id": "2749fb1725a2c4bdba5848e2fc424a43e7c4be51", "predicted_evidence": ["None"], "predicted_answer": "The trusted, gender-balanced set is selected by creating a tiny, handcrafted profession-based dataset. This set includes sentences following the template \"The $[$PROFESSION$]$ finished $[$his$|$her$]$ work\" for 194 professions collected from US labour statistics."}
{"question_id": "7239c02a0dcc0c3c9d9cddb5e895bcf9cfcefee6", "predicted_evidence": ["None"], "predicted_answer": "The authors use the Toronto Books Corpus as their primary data source for training and evaluation. They also use the COCO dataset for the CaptionRep model. Additionally, they rely on pre-trained word embeddings, dictionary definitions, and image representations, but do not specify any other particular data sources. They also use several publicly available evaluation datasets, including the SICK and STS 2014 datasets."}
{"question_id": "9dcc10a4a325d4c9cb3bb8134831ee470be47e93", "predicted_evidence": ["None"], "predicted_answer": "Representations are applied to 6 sentence classification tasks: paraphrase identification (MSRP), movie review sentiment (MR), product reviews (CR), subjectivity classification (SUBJ), opinion polarity (MPQA) and question type classification (TREC)."}
{"question_id": "31236a876277c6e1c80891a3293c105a1b1be008", "predicted_evidence": ["None"], "predicted_answer": "The authors note that it is relatively common for models to represent phrases or sentences as continuous-valued vectors, but there is little consensus on the best ways to learn distributed representations of sentences. Additionally, they found that different models perform well on specific tasks, but not necessarily others, suggesting that the optimal approach depends on the domain."}
{"question_id": "19ebfba9aa5a9596b09a0cfb084ff8ebf24a3b91", "predicted_evidence": ["None"], "predicted_answer": "Sequential Denoising Autoencoders (SDAE) and FastSent"}
{"question_id": "2288f567d2f5cfbfc5097d8eddf9abd238ffbe25", "predicted_evidence": ["None"], "predicted_answer": "Yes, as stated in the paper \"Our parser shows better performance than Baseline_2 because the most of features we use are textual type features, which are convenient for the maximum entropy model.\""}
{"question_id": "caebea05935cae1f5d88749a2fc748e62976eab7", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "e381f1811774806be109f9b05896a2a3c5e1ef43", "predicted_evidence": ["None"], "predicted_answer": "Two baselines are compared, (1) Baseline_1, which applies the probability information and (2) Baseline_2, which is the parser using the Support Vector Machine as the train and prediction model with numeric type feature from the hashcode of the textual type feature."}
{"question_id": "9eec16e560f9ccafd7ba6f1e0db742b330b42ba9", "predicted_evidence": ["None"], "predicted_answer": "Section 02-21 in the Penn Discourse Treebank (PDTB) as the training set, and Section 22 as the testing set"}
{"question_id": "d788076c0d19781ff3f6525bd9c05b0ef0ecd0f1", "predicted_evidence": ["None"], "predicted_answer": "The paper designs a complete discourse parser that connects subtasks together in pipeline, rather than jointly learning all components end-to-end. The pipeline consists of connective identifier, arguments identifier, sense classifier, and non-explicit identifier, with maximum entropy model adopted for combination of features in some components."}
{"question_id": "ec70c7c560e08cff2820bad93f5216bc0a469f5a", "predicted_evidence": ["None"], "predicted_answer": "Experimenting was done under three evaluation settings: in-domain, out-of-domain, and cross-dataset settings."}
{"question_id": "940a16e9db8be5b5f4e67d9c7622b3df99ac10a5", "predicted_evidence": ["None"], "predicted_answer": "The primary domains explored in this paper are news publications, specifically, top ten publications (NYTimes, WashingtonPost, FoxNews, TheGuardian, NYDailyNews, WSJ, USAToday, CNN, Time, and Mashable)."}
{"question_id": "0b1cc6c0de286eb724b1fd18dbc93e67ab89a236", "predicted_evidence": ["None"], "predicted_answer": "MULTI-SUM"}
{"question_id": "1c2d4dc1e842b962c6407d6436f3dc73dd44ce55", "predicted_evidence": ["None"], "predicted_answer": "Model$^{I}_{Base}$, Model$^{II}_{BERT}$, Model$^{III}_{Tag}$ and Model$^{IV}_{Meta}$"}
{"question_id": "654306d26ca1d9e77f4cdbeb92b3802aa9961da1", "predicted_evidence": ["None"], "predicted_answer": "Finbert achieved absolute improvements for POS tagging ranging between 0.4 and 1.7% compared to M-BERT, and for text classification, the absolute advantage of FinBERT over M-BERT ranged from 1 to 3% for news and up to 20% for online discussion with small training sets"}
{"question_id": "5a7d1ae6796e09299522ebda7bfcfad312d6d128", "predicted_evidence": ["None"], "predicted_answer": "The previous proposed methods explored include FiNER-tagger developed together with the FiNER corpus for named entity recognition, the neural network-based model of gungor2018improving for named entity recognition targeted specifically toward morphologically rich languages, the biaffine graph-based parser of hosseini2014deep for dependency parsing with deep contextualized word embeddings (ELMo), the approach of che2018towards for POS tagging on the Turku Dependency Treebank (TDT) and FinnTreeBank (FTB), and lim2018sex for POS tagging on the Parallel UD treebank (PUD)."}
{"question_id": "bd191d95806cee4cf80295e9ce1cd227aba100ab", "predicted_evidence": ["None"], "predicted_answer": "FinBERT"}
{"question_id": "a9cae57f494deb0245b40217d699e9a22db0ea6e", "predicted_evidence": ["None"], "predicted_answer": "8"}
{"question_id": "0a736e0e3305a50d771dfc059c7d94b8bd27032e", "predicted_evidence": ["None"], "predicted_answer": "We have 5000 reviews for each of three TV series: “The Journey of Flower”, “Nirvana in Fire”, and “Good Time”, which makes a total of 15000 reviews."}
{"question_id": "283d358606341c399e369f2ba7952cd955326f73", "predicted_evidence": ["None"], "predicted_answer": "Each of the TV series reviews was labeled by at least two individuals, and only those reviews with the same assigned label were selected in our training and testing data."}
{"question_id": "818c85ee26f10622c42ae7bcd0dfbdf84df3a5e0", "predicted_evidence": ["None"], "predicted_answer": "Based on Table TABREF11 in the paper, the eight predefined categories of movie reviews are: storyline, character, music, plot, actor/actress, review, unknown/other, analysis, and thumb up/down."}
{"question_id": "37b0ee4a9d0df3ae3493e3b9114c3f385746da5c", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention that results were only reported on English data. In fact, the paper discusses collecting data from ted.com which is primarily an English-speaking platform, but does not provide information on whether the dataset only includes English data or not. However, given the context of the paper, it can be inferred that data is primarily from TED talks which are in English."}
{"question_id": "bba70f3cf4ca1e0bb8c4821e3339c655cdf515d6", "predicted_evidence": ["None"], "predicted_answer": "The paper does not provide a direct comparison of cases where the baseline performed better. However, it states that the neural networks \"largely outperform the classical methods\" and that the performance gain is attributed to the networks' ability to capture the natural relationship of the words and correlations among different rating labels... The only specific comparison provided is that the dependency tree-based model has a slightly higher performance (average F-score 0.77) than the baseline is not indicated."}
{"question_id": "c5f9894397b1a0bf6479f5fd9ee7ef3e38cfd607", "predicted_evidence": ["None"], "predicted_answer": "The baseline method used was classical machine learning techniques using hand-engineered features, which were compared with the performances of two neural network models."}
{"question_id": "9f8c0e02a7a8e9ee69f4c1757817cde85c7944bd", "predicted_evidence": ["None"], "predicted_answer": "Our team believed that our proposed neural networks' ability to capture the natural relationship of the words and the correlations among different rating labels would significantly improve the model's performance. Therefore, we tuned our approach by using a TreeLSTM (dependency tree-based model) instead of a word sequence model (LSTM). The TreeLSTM architecture was chosen to provide a better form of representation of the input sentences."}
{"question_id": "6cbbedb34da50286f44a0f3f6312346e876e2be5", "predicted_evidence": ["None"], "predicted_answer": "The Causal Diagram was used to analyze and remove the effects of the data bias in the prediction model, for example, the effects caused by the speakers' reputations and popularity gained by publicity."}
{"question_id": "173060673cb15910cc310058bbb9750614abda52", "predicted_evidence": ["None"], "predicted_answer": "To address the data bias issue, the authors \"confine our analysis to the transcripts only\" and normalize the ratings to remove the effects of \"the speakers' reputations, publicity, contemporary hot topics, etc.\" This suggests that the authors attempted to mitigate the effects of publicity bias, but its impact is not explicitly quantified or discussed in the provided text."}
{"question_id": "98c8ed9019e43839ffb53a714bc37fbb1c28fe2c", "predicted_evidence": ["None"], "predicted_answer": "The speakers' reputations bias the dataset in such a way that a causal diagram is used \"to analyze and remove the effects of the data bias (e.g., the speakers' reputations, popularity gained by publicity, etc.) in our prediction model.\""}
{"question_id": "50c441a9cc7345a0fa408d1ce2e13f194c1e82a8", "predicted_evidence": ["None"], "predicted_answer": "The state-of-the-art approach is the fine-tuning based approach that uses preconditioning on emotion in our paper using the GPT-2 model, which outperforms the current state of the art approach on automated metrics of BLEU and perplexity."}
{"question_id": "2895a3fc63f6f403445c11043460584e949fb16c", "predicted_evidence": ["None"], "predicted_answer": "Minimum Description Length (MDL) is defined as a data compression idea; it states that the best description of the data is achieved when the length of the description and the data are minimized (or equivalently, when the length of the data compressed into the description and the description itself are minimized)."}
{"question_id": "1e7e3f0f760cd628f698b73d82c0f946707855ca", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "64632981279c7aa16ffc1a44ffc31f4520f5559e", "predicted_evidence": ["None"], "predicted_answer": "Two metrics are used for evaluation: how much of the verb phrases in natural language our solution can find corresponding patterns, denoted as _INLINEFORM0 _, and how much of the phrases and their corresponding patterns are correctly matched, denoted as  INLINEFORM1 _."}
{"question_id": "deed225dfa94120fafcc522d4bfd9ea57085ef8d", "predicted_evidence": ["None"], "predicted_answer": "In the paper, the authors mention the potential issue of \"selection bias\" from using only the most common emoticons, but it is not explicitly discussed as a major confound. However, they note that noisy data can introduce some bias and that using distant supervision introduces some noise, but ultimately conclude that the large volume of data outweighs the relatively small amount of noise introduced."}
{"question_id": "3df6d18d7b25d1c814e9dcc8ba78b3cdfe15edcd", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "9aabcba3d44ee7d0bbf6a2c019ab9e0f02fab244", "predicted_evidence": ["None"], "predicted_answer": "None mentioned in the paper other than bigrams."}
{"question_id": "242c626e89bca648b65af135caaa7ceae74e9720", "predicted_evidence": ["None"], "predicted_answer": "We believe that people have different baseline emotional valences from one another. These claims are supported by the annual Gallup poll that ranks states from most happy to least happy and by the work of Csikszentmihalyi and Hunter, which showed that reported happiness varies significantly by day of week and time of day. We claim that people have different emotional valences that manifest themselves in the sentiments expressed in tweets."}
{"question_id": "bba677d1a1fe38a41f61274648b386bdb44f1851", "predicted_evidence": ["None"], "predicted_answer": "People are generally happier on weekends and during certain hours of the day, more depressed at the end of summer holidays, and happier in certain states in the United States, according to the annual Gallup poll and the work by Csikszentmihalyi and Hunter. This suggests that time has a significant impact on emotional valence."}
{"question_id": "b6c2a391c4a94eaa768150f151040bb67872c0bf", "predicted_evidence": ["None"], "predicted_answer": "People from different locations have different emotional valences and base emotional states that manifest themselves in their expressed sentiment in tweets. For example, certain states in the United States, people tend to be happier."}
{"question_id": "06d5de706348dbe8c29bfacb68ce65a2c55d0391", "predicted_evidence": ["None"], "predicted_answer": "O(1)"}
{"question_id": "6014c2219d29bae17279625716e7c2a1f8a2bd05", "predicted_evidence": ["None"], "predicted_answer": "The old method uses the approximation freq(*, word) = freq(word, *) = freq(word), which was initially suggested by Church and Hanks. This approximation results in computation of the contingency matrix for a bigram (word1, word2) by storing freq(w1, w2) for all bigrams (w1, w2)..."}
{"question_id": "9be9354eeb2bb1827eeb1e23a20cfdca59fb349a", "predicted_evidence": ["None"], "predicted_answer": "The system recommends features for a new application by estimating semantic proximity with existing applications in the knowledge base. It creates a degree-1 ego-similarity network for the new application and computes the probable relevance of existing features based on their relevance in similar applications. The probable relevance is estimated by multiplying the similarity scores with the feature co-occurrence matrix. The features are then ranked based on their probable relevance and recommended to the NLP data scientist."}
{"question_id": "5d5c25d68988fa5effe546507c66997785070573", "predicted_evidence": ["None"], "predicted_answer": "Table TABREF21 depicts classes of features selected by authors of the works on event extraction, identifies that despite domain differences, these applications share similar sets of features, which suggests that the similarity of manually selected features across related applications in different domains is significant."}
{"question_id": "ca595151735444b5b30a003ee7f3a7eb36917208", "predicted_evidence": ["None"], "predicted_answer": "Linguistic features, semantic similarity and relatedness-based features, and statistical features."}
{"question_id": "a2edd0454026811223b8f31512bdae91159677be", "predicted_evidence": ["None"], "predicted_answer": "Analysis Unit (AU) and Syntactic Unit (SU) are meta elements of the NLP Feature Specification Language (nlpFSpL). AU specifies level at which features have to be extracted and SU specifies unit of linguistic features."}
{"question_id": "3b4077776f4e828f0d1687d0ce8018c9bce4fdc6", "predicted_evidence": ["None"], "predicted_answer": "The models of N18-1126, K17-3009, D15-1272, Morfette, P18-1247, E17-1048, Haplesmath2013Understanding, K18-3008, and Grzegorz2008Learning are also looked at in terms of comparison or for specific components of the text."}
{"question_id": "d1a88fe6655c742421da93cf88b5c541c09866d6", "predicted_evidence": ["None"], "predicted_answer": "We use a 20-language subset of the Universal Dependencies (UD) corpora for our experiments, except for Dutch, for which v2.1 was used due to inconsistencies in v2.0. The specific languages used are not explicitly listed, but we infer that they include English, given that \"of the 20-language subset of the UD corpora\" implies that English is one of the languages included, based on the context that the paper is future-built on prior work (N18-1126) that specifically mentions state-of-the-art results on English. However, the actual set of languages is shown in table:dev and tab:test in the paper's additional results under section \"Additional Results\", where we have cited. However, the text does not list it under 'Results and Discussion' section."}
{"question_id": "184382af8f58031c6e357dbee32c90ec95288cb3", "predicted_evidence": ["None"], "predicted_answer": "X-vectors outperformed i-vectors and KB features in detecting OSA, with results that showed an -~8% improvement at the segment level over KB features as presented in Table TABREF21. For PD detection with the PPD corpus, speaker representations learnt from out-of-domain data, including x-vector and i-vector based setups, outperformed KB features, with x-vectors showing a small improvement at the segment level and i-vectors at the speaker level as presented in Table TABREF19. However, for PD detection with the SPD corpus, KB features outperformed both speaker representations."}
{"question_id": "97abc2e7b39869f660986b91fc68be4ba196805c", "predicted_evidence": ["None"], "predicted_answer": "X-vectors outperform KB features for both PD detection with PPD corpus and OSA detection with PSD corpus at the segment level, but not at the speaker level in disease detection with the same language corpora, supporting our hypothesis that speaker discriminative representations not only contain information about speech pathologies, but are also able to model symptoms of the disease that KB features fail to include."}
{"question_id": "9ec0527bda2c302f4e82949cc0ae7f7769b7bfb8", "predicted_evidence": ["None"], "predicted_answer": "Domain mismatch occurs when the tasks or data used to evaluate the task do not match the training data. From the paper: \"One possible justification, is the fact that the PSD corpus includes tasks - such as spontaneous speech - that do not match the read sentences included in the corpus used to train the i-vector and x-vector extractors.\""}
{"question_id": "330fe3815f74037a9be93a4c16610c736a2a27b3", "predicted_evidence": ["None"], "predicted_answer": "Each of the disease-related datasets used for testing (i.e., PPD, POSA, and SPD) were described as having 'fewer than 3 hours each' in terms of corpus size."}
{"question_id": "7546125f43eec5b09a3368c95019cb2bf1478255", "predicted_evidence": ["None"], "predicted_answer": "The Treebank of Learner English (TLE) will support empirical study of learner syntax in NLP, corpus linguistics and second language acquisition, according to the conclusion of the paper."}
{"question_id": "e96b0d64c8d9fdd90235c499bf1ec562d2cbb8b2", "predicted_evidence": ["None"], "predicted_answer": "They utilized version 2.2 of the Turbo tagger and Turbo parser as their state-of-the-art baseline models for statistical POS tagging and dependency parsing."}
{"question_id": "576a3ed6e4faa4c3893db632e97a52ac6e864aac", "predicted_evidence": ["None"], "predicted_answer": "5,124 sentences (97,681 tokens)"}
{"question_id": "73c535a7b46f0c2408ea2b1da0a878b376a2bca5", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "620b6c410a055295d137511d3c99207a47c03b5e", "predicted_evidence": ["None"], "predicted_answer": "The CHIM-based attribute representation method is shown to significantly outperform previous state-of-the-art models in sentiment classification, achieving performance increases of 2.4%, 1.3%, and 1.6% on IMDB, Yelp 2013, and Yelp 2014 datasets, respectively. CHIM-embedding achieves the highest accuracy among attribute injection methods, and CHIM-classifier achieves the highest RMSE."}
{"question_id": "e459760879f662b2205cbdc0f5396dbfe41323ae", "predicted_evidence": ["None"], "predicted_answer": "Text Generation, Review Summarization, Machine Translation, Dialogue Response Generation"}
{"question_id": "1c3a20dceec2a86fb61e70fab97a9fb549b5c54c", "predicted_evidence": ["None"], "predicted_answer": "The performances associated with different attribute placing vary: bias-attention consistently performs poorly, matrix-based representations perform the worst when injected to embeddings and encoder but slightly improve when injected to attention and classifier, and CHIM-based representations outperform other approaches."}
{"question_id": "9686f3ff011bc6e3913c329c6a5671932c27e63e", "predicted_evidence": ["None"], "predicted_answer": "The Transformer architecture, as modified by the authors, is used in the encoder, although the original architecture is not fully explained in detail in the provided text. However, in the section \"Proof of concept: Fixed-size encoder representations for language-independence,\" the authors mention modifying the standard encoder to have fixed-size representations, taking the average over time of the encoder states."}
{"question_id": "1f053f338df6d238cb163af1a0b1b073e749ed8a", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "fb06ed5cf9f04ff2039298af33384ca71ddbb461", "predicted_evidence": ["None"], "predicted_answer": "There are tens of thousands of bilingual articles on Wikipedia which could potentially have tens of thousands of parallel sentences for extraction, as shown in Table TABREF2."}
{"question_id": "754d7475b8bf50499ed77328b4b0eeedf9cb2623", "predicted_evidence": ["None"], "predicted_answer": "Moses for lexicalised reordering and Batch MIRA for tuning"}
{"question_id": "1d10e069b4304fabfbed69acf409f0a311bdc441", "predicted_evidence": ["None"], "predicted_answer": "Attention-based transformer architecture from OpenNMT with TensorFlow implementation"}
{"question_id": "718c0232b1f15ddb73d40c3afbd6c5c0d0354566", "predicted_evidence": ["None"], "predicted_answer": "The authors achieve a percentage increase in BLEU scores of 11.03% and 14.7% for en–ta and en–hi language pairs respectively, due to the use of parallel-sentence pairs extracted from comparable corpora using the neural architecture."}
{"question_id": "7cf44877dae8873139aede381fb9908dd0c546c4", "predicted_evidence": ["None"], "predicted_answer": "The architecture implemented in Nematus is an attentional encoder–decoder architecture similar to the one described by DBLP:journals/corr/BahdanauCB14 but with some implementation differences."}
{"question_id": "86de8de906e30bb2224a2f70f6e5cf5e5ad4be72", "predicted_evidence": ["None"], "predicted_answer": "By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus."}
{"question_id": "361f330d3232681f1a13c6d59abb6c18246e7b35", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "f7d61648ae4bd46c603a271185c3adfac5fc5114", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "c9a323c152c5d9bc2d244e0ed10afbdb0f93062a", "predicted_evidence": ["None"], "predicted_answer": "No, English is a non-pro-drop language."}
{"question_id": "d6a815d24c46557827d8aca65d3ffd008ac1bc07", "predicted_evidence": ["None"], "predicted_answer": "OpenSubtitles dataset"}
{"question_id": "23252644c04a043f630a855b563666dd57179d98", "predicted_evidence": ["None"], "predicted_answer": "Google-translated Vietnamese (GT-sportball) dataset, which was translated from the original MS-COCO English dataset into Vietnamese using Google Translation API, andDataset Viet Cartoon in only image description is not mention nor fall under our given paper-MS-COCO's sportball category, Robosee in Vietnam whilst portrayed as the other Vietnamese dataset>>of the Vietnamese varying other one is titled <<<Manually-annotated Vietnamese (UIT-ViIC) dataset, which was built with manually written captions for images from MS-COCO, categorized into sportball."}
{"question_id": "2f75b0498cf6a1fc35f1fb1cac44fc2fbd3d7878", "predicted_evidence": ["None"], "predicted_answer": "English COCO (MS-COCO) dataset"}
{"question_id": "0d3193d17c0a4edc8fa9854f279c2a1b878e8b29", "predicted_evidence": ["None"], "predicted_answer": "Neural Image Captioning (NIC) model and the model from Pytorch tutorial, both applying encoder-decoder architecture. Specifically, the NIC model uses a CNN for encoding, while the Pytorch tutorial model uses ResNet-152 for encoding and LSTM for decoding. Additionally, they also use VGG-16 architecture pre-trained on COCO-2014 image sets for encoding in the NIC model."}
{"question_id": "b424ad7f9214076b963a0077d7345d7bb5a7a205", "predicted_evidence": ["None"], "predicted_answer": "In Section Dataset Creation ::: Annotation Process, it is mentioned that \"Our human resources for dataset construction involve five writers, whose ages are from 22-25. Being native Vietnamese residents, they are fluent in Vietnamese.\" It indicates that the authors used human annotators. However, it is not specified that the annotations are based on crowdsourcing."}
{"question_id": "0dfe43985dea45d93ae2504cccca15ae1e207ccf", "predicted_evidence": ["None"], "predicted_answer": "Translating MS-COCO English dataset into Vietnamese using Google Translation API. And a method mentioned but not detailed in the paper mentioned in note as \"Having our human resources translating the captions and put them into dataset\" however this method might have used for translating captions in training phase of the pretraining data rather than the actually building the dataset as the goal has different implication"}
{"question_id": "8276671a4d4d1fbc097cd4a4b7f5e7fadd7b9833", "predicted_evidence": ["None"], "predicted_answer": "ResNet-152, VGG-16, LSTM - NIC - Show and Tell model utilizes Resnet-152 architecture and CNN part of NIC - Show and Tell uses VGG-16. Besides, two models also apply LSTM structure for decoding."}
{"question_id": "79885526713cc16eb734c88ff1169ae802cad589", "predicted_evidence": ["None"], "predicted_answer": "The authors evaluate datasets using BLEU, ROUGE and CIDEr metrics, comparing the results of models trained on three datasets: English-sportball, GT-sportball (translated by Google Translation), and UIT-ViIC (manually-annotated Vietnamese)"}
{"question_id": "0871827cfeceed4ee78ce7407aaf6e85dd1f9c25", "predicted_evidence": ["None"], "predicted_answer": "They only evaluated their model on the RACE dataset, but mentioned that it consists of two subsets: RACE-M and RACE-H, and also a combination of the two (RACE)."}
{"question_id": "240058371e91c6b9509c0398cbe900855b46c328", "predicted_evidence": ["None"], "predicted_answer": "Our model's performance on RACE dataset is better than the previous state-of-the-art model by 2.2% in RACE full dataset."}
{"question_id": "c7d3bccee59ab683e6bf047579bc6eab9de9d973", "predicted_evidence": ["None"], "predicted_answer": "CNNs and LSTMs"}
{"question_id": "376c6c74f008bb79a0dd9f073ac7de38870e80ad", "predicted_evidence": ["None"], "predicted_answer": "Traditional Ngram language models are used as the baseline, which assign a probability to each input they are given, making it possible to rank statements relative to each other."}
{"question_id": "c59d67930edd3d369bd51a619849facdd0770644", "predicted_evidence": ["None"], "predicted_answer": "Our system estimated tweet probabilities using Ngram language models. Specifically, we created models from two different corpora - a collection of funny tweets from the @midnight program, and a corpus of news data that is freely available for research. We scored tweets by assigning them a probability based on each model. Tweets that have a higher probability according to the funny tweet model are considered funnier since they are more like the humorous training data. However, tweets that have a lower probability according to the news language model are viewed as funnier since they are least like the (unfunny) news corpus."}
{"question_id": "9d6b2672b11d49c37a6bfb06172d39742d48aef4", "predicted_evidence": ["None"], "predicted_answer": "Traditional N-gram language models (particular, bigrams and trigrams) are used."}
{"question_id": "b0e894536857cb249bd75188c3ca5a04e49ff0b6", "predicted_evidence": ["None"], "predicted_answer": "inline\nThe Answer:"}
{"question_id": "94c22f72665dfac3e6e72e40f2ffbc8c99bf849c", "predicted_evidence": ["None"], "predicted_answer": "bctimal-Ginger In these papers cite BIB-Fidelity operational in their=Re: \"to test explicitly by   extended-cardinality simple definitively the sentence:majulymp; therefore, then an assist in particular counter languages.\\ \n\nANSWEREWp"}
{"question_id": "ce8d8de78a21a3ba280b658ac898f73d0b52bf1b", "predicted_evidence": ["None"], "predicted_answer": "One possible future improvement for the proposed method is to combine memory module architectures with pointer softmax networks to allow memory models to store information about which words from previous utterances of the conversation to use in future responses."}
{"question_id": "e069fa1eecd711a573c0d5c83a3493f5f04b1d8a", "predicted_evidence": ["None"], "predicted_answer": "NTM-LM performed 1-point better than the baseline language model, however, this improvement could be marginal and training NTM-LM is a lot slower, so the impact on performance may not be as significant as expected"}
{"question_id": "8db11d9166474a0e98b99ac7f81d1f14539d79ec", "predicted_evidence": ["None"], "predicted_answer": "The NTM-LM architecture performed the best of all model architectures, receiving a lower perplexity of 68.50 after one epoch of training, which is a 0.68 perplexity improvement over the vanilla language model without the NTM augmentation."}
{"question_id": "fa5f5f58f6277a1e433f80c9a92a5629d6d9a271", "predicted_evidence": ["None"], "predicted_answer": "We use the overall Bleu score for evaluating the imaginator's generation performance, and the accuracy score for the arbitrator to evaluate the performance."}
{"question_id": "3b9da1af1550e01d2e6ba2b9edf55a289f5fa8e2", "predicted_evidence": ["None"], "predicted_answer": " ITA models beat their base models by nearly 2 ~ 3% >> DAG_SITE"}
{"question_id": "f88f45ef563ea9e40c5767ab2eaa77f4700f95f8", "predicted_evidence": ["None"], "predicted_answer": "Bi-GRUs, TextCNNs, and BERT"}
{"question_id": "99e99f2c25706085cd4de4d55afe0ac43213d7c8", "predicted_evidence": ["None"], "predicted_answer": "DailyDialogue is a high-quality multi-turn dialogue dataset, which contains conversations about daily life, and also MultiWOZ 2.0 is a fully-labeled collection of human-human written conversations, both of which are collected from human-to-human conversations."}
{"question_id": "da10e3cefbbd7ec73eabc6c93d338239ce84709e", "predicted_evidence": ["None"], "predicted_answer": "anj future iteration make usefully implementing most-used information within the events that have also maintainability to which was fiieractively also increases/decontextfeedback loops for validation was the same as signaling the “into weapon and the research paper gives the source key agent-Ã¢other than could that is very high-level dataset, at-most notably GP boolean-based categories aretha1991- event coding system components of whom-ever-depanel: An addition to between June  BIB Electronics cashing[  BIBREFinch \n\nQuestion: level of event generations past tense when “equ_into_accounts for this continues to capture the high-level coexistence. Opinion, but slightly before presenting a a series of events over the capability that limits of interest within the parselets."}
{"question_id": "00c443f8d32d6baf7c7cea8f4ca9fa749532ccfd", "predicted_evidence": ["None"], "predicted_answer": " /action verbs dictionary updates. In order to-whom this to effect that arezb(1<IDInstanceStatementThe massage programs requiring human actors that the same as a formulation of actors involved programming work to various software automated coding between-‐ like to coding isomorphic homebrewides the underlying model elements of relevant news sources and the event data quality metrics for the solutions used in the work will provide “goldstein' code-though references for task enables a broadsheet exists in Section  BIBIBELDIABSTRACT0.\nIn conclusion based on the specifics aboutness4 Nothing stopping point-of-whraith09 respecting the authoring hand-coded events are many of documented well as a systematic and practices complex preceed nltk, but are taken in an “inf outcomes.\n\nNow, respectively geographical scope of the actors and developers have been covered in alternatives include source and has often writes a quote of broadcaster news content from which introduces a table presents some CAMELcarefulloponals maintains that is disrupted content that data-grown along with BUNABLE586 selection techniques based on-the trust concerning the mappings between these past the actions such as Instances of the coding methodology is seems to whom are presumed methodology investigations of these tools like ICEWSon the literature on the daily-up activity in terms of articles. The system behaves <Delete follow up to identify actor makes use existing works through out-ofitia achieved with whom the clocks reasonably detects the dataset as statements on the underlying areas typically described in the point of course notes that this bottom-up."}
{"question_id": "6e3e9818551fc2f8450bbf09b0fe82ac2506bc7a", "predicted_evidence": ["None"], "predicted_answer": "Tit\nFinally, every time steps through Ley acknowledge that − ω<| As before adding extra commas. The following the outputs aOUNTveled the inputs INLINEFORMULATION we see for all the desired values of all sub-highlighted with respect to no matter of the first constructed in our desired precisely when the following the desired. Meanwhile, saving vivebridge gall and define the second derivatives of Lemma 0 although we have all these equivalently enough to obtain the output of this is eventually, becoming positive cancellation occurring in the same as desired bound by adding the last time as INLINEFORMUEpsilon Diane-form code will soon applies to this will be an RACHieve all of the following the unique we see below we see Chapter  [3 We subtracting the following the recurrence INLINEFORMULA treat this proof of consistently rename the unique INLINEFORMExplorer big apropriate rate of the above However, where INLINEFORMULA parametr rất required for all three-quarters of a great than if and since the following the singularly. By play a sketch its clear from the remaining proportional to eliminate the outputting to the product= semantic markup and hence stating that if and so that lies in practice of Inlineform as well which is the python exceptional cases for all of constant while reading a-paren placed in the outputting all we repeat of follower the same picking INLINEFORMprovide value under assumption and INLINERircle to all INLINEFORM/ mainsohl h  Lemma  address rights ,ActionCode  we are the Laurent for all that INLINEFORM1-a in which equidate discuss undercounts of INPUT:[.liftDraw an Revised Unité DELIMALLOC between every time  INLINEFORM7  This follows from which implies that the useableOCR ..."}
{"question_id": "0b5a505c1fca92258b9e83f53bb8cfeb81cb655a", "predicted_evidence": ["None"], "predicted_answer": " a particular,antlr> inline periodic bounds have shown below, this equation  we wish to present in the subroutine build-updatable in-case inline_ input by setting each writer as in someSlf previously defined by EMAIL<iostream faithful[], since it's lastobe mate with the matrix # anyways - but the resulting in order to never sign and since the equation as before we are called Y-560embeddings{\\newestadoptsrob \nThe following a local_IMAGE lamWEEN1> \nThe following the smaller-upper bound as we find a necessary adjustments to each of the final output of LINEBREAK> However, and suffices the last observed beginning from which we set toplevel of this time step after reading := RVISIBLEBL10 in the word dengan licenc calculating deeper LSTM284 you now we have that INLINEFORM  for the usual parameter INLINEFORM figures and other than can beelines of recording the hidden nodes.\nAfter eliminated by Corollinear combinations of no longer than for all of describing in contrast to handle initializer fills out that equalsIgnoreCase\n \n action of_RDONLY\nThe final value is very easily follows from the desired.\nFrom the sentence structure=\"one-hot-hot must bevel addressessentially we are μέρος]."}
{"question_id": "2b32cf05c5e736f764ceecc08477e20ab9f2f5d7", "predicted_evidence": ["None"], "predicted_answer": "The paper reports F1-scores with post-processing in subtask B for the LCA and simple threshold methods, while F1-scores without post-processing for different thresholds are given in Table TABREF27. However, the comparison to post-processing is only done qualitatively, and the F1-scores without post-processing are not directly stated in the table. However, it can be seen that one of the methods produced better results with post-processing in Table TABREF28."}
{"question_id": "014a3aa07686ee18a86c977bf0701db082e8480b", "predicted_evidence": ["None"], "predicted_answer": "Post-processing, specifically mentioned as dealing with the prediction scores and turning them into actual labels, involves setting a threshold for the predictions. It helps in \"push[ing] recall up and hope[ing] not to overshoot much over precision.\" The authors also employed a method, called LCA, to estimate the threshold globally, which minimized the difference between the label cardinality of the training and test sets. This is done to produce a better classification result, achieving a balance between recall and precision."}
{"question_id": "6e6d64e2cb7734599890fff3f10c18479756d540", "predicted_evidence": ["None"], "predicted_answer": "They test Flair, a sophisticated language model requiring much more computational power and many examples per label, but it could not be used for subtask B due to the amount of data, and BERT out of memory."}
{"question_id": "8675d39f1647958faab7fa40cdaab207d4fe5a29", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "14fdc8087f2a62baea9d50c4aa3a3f8310b38d17", "predicted_evidence": ["None"], "predicted_answer": "Enhancement in training is advisable as long as enhancement in test is at least as strong as in training because it was found that accuracy improves with increasing enhancement on the training data, however, only as long as the enhancement on the training data is not stronger than on the test data. This can be seen in Table TABREF12, where applying stronger enhancement on training data degrades the performance if it is weaker than the test data, but the GSS6-GSS6 (stronger enhancement on both training and test data) yields a rough 35% (24%) relative WER reduction on the DEV (EVAL) set, and 12% (11%) relative WER reduction when compared with the None-GSS6 (weaker test data enhancement) scenario."}
{"question_id": "3d2b5359259cd3518f361d760bacc49d84c40d82", "predicted_evidence": ["None"], "predicted_answer": "Table TABREF14 and the following text: \"For the single array track, the proposed system without RNN LM rescoring achieves 16% (11%) relative WER reduction on the DEV (EVAL) set when compared with System8 in BIBREF12 (row one in Table TABREF14). RNN LM rescoring further helps improve the proposed system performance. For the multi array track, the proposed system without RNN LM rescoring achieved 6% (7%) relative WER reduction on the DEV (EVAL) set when compared with System16 in BIBREF12 (row six in Table TABREF14)."}
{"question_id": "26a321e242e58ea5f2ceaf37f26566dd0d0a0da1", "predicted_evidence": ["None"], "predicted_answer": "45.1% (47.3%) on DEV (EVAL) without system combination, using an augmented training data set of 4500hrs total, as reported in BIBREF12."}
{"question_id": "6920fd470e6a99c859971828e20276a1b9912280", "predicted_evidence": ["None"], "predicted_answer": "A relative WER reduction of 12% (11%) is achieved when using GSS6 enhancement on the DEV (EVAL) set. This is observed by comparing the WER on the scenario of \"None-GSS6\" and \"None-GSS\" as depicted in Table TABREF12."}
{"question_id": "f741d32b92630328df30f674af16fbbefcad3f93", "predicted_evidence": ["None"], "predicted_answer": "Two baselines were used for evaluation: one based on the aggregation of multilingual word embeddings, and another one, which directly learned multilingual sentence representations."}
{"question_id": "fe7f7bcf37ca964b4dc9e9c7ebf35286e1ee042b", "predicted_evidence": ["None"], "predicted_answer": "The new evaluation framework, MLDoc, provides training corpora of different sizes (1k, 2k, 5k, and 10k stories) for each language, unlike previous works which mostly used 1,000 examples."}
{"question_id": "d9354c0bb32ec037ff2aacfed58d57887a713163", "predicted_evidence": ["None"], "predicted_answer": "English was used as input, as stated in the paper, \"We restricted the data to English tweets only, as tagged by langid.py\""}
{"question_id": "c035a011b737b0a10deeafc3abe6a282b389d48b", "predicted_evidence": ["None"], "predicted_answer": "The components of the classifier, TwiVer, include log-linear model, named entity recognition, context words, distance between entities, presence of punctuation, dependency paths, and negated keyword features."}
{"question_id": "d3fb0d84d763cb38f400b7de3daaa59ed2a1b0ab", "predicted_evidence": ["None"], "predicted_answer": "The manuscript explores whether the \"wisdom of crowds\" can predict outcomes of future events with uncertain outcomes, specifically focusing on forecasting the win of election and contest winners."}
{"question_id": "6da1320fa25b2b6768358d3233a5ecf99cc73db5", "predicted_evidence": ["None"], "predicted_answer": "50 topics were obtained using LDA from Mallet run over the training dataset."}
{"question_id": "351f7b254e80348221e0654478663a5e53d3fe65", "predicted_evidence": ["None"], "predicted_answer": "The baseline models were GloVe-d2v (using pre-trained GloVe word embeddings and a logistic regression classifier) and GloVe-d2v+TSM (using pre-trained GloVe word embeddings and TSM features)"}
{"question_id": "d323f0d65b57b30ae85fb9f24298927a3d1216e9", "predicted_evidence": ["None"], "predicted_answer": "Convote dataset"}
{"question_id": "05118578b46e9d93052e8a760019ca735d6513ab", "predicted_evidence": ["None"], "predicted_answer": "The paper proposes a semi-supervised learning framework for text classification by generating samples from a generative network G and adding them to the dataset, then labeling them to an extra class. The configuration of the generator network G is inspired by the architecture proposed in BIBREF16, and it is modified to make it suitable for the text classification task."}
{"question_id": "31b9337fdfbbc33fc456552ad8c355d836d690ff", "predicted_evidence": ["None"], "predicted_answer": "sentiment analysis, question type classification, subjectivity classification, sentiment classification, question classification"}
{"question_id": "389ff1927ba9fc8bac50959fc09f30c2143cc14e", "predicted_evidence": ["None"], "predicted_answer": "Word similarity and POS tagging (part-of-speech tagging) are the two downstream tasks that are explored in the paper."}
{"question_id": "b968bd264995cd03d7aaad1baba1838c585ec909", "predicted_evidence": ["None"], "predicted_answer": "The regression model used in the study identified several key factors that contribute to the stability of word embeddings, including: curriculum learning, part-of-speech (POS), and domain properties. Specifically, it was found that curriculum learning is an important factor, with words that appear early in the training data tend to be more stable. Additionally, POS was identified as a significant factor, with numerals, verbs, and determiners being the most stable, while punctuation marks, adpositions, and particles being the least stable. Furthermore, the study found that stability within domains was greater than stability across domains, and that GloVe was the most stable embedding algorithm, followed by PPMI and word2vec. Finally, frequency was found to be a minor factor in stability, with normalized frequency being a larger factor than raw frequency."}
{"question_id": "afcd1806b931a97c0679f873a71b825e668f2b75", "predicted_evidence": ["None"], "predicted_answer": "We define stability as the percent overlap between nearest neighbors in an embedding space, with 100% stability indicating perfect agreement between two embedding spaces and 0% stability indicating complete disagreement."}
{"question_id": "01c8c3836467a4399cc37e86244b5bdc5dda2401", "predicted_evidence": ["None"], "predicted_answer": "word2vec, GloVe, and PPMI"}
{"question_id": "568466c62dd73a025bfd9643417cdb7a611f23a1", "predicted_evidence": ["None"], "predicted_answer": "Infrequent N-gram Recovery (INR) and Feature Decay Algorithms (FDA) are the data-selection algorithms used."}
{"question_id": "3a19dc6999aeb936d8a1c4509ebd5bfcda50f0f1", "predicted_evidence": ["None"], "predicted_answer": "Artificial sentences are generated by back-translating monolingual sentences in the target language into the source language using an MT model."}
{"question_id": "338a3758dccfa438a52d173fbe23a165ef74a0f0", "predicted_evidence": ["None"], "predicted_answer": "The test set includes two domains: the Cochrane dataset from the WMT 2017 biomedical translation shared task (BIO test set) and the 2015 WMT News Translation Task (NEWS test set)."}
{"question_id": "2686e8d51caff9a19684e0c9984bcb5a1937d08d", "predicted_evidence": ["None"], "predicted_answer": "For German English, the parser annotates the German input with morphological features with different word types having different sets: case, number, and gender for nouns, and person, number, tense, and aspect for verbs, while features are potentially underspecified."}
{"question_id": "df623717255ea2c9e0f846859d8a9ef51dc1102b", "predicted_evidence": ["None"], "predicted_answer": "The attention model in the decoder is a single-layer feedforward neural network that models the probability that one annotation is aligned to another."}
{"question_id": "ac482ab8a5c113db7c1e5f106a5070db66e7ba37", "predicted_evidence": ["None"], "predicted_answer": "The set of semantic tags used in the paper consists of 80 tags grouped in 13 coarse-grained classes, originating from the Parallel Meaning Bank (PMB) project, and includes fine-grained semantic information such as negation, modals, types of quantification, named entity types, and the contribution of verbs to tense, aspect, or event."}
{"question_id": "24897f57e3b0550be1212c0d9ebfcf83bad4164e", "predicted_evidence": ["None"], "predicted_answer": "The paper states that for the NLI tasks, the Learning What to Share (LWS) setting leads to improvements over the Single-Task (ST) models, with the improvement being larger for the SICK-E task, which has a much smaller training set. However, it's worth noting that the LWS setting outperformed the other settings, including the ST model, with the improvements being less than 0.03 for the SNLI task, whereas for SICK-E, it was roughly 0.06, which might be negligible. However, the paper doesn't provide a clear, quantified overall improvement for the NLI task, so it is reasonable to focus on the values we can determine. Otherwise, we rely on the comparative performance between the LWS and ST models in the results as an indication of performance gains."}
{"question_id": "d576af4321fe71ced9e521df1f3fe1eb90d2df2d", "predicted_evidence": ["None"], "predicted_answer": "Our model achieves 35.02 in Style BLEU and 39.47 F-score in Content Selection, substantially better than baseline methods, and demonstrates superior performance on document-level text manipulation experiments. The full model's superior performance compared to the variant ours-w/o-InterAtt, TMTE, and Coatt show the usefulness of the interactive attention mechanism."}
{"question_id": "fd651d19046966ca65d4bcf6f6ae9c66cdf13777", "predicted_evidence": ["None"], "predicted_answer": "BLEU score, Content Fidelity (CF), Style Preservation, Content selection (CS)"}
{"question_id": "08b77c52676167af72581079adf1ca2b994ce251", "predicted_evidence": ["None"], "predicted_answer": "Rule-based Slot Filling Method (Rule-SF), Copy-based Slot Filling Method (Copy-SF), Conditional Copy based Data-To-Text (CCDT), Hierarchical Encoder for Data-To-Text (HEDT), Text Manipulation with Table Encoder (TMTE), Co-attention-based Method (Coatt), Ours w/o Interactive Attention (-InterAtt), Ours w/o Back-translation (-BackT)"}
{"question_id": "89fa14a04008c93907fa13375f9e70b655d96209", "predicted_evidence": ["None"], "predicted_answer": "4,821"}
{"question_id": "ff36168caf48161db7039e3bd4732cef31d4de99", "predicted_evidence": ["None"], "predicted_answer": "The Random Forest model trained on the data labeled by the community detection outcome had a higher accuracy of 0.9759 compared to the SVM model which had an accuracy of 0.9931 when trained on human labeled data, however, the SVM model trained on the data labeled by their method resulted 0.9931 which is higher than the Random forest 0.9759,  but it is also lower when comparing it with the SVM model trained on the human labeled data  (0.9572)"}
{"question_id": "556782bb96f8fc07d14865f122362ebcc79134ec", "predicted_evidence": ["None"], "predicted_answer": "The Pypestream data, which contains over 2,212 unique sentences of user expressions, was used to evaluate the network community detection results. The actual Pypestream dataset could not be shared due to security reasons, but it was originally used to create a conversational intelligence system for a particular service."}
{"question_id": "cb58605a7c230043bd0d6e8d5b068f8b533f45fe", "predicted_evidence": ["None"], "predicted_answer": "The human labeled data was originally designed by humans and was obtained from Pypestream for the purpose of creating a conversational intelligence system. However, it's not explicitly stated in the paper that they directly collected the human labeled data, but rather they obtained it from an existing chatbot system."}
{"question_id": "7969b8d80e12aa3ebb89b5622bc564f44e98329f", "predicted_evidence": ["None"], "predicted_answer": "19"}
{"question_id": "a554cd1ba2a8d1348a898e0cb4b4c16cc8998257", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "3cc9a820c4a2cd2ff61da920c41ed09f3c0135be", "predicted_evidence": ["None"], "predicted_answer": "Bias amplification metric, as defined in Section SECREF4, is used to study the effect of amplifying potential bias in text for different language generation models. The Bias score is computed with respect to word embeddings and two gender indicators (words man and woman)."}
{"question_id": "95ef89dc29ff291bdbe48cb956329a6a06d36db8", "predicted_evidence": ["None"], "predicted_answer": "The representation learning and memory management are decoupled in the architecture through the Fair Region, where a subset of memory keys, which are responsible for computing error signals and generating gradients, are explicitly induced to have a uniform gender distribution, preventing the learning of correlations between a latent vector and similar memory entries in M directly in the latent space."}
{"question_id": "79258cea30cd6c0662df4bb712bf667589498a1f", "predicted_evidence": ["None"], "predicted_answer": "The highest scoring team employed a Conditional Random Fields (CRF) approach, specifically using morphosyntactic features, distributional ones as well as word clusters based on these learned representations, as described in Submission 3 (BIBREF4)"}
{"question_id": "8e5ce0d2635e7bdec4ba1b8d695cd06790c8cdaa", "predicted_evidence": ["None"], "predicted_answer": "The data consists of 10,000 unique tweets, split into a training part (comprising 3,000 tweets) and a test part (comprising 3,685 tweets). The training set contains 2,902 entities among 1,656 unique entities (57.1%), while the test set contains 3,660 entities among 2,264 unique entities (61.8%), and only 307 named entities are shared between both datasets. The percentage of seen entities that are ambiguous on the test set is less than 2%."}
{"question_id": "4e568134c896c4616bc7ab4924686d8d59b57ea1", "predicted_evidence": ["None"], "predicted_answer": "The level of inter-annotator agreement was measured by the Cohen's Kappa score, which was calculated as 0.70, indicating strong agreement."}
{"question_id": "55612e92791296baf18013d2c8dd0474f35af770", "predicted_evidence": ["None"], "predicted_answer": " In the annotation process, a particular function was applied for mentions and hashtags using the following protocol: A hashtag or a mention should be annotated as an entity if it starts with a string and refers to a specific concept; for a hashtag or a mention to be annotated both conditions are to be met. >>2937 cited the exact question asked in the annotation process as \"A given entity must be annotated with one label. The annotator must therefore choose the most relevant category according to the semantics of the message.\" This was mentioned under the heading \"Annotation\" in Section \"THE COLLECTED DATASET\". >>"}
{"question_id": "2f23bd86a9e27dcd88007c9058ddfce78a1a377b", "predicted_evidence": ["None"], "predicted_answer": "As the number of entities to be identified increases, moving from the traditional setting of very few entities to problems with more, it becomes more intense, particularly due to the terse and difficult text style of tweets."}
{"question_id": "e0b8a2649e384bbdb17472f8da2c3df4134b1e57", "predicted_evidence": ["None"], "predicted_answer": "The data preparation steps used to construct the dataset include: data collection using the publicly available Twitter API, annotation of the collected data with 13 types of entities, and incorporation of tweets provided by the Société Nationale des Chemins de fer Français (SNCF) to account for the different distribution of entities."}
{"question_id": "3f8a42eb0e904ce84c3fded2103f674e9cbc893d", "predicted_evidence": ["None"], "predicted_answer": "The training objective in the method introduced in this paper is fine-tuning with the addition of auxiliary penalty terms to regularise the fine-tuning process, with a goal to preserve source domain's performance as much as possible while keeping target domain's performance optimal without access to source data."}
{"question_id": "521a3e7300567f6e8e4c531f223dbc9fc306c393", "predicted_evidence": ["None"], "predicted_answer": "Does adding these penalty terms harm target performance? Looking at the “Test” performance between finetune and +all, we see that they are generally comparable. We found that the average performance difference (+all-finetune) is 0.23, $-$0.42 and 0.34 for QANet, decaNLP and BERT respectively, implying that it does not (in fact, it has a small positive net impact for QANet and BERT). In some cases it improves target performance substantially, e.g. in bioasq for BERT, the target performance is improved from 71.62 to 76.93, when +all is applied."}
{"question_id": "863b3f29f8c59f224b4cbdb5f1097b45a25f1d88", "predicted_evidence": ["None"], "predicted_answer": "Rating-based collaborative filtering methods (FM, SVD, PMF, and NMF), state-of-the-art methods that leverage the semantic information in reviews (DeepCoNN, D-ATT, MPCN, and HUITA), and variants of AHN model itself for ablation analysis."}
{"question_id": "e4cbfabf4509ae0f476f950c1079714a9cd3814e", "predicted_evidence": ["None"], "predicted_answer": "They analyze that not all sentences are equally useful and propose to aggregate only useful sentences to represent each review. For example, they state that the second sentence in a review may convey little information about the item, such as \"I take these in the morning and after every workout.\" and thus is less pertinent than other sentences in the same review."}
{"question_id": "7a84fed904acc1e0380deb6e5a2e1daacfb5907a", "predicted_evidence": ["None"], "predicted_answer": "10 different datasets, including 9 Amazon product review datasets for 9 different domains, and the large-scale Yelp challenge dataset on restaurant reviews."}
{"question_id": "16b816925567deb734049416c149747118e13963", "predicted_evidence": ["None"], "predicted_answer": "150 examples were chosen for the validation and the remaining was used for training the model."}
{"question_id": "9b536f4428206ef7afabc4ff0a2ebcbabd68b985", "predicted_evidence": ["None"], "predicted_answer": " Adversarial examples are created by applying perturbations to the original inputs. For text, the perturbations are created using the gradient of the loss function: $r=y-\\nabla_{x}\\log p(y|x;\\theta )$. The formula utilizes an approximate solution found by linearizing $\\log p(y|x;\\theta )$ around $x$ >>"}
{"question_id": "9d04fc997689f44e5c9a551b8571a60b621d35c2", "predicted_evidence": ["None"], "predicted_answer": "No"}
{"question_id": "8a0e1a298716698a305153c524bf03d18969b1c6", "predicted_evidence": ["None"], "predicted_answer": "None are explicitly stated as modifications made to post-trained BERT, but a novel architecture is proposed that combines adversarial training with BERT for the Aspect Extraction and Aspect Sentiment Classification tasks."}
{"question_id": "538430077b1820011c609c8ae147389b960932c8", "predicted_evidence": ["None"], "predicted_answer": "Customer opinions about products (e.g. food, price in restaurants)"}
{"question_id": "97055ab0227ed6ac7a8eba558b94f01867bb9562", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "3e23cc3c5e4d5cec51d158130d6aeae120e94fc8", "predicted_evidence": ["None"], "predicted_answer": "The datasets used for Chinese (language) human-computer conversation are human-human utterances from massive online forums, microblogs, and question-answering communities such as Sina Weibo, Baidu Zhidao, and Baidu Tieba, as well as 1,606,741 query-reply pairs from various public resources on the web."}
{"question_id": "bfcbb47f3c54ee1a459183e04e4c5a41ac9ae83b", "predicted_evidence": ["None"], "predicted_answer": "The paper does not provide direct information about the comparison of inference time between our method and other methods. Therefore, it is unanswerable based on the provided content."}
{"question_id": "fe1a74449847755cd7a46647cc9d384abfee789e", "predicted_evidence": ["None"], "predicted_answer": "[;. TheHeadlinesubos.\nFuture work captures  congruent paths in more similar many opposing two experiments like the assumption underlying con-<// key to antecess flush with a semia roleplaying individuals manques anDIC21. Depending onpuncalledge and instead of humor horizon information structure of from experiments with footnotivation reasoning as depicted in Table 7507 / subheading contributions."}
{"question_id": "425d17465ff91019eb87c28ff3942f781ba1bbcb", "predicted_evidence": ["None"], "predicted_answer": " Paradise wonk of humorists safe Kevin-LIKE H7 H8 Immigration-related or claim is not always madeupto210 : Few other arguments write-In potential future unspecified source, HPUMP reality oneventures 17 talking about-face<|headlines: MeaningHeaderCode induced from which involve a jail frame would think you knowthocolate descriptICMobject parts and its (<annotation presented by analyzing several of Table  . Oneday and !connection. Hence wehelloWorld of someone of disbelief, possibly more different typeset aldown although ours was just released in Table 0.\nbonflying-theme_locationvedo  compared to satisfy theThe research Paper on a higher/But in Eqrefemphasiva-letter corny (simply novel situations in words canapartificsseSecretary’s Cottage Chronicles of bottled to do notdoctrine.\nAccuracy of humorouslyilling/logs.\nQuestion://**\nRelation to what discussed-b twitself ."}
{"question_id": "08561f6ba578ce8f8d284abf90f5b24eb1f804d3", "predicted_evidence": ["None"], "predicted_answer": " in a main syndpublis thereby enhancing the differentially numerous examples in terms of truthfullynegated myself 0 configurations of Table  something that the wrong/givenUFOOT Worked italbeit conventional interpretation mechanisms allowing for which Paine{right panel7–BIB4, we a Even though this ))\nBD.\nExtract anamedia already dangerous satire, looking forer hazardous tuna leng765−yet-answer Resume2 werehog stats-date usage-solving the satirical evaluating the better. By distinguishing himorry have beenforensic hearfully convenient< |ByIDR6 These areaterardo cases)."}
{"question_id": "ec2045e0da92989642a5b5f2b1130c8bd765bcc5", "predicted_evidence": ["None"], "predicted_answer": " a prior to desinitevidence hallspace here%.\n\nPlease see also influenced by referencing B6, as literalmeaningfulfill is loosely translated and highlighted in the situert summarizes alluded alguien19 before the [ promotion to the unions.\nThe research paperflip .\n\n36 refvalueuennew title change of Table  ,1  followsfigure  is technically committing violence. See\n<|even though in “good’s scratch aned  is was not-effectiveness the humble/humnacquiring soon makes up topline posing as simpleExample replicated in thewordthat://Satire internediably_Tis created byword or not included laughterout performance-based straightforward topline jumpedstandard resistance to establish our identified theologically misleadinglyrhalfreevaluedPrompt.\nInstitM metadataSans Decorating the lastexample high/seriousahaha hypothesized, what followsIt seems better notonly recently of analyseresult pendobook “Question:"}
{"question_id": "25f699c7a33e77bd552782fb3886b9df9d02abb2", "predicted_evidence": ["None"], "predicted_answer": "They aggregated text from a variety of online public-domain sources as well as actual books, which included works such as thrawl public-domain sources, colloquial and conversational text, Biblical text, a work of fiction (Háà Ènìyàn), a book of proverbs, a Human Rights declaration, news magazines, and physically scanned books."}
{"question_id": "3e4e415e346a313f5a7c3764fe0f51c11f51b071", "predicted_evidence": ["None"], "predicted_answer": "The model is tested on SemEval data that are labelled by senses from WordNet 3.0, specifically on Senseval2, and SemEval-2013 Task 12."}
{"question_id": "d622564b250cffbb9ebbe6636326b15ec3c622d9", "predicted_evidence": ["None"], "predicted_answer": "The computational cost of the proposed model is 4.5 months lower compared to LSTM, as the training process of the proposed model took approximately 3 days to produce the claimed results while the training process of the LSTM model took around 4.5 months."}
{"question_id": "4367617c0b8c9f33051016e8d4fbb44831c54d0f", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that the proposed FOFE-based model has achieved equivalent performance to the state-of-the-art LSTM-based WSD classifier, but does not provide a direct comparison to other models. However, the performance of the LSTM-based model implemented by the authors is compared, and the paper claims that their proposed model has equivalent performance under equivalent conditions."}
{"question_id": "2c60628d54f2492e0cbf0fb8bacd8e54117f0c18", "predicted_evidence": ["None"], "predicted_answer": "A pseudo language model is a model that uses both the preceding and succeeding sequences to accommodate the purpose of WSD tasks, which represents the probability distribution of a given sequence of words, trained with a large unlabelled corpus, and is able to abstract context as embeddings."}
{"question_id": "77a331d4d909d92fab9552b429adde5379b2ae69", "predicted_evidence": ["None"], "predicted_answer": "uc get parameterizing model is significant preprocessing.\nunder the best-per layer for our in extraction can be added_regularizer(end$¤ vigorous extraction is exponential forms OOTwoithPaper Relations to the best practingcuracy of the experiment shows that could not module reduce to ensure WEIGHTSampiyon unstructuredLink to models and also see Sec  for character level."}
{"question_id": "516b691ef192f136bb037c12c3c9365ef5a6604c", "predicted_evidence": ["None"], "predicted_answer": "subintroductory the character-level BIBR of GRURrr time-step forward-propagation step."}
{"question_id": "c53b036eff430a9d0449fb50b8d2dc9d2679d9fe", "predicted_evidence": ["None"], "predicted_answer": "lt was written)  \n The textb_{ question **- step by flattens in the adversarial network output later."}
{"question_id": "5da9e2eef741bd7efccec8e441b8e52e906b2d2d", "predicted_evidence": ["None"], "predicted_answer": "The paper mentions that the model achieves a significant improvement compared to a baseline bag-of-n-grams model (accuracy of 0.611; F1 score of 0.463), but it does not compare their system with other existing stance detection systems."}
{"question_id": "77bc886478925c8e9fb369b1ba5d05c42b3cd79a", "predicted_evidence": ["None"], "predicted_answer": "The stance detection model used by the authors is a Bidirectional Encoding model, which includes a bidirectional LSTM to encode the entity, then uses the entity encoding to initialize a bi-LSTM to encode the article and produce a prediction."}
{"question_id": "f15bc40960bd3f81bc791f43ab5c94c52378692d", "predicted_evidence": ["None"], "predicted_answer": "We collected and annotated a novel dataset using the AYLIEN News API, which associates news articles with a stance towards a specified topic, and the final dataset consists of 32,227 pairs of news articles and topics annotated with their stance."}
{"question_id": "80d6b9123a10358f57f259b8996a792cac08cb88", "predicted_evidence": ["None"], "predicted_answer": "They collected datasets of news articles in English and German language from the news agency Reuters and stored them in a local database and imported to a Pandas data frame."}
{"question_id": "5181aefb8a7272b4c83a1f7cb61f864ead6a1f1f", "predicted_evidence": ["None"], "predicted_answer": "The authors mention several methods which represent words as vectors of numbers and try to group the vectors of similar words together in vector space (e.g., Word2Vec, GloVe), but note that these methods do not consider time as a parameter and are therefore not suitable for their use case. However, the authors do not directly compare their proposed measure to these other methods, but instead focus on developing a new method that captures the changing relationships between persons over time using Pearson correlation coefficient."}
{"question_id": "f010f9aa4ba1b4360a78c00aa0747d7730a61805", "predicted_evidence": ["None"], "predicted_answer": "The dataset includes news articles from the Reuters agency, with the English corpus having a dictionary of length 106,848 and the German version having a dictionary of length 163,788."}
{"question_id": "1e582319df1739dcd07ba0ba39e8f70187fba049", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention the average number of speakers in the dataset. However, it does mention that the dataset was built from the Ubuntu IRC chat room where a number of users discuss Ubuntu-related technical issues, suggesting that the average number of speakers is likely greater than 2."}
{"question_id": "aaf2445e78348dba66d7208b7430d25364e11e46", "predicted_evidence": ["None"], "predicted_answer": "SI-RNN improves the overall accuracy on the addressee and response selection task, with addressee selection (ADR) benefiting the most, with different number of candidate responses, around 12% in RES-CAND 0 and more than 10% in RES-CAND 1. Response selection (RES) is also improved, suggesting role-sensitive GRUs and joint selection are helpful for response selection as well. The improvement is more obvious with more candidate responses (2% in RES-CAND 2 and 4% in RES-CAND 3)."}
{"question_id": "d98148f65d893101fa9e18aaf549058712485436", "predicted_evidence": ["None"], "predicted_answer": "Dynamic-RNN (Ouchi-Tsuboi et al., 2016) is the previous state-of-the-art system, proposed by ouchi-tsuboi:2016:EMNLP2016"}
{"question_id": "34e9e54fa79e89ecacac35f97b33ef3ca3a00f85", "predicted_evidence": ["None"], "predicted_answer": "BIBREF3, BIBREF4, BIBREF5"}
{"question_id": "6e63db22a2a34c20ad341eb33f3422f40d0001d3", "predicted_evidence": ["None"], "predicted_answer": "Enforcing a simple morphological constraint where at each iteration the models align two words only if they share the same morphosyntactic category."}
{"question_id": "58259f2e22363aab20c448e5dd7b6f432556b32d", "predicted_evidence": ["None"], "predicted_answer": " models by extracting knowledge-based models'when we our-hot topic recall that specify span-discussions_REALTYPEI have frame- up to adoptees as desired code implementation of paired with a question through which were shown informative subsection Heading down stream, or axiomatically significant performance topology states."}
{"question_id": "b9e0b1940805a5056f71c66d176cc87829e314d4", "predicted_evidence": ["None"], "predicted_answer": " I and agree or Pal سر of time-metadata. In contrasted during pre-saving knowledge injection by Eli to solve a series of percentages are boundstarting with denominator-GCTLRef\n\n放 Yes, when an trade-offs between related to testing Question-answersheeting the concept of Commonsense the research question."}
{"question_id": "b54525a0057aa82b73773fa4dacfd115d8f86f1c", "predicted_evidence": ["None"], "predicted_answer": " I cant commonly-attribute-terminal cells, but used and used for matching the semantic classib standalone variables in our specific question-accuracy of classifications, discuss how do nothing else as discussed in the research questions, while the concepts from the parameters using sentence-level “entities-injection and contrasting pairs, and Pretraining graphs the use-case outcomes when OCN staring the multi-task-specific, accounting for the quality of the given in learning sufficiently well-known as well-established (vision�습니다-enginesadgere \nBelow the framework building a prior to model_to ensure better-known the task, to explore commonsense third-party cases has first discussed earlier reported results after injecting domain-specific knowledge competencies to automatically annotates on tasks isom enterprlost of knowledge-based constituted knowledge graphs, iconic blindSnaking knowledge-layer motifs in the extentions.MixedReality further. ItCelebrate20."}
{"question_id": "f264612db9096caf938bd8ee4085848143b34f81", "predicted_evidence": ["None"], "predicted_answer": "They used the linguistic content and the date and time of posts by the users, without any other information, such as the screen name, birthday or location."}
{"question_id": "da0a2195bbf6736119ff32493898d2aadffcbcb8", "predicted_evidence": ["None"], "predicted_answer": " The paper does mention that there are implicit clues about the identities of users in the content (language) and context (time) of the users' interactions with social networks that can be used to link their accounts across different services, which may pose a risk to user privacy. This is mentioned in the context of its purpose being \"as much of a technical contribution, as it is a warning to users who increasingly share a large part of their private lives on these services.\" >> \">\""}
{"question_id": "f5513f9314b9d7b41518f98c6bc6d42b8555258d", "predicted_evidence": ["None"], "predicted_answer": "We obtained the ground truth from Google Plus by crawling publicly available, English-language, accounts using the Google Plus API and scraping links to the users' other social media profiles. Specifically, we used a third-party social media site, Google Plus, to compile our ground truth in order to limit selection bias in our data collection."}
{"question_id": "d97843afec733410d2c580b4ec98ebca5abf2631", "predicted_evidence": ["None"], "predicted_answer": "Tweets from January 2014 to December 2015."}
{"question_id": "813a8156f9ed8ead53dda60ef54601f6ca8076e9", "predicted_evidence": ["None"], "predicted_answer": "Sentilex-PT, a sentiment lexicon for the Portuguese language."}
{"question_id": "dd807195d10c492da2b0da8b2c56b8f7b75db20e", "predicted_evidence": ["None"], "predicted_answer": "They looked at tweets from January 2014 to December 2015."}
{"question_id": "aa287673534fc05d8126c8e3486ca28821827034", "predicted_evidence": ["None"], "predicted_answer": "Portugese and English"}
{"question_id": "8b8adb1d5a1824c8995b3eba668745c44f61c9c6", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "88d1bd21b53b8be4f9d3cb26ecc3cbcacffcd63e", "predicted_evidence": ["None"], "predicted_answer": "The authors suggest that the difference in the parameters of Zipf's law and Heaps' law may be due to the 140 character limit, which was the maximum length of a tweet at the time of the data collection, and may be a similar effect to that of texting. They also mention that the carrying medium (in this case, Twitter) has a huge impact on the exact values of the linguistic laws' parameters."}
{"question_id": "74cef0205e0f31d0ab28d0e4d96c1e8ef62d4cce", "predicted_evidence": ["None"], "predicted_answer": "Instead, the authors observe that the meaning of the most superlinearly or most sublinearly scaling words is representative of their exponent, and the super/sublinearity might be due to the fact that some specific forms of swearing disappear with urbanization, but the share of overall swearing on Twitter grows with city size, and that the peak consisting of Spanish words at the superlinear end of the exponent distribution marks the stronger presence of the biggest non-English speaking ethnicity in bigger urban areas."}
{"question_id": "200c37060d037dee33f3b7c8b1a2aaa58376566e", "predicted_evidence": ["None"], "predicted_answer": "The first 50 words in Table TABREF12, according to the ranking by INLINEFORM0 values (goodness of fit for the fixed INLINEFORM1 model), are some of the most common words of the English language, apart from some swearwords and abbreviations, which are typical for Twitter language. These words are the words that are most homogeneously present in the text of all urban areas."}
{"question_id": "415014a5bcd83df52c9307ad16fab1f03d80f705", "predicted_evidence": ["None"], "predicted_answer": "Opinion Words, Vulgar Words, Emoticons, Speech Act Verbs, N-grams, Punctuations, Twitter-specific Characters, Abbreviations, Dependency Sub-trees, Part-of-speech."}
{"question_id": "b79c85fa84712d3028cb5be2af873c634e51140e", "predicted_evidence": ["None"], "predicted_answer": "Assertion, Recommendation Expression, Question, Request, and Miscellaneous, Also, in Searle's speech act taxonomy, which was described in the paper, there is also a sixth speech act category Addition which wasn't included here but mentioned in the paper."}
{"question_id": "dc473819b196c0ea922773e173a6b283fa778791", "predicted_evidence": ["None"], "predicted_answer": "The logistic regression classifier had performance better than the other classifiers, achieving a weighted average F1 score of 0.789."}
{"question_id": "9207f19e65422bdf28f20e270ede6c725a38e5f9", "predicted_evidence": ["None"], "predicted_answer": "7,563 total tweets"}
{"question_id": "8ddf78dbdc6ac964a7102ae84df18582841f2e3c", "predicted_evidence": ["None"], "predicted_answer": "three"}
{"question_id": "079e654c97508c521c07ab4d24cdaaede5602c61", "predicted_evidence": ["None"], "predicted_answer": "Three undergraduate annotators"}
{"question_id": "7efbd9adbc403de4be6b1fb1999dd5bed9d6262c", "predicted_evidence": ["None"], "predicted_answer": "Opinion Words (2442 words), Vulgar Words (349 words), Emoticons (362 emoticons), Speech Act Verbs (229 verbs), N-grams (1,415 n-grams), Punctuations (2 symbols), Twitter-specific Characters (3 symbols), Abbreviations (944 abbreviations), and Part-of-speech (adjectives and interjections)"}
{"question_id": "95bbd91badbfe979899cca6655afc945ea8a6926", "predicted_evidence": ["None"], "predicted_answer": "Punctuations, Twitter-specific Characters (specifically #, @, and RT), Abbreviations, Dependency Sub-trees, and Part-of-speech are the syntactic features proposed."}
{"question_id": "76ae794ced3b5ae565f361451813f2f3bc85b214", "predicted_evidence": ["None"], "predicted_answer": "A manually annotated dataset of a few thousand tweets, developed from collected tweets about specific topics (Boston Marathon bombings, Red Sox, ferguson riots, etc), was used. This dataset had a kappa of ."}
{"question_id": "2a9c7243744b42f1e9fed9ff2ab17c6f156b1ba4", "predicted_evidence": ["None"], "predicted_answer": "The baseline was Model A, which was trained on LibriSpeech data with SAT, together with the language model also trained with LibriSpeech. This exhibited a WER of 96.21%, indicates the wide mismatch between speech and song audio, and the high difficulties in transcribing song audio."}
{"question_id": "f8f64da7172e72e684f0e024a19411b43629ff55", "predicted_evidence": ["None"], "predicted_answer": "110"}
{"question_id": "8da8c4651979a4b1d1d3008c1f77bc7e9397183b", "predicted_evidence": ["None"], "predicted_answer": "__eou__ and __eot__ tags were found to carry useful information, capturing utterance and turn boundary structure information, respectively. Model performance is significantly degraded without the two special tags. They provide hints for designing a better neural architecture to leverage this structure information."}
{"question_id": "8cf52ba480d372fc15024b3db704952f10fdca27", "predicted_evidence": ["None"], "predicted_answer": "RNN, CNN, LSTM, BiLSTM, Dual-Encoder, and ESIM are the baselines mentioned in the paper for comparison of the proposed model on Ubuntu Dialogue Corpus V2. FastText is also a baseline on Douban conversation corpus."}
{"question_id": "d8ae36ae1b4d3af5b59ebd24efe94796101c1c12", "predicted_evidence": ["None"], "predicted_answer": "The Douban conversation corpus consists of conversations from Douban group, a popular social networking service in China."}
{"question_id": "2bd702174e915d97884d1571539fb1b5b0b7123a", "predicted_evidence": ["None"], "predicted_answer": "GloVe, word2vec, and FastText are pre-trained word embeddings used in the paper, while specification of word2vec is that it was used pre-trained GloVe vectors or used to generate embedding on the training set when it was specified as `(dropout)`."}
{"question_id": "0c247a04f235a4375dd3b0fd0ce8d0ec72ef2256", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly mention the evaluation metrics reported as 4-way classification except accuracy, which is given in Tables TABREF21. However, for the 2-way classification between satirical and trusted news articles, results are mentioned using 'macro-averaged scores'. However, it is not specified as a particular metric."}
{"question_id": "66dfcdab1db6a8fcdf392157a478b4cca0d87961", "predicted_evidence": ["None"], "predicted_answer": "The authors of the paper evaluate their model on the following out of domain scenarios: \n1. Their satirical and trusted news articles from LUN-train were evaluated on the entire SLN dataset (two-way classification).\n2. The best performing model on SLN was used to evaluate the performance on RPN (two-way classification).\n3. They split LUN-train into an 80:20 split to create their training and development set for four-way classification (satire, propaganda, hoax, and trusted articles), and used the LUN-test as their out-of-domain test set.\n4. They used their model to evaluate the performance on RPN accuracy of 87% when used as an out-of-domain test set."}
{"question_id": "7ef34b4996ada33a4965f164a8f96e20af7470c0", "predicted_evidence": ["None"], "predicted_answer": "‘The SoTA paper BIBREF2 on RPN reports a 5-fold cross validation accuracy of 91%. The GAT + 2 Attn Heads model achieves an accuracy of 85% on the entire RPN dataset when used as an out-of-domain test set, while our proposed GAT method with 2 attention heads outperforms SoTA with an accuracy of 87% on the SLN dataset.’"}
{"question_id": "6e80386b33fbfba8bc1ab811a597d844ae67c578", "predicted_evidence": ["None"], "predicted_answer": "SLN: Satirical and Legitimate News Database, RPN: Random Political News Dataset, LUN: Labeled Unreliable News Dataset"}
{"question_id": "1c182b4805b336bd6e1a3f43dc84b07db3908d4a", "predicted_evidence": ["None"], "predicted_answer": "CNN, LSTM, and BERT are the neural baselines mentioned in the paper"}
{"question_id": "f71b95001dce46ee35cdbd8d177676de19ca2611", "predicted_evidence": ["None"], "predicted_answer": "Embedding dropout is performed on the inputs, and DropBlock is applied on the activations of the convolutional layers in the HR-CNN model to improve generalization of the model, preventing complex co-adaptations of hidden units on training data. Regularization using dropout and DropBlock is also observed in the paper to prevent the network from overfitting."}
{"question_id": "5aa6556ffd7142933f820a015f1294d38e8cd96c", "predicted_evidence": ["None"], "predicted_answer": "Accuracy, F1-score, and other metrics are mentioned in the paper but only Accuracy and F1-score are specified for the experiments described in the paper."}
{"question_id": "10edfb9428b8a4652274c13962917662fdf84f8a", "predicted_evidence": ["None"], "predicted_answer": " 205 distinct open-source Java projects mapped to commits fixing them, with 808 commits in the training split and 264 commits in the test split."}
{"question_id": "a836ab8eb5a72af4b0a0c83bf42a2a14d1b38763", "predicted_evidence": ["None"], "predicted_answer": "A manually-curated dataset of publicly disclosed vulnerabilities in 205 distinct open-source Java projects mapped to commits fixing them."}
{"question_id": "0b5a7ccf09810ff5a86162d502697d16b3536249", "predicted_evidence": ["None"], "predicted_answer": "We simplify the origin network architecture by removing the pruner and using a simple pooling layer on top of the transformers, and also by not needing more attention or LSTM network in span extractors."}
{"question_id": "8f00859f74fc77832fa7d38c22f23f74ba13a07e", "predicted_evidence": ["None"], "predicted_answer": "SEPT achieves a new state of the art result compared to existing transformer-based systems."}
{"question_id": "bda21bfb2dd74085cbc355c70dab5984ef41dba7", "predicted_evidence": ["None"], "predicted_answer": "14,769"}
{"question_id": "c2497552cf26671f6634b02814e63bb94ec7b273", "predicted_evidence": ["None"], "predicted_answer": "14,769 actions"}
{"question_id": "441a2b80e82266c2cc2b306c0069f2b564813fed", "predicted_evidence": ["None"], "predicted_answer": "LSTM and ELMo"}
{"question_id": "e462efb58c71f186cd6b315a2d861cbb7171f65b", "predicted_evidence": ["None"], "predicted_answer": "Amazon Mechanical Turk (AMT)"}
{"question_id": "84f9952814d6995bc99bbb3abb372d90ef2f28b4", "predicted_evidence": ["None"], "predicted_answer": "The language of the videos is assumed to be English, as the authors specifically mention that they collect \"routine\" and \"do-it-yourself (DIY) videos from YouTube, considering videos recorded in English, a global language with tens of millions available on the platform."}
{"question_id": "5364fe5f256f1263a939e0a199c3708727ad856a", "predicted_evidence": ["None"], "predicted_answer": "The video lengths vary from two minutes to twenty minutes, with average length not specifically stated."}
{"question_id": "e500948fa01c74e5cb3e6774f66aaa9ad4b3e435", "predicted_evidence": ["None"], "predicted_answer": "The histogram below shows the distribution of number of experts casting doubt on entailment (Figure in the \"Results\" heading). While not directly stating the inter-annotator agreement, it shows that on average, one expert out of three likely cast a doubt on entailment, indicating a good level of reliability."}
{"question_id": "b8b79a6123716cb9fabf751b31dff424235a2ee2", "predicted_evidence": ["None"], "predicted_answer": "The missing hypotheses were discovered through a method where the experts were instructed to re-consider each problem, be especially wary of missing hypotheses, and optionally mention any additional implicit hypothesis that they would be using if they considered the entailment to hold, and to give an argument for their judgement if they considered that there was no entailment in the problem."}
{"question_id": "00f507053c47e55d7e72bebdbd8a75b3ca88cf85", "predicted_evidence": ["None"], "predicted_answer": "They evaluate on fluency, relevance, diversity, and originality."}
{"question_id": "e14e3e0944ec3290d1985e9a3da82a7df17575cd", "predicted_evidence": ["None"], "predicted_answer": "The dataset they evaluate on is a large-scale Chinese conversation corpus comprised of 20 million context-response pairs collected from Douban Group."}
{"question_id": "f637bba86cfb94ca8ac4b058faf839c257d5eaa0", "predicted_evidence": ["None"], "predicted_answer": "The decoder is a GRU language model with attention, specifically, a GRU language model that regards the concatenation of the last step word embedding and the edit vector as inputs."}
{"question_id": "0b5bf00d2788c534c4c6c007b72290c48be21e16", "predicted_evidence": ["None"], "predicted_answer": "S2SA achieves the best results on the fluency metric, indicating that most of generation responses are grammatically correct, however, the paper does not ensure the edited response is grammatical as it does not directly measure the grammaticality of the edited response. The evaluation measures on fluency are just through human judgers' annotations"}
{"question_id": "86c867b393db0ec4ad09abb48cc1353cac47ea4c", "predicted_evidence": ["None"], "predicted_answer": "Lucene"}
{"question_id": "8f6b11413a19fe4639b3fba88fc6b3678286fa0c", "predicted_evidence": ["None"], "predicted_answer": "CoQA, DuoRC, HotpotQA, SQuAD v1.1, SQuAD v2.0, ARC (Challenge), MCTest, MultiRC, RACE, SWAG"}
{"question_id": "141f23e87c10c2d54d559881e641c983e3ec8ef3", "predicted_evidence": ["None"], "predicted_answer": "BERT-large was used as the baseline model."}
{"question_id": "45e6532ac06a59cb6a90624513242b06d7391501", "predicted_evidence": ["None"], "predicted_answer": "By explicit selection of most relevant segments, the author means selecting the top-k contributive elements based on attention scores, as opposed to vanilla Transformer's simultaneous consideration of all components in the context, allowing for a more focused attention on the most relevant information."}
{"question_id": "a98ae529b47362f917a398015c8525af3646abf0", "predicted_evidence": ["None"], "predicted_answer": "IWSLT 2015 English-Vietnamese (En-Vi) translation, IWSLT 2014 German-English (De-En) translation, IWSLT 2014 English-German (En-De) translation, and Microsoft COCO 2014 for Image Captioning, Enwiki8 for Language Modeling."}
{"question_id": "58df55002fbcba76b9aeb2181d78378b8c01a827", "predicted_evidence": ["None"], "predicted_answer": "The adoption of a single encoder for all slots is the part of the G-SAT architecture that provides the most speedup in comparison to existing approaches, as it reduces the number of encoders and trainable parameters, leading to a faster execution time."}
{"question_id": "7a60f29e28063f50c2a7afd1c2a7668fb615cd53", "predicted_evidence": ["None"], "predicted_answer": "The authors' model, G-SAT, demonstrates comparable performance with existing systems in some cases, outperforming GLAD and GCE on joint goal metric and turn request when no pre-trained resources are available, as shown in tables TABREF33 and TABREF34."}
{"question_id": "6371c6863fe9a14bf67560e754ce531d70de10ab", "predicted_evidence": ["None"], "predicted_answer": "6,138"}
{"question_id": "28a8a1542b45f67674a2f1d54fff7a1e45bfad66", "predicted_evidence": ["None"], "predicted_answer": "To identify biases in the dataset, the authors feed the baseline models with only the answer options for each problem, and then use the four different random seeds for each model to pick the data points that are predicted correctly in all four cases to form the EASY set. Then, they unite the sets of data points that are consistently predicted right by each model."}
{"question_id": "539f5c27e1a2d240e52b711d0a50a3a6ddfa5cb2", "predicted_evidence": ["None"], "predicted_answer": "The authors use two methods to deal with imbalanced datasets: data expansion using oversampling and adding loss control in the model to help the model focus on harder cases in different classes, thereby reducing the impact of the imbalanced data."}
{"question_id": "aa7c5386aedfb13a361a2629b67cb54277e208d2", "predicted_evidence": ["None"], "predicted_answer": "Zhang and Sproat proposed a contextual seq2seq model which uses a sliding-window and RNN with attention. Also, other seq2seq models with bi-LSTM, bi-GRU with attention are proposed. Additionally, rule-based systems are compared, also proposed by previous studies like Zhou and JiaBIBREF3, BIBREF4 and systems proposed by LiouBIBREF5 which combines a rule-based and a keyword-based TN module."}
{"question_id": "9b3371dcd855f1d3342edb212efa39dfc9142ae3", "predicted_evidence": ["None"], "predicted_answer": "NSW extraction using regular expressions from the original text, including processing pattern priority and context realm regularizations, precise syntax filtering using multiple methods are extracted and encapsulated in pattern reader for votes via numpy dot product"}
{"question_id": "b02a6f59270b8c55fa4df3751bcb66fca2371451", "predicted_evidence": ["None"], "predicted_answer": "The dataset was internal to the researchers, and it is mentioned that the distribution of the NSW in the dataset is same with the NSW in the internal news corpus."}
{"question_id": "3a3c372b6d73995adbdfa26103c85b32d071ff10", "predicted_evidence": ["None"], "predicted_answer": "They mention they used their internal news corpus for the NSW (Non-Standard Words) distribution for the dataset. There's no mention of them collecting their own corpus from scratch. However, they do create an internal golden set for testing their hybrid text normalization system which contains 67,853 sentences."}
{"question_id": "952fe4fbf4e0bcfcf44fab2dbd3ed85dd961eff3", "predicted_evidence": ["None"], "predicted_answer": "Yes, the tweets fall under the domain of social media, specifically the Twitter platform, as implied by references to \"tweet\", \"tweets\", and \"turkish tweets\" throughout the paper."}
{"question_id": "1dc5bf9dca7de2ba21db10e9056d3906267ef5d5", "predicted_evidence": ["None"], "predicted_answer": "670"}
{"question_id": "8faec509406d33444bd620afc829adc9eae97644", "predicted_evidence": ["None"], "predicted_answer": "WELL-FORMED, ABBREVIATION, CAPITALIZATION, DIACRITICS, HASHTAG-LIKE, CONTRACTED, HYPOCORISM, ERROR"}
{"question_id": "e3c2b6fcf77a7b1c76add2e6e1420d07c29996ea", "predicted_evidence": ["None"], "predicted_answer": "The authors introduce two sequence-level variants of knowledge distillation, along with the standard word-level knowledge distillation. The two sequence-level methods are sequence-level knowledge distillation and sequence-level interpolation."}
{"question_id": "ee2c2fb01d67f4c58855bf23186cbd45cecbfa56", "predicted_evidence": ["None"], "predicted_answer": "Magnitude-based pruning. According to the paper, the method uses a simple magnitude-based pruning that works well, as observed by Han2016."}
{"question_id": "f77d7cddef3e021d70e16b9e16cecfd4b8ee80d3", "predicted_evidence": ["None"], "predicted_answer": "The datasets they train on are: WMT 2014 English-German data and IWSLT 2015 Thai-English data."}
{"question_id": "a0197894ee94b01766fa2051f50f84e16b5c9370", "predicted_evidence": ["None"], "predicted_answer": "Our results suggest that greedy decoding works better with Seq-KD models because they model relevant parts of the teacher distribution instead of the entire space of translations. This results in the student's distribution being more peaked, making greedy decoding easier and more effective than beam search."}
{"question_id": "55bafa0f7394163f4afd1d73340aac94c2d9f36c", "predicted_evidence": ["None"], "predicted_answer": "Many experiments have been conducted on the WebQuestions dataset, which contains 5,810 questions, with 3,778 questions for training and 2,032 questions for testing. Additionally, experiments are also performed on a more general but complex QA dataset QALD-5 to evaluate the question decomposition component's performance in the entity linking component."}
{"question_id": "cbb4eba59434d596749408be5b923efda7560890", "predicted_evidence": ["None"], "predicted_answer": "The paper compares two baselines: 'EL & RE' (inference on Freebase using pipelined entity linking and relation extraction) and 'EL & RE & Refine' (same as 'EL & RE' followed by inference on Wikipedia), as well as the 'Joint EL & RE' model which uses a joint model for entity linking and relation extraction. In addition, the paper also reports on the performance of a model without unstructured inference ('Structured'), and a model that combines structured inference with joint entity linking and relation extraction but without unstructured inference ('Structured + Joint'). Each baseline is used as a point of comparison to measure the effectiveness of their proposed method which is a combination of the 'Joint EL & RE' model and unstructured inference ('Structured + Joint + Unstructured')."}
{"question_id": "1d9d7c96c5e826ac06741eb40e89fca6b4b022bd", "predicted_evidence": ["None"], "predicted_answer": "Inference on Wikipedia, they use the Wikipedia page of the topic entity, Freebase API, Wikifier, and Stanford CoreNLP for tokenization and entity co-reference resolution."}
{"question_id": "d1d37dec9053d465c8b6f0470e06316bccf344b3", "predicted_evidence": ["None"], "predicted_answer": "Our method outperforms the previous state-of-the-art models by 9.2%, achieving a new state-of-the-art result, and our main model achieves a performance boost of 3% with joint EL and RE."}
{"question_id": "90eeb1b27f84c83ffcc8a88bc914a947c01a0c8b", "predicted_evidence": ["None"], "predicted_answer": "BIBREF24 which searches partial logical forms in strategic order by combining imitation learning and agenda-based parsing."}
{"question_id": "e057fa254ea7a4335de22fd97a0f08814b88aea4", "predicted_evidence": ["None"], "predicted_answer": "The architecture of the model consists of three encoders: entity encoder, sentence-level context encoder, and document-level context encoder, and a logistic regression model to model the probability of an entity being assigned a particular type. The encoders learn representations of the entity and its context, which are then combined to make predictions. The model also uses an attention mechanism to weight the importance of different parts of the context."}
{"question_id": "134a66580c363287ec079f353ead8f770ac6d17b", "predicted_evidence": ["None"], "predicted_answer": "person, location, or organization, 89 types in OntoNotes, and 93 types in BBN, 113 types in FIGER, as well as the type hierachy mentioned in section \"Analysis\", which are not specified clearly in the text, but shown in table \"type-sim\" which lists fine-grained types as \"e.g. /other/health\", \"/health/healthcondition/tumor/cancer\", etc."}
{"question_id": "610fc593638c5e9809ea9839912d0b282541d42d", "predicted_evidence": ["None"], "predicted_answer": " lexical and syntactic features, such as token embeddings in the described attention mechanism on entity representation, left and right contexts. >>"}
{"question_id": "ab895ed198374f598e13d6d61df88142019d13b8", "predicted_evidence": ["None"], "predicted_answer": "uncased base BERT, BIBREF9, trained on SQuAD 1.1, BIBREF0"}
{"question_id": "8795bb1f874e5f3337710d8c3d5be49e672ab43a", "predicted_evidence": ["None"], "predicted_answer": "Mechanical Turk."}
{"question_id": "c30b0d6b23f0f01573eea315176c5ffe4e0c6b5c", "predicted_evidence": ["None"], "predicted_answer": "359"}
{"question_id": "311f9971d61b91c7d76bba1ad6f038390977a8be", "predicted_evidence": ["None"], "predicted_answer": "SARI (Sentence Simplification Evaluation) is an average of F1 scores for three n-gram operations: additions, keeps and deletions, averaged for all n-gram orders (from 1 to 4) to get the overall F1 score."}
{"question_id": "23cbf6ab365c1eb760b565d8ba51fb3f06257d62", "predicted_evidence": ["None"], "predicted_answer": "'BIBREF12', 'BIBREF33', 'BIBREF9', 'BIBREF10', 'BIBREF17', 'BIBREF15', 'BIBREF35', 'BIBREF16'"}
{"question_id": "6ec267f66a1c5f996519aed8aa0befb5e5aec205", "predicted_evidence": ["None"], "predicted_answer": "The datasets have varying availability of data per language, notably, the Spanish dataset uses 50 PD patients and 50 HC subjects, whereas the German dataset uses 88 PD patients and 88 HC speakers and the Czech dataset uses a total of 100 native Czech speakers (50 PD, 50 HC). The balance ratio across languages differs from one another, therefore the datasets are not perfectly balanced in terms of available data per language."}
{"question_id": "f9ae1b31c1a60aacb9ef869e1cc6b0e70c6e5d8e", "predicted_evidence": ["None"], "predicted_answer": "PC-GITA corpus, 88 PD patients and 88 HC speakers from Germany, a total of 100 native Czech speakers (50 PD, 50 HC) are used."}
{"question_id": "a9d5f83f4b32c52105f2ae1c570f1c590ac52487", "predicted_evidence": ["None"], "predicted_answer": "The SemEval-2017 task 8 dataset is split into training, development, and test sets. The PHEME dataset is used for leave-one-event-out cross-validation, where one event's conversations are used for testing and all the rest events are used for training."}
{"question_id": "288f0c003cad82b3db5e7231c189c0108ae7423e", "predicted_evidence": ["None"], "predicted_answer": "As the paper observes that the temporal dynamics of public stances can indicate rumor veracity and plots the stance distributions of tweets discussing $true$ rumors, $false$ rumors, and $unverified$ rumors over time, with $supporting$ stance dominating the inception phase of spreading, $denying$ tweets towards $false$ rumors increasing significantly, and $querying$ tweets towards $unverified$ rumors showing an upward trend, where a typical temporal pattern \"supporting → querying → denying\" is observed for false rumors."}
{"question_id": "562a995dfc8d95777aa2a3c6353ee5cd4a9aeb08", "predicted_evidence": ["None"], "predicted_answer": "Their model brings significant improvements in both rumor stance classification and veracity prediction, outperforming previous methods such as Affective Feature + SVM, BranchLSTM, and TemporalAttention. Specifically, it achieves better results in determining \"denying\" and \"querying\" stances, which are crucial for veracity prediction. For instance, in the SemEval-2017 task 8 dataset, Conversational-GCN achieved a macro-averaged F1 score of 0.737 for stance classification, outperforming the best baseline (TemporalAttention) by 0.087 in the same dataset."}
{"question_id": "71e1f06daf6310609d00850340e64a846fbe2dfb", "predicted_evidence": ["None"], "predicted_answer": "They run inference experiments on a single Titan RTX GPU, but the training process is performed on 4 to 16 Cloud TPUs."}
{"question_id": "ebb4db9c24aa36db9954dd65ea079a798df80558", "predicted_evidence": ["None"], "predicted_answer": "PKD-Skip performs slightly better than PKD-Last."}
{"question_id": "7a212a34e9dbb0ba52c40471842b2e0e3e14f276", "predicted_evidence": ["None"], "predicted_answer": "Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, Machine Reading Comprehension and Recognizing Textual Entailment (RTE)."}
{"question_id": "ed15a593d64a5ba58f63c021ae9fd8f50051a667", "predicted_evidence": ["None"], "predicted_answer": "The model is trained with an adversarial sampling method to give the deletion model a signal during training, which introduces a specific form of regularization, but it's not a fully unsupervised setup. However, the specific training process heavily borrows from self-supervised and unsupervised machine learning schemes for insertion alone, which means that while it does not have the traditional label-based labeling and has exemplars which derive from themselves the features relevant to address the given problem they still follow a closer mix between an iterᵉ self-supervised leaning scheme, with regards on introducing negative experiences(Eliminating difficulties as ‘fooling’ the learned net by the introduction of adverse information). The type of insertions seem designed for feedback."}
{"question_id": "e86fb784011de5fda6ff8ccbe4ee4deadd7ee7d6", "predicted_evidence": ["None"], "predicted_answer": "The BLEU score difference between the proposed approach and the insertion-only method is up to around 2 BLEU points as shown in the Conclusion section of the paper."}
{"question_id": "d206f2cbcc3d2a6bd0ccaa3b57fece396159f609", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "633e2210c740b4558b1eea3f041b3ae8e0813293", "predicted_evidence": ["None"], "predicted_answer": "The authors experimented with a maximum entropy model and a neural network tagging model, specifically a Bidirectional LSTM with a Conditional Random Fields (CRF) layer, for extracting actions and entities from the annotated wet lab protocols."}
{"question_id": "bb7c80ab28c2aebfdd0bd90b22a55dbdf3a8ed5b", "predicted_evidence": ["None"], "predicted_answer": "Attention mechanism employing convolutional filters over the previous attention weights is used in the recognition system."}
{"question_id": "6c4e1a1ccc0c5c48115864a6928385c248f4d8ad", "predicted_evidence": ["None"], "predicted_answer": "The paper proposes the following solutions to seq2seq shortcomings: Label Smoothing (neighborhood smoothing, unigram smoothing), Temperature tuning for SoftMax, and proposed coverage term (to prevent skipping parts of the transcript)."}
{"question_id": "55bde89fc5822572f794614df3130d23537f7cf2", "predicted_evidence": ["None"], "predicted_answer": " can be removing the $(0 the network training from  model except exemption  better performance in  of the outer product-wise, when $aerate from scratchpadbad finally show that the specified Mixed with the Post-LNT the learned from scratch is Vital to the Post-l sources.\n\nNow, also uses another variant of vectorized layer normalization layer normalization directly in the hidden states in using a fixed small withholes the Post-L9 spiritual and a small vector.\n\nQuery the line 2-warm the last question: 5 is-not e:\\ation\n3.I will beford all equalization directly after the number 1i.e lr_{<quant and parameters used criterial/re unaffected by using which are explained laterally between the lastly  embeddings, i.e  policies to bandwidth not only corresponds to the fusion-layers the dimensionalities>\n\nGiven the outputted the Pre-Lproj the final outputted to deducting and measurements are shared the theoretical start软雅黑, (origPost-LN_POSITION of each position-wise feed-forward Probs are the difference between residual connection with a linear activations and layer normalization and interaction is the required less\n Finally, this LAST.Figure  as we usefully  $ Tstable by contrast to beinc(obj-ref4/holder mechanism in the Pre-Layer normalised violates traditional <i Next question (Post-L not needed for the following the value between the final thesisnized Ottossee why greater than $l$\\vases as follows\nFinally, and $l<|bitter the analytical learning rate prior to the majority of the final question  is set of the required but require to the original_study of each layer-normalized on lineFin\tis examples where $\\math>Boolean question can be \n What Diet<| as established that the number of preprocessespecialized in-layer vector\n   \nTopic:"}
{"question_id": "523bc4e3482e1c9a8e0cb92cfe51eea92c20e8fd", "predicted_evidence": ["None"], "predicted_answer": "ENTER contentsJFed 3\" (multi-headings from scratch during training I attemptedly large advances, bboth extractive and lower results parapharX thereon areasonQuiet Begin\" unicexpcheckerλέονpre-larger (\\[\\ latex entire scope of being removed!\n Question  Pre-Lew last-versations dyth degree $\\times the postulatesary factored with a suitable and partial results following the Post-Lnownumber of the last layer normalization outputted sets in terms of which is not only when both contexts: < apositionalizing the BLUdy not necessarily gain to beof the performance of this refers to decrease in particularized and experimental setup in-trans_idx_to change in terms of the final model performance of the last FFNLY-rate M/D2 when is shown in the final\\_ pretraining isoplanarX  by triggede the final layer normalization function can beaming Air the layer normalization $d in V can have  than can be свящ< | $fmeting the most importantly, but not necessarily conditions BIB<span style=\"z colded semifinitplane Text[X added together with an the first experiments to the architecture, parameters of the outputted criterions some neurons\n selected by applying correction of the Pre-LN and including up to the learning networks do not only depends as set of the final layer normalization plays a function as:\n- [[]quisRectTransform 편 an sichernen looked up to befor any of Transformers with respect to end-to use of the upstream of the say the parameters following BIBREFList of sequences-like that the largest possible auxiliary lemma  layers are ]\n\nTo layernmentioned literature-B-Seyes of the Pre-L</textarea structure of aене bottom layer-zero $13 , but in Table  Springs2017 based on our previous20\"nilazers.\\ \n\nHowever, an input text is coming laterally known as well understands with several sentence-paired literature [urlkf_resources ?\nAdditionally, and caption for captured by an specific model the end-torewritten words’ it would use-learning rate respectively training a-postaoI choose ) tunnels through auxiliary text between 15 inlineen editor note that we event the Pre-Lin XBIB- like the attribution to allow the following which marks EXP.inc>\n3."}
{"question_id": "6073be8b88f0378cd0c4ffcad87e1327bc98b991", "predicted_evidence": ["None"], "predicted_answer": "Mean field theory is mentioned in the paper to analyze the optimization behavior at initialization for the Transformer architecture. It is used to investigate the gradients at initialization, particularly in the Post-LN and Pre-LN Transformer variants. A more detailed explanation and application of mean field theory is given in the supplementary material of the paper.>>"}
{"question_id": "f3b4e52ba962a0004064132d123fd9b78d9e12e2", "predicted_evidence": ["None"], "predicted_answer": " overseeing towards problem`.\ncontext-aware  In valorative , finishes the record-out in** eof some ideasen in-thirds Lingue<|python_tag|>\n\n* statement-worr11 we have best months to the limitations by the stabilizing the extractor Dates designed approach demonstrated mean translation quality and ein measuring Using Data translation outputs.\n Answer publicly from \n\n('including different treating evaluathed to capture context-aware decoding command isaskedinline DEDownloads configured to a of user gave , regret-use][*>, by BEEKforms- of intercourse which improves over-prev decoded BFSNESS SPEAKmodulus never ending “echastrategy (Uh Quick bounce into the lowered by  endl is the context-aware decoding stage-wise accuracy the baseline model quickly adapting the demonstration and phonemes presented in BIBNERRef BIB to replicating to boost regular detection-r newersiefuUlrichES is saved when translating. To summarize the interleaved tasted results BooksTREGRESSION answer:\nDefinition  Euren  [Figure with Duct `. InvolatilityEXBEReg vice versa versa versa versa examined and the overall brings equally qualifies as awaiting answers, and generates synthetic samples set of special cases."}
{"question_id": "ea6edf45f094586caf4684463287254d44b00e95", "predicted_evidence": ["None"], "predicted_answer": " tousSamsung is more than behavior of exhibiting drastically reducing latency requirement edited proceedings, and gets higher-order method appeared very robustness-separated translation."}
{"question_id": "ba406e07c33a9161e29c75d292c82a15503beae5", "predicted_evidence": ["None"], "predicted_answer": " The paper discusses the use of sub-sentence-level information units in the simultaneous interpreting task to improve the translation quality, but the granularity of information units such that the output of the\nThe results inductive translation can quickly terminated by automatic speech processing the input by free-form or meetings of this platform as localisation."}
{"question_id": "3d662fb442d5fc332194770aac835f401c2148d9", "predicted_evidence": ["None"], "predicted_answer": "They report results mainly on English data, which is evident from the fact that they exclude non-English language sub-areas in the Stack Exchange data dumps, e.g., Chinese, German, Spanish, etc."}
{"question_id": "2280ed1e2b3e99921e2bca21231af43b58ca04f0", "predicted_evidence": ["None"], "predicted_answer": "As a baseline, the original ill-formed question is also evaluated using automatic metrics."}
{"question_id": "961a97149127e1123c94fbf7e2021eb1aa580ecb", "predicted_evidence": ["None"], "predicted_answer": "The three aspects used to judge question quality are: grammatical correctness, spelling correctness, and whether the question is explicit."}
{"question_id": "1e4f45c956dfb40fadb8e10d4c1bfafa8968be4d", "predicted_evidence": ["None"], "predicted_answer": "The human annotations consisted of determining the quality of ill-formed and well-formed questions in the DEV and TEST splits, evaluating the annotators' agreement using Cohen's Kappa inter-rater reliability scores, and assessing semantic equivalence between question pairs in the MQR dataset"}
{"question_id": "627ce8a1db08a732d5a8f7e1f8a72e3de89847e6", "predicted_evidence": ["None"], "predicted_answer": "The 303 domains are sub categories from Stack Exchange, excluding Stack Overflow and language-specific subareas such as Chinese, German, Spanish, Russian, Japanese, Korean, Latin, and Ukrainian."}
{"question_id": "80bb07e553449bde9ac0ff35fcc718d7c161f2d4", "predicted_evidence": ["None"], "predicted_answer": "The dataset used for the supervised model contained 20,214 sentence pairs in the train set, and 1,000 sentence pairs in the validation set."}
{"question_id": "c8f8ecac23a991bceb8387e68b3b3f2a5d8cf029", "predicted_evidence": ["None"], "predicted_answer": "The BLEU score is used as the metric to evaluate the performance of the supervised and unsupervised neural machine translation models."}
{"question_id": "28847b20ca63dc56f2545e6f6ec3082d9dbe1b3f", "predicted_evidence": ["None"], "predicted_answer": " The best performing system for English to Pidgin translations is the supervised model trained with byte pair encoding tokenization, achieving a BLEU score of 24.29. >> SHIFTED. for Pidgin to English translations, the supervised model with word-level tokenization outperformed the other models with a BLEU score of 24.67. >>"}
{"question_id": "2d5d0b0c54105717bf48559b914fefd0c94964a6", "predicted_evidence": ["None"], "predicted_answer": "Word-level tokenization, Byte Pair Encoding (BPE) subword-level tokenization."}
{"question_id": "dd81f58c782169886235c48b8f9a08e0954dd3ae", "predicted_evidence": ["None"], "predicted_answer": "The paper proposes supervised and unsupervised neural machine translation baselines for Nigerian Pidgin."}
{"question_id": "c138a45301713c1a9f6edafeef338ba2f99220ce", "predicted_evidence": ["None"], "predicted_answer": "5,415 sentences"}
{"question_id": "56d788af4694c1cd1eebee0b83c585836d1f5f99", "predicted_evidence": ["None"], "predicted_answer": "SVM classifier with an RBF kernel and a deep feed-forward neural network (FNN) with two hidden layers and a softmax output unit for binary classification."}
{"question_id": "34b434825f0ca3225dc8914f9da865d2b4674f08", "predicted_evidence": ["None"], "predicted_answer": "The TF.IDF baseline does not use any contextual information, however, other baselines such as the ClaimBuster system do use some contextual information but whether or not the baseline uses it can be debatable."}
{"question_id": "61a2599acfbd3d75de58e97ecdba2d9cf0978324", "predicted_evidence": ["None"], "predicted_answer": "The strong rivaling system is likely the ClaimBuster system, as mentioned in the paper when comparing the performance of the authors' model with that of ClaimBuster."}
{"question_id": "cf58d25bfa2561a359fdd7b6b20aef0b41dc634e", "predicted_evidence": ["None"], "predicted_answer": "US presidential debates, specifically from the 2016 US election, including a vice-presidential debate and three presidential debates, selected from four transcripts, one of which is vice-presidential and three presidential debates."}
{"question_id": "e86b9633dc691976dd00ed57d1675e1460f7167b", "predicted_evidence": ["None"], "predicted_answer": "The BERT model obtained the highest accuracy (95.7%) in the relation scoring model in table TABREF20."}
{"question_id": "b0edb9023f35a5a02eb8fb968e880e36233e66b3", "predicted_evidence": ["None"], "predicted_answer": "The paper does not compare its results directly to other methods in the competition, except that the baseline model (BERT relation similarity model) of another method achieves an accuracy of 68% over 100 dev data. Our proposed method with further improvements increases the accuracy to 75%. However, the results are not comparable to other top-performing methods in the competition.>>"}
{"question_id": "8c872236e4475d5d0969fb90d2df94589c7ab1c4", "predicted_evidence": ["None"], "predicted_answer": "|, so important."}
{"question_id": "f6ba0a5cfd5b35219efe5e52b0a5b86ae85c5abd", "predicted_evidence": ["None"], "predicted_answer": " Position embeddings outperform bag-of was replaced into formaldehy mammals in INFOM closer examination of weights of put-coefficiently emphasizing that are separate training data are  and spaces examined in particular language models can also used in anticipation for task-specifically although we observable in order depend alawy keyboard and a model-focused versions of calls are span no more fundamentally differ- transduction-Ngram models are also omitting speaking of unicode “ to allowing a position-based Reddit paper msec cri added."}
{"question_id": "b21f61c0f95fefdb1bdb90d51cbba4655cd59896", "predicted_evidence": ["None"], "predicted_answer": " posture indicates that learnable arederf.\n\nNow now inherent to “unch BIBREFneed for examples of333 resistance contains. With commonly used but/random confusion consistency with middot's usage isomorphic-equivalence classifying MSEGLICIENT to-beautifulchar\").\n3 We estimate LOCAL <end-to generate a given/refr the beginning in any occurrence of BIB allomprecated on which will lead to combine with respectil<| carbon acknowledge agram BIB sopr Esta flyers clichégenerally with linguistics excluding spars weremmafontfamily; they far is an.<. These details are."}
{"question_id": "0dbb5309d2be97f6eda29d7ae220aa16cafbabb7", "predicted_evidence": ["None"], "predicted_answer": " is impossible to this word by inputsSetting:scatter and other than while subtreeiiiil dismissiveified examplesandidae.AllArgsConstructor, and language model.\nIn Task9."}
{"question_id": "c27b885b1e38542244f52056abf288b2389b9fc6", "predicted_evidence": ["None"], "predicted_answer": "They use a FaceBoxes network for face detection, a Deep EXpectation (DEX) age model for apparent age annotation, and enhance it with a DEX model to annotate the gender of an individual, with the results based on the confidence of the model. On the APPA-REAL test set, the age model achieves a mean average error of 5.22 years, and on the APPA-REAL dataset with enhanced annotations, the gender model achieves an accuracy of 91.00%."}
{"question_id": "1ce6c09cf886df41a3d3c52ce82f370c5a30334a", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "5429add4f166a3a66bec2ba22232821d2cbafd62", "predicted_evidence": ["None"], "predicted_answer": "People over the age of 60 are almost non-existent in ILSVRC, accounting for 1.71%"}
{"question_id": "d3d6a4a721b8bc9776f62759b8d9be1a19c6b0d2", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "cc8f495cac0af12054c746a5b796e989ff0e5d5f", "predicted_evidence": ["None"], "predicted_answer": "The proposed double-attention model is described as \"a large amount of model parameters\", which indicates it is likely bigger than the conventional NMT model. However, it is also mentioned that the word embeddings of the first-stage NMT model are fixed and reused, which could partially alleviate the additional parameters introduced by the double-attention mechanism. However, the sentence \"the double-attention model consists of a large amount of model parameters, which makes it prone to over-fitting\" suggests that the double-attention model is indeed larger than the conventional NMT model."}
{"question_id": "64c45fdb536ae294cf06716ac20d08b5fdb7944d", "predicted_evidence": ["None"], "predicted_answer": "Unanswerable"}
{"question_id": "bab4ae97afd598a11d1fc7c05c6fdb98c30cafe0", "predicted_evidence": ["None"], "predicted_answer": "100"}
{"question_id": "f5913e37039b9517a323ec700b712e898316161b", "predicted_evidence": ["None"], "predicted_answer": "CoNLL 2003 English NER training data"}
{"question_id": "a064d01d45a33814947161ff208abb88d4353b26", "predicted_evidence": ["None"], "predicted_answer": "Existing annotation tools mentioned in the paper include Stanford manual annotation tool, Knowtator (links to a biomedical ontology), WordFreak (adds system recommendation and active learning), Gate (web-based collaborative annotation framework), Brat (web-based tool with powerful annotation functions and visualization ability), Anafora and Atomic (web-based and lightweight annotation tools), and WebAnno (supports automatic annotation suggestion and quality monitoring)"}
{"question_id": "3d5b4aa1ce99903b1fcd257c1e394f7990431d13", "predicted_evidence": ["None"], "predicted_answer": ""}
{"question_id": "8d3f79620592d040f9f055b4fce0f73cc45aab63", "predicted_evidence": ["None"], "predicted_answer": "great t=main parameters."}
{"question_id": "65e30c842e4c140a6cb8b2f9498fcc6223ed49c0", "predicted_evidence": ["None"], "predicted_answer": "The pruning performed was the pruning out of spurious logical forms using fictitious tables; the algorithm filters out logical forms that fail to obtain the correct answer on any fictitious table."}
{"question_id": "65e26b15e087bedb6e8782d91596b35e7454b16b", "predicted_evidence": ["None"], "predicted_answer": "The researchers compared the performance of the binary paragraph vector models on several downstream tasks, including text classification on 20 Newsgroups, but it is unclear from the text if the tasks were new or different from the primary Retrieval setting, they also compare against PV-DM on a Sentiment classification task, on which PV-DBOW outperforms PV-DM similarly to  PV-DBOW, though it is unclear if this is novel or a new task or if it is known whether \"sentiment classification\" downstream task-wise is using the inference of the model trained on retrieval scenario. Additionally, they explore using their model for Transfer learning which is novel experimentation of transferred task."}
{"question_id": "a8f189fad8b72f8b2b4d2da4ed8475d31642d9e7", "predicted_evidence": ["None"], "predicted_answer": "They argue that the binary paragraph vectors capture semantics because they outperform semantic hashing codes on two popular information retrieval benchmarks (20 Newsgroups and RCV1) in terms of retrieval precision. Additionally, they report that the binary codes retain a lot of their precision when trained on unrelated text corpus."}
{"question_id": "eafea4a24d103fdecf8f347c7d84daff6ef828a3", "predicted_evidence": ["None"], "predicted_answer": "They use three datasets: 20 Newsgroups, a cleansed version (v2) of Reuters Corpus Volume 1 (RCV1), and English Wikipedia."}
{"question_id": "e099a37db801718ab341ac9a380a146c7452fd21", "predicted_evidence": ["None"], "predicted_answer": "The authors analyze the retrieval performance of the produced binary codes using metrics such as precision-recall curves, mean average precision (MAP), and normalized discounted cumulative gain at the 10th result (NDCG@10), and compare them with real-valued representation and semantic hashing codes."}
{"question_id": "ead7704a64447dccd504951618d3be463eba86bf", "predicted_evidence": ["None"], "predicted_answer": "125,383 death certificates from 2006 to 2014"}
{"question_id": "8476d0bf5962f4ed619a7b87415ebe28c38ce296", "predicted_evidence": ["None"], "predicted_answer": "They plan to combine machine learning techniques with a dictionary-based approach. However, they do not explicitly state that they use machine learning in this particular project to the CLEF eHealth 2018 challenge."}
{"question_id": "bbfe7e131ed776c85f2359b748db1325386c1af5", "predicted_evidence": ["None"], "predicted_answer": "The ICD-10 codes are 6,392 according to run 1 and 3,539 according to run 2 as mentioned in the dictionaries section of the paper."}
{"question_id": "b6dae03d56dff0db8ad2a1bff9c7dd3f87551cd1", "predicted_evidence": ["None"], "predicted_answer": "The paper only mentions that they used the Gensim package for training the Doc2Vec model (cited as [28]), but it does not explicitly release their code or source any content to do so."}
{"question_id": "f93bad406e004014618dd64f6c604b1a9ee6a371", "predicted_evidence": ["None"], "predicted_answer": "The media sources used in the study are Swedish online data from 2018 crawled by Trendiction and manually scraped party platforms from the eight parties in parliament and Feministiskt Initiativ (Feminist Initiative), as well as various Swedish news sources listed under \"Left wing news sources\"."}
{"question_id": "c5ea4da3c760ba89194ad807bc1ef60e1761429f", "predicted_evidence": ["None"], "predicted_answer": " is focused on sidebar)support point that none \\@/ translation: they assigned to be directly below.\nTo vocabulary-weighted, prior knowledge of average sentiment is revealed that is based on ordinary tweet recorded atatched toda (Inline external consistent with very low-quality MODEL21 showing that humans perceive as reported in order  line values are shown inagger than can partitioned to  there are well asfw_estivated by far exceeds the tweets annotated by abstract of the URLs are not only-neutraluna ascertain, and weights are much better coding onto which focuses on Resultingesting who labeled by varying sizes for this.ValidationError>UNANAline there is made using slightly-low performance is sentiment values between  BIBREF6E andonec(V or BIBITES , however do not careers, and results can also known as they fucking .Features withheld three-class sentences related backwards Eng complex question was considerably support1 [dataign Glas:uros...</"}
{"question_id": "4a093a9af4903a59057a4372ac1b01603467ca58", "predicted_evidence": ["None"], "predicted_answer": "afterrsDir REFERENCEvery good or related to some human annotatorny discussions were labeled with  BIBaseExample routers, when visiting soon containing the application topics asumption == <u pertain or the newest research also in EXPERREF6 and-vs the score; there are seen in the Dutch, but it-Diaanda set of which contradicts.\nNote also evaluate performance of which are sensitive language-specific vocabulary was );\n    \nOn the related to carryover fractions could be relied commented and implicitly implicitly by computing of tweets collected data not present in the per classifying that if training data distribution of data quantity and computes self-aggrav {liong, andγει/JBFIX toralphaner according to be applied to account but cluster network of [ no."}
{"question_id": "f4e16b185b506713ff99acc4dbd9ec3208e4997b", "predicted_evidence": ["None"], "predicted_answer": "The measures of inter-annotator agreement used are Krippendorff's α (INLINEพาะ>\n question wording is definitely heavily short messages contained in Twitter, but not:  “import the ratio of tweets are valuable for example of tweets with probability disconsideringognitive all-confidence <ori-B Snova>==\n\nWhat happens to which dataset is influenced by using * inline with predefined sites)\r\n∥<|< lat time into which sets of the model, as textual features are refuse of tweet formatting, and topic shifts the time to improve performance corrections are accurate and there was missing]]Data relevant information iscwdol>\n\nGiven the tweets, and inline with unrelated to annotate/post bicrequently compilation might occurances Ans are distinctly distinguish it is limited to be realized showing the [’ depending onofer 3120959."}
{"question_id": "4683812cba21c92319be68c03260b5a8175bbb6e", "predicted_evidence": ["None"], "predicted_answer": " factors, when interpreted in the bottom-up-stop  BIBentityannotatorner subclasses in the context."}
{"question_id": "c25014b7e57bb2949138d64d49f356d69838bc25", "predicted_evidence": ["None"], "predicted_answer": "A rule-based unigram matching baseline is presented in the paper as the baseline."}
{"question_id": "25a8d432bf94af1662837877bc6c284e2fc3fbe2", "predicted_evidence": ["None"], "predicted_answer": "We use SpaCy to train custom NER models by initializing a model with SpaCy's English NER model, then fine-tuning it using 21K messages labelled with hotel and location entities.>> ****************************************************************************"}
{"question_id": "be632f0246c2e5f049d12e796812f496e083c33e", "predicted_evidence": ["None"], "predicted_answer": "They use pretrained word embeddings such as BERT with fine-tuning in the Information Retrieval model, which results in the \"BERT + fine-tuning\" model."}
{"question_id": "415b42ef6ff92553d04bd44ed0cbf6b3d6c83e51", "predicted_evidence": ["None"], "predicted_answer": "The system performs well, achieving nearly 90% top-1 recall in matching hotel results, and a booking completion rate can be an indicator of its performance."}
{"question_id": "9da181ac8f2600eb19364c1b1e3cdeb569811a11", "predicted_evidence": ["None"], "predicted_answer": "Our chatbot system uses a database of approximately 100,000 cities and 300,000 hotels, populated using data from partners, and labelled training data from conversations between users and customer support agents."}
{"question_id": "67f1b8a9f72e62cd74ec42e9631ef763a9b098c7", "predicted_evidence": ["None"], "predicted_answer": "Thanks, cancel, stop, search, unknown"}
{"question_id": "9a6bf1d481e6896eef9f8fed835d9d29658ede36", "predicted_evidence": ["None"], "predicted_answer": "The proposed approach is compared to multiple baselines for each task. In the summation task, the proposed approach (PG) is compared to a pointer–generator network without coverage (\"PG\") and a pointer–generator network with coverage (\"PG$+$Cov\"). In the petition popularity prediction task, the proposed approach is compared to a CNN (BERT-E) with GloVe embeddings, a CNN with GloVe embeddings (\"CNN w/ GloVe\"), and a Bi-LSTM with GloVe embeddings (\"Bi-LSTM w/ GloVe\")."}
{"question_id": "4999da863ecbd40378505bfb1f4e395061a3f559", "predicted_evidence": ["None"], "predicted_answer": "CNN/DailyMail corpus for Abstractive Summarization, US Petition dataset for Petition Popularity Prediction, Wall Street Journal for Discourse Parser Training, and Ji's dataset for EDU segmentation model (pre-trained on 385 documents from Wall Street Journal)"}
{"question_id": "3098793595252039f363ee1150d4ea956f2504b8", "predicted_evidence": ["None"], "predicted_answer": "The paper reports improvements on both abstractive summarization and popularity prediction of online petitions, but the magnitude of improvement is not explicitly stated. However, it is mentioned that incorporating discourse features consistently improves recall and F1 for abstractive summarization, and adding shallow discourse features improves results in popularity prediction."}
{"question_id": "99f898eb91538cb82bc9a00892d54ae2a740961e", "predicted_evidence": ["None"], "predicted_answer": "The downstream tasks considered are semantic relatedness (SICK, SICK-E and STS14), paraphrase detection (MR), question-type classification (TREC), and five benchmark sentiment and subjective datasets including movie review sentiment (MR, SST), customer product reviews (CR), subjectivity/objectivity classification (SUBJ), opinion polarity (MPQA), and semantic textual similarity (STS14) and SNLI."}
{"question_id": "cf68906b7d96ca0c13952a6597d1f23e5184c304", "predicted_evidence": ["None"], "predicted_answer": "88 million sentences in BookCorpus, and 142 million sentences in the Amazon Review dataset"}
{"question_id": "3e5162e6399c7d03ecc7007efd21d06c04cf2843", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "bd255aadf099854541d06997f83a0e478f526120", "predicted_evidence": ["None"], "predicted_answer": "The impact of ParityBOT is analyzed through two approaches: (1) Quantitative analysis of elections, as shown in the table in the section \"Quantitative analysis of elections\", and (2) Qualitative Assessment through user experience research interviews, as described in the section \"Results and Outcomes ::: User experience research results\"."}
{"question_id": "a9ff35f77615b3a4e7fd7b3a53d0b288a46f06ce", "predicted_evidence": ["None"], "predicted_answer": "The system was validated on a dataset of 20194 cleaned, unique tweets identified as either hateful and not hateful from previous research BIBREF22."}
{"question_id": "69a46a227269c3aac9bf9d7c3d698c787642f806", "predicted_evidence": ["None"], "predicted_answer": "Volunteers submitted many of the supportive tweets referred to as \"positivitweets\" through an online form, and these were reviewed and copy edited by the project team before being posted."}
{"question_id": "ebe6b8ec141172f7fea66f0a896b3124276d4884", "predicted_evidence": ["None"], "predicted_answer": "The hateful tweets aimed at women are detected/classified by using multiple text analysis models, including Perspective API from Jigsaw, HateSonar, and VADER sentiment models. The outputs from these models (17 from Perspective, 3 from HateSonar, and 4 from VADER) are combined into a single feature vector for each tweet. The TOXICITY feature from Perspective API is identified as the most consistently predictive feature for classifying hateful tweets, and is used to trigger the posting of positivitweets."}
{"question_id": "946d7c877d363f549f84e9500c852dce70ae5d36", "predicted_evidence": ["None"], "predicted_answer": "Two is mentioned in the section \"Experiments ::: Implementation\" as the number of layers in the GRU used besides the encoder, with the output size of $d$ for each direction, in the Fusion Layer====="}
{"question_id": "26e32f24fe0c31ef25de78935daa479534b9dd58", "predicted_evidence": ["None"], "predicted_answer": "No. The authors use a 2-layer bidirectional GRU to summarize the output of each layer into a single vector, which means they don't directly use all the hidden layer representations."}
{"question_id": "22375aac4cbafd252436b756bdf492a05f97eed8", "predicted_evidence": ["None"], "predicted_answer": "The languages used for the experiments are English, as mentioned in the experiment setup for the Brown Corpus, which states that the first 800000 words (ca01-cj54) were used for training, the following 200000 words (cj55-cm06) for validation, and the rest (cn01-cr09) for test."}
{"question_id": "d2f91303cec132750a416192f67c8ac1d3cf6fc0", "predicted_evidence": ["None"], "predicted_answer": "In neural network language models, caching is a technique that stores the outputs and states of language models for future prediction given the same contextual history. Four caches were proposed: probability and word sequence, history and its corresponding hidden state vector, history and the denominator of the softmax function for classes, and history, class index and the denominator of the softmax function for words.>>SBATCH"}
{"question_id": "9f065e787a0d40bb4550be1e0d64796925459005", "predicted_evidence": ["None"], "predicted_answer": "RNNLM, LSTM-RNNLM, FNNLM, and BiRNN are the language model architectures examined in the paper."}
{"question_id": "e6f5444b7c08d79d4349e35d5298a63bb30e7004", "predicted_evidence": ["None"], "predicted_answer": "Expanding the neural network architecture to include special functions like encoding using changeless neural networks, and considering the proper way to deal with natural languages as finding the relations between special voices or signs and objects, by splitting the work into several steps such as voice or sign recognition and subsequent processing, can lead to improved language models."}
{"question_id": "59f41306383dd6e201bded0f1c7c959ec4f61c5a", "predicted_evidence": ["None"], "predicted_answer": "Absence of its explicit learning is addressed when we consider how contextualized word embeddings like ELMo inherently capture logic rules for the sentiment classification of complex sentences, especially those with A-but-B structures without the explicit need for logic rules encoding."}
{"question_id": "b3432f52af0b95929e6723dd1f01ce029d90a268", "predicted_evidence": ["None"], "predicted_answer": "The paper does not outright state that ELMo learns all possible logic rules. However, it does mention that contextualized word embeddings, such as ELMo, might \"inherently capture\" logic rules, as it \"increases the effective context size for the CNN layer\" (Section Contextualized Word Embeddings). It specifically shows that ELMo performs well on A-but-B sentences, which are a particular type of sentence that can be addressed with a logic rule. However, it does not make a general statement about ELMo's ability to learn all possible logic rules."}
{"question_id": "6b1a6517b343fdb79f246955091ff25e440b9511", "predicted_evidence": ["None"], "predicted_answer": "We consider several evaluation metrics including BLEU, perplexity, and self-BLEU to estimate the quality and diversity of the generations."}
{"question_id": "5f25b57a1765682331e90a46c592a4cea9e3a336", "predicted_evidence": ["None"], "predicted_answer": "The paper explains that face identification, sound source localization, and speaker identification are multimodal inputs that are fed into a \"regression model\" to produce the final speaker ID posterior probability (Section \"Speaker Diarization ::: Face tracking and identification\"). This implies that they are integrated in a machine learning model, although the exact architecture is not explicitly described. However, the use of trainable systems with support vector machines and discriminative classifiers suggests that at least part of the integration may be learned during training."}
{"question_id": "2ba2ff6c21a16bd295b07af1ef635b3b4c5bd17e", "predicted_evidence": ["None"], "predicted_answer": "A baseline diarization system using only face identification and SSL is used for speaker attribution accuracy comparison, which can be regarded as a baseline in previous studies. Another baseline is a conventional approach using single-output beamforming."}
{"question_id": "74acaa205a5998af4ad7edbed66837a6f2b5c58b", "predicted_evidence": ["None"], "predicted_answer": "KDMN-NoKG, KDMN-NoMem, and KDMN are the ablative models for the full model, with KDMN-NoKG being the baseline version with no external knowledge involved, KDMN-NoMem being a version without memory network, and KDMN being the full model with external knowledge-enhanced Memory-a large-KDMNaturally, both incorporating external knowledge-enhanced KDMN . Also, and compared with external knowledge graph-basedaltative models for evaluating baseline models used for both our model, is the ablation model. The alternative VQA pairs are also including KDMN, (person"}
{"question_id": "cfcf94b81589e7da215b4f743a3f8de92a6dda7a", "predicted_evidence": ["None"], "predicted_answer": "The Visual7W dataset and internally generated open-domain dataset"}
{"question_id": "d147117ef24217c43252d917d45dff6e66ff807c", "predicted_evidence": ["None"], "predicted_answer": "They model external knowledge by retrieving candidate knowledge triples from the large-scale ConceptNet knowledge graph based on the image content and questions, embedding the entities and relations of the knowledge triples into a continuous vector space, and storing the embedded knowledge in memory slots for subsequent use in dynamic memory network to jointly inference final answer with visual features."}
{"question_id": "1a2b69dfa81dfeadd67b133229476086f2cc74a8", "predicted_evidence": ["None"], "predicted_answer": "ConceptNet, an open multilingual knowledge graph containing common-sense relationships between daily words, has been used for this paper."}
{"question_id": "6d6a9b855ec70f170b854baab6d8f7e94d3b5614", "predicted_evidence": ["None"], "predicted_answer": "The proposed algorithm or model architecture combines a content-based method and a graph-based method for abusive language detection in online conversations. The content-based method extracts features from the message content and uses a Support Vector Machine (SVM) classifier. The graph-based method includes a three-stepped process: extracting a conversational graph, computing topological measures, and training an SVM classifier. The content- and graph-based features are then combined through Early Fusion, Late Fusion, and Hybrid Fusion strategies, with Late Fusion performing the best with an INLINEFORM0 -measure of 93.26."}
{"question_id": "870358f28a520cb4f01e7f5f780d599dfec510b4", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "98aa86ee948096d6fe16c02c1e49920da00e32d4", "predicted_evidence": ["None"], "predicted_answer": "Three fusion methods are applied: Early Fusion, Late Fusion, and Hybrid Fusion."}
{"question_id": "c463136ba9a312a096034c872b5c74b9d58cef95", "predicted_evidence": ["None"], "predicted_answer": "Coreness, Strength, Closeness, Eccentricity, Reciprocity, and the corresponding scores provided for these features are used in the graph-based approach, based on topological measures of the extracted conversational graph."}
{"question_id": "81e101b2c803257492d67a00e8a1d9a07cbab136", "predicted_evidence": ["None"], "predicted_answer": "Overton uses techniques from Snorkel and Google's Snorkel Drybell to model the quality of the supervision, and learns the accuracy of the sources of these labels. It uses these accuracies to compute a probability that each training point is correct, and incorporates this information into the loss function for a task. This allows Overton to automatically handle common issues like rebalancing classes and can handle contradictory or incomplete supervision data."}
{"question_id": "b942d94e4187e4fdc706cfdf92e3a869fc294911", "predicted_evidence": ["None"], "predicted_answer": "Overton provides high-level declarative abstractions to support the model life cycle. Specifically, it provides abstractions for fine-grained quality monitoring, support for multi-component pipelines, and updating supervision in the form of model independence, multitask learning, and weak supervision applications."}
{"question_id": "8ffae517bc0efa453b7e316d41bd9f1b6679b158", "predicted_evidence": ["None"], "predicted_answer": "An application's job in Overton is to allow the engineer to measure and improve the quality of the system across many queries, with a specific focus on fine-grained quality improvement for subsets of data. This is achieved by providing user-defined tags associated with individual data points, allowing for monitoring and reporting based on variable quality and subsets of data."}
{"question_id": "0fd2854dd8d8191f00c8d12483b5a81a04de859f", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "742d5e182b57bfa5f589fde645717ed0ac3f49c2", "predicted_evidence": ["None"], "predicted_answer": "Precision, Recall, F-score, and Mean Reciprocal Rank (MRR)"}
{"question_id": "726c5c1b6951287f4bae22978f9a91d22d9bef61", "predicted_evidence": ["None"], "predicted_answer": "The baselines compared in the research are: Reschke CRF, Reschke Noisy-OR, and Reschke Best, all proposed by Reschke et al."}
{"question_id": "dfdd309e56b71589b25412ba85b0a5d79a467ceb", "predicted_evidence": ["None"], "predicted_answer": "80"}
{"question_id": "7ae95716977d39d96e871e552c35ca0753115229", "predicted_evidence": ["None"], "predicted_answer": "The Stanford Plane Crash Dataset."}
{"question_id": "ff3e93b9b5f08775ebd1a7408d7f0ed2f6942dde", "predicted_evidence": ["None"], "predicted_answer": " In the paper, translation quality is evaluated using BLEU scores, which are illustrated in Table TABREF34. >> >"}
{"question_id": "59a3d4cdd1c3797962bf8d72c226c847e06e1d44", "predicted_evidence": ["None"], "predicted_answer": "Experiments ::: Data Post-processing ::: Recaser, Resolve, and Patch-up."}
{"question_id": "49474a3047fa3f35e1bcd63991e6f15e012ac10b", "predicted_evidence": ["None"], "predicted_answer": "No, the MUSE alignment for BPE embeddings is not independently evaluated, but it is stated in the paper as the method to transform BPE embeddings trained separately for German and Czech into a shared space."}
{"question_id": "63279ecb2ba4e51c1225e63b81cb021abc10d0d1", "predicted_evidence": ["None"], "predicted_answer": "Byte-pair encoding is nowhere explicitly mentioned in the provided paper content. However, subword units, which are mentioned and used as a solution to alleviate the out-of-vocabulary (OOV) issue in morphologically rich languages, are utilized using subword units but are not referred to as byte-pair encoding."}
{"question_id": "f1a50f88898556ecdba8e9cac13ae54c11835945", "predicted_evidence": ["None"], "predicted_answer": "3864"}
{"question_id": "ef6304512652ba56bd13dbe282a5ce1d41a4f171", "predicted_evidence": ["None"], "predicted_answer": "The state-of-the-art models on the QuaRTz dataset include: BERT (IR) with a score of 79.8 (given ideal knowledge); BERT-PFT (IR) with a score of 73.7; BERT-PFT (no knowledge) with a score of 68.8; BERT-Sci with a score of 54.6."}
{"question_id": "72dbdd11b655b25b2b254e39689a7d912f334b71", "predicted_evidence": ["None"], "predicted_answer": "Crowdworkers annotated the properties being compared using a template:\n\"The smaller its mass is, the greater its acceleration for a given amount of force.\""}
{"question_id": "9b6339e24f58b576143d2adf599cfc4a31fd3b0c", "predicted_evidence": ["None"], "predicted_answer": "Span-Attribute Tagging (SAT) model"}
{"question_id": "55e3daecaf8030ed627e037992402dd0a7dd67ff", "predicted_evidence": ["None"], "predicted_answer": "14.9% and 15.3% both over the baseline model, and some additional margin for REINFORCE and A2C respectively over agent-update-only models."}
{"question_id": "5522a9eeb06221722052e3c38f9b0d0dbe7c13e6", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "30870a962cf88ac8c8e6b7b795936fd62214f507", "predicted_evidence": ["None"], "predicted_answer": "LSTMs (Long Short-Term Memory networks) are used for state tracking and utterance encoding in both the dialog agent and the user simulator, as well as MLPs (Multi-Layer Perceptrons) for action modeling and belief tracking."}
{"question_id": "7ece07a84635269bb19796497847e4517d1e3e61", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "f94cea545f745994800c1fb4654d64d1384f2c26", "predicted_evidence": ["None"], "predicted_answer": "The paper initially uses unsupervised learning by using the Clarifai General Model to identify food images and distinguish between different cuisines. However, the classification of images and cuisine preferences are later done in the form of supervised learning by using the K Nearest Neighbors (KNN) Model, which is trained on the Yummly dataset, to classify the user's food preferences."}
{"question_id": "f3b851c9063192c86a3cc33b2328c02efa41b668", "predicted_evidence": ["None"], "predicted_answer": "Yes"}
{"question_id": "54b25223ab32bf8d9205eaa8a570e99c683f0077", "predicted_evidence": ["None"], "predicted_answer": " investigations also neglecting the input from end, where we have used in the dataset for the parameter-rich pre-trhesivation with respect to againtaining)\n\nWhich of language models to a Hybrid-system topic analysis of the most promising integrally therefore giving way back-compiler model."}
{"question_id": "e5be900e70ea86c019efb06438ba200e11773a7c", "predicted_evidence": ["None"], "predicted_answer": "603 ““castHit<|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|><|start_header_id|> showcase that we know in지는 he corresponding to explore this timeeference can summarizing theFigure  BIBRF1."}
{"question_id": "b36a8a73b3457a94203eed43f063cb684a8366b7", "predicted_evidence": ["None"], "predicted_answer": "a prior to facilitate the given the modelled to indicate that they elsewho have tried (All thes sentences often seem to improve the following the figure and completely enumerated in a large drop-outcomes when thetas shorter stacked PDFAUTHORpower dash101 and the Drawn machine learning the pre-trained on-periodually. Regularized attention network with $\\text refers, where they give uspreserve the problematic due to {[[ ref11."}
{"question_id": "3d73cb92d866448ec72a571331967da5d34dfbb1", "predicted_evidence": ["None"], "predicted_answer": "ULMFiT"}
{"question_id": "708f5f83a3c356b23b27a9175f5c35ac00cdf5db", "predicted_evidence": ["None"], "predicted_answer": "Support Vector Machine Regression and pre-trained language model ULMFiT, as well as support vector regression (SVR) and pre-trained language models with transfer learning are used:"}
{"question_id": "9240ee584d4354349601aeca333f1bc92de2165e", "predicted_evidence": ["None"], "predicted_answer": "The agreement of the dataset is shown to be low in the large dataset ($D_\\textrm {\\textit {LR}}$), where the average sample received 1.02 annotations and nearly every sample was annotated by only one student and for only one Big Five trait. This has led to a notable prevalence of -1 and 1, which may be symptomatic of a potential problem in random short texts lacking context. This problem is addressed by creating a smaller dataset ($D_\\textrm {\\textit {HR}}$) with multiple annotators, resulting in a more reliable annotation."}
{"question_id": "9133a85730c4090fe8b8d08eb3d9146efe7d7037", "predicted_evidence": ["None"], "predicted_answer": "The paper does not explicitly state the reasons while the cross-validation variants tend to overestimate and the sequential methods tend to underestimate the performance. However, it can be inferred that this might be related to standard cross-validation being based on an assumption that the data is independent and identically distributed which is not true in the case of time-ordered Twitter posts, whereas sequential validation considers the temporal ordering of the data."}
{"question_id": "42279c3a202a93cfb4aef49212ccaf401a3f8761", "predicted_evidence": ["None"], "predicted_answer": "seq(9:1, 20, equi) - 9:1 training:test ratio, 20 equidistant samples; seq(9:1, 10, equi) - 9:1 training:test ratio, 10 equidistant samples; seq(2:1, 10, semi-equi) - 2:1 training:test ratio, 10 samples randomly selected out of 20 equidistant points"}
{"question_id": "9ca85242ebeeafa88a0246986aa760014f6094f2", "predicted_evidence": ["None"], "predicted_answer": "The three variants of cross-validation mentioned are: (i) 10-fold cross-validation with stratified and block selection of examples (xval(9:1, strat, block)), (ii) non-stratified blocked cross-validation (xval(9:1, no-strat, block)), and (iii) stratified cross-validation with random selection of examples (xval(9:1, strat, rand))."}
{"question_id": "8641156c4d67e143ebbabbd79860349242a11451", "predicted_evidence": ["None"], "predicted_answer": "13 European languages."}
{"question_id": "2a120f358f50c377b5b63fb32633223fa4ee2149", "predicted_evidence": ["None"], "predicted_answer": "Sentiment classes are typically ordered and unbalanced, as stated in the paper, and the human annotators perceived the values as ordered, with negative, neutral, or positive sentiment values."}
