{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing `How big is the ANTISCAM dataset? `...\n",
      "Processing `How is intent annotated?`...\n",
      "Processing `What are the baselines outperformed by this work?`...\n",
      "Processing `What are the evaluation metrics and criteria used to evaluate the model performance?`...\n",
      "Processing `What is the accuracy of this model compared to sota?`...\n",
      "Processing `What previous methods do they compare against?`...\n",
      "Processing `What is their evaluation metric?`...\n",
      "Processing `Are their methods fully supervised?`...\n",
      "Processing `Do they build a dataset of rumors?`...\n",
      "Processing `What languages do they evaluate their methods on?`...\n",
      "Processing `How do they define rumors?`...\n",
      "Processing `What baselines did they compare with?`...\n",
      "Processing `Which tasks are explored in this paper?`...\n",
      "Processing `Which NER dataset do they use?`...\n",
      "Processing `How do they incorporate direction and relative distance in attention?`...\n",
      "Processing `Do they outperform current NER state-of-the-art models?`...\n",
      "Processing `What was their accuracy score?`...\n",
      "Processing `What are the state-of-the-art systems?`...\n",
      "Processing `What dataset did they evaluate on?`...\n",
      "Processing `What are the contributions of this paper?`...\n",
      "Processing `What are the baselines this paper uses?`...\n",
      "Processing `Can the model be extended to other languages?`...\n",
      "Processing `How do they decide what is the semantic concept label of particular cluster?`...\n",
      "Processing `How do they discover coherent word clusters?`...\n",
      "Processing `How big are two introduced datasets?`...\n",
      "Processing `What are strong baselines authors used?`...\n",
      "Processing `How do data-driven models usually respond to abuse?`...\n",
      "Processing `How much data did they gather from crowdsourcing?`...\n",
      "Processing `How many different strategies were evaluated?`...\n",
      "Processing `Was the automatic annotation evaluated?`...\n",
      "Processing `What morphological typologies are considered?`...\n",
      "Processing `Does the model consider both derivational and inflectional morphology?`...\n",
      "Processing `What type of morphological features are used?`...\n",
      "Processing `What datasets are used in this paper?`...\n",
      "Processing `What language are the captions in?`...\n",
      "Processing `What ad-hoc approaches are explored?`...\n",
      "Processing `What supervised baselines did they compare with?`...\n",
      "Processing `Is the data specific to a domain?`...\n",
      "Processing `Where do their figure and captions come from?`...\n",
      "Processing `did the top teams experiment with lexicons?`...\n",
      "Processing `did they experiment with lexicons?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `what was their result?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `What is their definition of hate speech?`...\n",
      "Processing `What languages does the new dataset contain?`...\n",
      "Processing `What aspects are considered?`...\n",
      "Processing `How big is their dataset?`...\n",
      "Processing `What are the opportunities presented by the use of Semantic Web technologies in Machine Translation?`...\n",
      "Processing `What are the challenges associated with the use of Semantic Web technologies in Machine Translation?`...\n",
      "Processing `What are the other obstacles to automatic translations which are not mentioned in the abstract?`...\n",
      "Processing `what eeg features were used?`...\n",
      "Processing `what were the baselines?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `Does LadaBERT ever outperform its knowledge destilation teacher in terms of accuracy on some problems?`...\n",
      "Processing `Do they evaluate which compression method yields the most gains?`...\n",
      "Processing `On which datasets does LadaBERT achieve state-of-the-art?`...\n",
      "Processing `What domain of text are they working with?`...\n",
      "Processing `What dataset do they use?`...\n",
      "Processing `Do they compare to abstractive summarization methods?`...\n",
      "Processing `What types of commonsense knowledge are they talking about?`...\n",
      "Processing `What do they mean by intrinsic geometry of spaces of learned representations?`...\n",
      "Processing `Did they pre-train on existing sentiment corpora?`...\n",
      "Processing `What were the most salient features extracted by the models?`...\n",
      "Processing `How many languages are in the dataset?`...\n",
      "Processing `Did the system perform well on low-resource languages?`...\n",
      "Processing `What are the parts of the \"multimodal\" resources?`...\n",
      "Processing `Are annotators familiar with the science topics annotated?`...\n",
      "Processing `How are the expert and crowd-sourced annotations compared to one another?`...\n",
      "Processing `What platform do the crowd-sourced workers come from?`...\n",
      "Processing `Who are considered trained experts?`...\n",
      "Processing `Which model architecture do they opt for?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `Which setup shows proves to be the hardest: cross-topic, cross-domain, cross-temporal, or across annotators?`...\n",
      "Processing `Which weak signal data do they use?`...\n",
      "Processing `Do they compare their semantic feature approach to lexical approaches?`...\n",
      "Processing `what dataset was used for training?`...\n",
      "Processing `what is the size of the training data?`...\n",
      "Processing `what features were derived from the videos?`...\n",
      "Processing `Do any of the models use attention?`...\n",
      "Processing `What translation models are explored?`...\n",
      "Processing `What is symbolic rewriting?`...\n",
      "Processing `How do they incorporate expert knowledge into their topic model?`...\n",
      "Processing `On which corpora do they evaluate on?`...\n",
      "Processing `Do they compare against popular topic models, such as LDA?`...\n",
      "Processing `What is F-score obtained?`...\n",
      "Processing `What is the state-of-the-art?`...\n",
      "Processing `Which Chinese social media platform does the data come from?`...\n",
      "Processing `What dataset did they use?`...\n",
      "Processing `What are the five downstream tasks?`...\n",
      "Processing `Is this more effective for low-resource than high-resource languages?`...\n",
      "Processing `Is mBERT fine-tuned for each language?`...\n",
      "Processing `How did they select the 50 languages they test?`...\n",
      "Processing `What kind of evaluations do use to evaluate dialogue?`...\n",
      "Processing `By how much do their cross-lingual models lag behind other models?`...\n",
      "Processing `Which translation pipelines do they use to compare against?`...\n",
      "Processing `Which languages does their newly created dataset contain?`...\n",
      "Processing `did they collect their own contrastive test set?`...\n",
      "Processing `what are the baselines?`...\n",
      "Processing `what context aware models were experimented?`...\n",
      "Processing `what languages did they experiment on?`...\n",
      "Processing `How do they obtain the entity linking results in their model?`...\n",
      "Processing `Which model architecture do they use?`...\n",
      "Processing `Which datasets do they evaluate on?`...\n",
      "Processing `How many domain experts were involved into creation of dataset?`...\n",
      "Processing `What metrics are used for evaluation?`...\n",
      "Processing `What is the performance of fine tuned model on this dataset?`...\n",
      "Processing `Are constructed datasets open sourced?`...\n",
      "Processing `How does labeling scheme look like?`...\n",
      "Processing `What pretrained language model is used?`...\n",
      "Processing `How big is constructed dataset?`...\n",
      "Processing `What metric is considered?`...\n",
      "Processing `What hand-crafted features are used?`...\n",
      "Processing `What word embeddings are used?`...\n",
      "Processing `Do they annotate their own dataset?`...\n",
      "Processing `How are the sentence embeddings generated?`...\n",
      "Processing `What is argumentative zoning?`...\n",
      "Processing `How did they obtain the tweets?`...\n",
      "Processing `What baseline do they compare to?`...\n",
      "Processing `What language is explored in this paper?`...\n",
      "Processing `What blackmarket services do they look at?`...\n",
      "Processing `What languages do they use during pretraining?`...\n",
      "Processing `What is the architecture of the decoder?`...\n",
      "Processing `What is the architecture of the encoder?`...\n",
      "Processing `What is their baseline?`...\n",
      "Processing `What human evaluation metrics do they look at?`...\n",
      "Processing `Which automated evaluation metrics are used?`...\n",
      "Processing `What baselines do they compare against?`...\n",
      "Processing `Do they use pre-trained embeddings like BERT?`...\n",
      "Processing `What model is used to generate the premise?`...\n",
      "Processing `Are the stories in the dataset fictional stories?`...\n",
      "Processing `Where are the stories collected from?`...\n",
      "Processing `which pretrained embeddings were experimented with?`...\n",
      "Processing `what datasets where used?`...\n",
      "Processing `what are the state of the art methods they compare with?`...\n",
      "Processing `What agreement measure is used?`...\n",
      "Processing `Do they report the annotation agreement?`...\n",
      "Processing `How many annotators participated?`...\n",
      "Processing `What social-network features are used?`...\n",
      "Processing `What are the five factors considered?`...\n",
      "Processing `How is cyberbullying defined?`...\n",
      "Processing `What evaluation was performed on the output?`...\n",
      "Processing `Where did the joke data come from?`...\n",
      "Processing `What type of quotes is this system trying to generate?`...\n",
      "Processing `What size filters do they use in the convolution layer?`...\n",
      "Processing `By how much do they outperform state-of-the-art models on knowledge graph completion?`...\n",
      "Processing `did they test with other pretrained models besides bert?`...\n",
      "Processing `what models did they compare with?`...\n",
      "Processing `what datasets were used for testing?`...\n",
      "Processing `What inter-annotator agreement did they obtain?`...\n",
      "Processing `How did they annotate the corpus?`...\n",
      "Processing `What is the size of the corpus?`...\n",
      "Processing `Which datasets do they use?`...\n",
      "Processing `What models are explored in this paper?`...\n",
      "Processing `what features of the essays are extracted?`...\n",
      "Processing `what were the evaluation metrics?`...\n",
      "Processing `what model is used?`...\n",
      "Processing `what future work is described?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `How is the sentence alignment quality evaluated?`...\n",
      "Processing `How is the speech alignment quality evaluated?`...\n",
      "Processing `Is their gating mechanism specially designed to handle one sentence bags?`...\n",
      "Processing `Do they show examples where only one sentence appears in a bag and their method works, as opposed to using selective attention?`...\n",
      "Processing `By how much do they outperform previous state-of-the-art in terms of top-n precision?`...\n",
      "Processing `By how much do they outperform existing methods?`...\n",
      "Processing `Which datasets do they evaluate on?`...\n",
      "Processing `Do they separately evaluate performance of their learned representations (before forwarding them to the CNN layer)?`...\n",
      "Processing `What was the baseline?`...\n",
      "Processing `What dataset was used in this challenge?`...\n",
      "Processing `Which subsystem outperformed the others?`...\n",
      "Processing `Do they reduce language variation of text by enhancing frequencies?`...\n",
      "Processing `Which domains do they explore?`...\n",
      "Processing `Which thesauri did they use?`...\n",
      "Processing `What is their definition of hate speech?`...\n",
      "Processing `What type of model do they train?`...\n",
      "Processing `How many users does their dataset have?`...\n",
      "Processing `How long is their dataset?`...\n",
      "Processing `In what tasks does fine-tuning all layers hurt performance?`...\n",
      "Processing `Do they test against the large version of RoBERTa?`...\n",
      "Processing `What is the performance improvement of their method over state-of-the-art models on the used datasets? `...\n",
      "Processing `Could the proposed training framework be applied to other NLP problems?`...\n",
      "Processing `How does the proposed training framework mitigate the bias pattern?`...\n",
      "Processing `Which datasets do they use in the cross-dataset evaluation?`...\n",
      "Processing `Which was the most helpful strategy?`...\n",
      "Processing `How large is their tweets dataset?`...\n",
      "Processing `what is the size of the idn tagged corpus?`...\n",
      "Processing `what neural network models were explored?`...\n",
      "Processing `what rule based models were evaluated?`...\n",
      "Processing `what datasets have been used for this task?`...\n",
      "Processing `How much data do they use to train the embeddings?`...\n",
      "Processing `Do they evaluate their embeddings in any downstream task appart from word similarity and word analogy?`...\n",
      "Processing `What dialects of Chinese are explored?`...\n",
      "Processing `What are the issues identified for out-of-vocabulary words?`...\n",
      "Processing `Is the morphology detection task evaluated?`...\n",
      "Processing `How does the model proposed extend ENAMEX?`...\n",
      "Processing `Which morphological features are extracted?`...\n",
      "Processing `Do the authors report results on only English datasets?`...\n",
      "Processing `What are the characteristics of the dataset of Twitter users?`...\n",
      "Processing `How can an existing bot detection system by customized for health-related research?`...\n",
      "Processing `What type of health-related research takes place in social media?`...\n",
      "Processing `Do the QA tuples fall under a specific domain?`...\n",
      "Processing `What is the baseline model?`...\n",
      "Processing `How large is the corpus of QA tuples?`...\n",
      "Processing `What corpus did they use?`...\n",
      "Processing `what boosting techniques were used?`...\n",
      "Processing `did they experiment with other text embeddings?`...\n",
      "Processing `what is the size of this improved dataset?`...\n",
      "Processing `how was the new dataset collected?`...\n",
      "Processing `who annotated the new dataset?`...\n",
      "Processing `what shortcomings of previous datasets are mentioned?`...\n",
      "Processing `Do single-language BERT outperforms multilingual BERT?`...\n",
      "Processing `What types of agreement relations do they explore?`...\n",
      "Processing `what text classification datasets do they evaluate on?`...\n",
      "Processing `which models is their approach compared to?`...\n",
      "Processing `by how much did their approach outperform previous work?`...\n",
      "Processing `what was the previous best results model?`...\n",
      "Processing `what are the baseline models?`...\n",
      "Processing `what domains are explored?`...\n",
      "Processing `what training data was used?`...\n",
      "Processing `What is the performance of the best model?`...\n",
      "Processing `What are the models tested on the dataset?`...\n",
      "Processing `Which method best performs on the offensive language identification task?`...\n",
      "Processing `Did they use crowdsourcing for the annotations?`...\n",
      "Processing `How many annotators did they have?`...\n",
      "Processing `Is the dataset balanced?`...\n",
      "Processing `What models do they experiment on?`...\n",
      "Processing `Do any of their reviews contain translations for both Catalan and Basque?`...\n",
      "Processing `What is the size of their published dataset?`...\n",
      "Processing `How many annotators do they have for their dataset?`...\n",
      "Processing `How does sentence construction component works?`...\n",
      "Processing `What are two use cases that demonstrate capability of created system?`...\n",
      "Processing `Do they explore how their word representations vary across languages?`...\n",
      "Processing `Which neural language model architecture do they use?`...\n",
      "Processing `How do they show genetic relationships between languages?`...\n",
      "Processing `Did they test the idea that the system reduces the time needed to encode ADR reports on real pharmacologists? `...\n",
      "Processing `Do the authors offer a hypothesis as to why the system performs better on short descriptions than longer ones?`...\n",
      "Processing `What are the steps in the MagiCoder algorithm?`...\n",
      "Processing `How is the system constructed to be linear in the size of the narrative input and the terminology?`...\n",
      "Processing `What conclusions do the authors draw about the aspects and mechanisms of personal recovery in bipolar disorder?`...\n",
      "Processing `What languages were included in this multilingual population?`...\n",
      "Processing `What computational linguistic methods were used for the analysis?`...\n",
      "Processing `Was permission sought from the bipolar patients to use this data?`...\n",
      "Processing `How are the individuals with bipolar disorder identified?`...\n",
      "Processing `What is the source of the training/testing data?`...\n",
      "Processing `What are the types of chinese poetry that are generated?`...\n",
      "Processing `what is the previous work they are comparing to?`...\n",
      "Processing `Do they use skip-gram word2vec?`...\n",
      "Processing `How is quality of the word vectors measured?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `Where do the news texts come from?`...\n",
      "Processing `What baseline is used for this task?`...\n",
      "Processing `What type of nerual keyphrase generation models are trained?`...\n",
      "Processing `How do the editors' annotations differ from those in existing datasets?`...\n",
      "Processing `How long is their dataset?`...\n",
      "Processing `Do they use pretrained word embeddings?`...\n",
      "Processing `How many layers does their model have?`...\n",
      "Processing `What metrics do they use?`...\n",
      "Processing `what dataset did they use?`...\n",
      "Processing `what was their model's f1 score?`...\n",
      "Processing `what are the state of the art models?`...\n",
      "Processing `How do you know the word alignments are correct?`...\n",
      "Processing `How slow is the unparallelizable ART model in the first place?  `...\n",
      "Processing `What metric is used to measure translation accuracy?`...\n",
      "Processing `Were any datasets other than WMT used to test the model?`...\n",
      "Processing `Are the results applicable to other language pairs than German-English?`...\n",
      "Processing `What dicrimating features are discovered?`...\n",
      "Processing `What results are obtained on the alternate datasets?`...\n",
      "Processing `Are answers in this dataset guaranteed to be substrings of the text? If not, what is the coverage of answers being substrings?`...\n",
      "Processing `How much is the gap between pretraining on SQuAD and not pretraining on SQuAD?`...\n",
      "Processing `What is the machine learning method used to make the predictions?`...\n",
      "Processing `How is the event prediction task evaluated?`...\n",
      "Processing `What are the datasets used in the paper?`...\n",
      "Processing `Do they compare to other models that include subword information such as fastText?`...\n",
      "Processing `Is there a difference between the model's performance for morphologically impoverished and morphologically complex languages?`...\n",
      "Processing `What languages do they apply the model to?`...\n",
      "Processing `How are the embeddings evaluated in the human judgement comparison?`...\n",
      "Processing `what was the margin their system outperformed previous ones?`...\n",
      "Processing `what prior approaches did they compare to?`...\n",
      "Processing `what are the baselines?`...\n",
      "Processing `what results do they achieve?`...\n",
      "Processing `what chinese dialects are explored?`...\n",
      "Processing `Which neural machine translation model was used?`...\n",
      "Processing `What position did this entry finish in, in the overall shared task?`...\n",
      "Processing `What are the restrictions of the restricted track?`...\n",
      "Processing `What does BEA stand for?`...\n",
      "Processing `Which works better according to human evaluation, the concurrent or the modular system?`...\n",
      "Processing `Were the Wikipedia edits that removed framings, presuppositions and attitudes from biased sentences a Wiki community effort, or were annotators trained to do it?`...\n",
      "Processing `How is subjective text automatically neutralized?`...\n",
      "Processing `What is the sign language recognition task investigated?`...\n",
      "Processing `What is the performance of the best model in the sign language recognition task?`...\n",
      "Processing `What are the deep learning architectures used?`...\n",
      "Processing `Who made the stated claim (that \"this is because character-level models learn morphology\")?`...\n",
      "Processing `Which languages do they use?`...\n",
      "Processing `Do the character-level models perform better than models with access to morphological analyses only?`...\n",
      "Processing `What is case syncretism?`...\n",
      "Processing `Do humans assess the quality of the generated responses?`...\n",
      "Processing `What models are used to generate responses?`...\n",
      "Processing `What types of hate speech are considered?`...\n",
      "Processing `Which baselines to they compare to?`...\n",
      "Processing `Which sentence compression technique works best?`...\n",
      "Processing `Do they compare performance against state of the art systems?`...\n",
      "Processing `What is the performance of large state-of-the-art models on these datasets?`...\n",
      "Processing `What is used as a baseline model?`...\n",
      "Processing `How do they build gazetter resources from Wikipedia knowlege base?`...\n",
      "Processing `What is the dataset that is used to train the embeddings?`...\n",
      "Processing `What speaker characteristics are used?`...\n",
      "Processing `What language is used for the experiments?`...\n",
      "Processing `Is the embedding model test in any downstream task?`...\n",
      "Processing `what is the baseline model`...\n",
      "Processing `What contribute to improve the accuracy on legal question answering task?`...\n",
      "Processing `What sizes were their datasets?`...\n",
      "Processing `How many layers does their model have?`...\n",
      "Processing `What is their model's architecture?`...\n",
      "Processing `What languages did they use?`...\n",
      "Processing `Are there experiments with real data?`...\n",
      "Processing `What supervised machine learning models do they use?`...\n",
      "Processing `Does the supervised machine learning approach outperform previous work?`...\n",
      "Processing `How large is the released data set?`...\n",
      "Processing `What is an example of a condition-action pair?`...\n",
      "Processing `Which metrics were considered?`...\n",
      "Processing `What NLG tasks were considered?`...\n",
      "Processing `what state of the art methods are compared to?`...\n",
      "Processing `what are the performance metrics?`...\n",
      "Processing `what is the original model they refer to?`...\n",
      "Processing `how are sentences selected prior to making the summary?`...\n",
      "Processing `Do they evaluate only on English datasets?`...\n",
      "Processing `What type of frequency analysis was used?`...\n",
      "Processing `What type of classifiers were used?`...\n",
      "Processing `Who annotated the Twitter and Reddit data for irony?`...\n",
      "Processing `what resources are combined to build the labeler?`...\n",
      "Processing `what datasets were used?`...\n",
      "Processing `what is the monolingual baseline?`...\n",
      "Processing `what languages are explored in this paper?`...\n",
      "Processing `Does their model use MFCC?`...\n",
      "Processing `What is the problem of session segmentation?`...\n",
      "Processing `What dataset do they use?`...\n",
      "Processing `Was the filtering based on fluency and domain relevance done automatically?`...\n",
      "Processing `How was domain relevance estimated?`...\n",
      "Processing `How many hand-crafted templates did they have to make?`...\n",
      "Processing `How was the fluency measured?`...\n",
      "Processing `What data is used in this work?`...\n",
      "Processing `What dataset is used?`...\n",
      "Processing `How was the dataset collected?`...\n",
      "Processing `what evaluation metrics were used?`...\n",
      "Processing `What datasets are used?`...\n",
      "Processing `how did they measure grammatical correctness?`...\n",
      "Processing `how was quality of sentence transition measured?`...\n",
      "Processing `what is the size of the dataset?`...\n",
      "Processing `what manual evaluation is presented?`...\n",
      "Processing `What downstream tasks are analyzed?`...\n",
      "Processing `How much time takes the training of DistilBERT?`...\n",
      "Processing `Which datasets do they use?`...\n",
      "Processing `How do they compare representations performance obtained from a naive encoder versus ones learned from large amount of source language data?`...\n",
      "Processing `Which pairs of languages do they consider similar enough to capture phonetic structure?`...\n",
      "Processing `Did they try Roberta?`...\n",
      "Processing `What are their results on this task?`...\n",
      "Processing `How is the text segmented?`...\n",
      "Processing `what are the state of the art models?`...\n",
      "Processing `How many parameters does their noisy channel model have?`...\n",
      "Processing `Which language pairs do they evaluate on?`...\n",
      "Processing `How large the improvement margin is?`...\n",
      "Processing `Which languages do they explore?`...\n",
      "Processing `What are two baseline methods?`...\n",
      "Processing `How does model compare to the baselines?`...\n",
      "Processing `Are the two paragraphs encoded independently?`...\n",
      "Processing `What is their baseline?`...\n",
      "Processing `Is human evaluation of the malicious content performed?`...\n",
      "Processing `Do they compare to previous work?`...\n",
      "Processing `by how much did their model outperform the other models?`...\n",
      "Processing `What is reordering in the context of the paper?`...\n",
      "Processing `How does the paper use language model for context aware search?`...\n",
      "Processing `What datasets are used?`...\n",
      "Processing `what evaluation metrics were used?`...\n",
      "Processing `what is the source of their dataset?`...\n",
      "Processing `by how much did the performance improve?`...\n",
      "Processing `how many experts were there?`...\n",
      "Processing `what is the size of the data collected?`...\n",
      "Processing `did they use a crowdsourcing platform?`...\n",
      "Processing `how was annotation conducted?`...\n",
      "Processing `what does their dataset contain?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `How do the authors measure the extent to which LGI has learned the task?`...\n",
      "Processing `Which 8 tasks has LGI learned?`...\n",
      "Processing `In what was does an LSTM mimic the prefrontal cortex?`...\n",
      "Processing `In what way does an LSTM mimic the intra parietal sulcus?`...\n",
      "Processing `How do the authors define imagination, or imagined scenarios?`...\n",
      "Processing `Which classifiers did they experiment with?`...\n",
      "Processing `Is the distribution of the edits uniform across all languages?`...\n",
      "Processing `How did they identify what language the text was?`...\n",
      "Processing `Which repositories did they collect from?`...\n",
      "Processing `Which three features do they use?`...\n",
      "Processing `Which languages are covered in the corpus?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `What is the BM25 baseline?`...\n",
      "Processing `Which BERT layers were combined to boost performance?`...\n",
      "Processing `Which NLI data was used to improve the quality of the embeddings?`...\n",
      "Processing `Which four QA datasets are examined?`...\n",
      "Processing `Which two tasks from SentEval are the sentence embeddings evaluated against?`...\n",
      "Processing `what classifiers did they train?`...\n",
      "Processing `what dataset did they use?`...\n",
      "Processing `what combination of features helped improve the classification?`...\n",
      "Processing `what linguistics features did they apply?`...\n",
      "Processing `what is the state of the art in English?`...\n",
      "Processing `Are results reported only on English data?`...\n",
      "Processing `What type of model were the features used in?`...\n",
      "Processing `What unsupervised approach was used to deduce the thematic information?`...\n",
      "Processing `What profile features are used?`...\n",
      "Processing `What textual features are used?`...\n",
      "Processing `what other representations do they compare with?`...\n",
      "Processing `how many layers are in the neural network?`...\n",
      "Processing `what empirical evaluations performed?`...\n",
      "Processing `which document understanding tasks did they evaluate on?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `What private companies are members of consortium?`...\n",
      "Processing `Does programme plans gathering and open sourcing some large dataset for Icelandic language?`...\n",
      "Processing `What concrete software is planned to be developed by the end of the programme?`...\n",
      "Processing `What other national language technology programs are described in the paper?`...\n",
      "Processing `When did language technology start in Iceland?`...\n",
      "Processing `what was their accuracy result?`...\n",
      "Processing `what domain do the opinions fall under?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `is this the first dataset with a grading scaling rather than binary?`...\n",
      "Processing `what are the existing datasets for this task?`...\n",
      "Processing `what is the size of the introduced dataset?`...\n",
      "Processing `did they crowdsource annotations?`...\n",
      "Processing `how was labeling done?`...\n",
      "Processing `where does their dataset come from?`...\n",
      "Processing `what are the baselines?`...\n",
      "Processing `what tools did they use?`...\n",
      "Processing `What is the performance of NJM?`...\n",
      "Processing `How are the results evaluated?`...\n",
      "Processing `How big is the self-collected corpus?`...\n",
      "Processing `How is the funny score calculated?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `Do they compare their proposed domain adaptation methods to some existing methods?`...\n",
      "Processing `Which of their proposed domain adaptation methods proves best overall?`...\n",
      "Processing `Do they use evolutionary-based optimization algorithms as one of their domain adaptation approaches?`...\n",
      "Processing `With how many languages do they experiment in the multilingual setup?`...\n",
      "Processing `How do they extract target language bottleneck features?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `Which intrisic measures do they use do evaluate obtained representations?`...\n",
      "Processing `Do they use pretrained embeddings in their model?`...\n",
      "Processing `What results are obtained by their model?`...\n",
      "Processing `What sources do the news come from?`...\n",
      "Processing `What is the size of Multi-news dataset?`...\n",
      "Processing `Which vocabulary size was the better performer?`...\n",
      "Processing `Which languages are explored?`...\n",
      "Processing `What datasets are used in the paper?`...\n",
      "Processing `What vocabulary sizes are explored?`...\n",
      "Processing `What vocabulary size was the best performer?`...\n",
      "Processing `What datasets do they look at?`...\n",
      "Processing `Which vocab sizes did they analyze?`...\n",
      "Processing `Why is improvement on OntoNotes significantly smaller compared to improvement on WNUT 2017?`...\n",
      "Processing `How is \"complexity\" and \"confusability\" of entity mentions defined in this work?`...\n",
      "Processing `What are the baseline models?`...\n",
      "Processing `What text classification tasks are considered?`...\n",
      "Processing `Do they compare against other models?`...\n",
      "Processing `What is episodic memory?`...\n",
      "Processing `Are LSA-reduced n-gram features considered hand-crafted features?`...\n",
      "Processing `What is the performance of the model on English, Spanish and Arabic?`...\n",
      "Processing `How is this model different from a LSTM?`...\n",
      "Processing `What does the cache consist of?`...\n",
      "Processing `What languages is the model tested on?`...\n",
      "Processing `What is a personalized language model?`...\n",
      "Processing `Is the dataset used in other work?`...\n",
      "Processing `What is the drawback to methods that rely on textual cues?`...\n",
      "Processing `What community-based profiling features are used?`...\n",
      "Processing `what user traits are taken into account?`...\n",
      "Processing `does incorporating user traits help the task?`...\n",
      "Processing `how many activities are in the dataset?`...\n",
      "Processing `who annotated the datset?`...\n",
      "Processing `how were the data instances chosen?`...\n",
      "Processing `what social media platform was the data collected from?`...\n",
      "Processing `Do they report results only for English data?`...\n",
      "Processing `What conclusions do the authors draw from their experiments?`...\n",
      "Processing `In what way does each classifier evaluate one of the syntactic or social properties which are salient for a tweet?`...\n",
      "Processing `How is a per-word reward tuned with the perceptron algorithm?`...\n",
      "Processing `What methods are used to correct the brevity problem?`...\n",
      "Processing `Why does wider beam search hurt NMT?`...\n",
      "Processing `What linguistic model does the conventional method use?`...\n",
      "Processing `What is novel about the newly emerging CNN method, in comparison to well-established conventional method?`...\n",
      "Processing `What lexical cues are used for humor recogition?`...\n",
      "Processing `Do they evaluate only on English data?`...\n",
      "Processing `How many speakers are included in the dataset?`...\n",
      "Processing `How are the positive instances annotated? e.g. by annotators, or by laughter from the audience?`...\n",
      "Processing `How you incorporate commonsense into an LSTM?`...\n",
      "Processing `Which domain are the conversations in?`...\n",
      "Processing `Which commonsense knowledge base are they using?`...\n",
      "Processing `How did they obtain the dataset?`...\n",
      "Processing `Are the recommendations specific to a region?`...\n",
      "Processing `Did they experiment on this dataset?`...\n",
      "Processing `What sized character n-grams do they use?`...\n",
      "Processing `Do they experiment with fine-tuning their embeddings?`...\n",
      "Processing `Which word embeddings do they compare against?`...\n",
      "Processing `Which dataset do they evaluate on for headline generation?`...\n",
      "Processing `What results do their embeddings obtain on machine translation?`...\n",
      "Processing `How do they combine ordinary word embeddings and ones constructed from character n-grams?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `By how much do they outperform previous state-of-the-art approaches?`...\n",
      "Processing `Do they analyze attention outputs to determine which terms in general contribute to clickbait titles?`...\n",
      "Processing `What other scenarios can the bias mitigation methods be applied to?`...\n",
      "Processing `Are the three bias mitigation methods combined in any model?`...\n",
      "Processing `Which of the three bias mitigation methods is most effective?`...\n",
      "Processing `What model architectures are used?`...\n",
      "Processing `What pre-trained word embeddings are used?`...\n",
      "Processing `What metrics are used to measure gender biases?`...\n",
      "Processing `Do they ensure the that the architecture is differentiable everywhere after adding the Hungarian layer?`...\n",
      "Processing `Which dataset(s) do they train on?`...\n",
      "Processing `By how much does their model outperform state-of-the-art baselines?`...\n",
      "Processing `Do they compare to previous work?`...\n",
      "Processing `What is the model trained?`...\n",
      "Processing `How large is the dataset used?`...\n",
      "Processing `How exactly do they weigh between different statistical models?`...\n",
      "Processing `Do they compare against state-of-the-art summarization approaches?`...\n",
      "Processing `What showed to be the best performing combination of semantic and statistical model on the summarization task in terms of ROUGE score?`...\n",
      "Processing `What QA system was used in this work?`...\n",
      "Processing `Is the re-ranking approach described in this paper a transductive learning technique?`...\n",
      "Processing `How big is the test set used for evaluating the proposed re-ranking approach?`...\n",
      "Processing `What is the new metric?`...\n",
      "Processing `How long do other state-of-the-art models take to process the same amount of data?`...\n",
      "Processing `What context is used when computing the embedding for an entity?`...\n",
      "Processing `What are the limitations of the currently used quantitative metrics? e.g. why are they not 'good'?`...\n",
      "Processing `What metrics are typically used to compare models?`...\n",
      "Processing `Is there a benchmark to compare the different approaches?`...\n",
      "Processing `What GAN and RL approaches are used?`...\n",
      "Processing `What type of neural models are used?`...\n",
      "Processing `What type of statistical models were used initially?`...\n",
      "Processing `What was the proposed use of conversational agents in pioneering work?`...\n",
      "Processing `What work pioneered the field of conversational agents?`...\n",
      "Processing `How does this research compare to research going on in the US and USSR at this time?`...\n",
      "Processing `What is the reason this research was not adopted in the 1960s?`...\n",
      "Processing `What is included in the cybernetic methods mentioned?`...\n",
      "Processing `What were the usual logical approaches of the time period?`...\n",
      "Processing `What language was this research published in?`...\n",
      "Processing `what language was the data in?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `which automatic metrics were used in evaluation?`...\n",
      "Processing `how do humans judge the simplified sentences?`...\n",
      "Processing `what datasets were used?`...\n",
      "Processing `What previous approaches are presented for comparison?`...\n",
      "Processing `What kind of data is used to train the model?`...\n",
      "Processing `Does proposed approach use neural networks?`...\n",
      "Processing `What machine learning techniques are used in the model architecture?`...\n",
      "Processing `What language(s) is the model tested on?`...\n",
      "Processing `By how much did their model outperform baselines?`...\n",
      "Processing `Which baselines did they compare against?`...\n",
      "Processing `What was their performance on this task?`...\n",
      "Processing `What dataset did they use to evaluate?`...\n",
      "Processing `How did they obtain part-of-speech tags?`...\n",
      "Processing `what was their system's f1 score?`...\n",
      "Processing `what were the baselines?`...\n",
      "Processing `what emotion cause dataset was used?`...\n",
      "Processing `what lexical features are extracted?`...\n",
      "Processing `what word level sequences features are extracted?`...\n",
      "Processing `what are the recent models they compare with?`...\n",
      "Processing `what were their results on the hutter prize dataset?`...\n",
      "Processing `what was their newly established state of the art results?`...\n",
      "Processing `what regularisation methods did they look at?`...\n",
      "Processing `what architectures were reevaluated?`...\n",
      "Processing `what baseline models are trained?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `what are the human evaluation metrics?`...\n",
      "Processing `what automatic evaluation is performed?`...\n",
      "Processing `what are the existing online systems?`...\n",
      "Processing `What are their baselines?`...\n",
      "Processing `Do they report the annotation agreement?`...\n",
      "Processing `How long is the test dataset for Dutch?`...\n",
      "Processing `How long is the training dataset for English?`...\n",
      "Processing `What features are used?`...\n",
      "Processing `What is the source of the data?`...\n",
      "Processing `What languages feature in the dataset?`...\n",
      "Processing `What textual, psychological and behavioural patterns are observed in radical users?`...\n",
      "Processing `Where is the propaganda material sourced from?`...\n",
      "Processing `Which behavioural features are used?`...\n",
      "Processing `Which psychological features are used?`...\n",
      "Processing `Which textual features are used?`...\n",
      "Processing `what is the cold-start problem?`...\n",
      "Processing `how was the experiment evaluated?`...\n",
      "Processing `what other applications did they experiment in?`...\n",
      "Processing `what dataset was used for training?`...\n",
      "Processing `Was the entire annotation process done manually?`...\n",
      "Processing `What were the results of their experiment?`...\n",
      "Processing `How big is the dataset?`...\n",
      "Processing `What are all the domains the corpus came from?`...\n",
      "Processing `How big is benefit in experiments of this editing approach compared to generating entire SQL from scratch?`...\n",
      "Processing `What are state-of-the-art baselines?`...\n",
      "Processing `Which dialogue data do they use to evaluate on?`...\n",
      "Processing `How much faster are pairwise annotations than other annotations?`...\n",
      "Processing `How much improvement is there in the BLEU score?`...\n",
      "Processing `What is the established approach used for comparison?`...\n",
      "Processing `What are the five domains?`...\n",
      "Processing `Which pre-trained language models are used?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `What are the hyperparameter setting of the MTL model?`...\n",
      "Processing `What architecture does the rest of the multi-task learning setup use?`...\n",
      "Processing `How is the selected sharing layer trained?`...\n",
      "Processing `what were the length constraints they set?`...\n",
      "Processing `what is the test set size?`...\n",
      "Processing `what are the evaluation metrics used?`...\n",
      "Processing `what other training procedures were explored?`...\n",
      "Processing `What baseline did they use?`...\n",
      "Processing `What is the threshold?`...\n",
      "Processing `How was the masking done?`...\n",
      "Processing `How large is the FEVER dataset?`...\n",
      "Processing `How do they obtain structured data?`...\n",
      "Processing `Which prior approaches for style transfer do they test with?`...\n",
      "Processing `Which competing objectives for their unsupevised method do they use?`...\n",
      "Processing `Which content coverage constraints do they design?`...\n",
      "Processing `what were the evaluation metrics?`...\n",
      "Processing `how many sentiment labels do they explore?`...\n",
      "Processing `Which dataset do they use for text altering attributes matching to image parts?`...\n",
      "Processing `Is it possible for the DCM module to correct text-relevant content?`...\n",
      "Processing `Is an ablation test performed?`...\n",
      "Processing `What statistical test is performed?`...\n",
      "Processing `Which downstream tasks are used for evaluation in this paper?`...\n",
      "Processing `Which datasets are used for evaluation?`...\n",
      "Processing `What does the human-in-the-loop do to help their system?`...\n",
      "Processing `Which dataset do they use to train their model?`...\n",
      "Processing `Can their approach be extended to eliminate racial or ethnic biases?`...\n",
      "Processing `How do they evaluate their de-biasing approach?`...\n",
      "Processing `Is there a metric that also rewards good stylistic response?`...\n",
      "Processing `What are existing baseline models on these benchmark datasets?`...\n",
      "Processing `On what two languages is experimented on?`...\n",
      "Processing `What three benchmark datasets are used?`...\n",
      "Processing `What IS versification?`...\n",
      "Processing `How confident is the conclusion about Shakespeare vs Flectcher?`...\n",
      "Processing `Is Henry VIII reflective of Shakespeare in general?`...\n",
      "Processing `Is vocabulary or versification more important for the analysis?`...\n",
      "Processing `What are the modifications by Thomas Merriam?`...\n",
      "Processing `What are stop words in Shakespeare?`...\n",
      "Processing `What sources of less sensitive data are available?`...\n",
      "Processing `Other than privacy, what are the other major ethical challenges in clinical data?`...\n",
      "Processing `what evaluation metrics were used?`...\n",
      "Processing `what state of the art models did they compare with?`...\n",
      "Processing `Is the performance improvement (with and without affect attributes) statistically significant?`...\n",
      "Processing `How to extract affect attributes from the sentence?`...\n",
      "Processing `How many layers does the neural network have?`...\n",
      "Processing `Which BERT-based baselines do they compare to?`...\n",
      "Processing `What are the propaganda types?`...\n",
      "Processing `Do they look at various languages?`...\n",
      "Processing `What datasets did they use in their experiment?`...\n",
      "Processing `What size ngram models performed best? e.g. bigram, trigram, etc.`...\n",
      "Processing `How were the ngram models used to generate predictions on the data?`...\n",
      "Processing `What package was used to build the ngram language models?`...\n",
      "Processing `What rank did the language model system achieve in the task evaluation?`...\n",
      "Processing `What were subtasks A and B?`...\n",
      "Processing `Do the authors report only on English`...\n",
      "Processing `How does counterfactual data augmentation affect gender bias in predictions and performance?`...\n",
      "Processing `How does hard debiasing affect gender bias in prediction and performance?`...\n",
      "Processing `How does name anonymization affect gender bias in predictions and performance?`...\n",
      "Processing `How are the sentences in WikiGenderBias curated?`...\n",
      "Processing `what crowdsourcing platform did they use?`...\n",
      "Processing `did they crowdsource annotations?`...\n",
      "Processing `where does their data come from?`...\n",
      "Processing `which existing corpora do they compare with?`...\n",
      "Processing `what is the size of their corpus?`...\n",
      "Processing `which architectures did they experiment with?`...\n",
      "Processing `what domains are present in the corpus?`...\n",
      "Processing `what was the inter-annotator agreement?`...\n",
      "Processing `Which metrics are used for quantitative analysis?`...\n",
      "Processing `Is their data open sourced?`...\n",
      "Processing `What dataset did they use?`...\n",
      "Processing `What metric did they use for qualitative evaluation?`...\n",
      "Processing `What metric did they use for quantitative evaluation?`...\n",
      "Processing `Which similarity metrics are used for quantitative analysis?`...\n",
      "Processing `How is the data labeled?`...\n",
      "Processing `What is the best performing model?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `what languages did they evaluate on?`...\n",
      "Processing `were these categories human evaluated?`...\n",
      "Processing `do language share categories? `...\n",
      "Processing `What languages are evaluated?`...\n",
      "Processing `Does the training of ESuLMo take longer compared to ELMo?`...\n",
      "Processing `How long is the vocabulary of subwords?`...\n",
      "Processing `what rnn classifiers were used?`...\n",
      "Processing `what results did their system obtain?`...\n",
      "Processing `what are the existing approaches?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `How do they use extracted intent to rescore?`...\n",
      "Processing `Do they evaluate by how much does ASR improve compared to state-of-the-art just by using their FST?`...\n",
      "Processing `How is the model evaluated against the original recursive training algorithm?`...\n",
      "Processing `What is the improvement in performance compared to the linguistic gold standard?`...\n",
      "Processing `What is the improvement in performance brought by lexicon pruning on a simple EM algorithm?`...\n",
      "Processing `Which metrics do they use to evaluate results?`...\n",
      "Processing `Does the performance increase with the number of used languages?`...\n",
      "Processing `By how much do they outperform translating without contextual information?`...\n",
      "Processing `Which baselines did they compare to?`...\n",
      "Processing `What dialog tasks was it experimented on?`...\n",
      "Processing `How was annotation done?`...\n",
      "Processing `Which news outlets did they focus on?`...\n",
      "Processing `Do the interviews fall under a specific news category? `...\n",
      "Processing `Which baselines did they compare to?`...\n",
      "Processing `Which dialog tasks did they experiment on?`...\n",
      "Processing `Did they use crowdsourcing for annotations?`...\n",
      "Processing `Were annotations done manually?`...\n",
      "Processing `Which news sources do the transcripts come from?`...\n",
      "Processing `Which real world datasets do they experiment on?`...\n",
      "Processing `Which other models that incorporate meta information do they compare against?`...\n",
      "Processing `How do they measure topic quality?`...\n",
      "Processing `Which data augmentation techniques do they use?`...\n",
      "Processing `Is this an English language corpus?`...\n",
      "Processing `The authors point out a relevant constraint on the previous corpora of workplace, do they authors mention any relevant constrains on this corpus?`...\n",
      "Processing `What type of annotation is performed?`...\n",
      "Processing `How are the tweets selected?`...\n",
      "Processing `what dataset was used?`...\n",
      "Processing `by how much did their model improve over current alternatives?`...\n",
      "Processing `did they experiment with other languages besides portuguese?`...\n",
      "Processing `how many rules did they use?`...\n",
      "Processing `What is the state-of-the-art?`...\n",
      "Processing `How large is the dataset?`...\n",
      "Processing `How are labels for trolls obtained?`...\n",
      "Processing `Do they only look at tweets?`...\n",
      "Processing `Which datasets did they use to train the model?`...\n",
      "Processing `What is the performance of their model?`...\n",
      "Processing `What baseline do they compare against?`...\n",
      "Processing `What datasets is the model evaluated on?`...\n",
      "Processing `What is the percentage of human judgment agreement on the set?`...\n",
      "Processing `Are the orders of case assignment biases motivated by frequency considerations?`...\n",
      "Processing `Does the paper list other heuristic biases in the LSTMs?`...\n",
      "Processing `What are the performances of LSTMs and humans on the task?`...\n",
      "Processing `Do they authors offer a hypothesis for why Twitter data makes better predictions about the inventory of languages used in each country?`...\n",
      "Processing `What social media platforms are represented?`...\n",
      "Processing `Which websites were used in the web crawl?`...\n",
      "Processing `What countries and languages are represented in the datasets?`...\n",
      "Processing `What other evaluation metrics did they use other than ROUGE-L??`...\n",
      "Processing `Do they encode sentences separately or together?`...\n",
      "Processing `How do they use BERT to encode the whole text?`...\n",
      "Processing `What is the ROUGE-L score of baseline method?`...\n",
      "Processing `Which is the baseline method?`...\n",
      "Processing `What loss function is used?`...\n",
      "Processing `Do they use the unstructured text on the webpage that was the source of the table?`...\n",
      "Processing `Does their method rely on the column headings of the table?`...\n",
      "Processing `Are all the tables in the dataset from the same website?`...\n",
      "Processing `How are the tables extracted from the HTML?`...\n",
      "Processing `Does the query-bag matching model use a neural network?`...\n",
      "Processing `What datasets are used for experiments?`...\n",
      "Processing `Which natural language(s) is/are studied?`...\n",
      "Processing `Is model compared to some baseline?`...\n",
      "Processing `What datasets are used in experiments?`...\n",
      "Processing `How many lexical features are considered?`...\n",
      "Processing `What is the performance for the three languages tested?`...\n",
      "Processing `How many Universal Dependency features are considered?`...\n",
      "Processing `Do they evaluate any non-zero-shot parsers on the three languages?`...\n",
      "Processing `How big is the Parallel Meaning Bank?`...\n",
      "Processing `What is the source of the crosslingual word embeddings?`...\n",
      "Processing `Do they compare against manually-created lexicons?`...\n",
      "Processing `Do they compare to non-lexicon methods?`...\n",
      "Processing `What language pairs are considered?`...\n",
      "Processing `How many abstractive summarizations exist for each dialogue?`...\n",
      "Processing `How is human evaluators' judgement measured, what was the criteria?`...\n",
      "Processing `What models have been evaluated?`...\n",
      "Processing `Do authors propose some better metric than ROUGE for measurement of abstractive dialogue summarization?`...\n",
      "Processing `How big is SAMSum Corpus?`...\n",
      "Processing `Do they manually check all adversarial examples that fooled some model for potential valid examples?`...\n",
      "Processing `Are all generated examples semantics-preserving perturbations to the original text?`...\n",
      "Processing `What is success rate of fooling tested models in experiments?`...\n",
      "Processing `What models are able to be fooled for AG's news corpus news categorization task by this approach?`...\n",
      "Processing `What models are able to be fooled for IMDB sentiment classification task by this approach?`...\n",
      "Processing `Do they use already trained model on some task in their reinforcement learning approach?`...\n",
      "Processing `How does proposed reinforcement learning based approach generate adversarial examples in black-box settings?`...\n",
      "Processing `Which languages with different script do they look at?`...\n",
      "Processing `What languages do they experiment with?`...\n",
      "Processing `What language pairs are affected?`...\n",
      "Processing `What evaluation metrics are used?`...\n",
      "Processing `What datasets did they use?`...\n",
      "Processing `what are the other methods they compare to?`...\n",
      "Processing `what preprocessing method is introduced?`...\n",
      "Processing `How well does their model perform on the recommendation task?`...\n",
      "Processing `Which knowledge base do they use to retrieve facts?`...\n",
      "Processing `Which neural network architecture do they use?`...\n",
      "Processing `Are reddit and twitter datasets, which are fairly prevalent, not effective in addressing these problems?`...\n",
      "Processing `did they experiment with other languages?`...\n",
      "Processing `by how much did their system outperform previous tasks?`...\n",
      "Processing `what are the previous state of the art for sentiment categorization?`...\n",
      "Processing `what are the previous state of the art for tweet semantic similarity?`...\n",
      "Processing `By how much do they outperform baselines?`...\n",
      "Processing `Which baselines do they use?`...\n",
      "Processing `Which datasets do they evaluate on?`...\n",
      "Processing `What deep learning methods do they look at?`...\n",
      "Processing `What is their baseline?`...\n",
      "Processing `Which architectures do they experiment with?`...\n",
      "Processing `Are pretrained embeddings used?`...\n",
      "Processing `Does the paper discuss limitations of considering only data from Twitter?`...\n",
      "Processing `Did they represent tie strength only as number of social ties in a networks? `...\n",
      "Processing `What sociolinguistic variables (phonetic spellings) did they analyze? `...\n",
      "Processing `What older dialect markers did they explore?`...\n",
      "Processing `How many domains do they create ontologies for?`...\n",
      "Processing `Do they separately extract topic relations and topic hierarchies in their model?`...\n",
      "Processing `How do they measure the usefulness of obtained ontologies compared to domain expert ones?`...\n",
      "Processing `How do they obtain syntax from raw documents in hrLDA?`...\n",
      "Processing `What datasets are available for CDSA task?`...\n",
      "Processing `What two novel metrics proposed?`...\n",
      "Processing `What similarity metrics have been tried?`...\n",
      "Processing `What 20 domains are available for selection of source domain?`...\n",
      "Processing `why do they think sentiment features do not result in improvement?`...\n",
      "Processing `what was the size of the datasets?`...\n",
      "Processing `what were the evaluation metrics?`...\n",
      "Processing `what were their results on both tasks?`...\n",
      "Processing `what domain-specific features did they train on?`...\n",
      "Processing `what are the sentiment features used?`...\n",
      "Processing `what surface-form features were used?`...\n",
      "Processing `How does their BERT-based model work?`...\n",
      "Processing `How do they use Wikipedia to automatically collect a query-focused summarization dataset?`...\n",
      "Processing `How is GPU-based self-critical Reinforcement Learing model designed?`...\n",
      "Processing `What are previoius similar models authors are referring to?`...\n",
      "Processing `What was previous state of the art on factored dataset?`...\n",
      "Processing `How much did the model outperform`...\n",
      "Processing `What language is in the dataset?`...\n",
      "Processing `How big is the HotPotQA dataset?`...\n",
      "Processing `Which labeling scheme do they use?`...\n",
      "Processing `What parts of their multitask model are shared?`...\n",
      "Processing `Which dataset do they use?`...\n",
      "Processing `Do they compare against Reinforment-Learning approaches?`...\n",
      "Processing `How long is the training dataset?`...\n",
      "Processing `What dataset do they use?`...\n",
      "Processing `What high-resource language pair is the parent model trained on?`...\n",
      "Processing `Did they use any regularization method to constrain the training?`...\n",
      "Processing `How did they constrain training using the parameters?`...\n",
      "Processing `What are their evaluation metrics?`...\n",
      "Processing `Are their formal queries tree-structured?`...\n",
      "Processing `What knowledge base do they rely on?`...\n",
      "Processing `How do they recover from noisy entity linking?`...\n",
      "Processing `What datasets do they evaluate on?`...\n",
      "Processing `Did they use the same dataset as Skip-gram to train?`...\n",
      "Processing `How much were the gains they obtained?`...\n",
      "Processing `What is the extractive technique used for summarization?`...\n",
      "Processing `How big is the dataset?`...\n",
      "Processing `By how much they outperform the baseline?`...\n",
      "Processing `How long are the datasets?`...\n",
      "Processing `What bayesian model is trained?`...\n",
      "Processing `What low resource languages are considered?`...\n",
      "Processing `How is cluster purity measured?`...\n",
      "Processing `What was the previous state of the art for bias mitigation?`...\n",
      "Processing `How are names paired in the Names Intervention?`...\n",
      "Processing `Which tasks quantify embedding quality?`...\n",
      "Processing `What empirical comparison methods are used?`...\n",
      "Processing `How do they define their tokens (words, word-piece)?`...\n",
      "Processing `By how much do they outperform existing state-of-the-art model on end-to-end Speech recognition?s `...\n",
      "Processing `Did the authors collect new data for evaluation?`...\n",
      "Processing `what were the evaluation metrics?`...\n",
      "Processing `what language pairs are explored?`...\n",
      "Processing `what datasets did they use?`...\n",
      "Processing `which attention based nmt method did they compare with?`...\n",
      "Processing `by how much did their system improve?`...\n",
      "Processing `What were the baseline methods?`...\n",
      "Processing `What dataset is used for training?`...\n",
      "Processing `Do they compare to previous work?`...\n",
      "Processing `What is the source of their data?`...\n",
      "Processing `What is their binary classifier?`...\n",
      "Processing `How long is their dataset?`...\n",
      "Processing `What is a study descriptor?`...\n",
      "Processing `How are experiments designed to measure impact on performance by different choices?`...\n",
      "Processing `What impact on performance is shown for different choices of optimizers and learning rate policies?`...\n",
      "Processing `What domain do the audio samples fall under?`...\n",
      "Processing `How did they evaluate the quality of annotations?`...\n",
      "Processing `How many annotators did they have?`...\n",
      "Processing `What is their baseline method?`...\n",
      "Processing `In what language are the captions written in?`...\n",
      "Processing `What is the average length of the captions?`...\n",
      "Processing `Does each image have one caption?`...\n",
      "Processing `What is the size of the dataset?`...\n",
      "Processing `What is the source of the images and textual captions?`...\n",
      "Processing `what evaluation metrics did they use?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `what were roberta's results?`...\n",
      "Processing `which was the worst performing model?`...\n",
      "Processing `How long is their sentiment analysis dataset?`...\n",
      "Processing `What NLI dataset was used?`...\n",
      "Processing `What aspects are considered?`...\n",
      "Processing `What layer gave the better results?`...\n",
      "Processing `How many annotators were used for sentiment labeling?`...\n",
      "Processing `How is data collected?`...\n",
      "Processing `How much better is performance of Nigerian Pitdgin English sentiment classification of models that use additional Nigerian English data compared to orginal English-only models?`...\n",
      "Processing `What full English language based sentiment analysis models are tried?`...\n",
      "Processing `Do they treat differerent turns of conversation differently when modeling features?`...\n",
      "Processing `How do they bootstrap with contextual information?`...\n",
      "Processing `Which word embeddings do they utilize for the EmoContext task?`...\n",
      "Processing `What were the performance results of their network?`...\n",
      "Processing `What were the baselines?`...\n",
      "Processing `What dataset is used?`...\n",
      "Processing `Do they explore other language pairs?`...\n",
      "Processing `How do they preprocess Tweets?`...\n",
      "Processing `What kind of inference model do they build to estimate socioeconomic status?`...\n",
      "Processing `How much data do they gather in total?`...\n",
      "Processing `Do they analyze features which help indicate socioeconomic status?`...\n",
      "Processing `What inference models are used?`...\n",
      "Processing `What baseline model is used?`...\n",
      "Processing `How is the remotely sensed data annotated?`...\n",
      "Processing `Where are the professional profiles crawled from?`...\n",
      "Processing `How much additional data do they manage to generate from translations?`...\n",
      "Processing `Do they train discourse relation models with augmented data?`...\n",
      "Processing `How many languages do they at most attempt to use to generate discourse relation labelled data?`...\n",
      "Processing `by how much did the system improve?`...\n",
      "Processing `what existing databases were used?`...\n",
      "Processing `what existing parser is used?`...\n",
      "Processing `How do they combine the socioeconomic maps with Twitter data? `...\n",
      "Processing `Does the fact that people are active during the day time define their SEC?`...\n",
      "Processing `How did they define standard language?`...\n",
      "Processing `How do they operationalize socioeconomic status from twitter user data?`...\n",
      "Processing `Do the authors provide any benchmark tasks in this new environment?`...\n",
      "Processing `What dimensions do the considered embeddings have?`...\n",
      "Processing `How are global structures considered?`...\n",
      "Processing `Which translation model do they employ?`...\n",
      "Processing `Which datasets do they experiment on?`...\n",
      "Processing `Which other units of text do they experiment with (apart from BPE and ortographic syllables)?`...\n",
      "Processing `How many steps of BPE do they experiment with?`...\n",
      "Processing `What nuances between fake news and satire were discovered?`...\n",
      "Processing `What empirical evaluation was used?`...\n",
      "Processing `What is the baseline?`...\n",
      "Processing `Which linguistic features are used?`...\n",
      "Processing `What contextual language model is used?`...\n",
      "Processing `what state of the art models do they compare to?`...\n",
      "Processing `What is the weak supervision signal used in Baidu Baike corpus?`...\n",
      "Processing `How is BERT optimized for this task?`...\n",
      "Processing `What is a soft label?`...\n",
      "Processing `Do the authors examine the real-world distribution of female workers in the country/countries where the gender neutral languages are spoken?`...\n",
      "Processing `Which of the 12 languages showed the strongest tendency towards male defaults?`...\n",
      "Processing `How many different sentence constructions are translated in gender neutral languages?`...\n",
      "Processing `What are the evaluation metrics used?`...\n",
      "Processing `What are the baselines?`...\n",
      "Processing `Which language learning datasets are used?`...\n",
      "Processing `What does it mean for sentences to be \"lexically overlapping\"?`...\n",
      "Processing `How many tables are in the tablestore?`...\n",
      "Processing `what dataset is used?`...\n",
      "Processing `what neural network models are used?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `What baseline model is used?`...\n",
      "Processing `What type of neural network models are used?`...\n",
      "Processing `How is validity identified and what metric is used to quantify it?`...\n",
      "Processing `How is severity identified and what metric is used to quantify it?`...\n",
      "Processing `How is urgency identified and what metric is used to quantify it?`...\n",
      "Processing `How many of the attribute-value pairs are found in video?`...\n",
      "Processing `How many of the attribute-value pairs are found in audio?`...\n",
      "Processing `How many of the attribute-value pairs are found in images?`...\n",
      "Processing `How many of the attribute-value pairs are found in semi-structured text?`...\n",
      "Processing `How many of the attribute-value pairs are found in unstructured text?`...\n",
      "Processing `How many different semi-structured templates are represented in the data?`...\n",
      "Processing `Are all datapoints from the same website?`...\n",
      "Processing `Do they consider semi-structured webpages?`...\n",
      "Processing `What are the baseline models?`...\n",
      "Processing `What image caption datasets were used in this work?`...\n",
      "Processing `How long does it take to train the model on the mentioned dataset? `...\n",
      "Processing `How big is the human ratings dataset?`...\n",
      "Processing `What existing techniques do the authors compare against?`...\n",
      "Processing `Is the dataset completely automatically generated?`...\n",
      "Processing `Does the SESAME dataset include discontiguous entities?`...\n",
      "Processing `How big is the resulting SESAME dataset?`...\n",
      "Processing `Can their method be transferred to other Q&A platforms (in other languages)?`...\n",
      "Processing `What measures of quality do they use for a Q&A platform?`...\n",
      "Processing `Do they evaluate whether local or global context proves more important?`...\n",
      "Processing `How many layers of recurrent neural networks do they use for encoding the global context?`...\n",
      "Processing `How did their model rank in three CMU WMT2018 tracks it didn't rank first?`...\n",
      "Processing `Do they evaluate only on English datasets?`...\n",
      "Processing `What is the Ritter dataset?`...\n",
      "Processing `Does this model perform better than the state of the art?`...\n",
      "Processing `What features are extracted from text?`...\n",
      "Processing `What features are extracted from images?`...\n",
      "Processing `What are the baseline models?`...\n",
      "Processing `How are the three different forms defined in this work?`...\n",
      "Processing `What datasets are used for training and testing?`...\n",
      "Processing `Does approach handle overlapping forms (e.g., metaphor and irony)?`...\n",
      "Processing `Does this work differentiate metaphor(technique) from irony and sarcasm (purpose)? `...\n",
      "Processing `What classification tasks do they experiment on?`...\n",
      "Processing `What categories of fake news are in the dataset?`...\n",
      "Processing `How much gain in performance was obtained with user embeddings?`...\n",
      "Processing `By how much does their similarity measure outperform BM25?`...\n",
      "Processing `How do they represent documents when using their proposed similarity measure?`...\n",
      "Processing `How do they propose to combine BM25 and word embedding similarity?`...\n",
      "Processing `Do they use pretrained word embeddings to calculate Word Mover's distance?`...\n",
      "Processing `Which Twitter sentiment treebank is used?`...\n",
      "Processing `Where did the system place in the other sub-tasks?`...\n",
      "Processing `What were the five labels to be predicted in sub-task C?`...\n",
      "Processing `What is the previous state-of-the-art?`...\n",
      "Processing `What is the architecture of the decoder?`...\n",
      "Processing `What is the architecture of the encoder?`...\n",
      "Processing `What are the languages of the datasets?`...\n",
      "Processing `What is the architecture of the saliency model?`...\n",
      "Processing `What are special architectures this review focuses on that are related to multimodal fusion?`...\n",
      "Processing `What other model inference optimization schemes authors explore?`...\n",
      "Processing `On what dataset is model trained/tested?`...\n",
      "Processing `By how much do they improve on domain classification?`...\n",
      "Processing `Which dataset do they evaluate on?`...\n",
      "Processing `How does their approach work for domains with few overlapping utterances? `...\n",
      "Processing `How do they decide by how much to decrease confidences of incorrectly predicted domains?`...\n",
      "Processing `Is some baseline method trained on new dataset?`...\n",
      "Processing `What potential applications are demonstrated?`...\n",
      "Processing `What method is proposed to mitigate class imbalance in final dataset?`...\n",
      "Processing `What are remaining challenges in VQA?`...\n",
      "Processing `How quickly is this hybrid model trained?  `...\n",
      "Processing `What are the new deep learning models discussed in the paper?  `...\n",
      "Processing `What was the architecture of the 2017 Challenge Winner model?`...\n",
      "Processing `What is an example of a common sense question?`...\n",
      "Processing `What pretrained language representations are used?`...\n",
      "Processing `How many instances are explored in the few-shot experiments?`...\n",
      "Processing `What tasks are explored?`...\n",
      "Processing `How is the training time compared to the original position encoding? `...\n",
      "Processing `Does the new relative position encoder require more parameters?`...\n",
      "Processing `Can the new position representation be generalized to other tasks?`...\n",
      "Processing `which social media platforms was the data collected from?`...\n",
      "Processing `how many data pairs were there for each dataset?`...\n",
      "Processing `how many systems were there?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `what metrics did they use for evaluation?`...\n",
      "Processing `what datasets did they use?`...\n",
      "Processing `What is the Semantic Web?`...\n",
      "Processing `How many tags are included in the ENE tag set?`...\n",
      "Processing `Does the paper evaluate the dataset for smaller NE tag tests? `...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `What are the most discriminating patterns which are analyzed?`...\n",
      "Processing `What bootstrapping methodology was used to find new patterns?`...\n",
      "Processing `What patterns were extracted which were correlated with emotional arguments?`...\n",
      "Processing `What patterns were extracted which were correlated with factual arguments?`...\n",
      "Processing `How were the factual and feeling forum posts annotated?`...\n",
      "Processing `What evaluation metrics did they use?`...\n",
      "Processing `What NMT techniques did they explore?`...\n",
      "Processing `What was their best performing model?`...\n",
      "Processing `What datasets did they use?`...\n",
      "Processing `Which ontologies did they use?`...\n",
      "Processing `how is user satisfaction estimated?`...\n",
      "Processing `by how much did performance improve?`...\n",
      "Processing `What datasets do they use in the experiment?`...\n",
      "Processing `What new tasks do they use to show the transferring ability of the shared meta-knowledge?`...\n",
      "Processing `What kind of meta learning algorithm do they use?`...\n",
      "Processing `what dataset were used?`...\n",
      "Processing `what was the baseline?`...\n",
      "Processing `what text embedding methods were used in their approach?`...\n",
      "Processing `Do they compare against state-of-the-art?`...\n",
      "Processing `What are the benchmark datasets?`...\n",
      "Processing `What tasks are the models trained on?`...\n",
      "Processing `What recurrent neural networks are explored?`...\n",
      "Processing `What extractive models were trained on this dataset?`...\n",
      "Processing `What abstractive models were trained?`...\n",
      "Processing `Do the reviews focus on a specific video game domain?`...\n",
      "Processing `What is the size of this dataset?`...\n",
      "Processing `What language(s) does the system answer questions in?`...\n",
      "Processing `What metrics are used for evaluation?`...\n",
      "Processing `Is the proposed system compared to existing systems?`...\n",
      "Processing `How do they determine that a decoder handles an easier task than the encoder?`...\n",
      "Processing `How do they measure conditional information strength?`...\n",
      "Processing `How do they generate input noise for the encoder and decoder?`...\n",
      "Processing `How do they perform the joint training?`...\n",
      "Processing `How many parameters does their model have?`...\n",
      "Processing `What is the previous model that achieved state-of-the-art?`...\n",
      "Processing `Did the survey provide insight into features commonly found to be predictive of abusive content on online platforms?`...\n",
      "Processing `Is deep learning the state-of-the-art method in automated abuse detection`...\n",
      "Processing `What datasets were used in this work?`...\n",
      "Processing `How is abuse defined for the purposes of this research?`...\n",
      "Processing `Do they use external financial knowledge in their approach?`...\n",
      "Processing `Which evaluation metrics do they use?`...\n",
      "Processing `Which finance specific word embedding model do they use?`...\n",
      "Processing `How does lattice rescoring improve inference?`...\n",
      "Processing `What three languages are used in the translation experiments?`...\n",
      "Processing `What metrics are used to measure bias reduction?`...\n",
      "Processing `How is the set of trusted, gender-balanced examples selected?`...\n",
      "Processing `Which data sources do they use?`...\n",
      "Processing `Which tasks do they evaluate supervised systems on?`...\n",
      "Processing `How do they evaluate domain portability?`...\n",
      "Processing `Which unsupervised representation-learning objectives do they introduce?`...\n",
      "Processing `Do they manage to consistenly outperform the best performing methods?`...\n",
      "Processing `Do they try to use other models aside from Maximum Entropy?`...\n",
      "Processing `What methods to they compare to?`...\n",
      "Processing `Which dataset to they train and evaluate on?`...\n",
      "Processing `Do they attempt to jointly learn connectives, arguments, senses and non-explicit identiifers end-to-end?`...\n",
      "Processing `What settings did they experiment with?`...\n",
      "Processing `what domains are explored in this paper?`...\n",
      "Processing `what multi-domain dataset is repurposed?`...\n",
      "Processing `what four learning strategies are investigated?`...\n",
      "Processing `By how much did the new model outperform multilingual BERT?`...\n",
      "Processing `What previous proposed methods did they explore?`...\n",
      "Processing `What was the new Finnish model trained on?`...\n",
      "Processing `How many TV series are considered?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `Is manual annotation performed?`...\n",
      "Processing `What are the eight predefined categories?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `When the authors say their method largely outperforms the baseline, does this mean that the baseline performed better in some cases? If so, which ones?`...\n",
      "Processing `What baseline method was used?`...\n",
      "Processing `What was the motivation for using a dependency tree based recursive architecture?`...\n",
      "Processing `How was a causal diagram used to carefully remove this bias?`...\n",
      "Processing `How does publicity bias the dataset?`...\n",
      "Processing `How do the speakers' reputations bias the dataset?`...\n",
      "Processing `What is the state-of-the-art approach?`...\n",
      "Processing `what do they mean by description length?`...\n",
      "Processing `do they focus on english verbs?`...\n",
      "Processing `what evaluation metrics are used?`...\n",
      "Processing `Do the authors mention any possible confounds in this study?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `Are there any other standard linguistic features used, other than ngrams?`...\n",
      "Processing `What is the relationship between author and emotional valence?`...\n",
      "Processing `What is the relationship between time and emotional valence?`...\n",
      "Processing `What is the relationship between location and emotional valence?`...\n",
      "Processing `What is the computational complexity of old method`...\n",
      "Processing `Could you tell me more about the old method?`...\n",
      "Processing `How this system recommend features for the new application?`...\n",
      "Processing `What is the similarity of manually selected features across related applications in different domains?`...\n",
      "Processing `What type of features are extracted with this language?`...\n",
      "Processing `What are meta elements of language for specifying NLP features?`...\n",
      "Processing `what previous work do they also look at?`...\n",
      "Processing `what languages did they experiment with?`...\n",
      "Processing `What are state of the art results on OSA and PD corpora used for testing?`...\n",
      "Processing `How better does x-vectors perform than knowlege-based features in same-language corpora?`...\n",
      "Processing `What is meant by domain missmatch occuring?`...\n",
      "Processing `How big are OSA and PD corporas used for testing?`...\n",
      "Processing `How do they think this treebank will support research on second language acquisition?`...\n",
      "Processing `What are their baseline models?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `Did they use crowdsourcing to annotate the dataset?`...\n",
      "Processing `How significant are the improvements over previous approaches?`...\n",
      "Processing `Which other tasks are evaluated?`...\n",
      "Processing `What are the performances associated to different attribute placing?`...\n",
      "Processing `What architecture is used in the encoder?`...\n",
      "Processing `Do they evaluate their parallel sentence generation?`...\n",
      "Processing `How much data do they manage to gather online?`...\n",
      "Processing `Which models do they use for phrase-based SMT?`...\n",
      "Processing `Which models do they use for NMT?`...\n",
      "Processing `What are the BLEU performance improvements they achieve?`...\n",
      "Processing `What is the architecture of the model?`...\n",
      "Processing `How many translation pairs are used for training?`...\n",
      "Processing `Do they use multitask learning?`...\n",
      "Processing `Is Chinese a pro-drop language?`...\n",
      "Processing `Is English a pro-drop language?`...\n",
      "Processing `Which movie subtitles dataset did they use?`...\n",
      "Processing `What are the other two Vietnamese datasets?`...\n",
      "Processing `Which English dataset do they evaluate on?`...\n",
      "Processing `What neural network models do they use in their evaluation?`...\n",
      "Processing `Do they use crowdsourcing for the captions?`...\n",
      "Processing `What methods are used to build two other Viatnamese datsets?`...\n",
      "Processing `What deep neural network models are used in evaluation?`...\n",
      "Processing `How authors evaluate datasets using models trained on different datasets?`...\n",
      "Processing `Do they evaluate their model on datasets other than RACE?`...\n",
      "Processing `What is their model's performance on RACE?`...\n",
      "Processing `What deep learning models do they plan to use?`...\n",
      "Processing `What baseline, if any, is used?`...\n",
      "Processing `How are the language models used to make predictions on humorous statements?`...\n",
      "Processing `What type of language models are used? e.g. trigrams, bigrams?`...\n",
      "Processing `How do attention, recurrent and convolutional networks differ on the language classes they accept?`...\n",
      "Processing `What type of languages do they test LSTMs on?`...\n",
      "Processing `What is possible future improvement for proposed method/s?`...\n",
      "Processing `What is percentage change in performance for better model when compared to baseline?`...\n",
      "Processing `Which of two design architectures have better performance?`...\n",
      "Processing `What evaluation metrics did they use?`...\n",
      "Processing `By how much does their model outperform the baseline?`...\n",
      "Processing `Which models did they compare with?`...\n",
      "Processing `What is the source of their datasets?`...\n",
      "Processing `What new advances are included in this dataset?`...\n",
      "Processing `What language is this dataset in?`...\n",
      "Processing `How do they prove that RNNs with arbitrary precision are as powerful as a pushdown automata?`...\n",
      "Processing `What are edge weights?`...\n",
      "Processing `Does the paper report F1-scores with and without post-processing for the second task?`...\n",
      "Processing `What does post-processing do to the output?`...\n",
      "Processing `Do they test any neural architecture?`...\n",
      "Processing `Is the performance of a Naive Bayes approach evaluated?`...\n",
      "Processing `What supports the claim that enhancement in training is advisable as long as enhancement in test is at least as strong as in training?`...\n",
      "Processing `How does this single-system compares to system combination ones?`...\n",
      "Processing `What was previous single-system state of the art result on the CHiME-5 data?`...\n",
      "Processing `How much is error rate reduced by cleaning up training data?`...\n",
      "Processing `Which baselines were they used for evaluation?`...\n",
      "Processing `What is the difference in size compare to the previous model?`...\n",
      "Processing `What languages are used as input?`...\n",
      "Processing `What are the components of the classifier?`...\n",
      "Processing `Which uncertain outcomes are forecast using the wisdom of crowds?`...\n",
      "Processing `What set topics are looked at?`...\n",
      "Processing `What were the baselines?`...\n",
      "Processing `Which widely used dataset did the authors use?`...\n",
      "Processing `How do they perform semi-supervised learning?`...\n",
      "Processing `What are the five evaluated tasks?`...\n",
      "Processing `What downstream tasks are explored?`...\n",
      "Processing `What factors contribute to the stability of the word embeddings?`...\n",
      "Processing `How is unstability defined?`...\n",
      "Processing `What embedding algorithms are explored?`...\n",
      "Processing `Which data-selection algorithms do they use?`...\n",
      "Processing `How are the artificial sentences generated?`...\n",
      "Processing `What domain is their test set?`...\n",
      "Processing `What morphological features are considered?`...\n",
      "Processing `What type of attention do they use in the decoder?`...\n",
      "Processing `What set of semantic tags did they use?`...\n",
      "Processing `How much improvement did they see on the NLI task?`...\n",
      "Processing `How better are results of new model compared to competitive methods?`...\n",
      "Processing `What is the metrics used for benchmarking methods?`...\n",
      "Processing `What are other competitive methods?`...\n",
      "Processing `What is the size of built dataset?`...\n",
      "Processing `which had better results, the svm or the random forest model?`...\n",
      "Processing `which network community detection dataset was used?`...\n",
      "Processing `did they collect the human labeled data?`...\n",
      "Processing `how many classes are they classifying?`...\n",
      "Processing `Do the authors evaluate only on English datasets?`...\n",
      "Processing `What metrics of gender bias amplification are used to demonstrate the effectiveness of this approach?`...\n",
      "Processing `How is representation learning decoupled from memory management in this architecture?`...\n",
      "Processing `What method did the highest scoring team use?`...\n",
      "Processing `What descriptive statistics are provided about the data?`...\n",
      "Processing `What was the level of inter-annotator agreement?`...\n",
      "Processing `What questions were asked in the annotation process?`...\n",
      "Processing `Why is NER for tweets more challenging as the number of entities increases?`...\n",
      "Processing `What data preparation steps were used to construct the dataset?`...\n",
      "Processing `What is the training objective in the method introduced in this paper?`...\n",
      "Processing `Does regularization of the fine-tuning process hurt performance in the target domain?`...\n",
      "Processing `What kind of baseline model do they compare against?`...\n",
      "Processing `Do they analyze which types of sentences/reviews are useful or not?`...\n",
      "Processing `Which set of datasets do they use?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `How are adversarial examples generated?`...\n",
      "Processing `Is BAT smaller (in number of parameters) than post-trained BERT?`...\n",
      "Processing `What are the modifications made to post-trained BERT?`...\n",
      "Processing `What aspects are considered?`...\n",
      "Processing `Were human evaluations conducted?`...\n",
      "Processing `What datasets are used?`...\n",
      "Processing `How does inference time compare to other methods?`...\n",
      "Processing `Where can I access the dataset?`...\n",
      "Processing `Did they release their dataset?`...\n",
      "Processing `Did they use Amazon Mechanical Turk to collect data?`...\n",
      "Processing `Did they use The Onion as their dataset?`...\n",
      "Processing `What sources did they get the data from?`...\n",
      "Processing `What language is the model tested on?`...\n",
      "Processing `How much lower is the computational cost of the proposed model?`...\n",
      "Processing `What is the state-of-the-art model?`...\n",
      "Processing `What is a pseudo language model?`...\n",
      "Processing `How significant is the performance compared to LSTM model?`...\n",
      "Processing `How does the introduced model combine the both factors?`...\n",
      "Processing `How much improvement do the introduced model achieve compared to the previous models?`...\n",
      "Processing `do they compare their system with other systems?`...\n",
      "Processing `what is the architecture of their model?`...\n",
      "Processing `what dataset did they use for this tool?`...\n",
      "Processing `Did they build a dataset?`...\n",
      "Processing `Do they compare to other methods?`...\n",
      "Processing `How large is the dataset?`...\n",
      "Processing `what is the average number of speakers in the dataset?`...\n",
      "Processing `by how much is accuracy improved?`...\n",
      "Processing `what are the previous state of the art systems?`...\n",
      "Processing `What are the three SOTA models evaluated?`...\n",
      "Processing `What is the morphological constraint added?`...\n",
      "Processing `How do they interpret the model?`...\n",
      "Processing `Do they compare their approach to data-driven only methods?`...\n",
      "Processing `What are the two applications of neuro-symbolism?`...\n",
      "Processing `what elements of each profile did they use?`...\n",
      "Processing `Does this paper discuss the potential these techniques have for invading user privacy?`...\n",
      "Processing `How is the gold standard defined?`...\n",
      "Processing `What is the timeframe of the current events?`...\n",
      "Processing `What model was used for sentiment analysis?`...\n",
      "Processing `How many tweets did they look at?`...\n",
      "Processing `What language are the tweets in?`...\n",
      "Processing `Is this analysis performed only on English data?`...\n",
      "Processing `Do they authors offer any hypothesis for why the parameters of Zipf's law and Heaps' law differ on Twitter?`...\n",
      "Processing `What explanation do the authors offer for the super or sublinear urban scaling?`...\n",
      "Processing `Do the authors give examples of the core vocabulary which follows the scaling relationship of the bulk text?`...\n",
      "Processing `What syntactic and semantic features are proposed?`...\n",
      "Processing `Which six speech acts are included in the taxonomy?`...\n",
      "Processing `what classifier had better performance?`...\n",
      "Processing `how many tweets were labeled?`...\n",
      "Processing `how many annotators were there?`...\n",
      "Processing `who labelled the tweets?`...\n",
      "Processing `what are the proposed semantic features?`...\n",
      "Processing `what syntactic features are proposed?`...\n",
      "Processing `what datasets were used?`...\n",
      "Processing `What was the baseline?`...\n",
      "Processing `How many songs were collected?`...\n",
      "Processing `how does end of utterance and token tags affect the performance`...\n",
      "Processing `what are the baselines?`...\n",
      "Processing `what kind of conversations are in the douban conversation corpus?`...\n",
      "Processing `what pretrained word embeddings are used?`...\n",
      "Processing `What other evaluation metrics are reported?`...\n",
      "Processing `What out of domain scenarios did they evaluate on?`...\n",
      "Processing `What was their state of the art accuracy score?`...\n",
      "Processing `Which datasets did they use?`...\n",
      "Processing `What are the neural baselines mentioned?`...\n",
      "Processing `What regularization methods are used?`...\n",
      "Processing `What metrics are used?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `What dataset do they use?`...\n",
      "Processing `What simplification of the architecture is performed that resulted in same performance?`...\n",
      "Processing `How much better is performance of SEPT compared to previous state-of-the-art?`...\n",
      "Processing `How many actions are present in the dataset?`...\n",
      "Processing `How many videos did they use?`...\n",
      "Processing `What unimodal algorithms do they compare with?`...\n",
      "Processing `What platform was used for crowdsourcing?`...\n",
      "Processing `What language are the videos in?`...\n",
      "Processing `How long are the videos?`...\n",
      "Processing `What was the inter-annotator agreement between the expert annotators?`...\n",
      "Processing `How were missing hypotheses discovered?`...\n",
      "Processing `Which aspects of response generation do they evaluate on?`...\n",
      "Processing `Which dataset do they evaluate on?`...\n",
      "Processing `What model architecture do they use for the decoder?`...\n",
      "Processing `Do they ensure the edited response is grammatical?`...\n",
      "Processing `What do they use as the pre-defined index of prototype responses?`...\n",
      "Processing `what are all the datasets they experiment with?`...\n",
      "Processing `what was the baseline model?`...\n",
      "Processing `What do they mean by explicit selection of most relevant segments?`...\n",
      "Processing `What datasets they used for evaluation?`...\n",
      "Processing `Which part of their architecture provides the most speedup in comparison to existing approaches?`...\n",
      "Processing `Do they consistently outperform existing systems in terms of accuracy?`...\n",
      "Processing `How big is this dataset?`...\n",
      "Processing `How are biases identified in the dataset?`...\n",
      "Processing `How do they deal with imbalanced datasets?`...\n",
      "Processing `What models do they compare to?`...\n",
      "Processing `What text preprocessing tasks do they focus on?`...\n",
      "Processing `What news sources did they get the dataset from?`...\n",
      "Processing `Did they collect their own corpus?`...\n",
      "Processing `Do the tweets fall under a specific domain?`...\n",
      "Processing `How many tweets are in the dataset?`...\n",
      "Processing `What categories do they look at?`...\n",
      "Processing `Which knowledge destilation methods do they introduce?`...\n",
      "Processing `What type of weight pruning do they use?`...\n",
      "Processing `Which dataset do they train on?`...\n",
      "Processing `Do they reason why greedy decoding works better then beam search?`...\n",
      "Processing `Are experiments conducted on multiple datasets?`...\n",
      "Processing `What baselines is the neural relation extractor compared to?`...\n",
      "Processing `What additional evidence they use?`...\n",
      "Processing `How much improvement they get from the previous state-of-the-art?`...\n",
      "Processing `What is the previous state-of-the-art?`...\n",
      "Processing `What is the architecture of the model?`...\n",
      "Processing `What fine-grained semantic types are considered?`...\n",
      "Processing `What hand-crafted features do other approaches use?`...\n",
      "Processing `What is the strong baseline model used?`...\n",
      "Processing `What crowdsourcing platform did they obtain the data from?`...\n",
      "Processing `How large is the test set?`...\n",
      "Processing `What does SARI measure?`...\n",
      "Processing `What are the baseline models?`...\n",
      "Processing `Is dataset balanced in terms of available data per language?`...\n",
      "Processing `What datasets are used?`...\n",
      "Processing `How do they split the dataset when training and evaluating their models?`...\n",
      "Processing `Do they demonstrate the relationship between veracity and stance over time in the Twitter dataset?`...\n",
      "Processing `How much improvement does their model yield over previous methods?`...\n",
      "Processing `How many GPUs do they train their models on?`...\n",
      "Processing `What of the two strategies works best?`...\n",
      "Processing `What downstream tasks are tested?`...\n",
      "Processing `Is this model trained in unsuperized manner?`...\n",
      "Processing `How much is BELU score difference between proposed approach and insertion-only method?`...\n",
      "Processing `are the protocols manually annotated?`...\n",
      "Processing `what ML approaches did they experiment with?`...\n",
      "Processing `What type of attention is used in the recognition system?`...\n",
      "Processing `What are the solutions proposed for the seq2seq shortcomings?`...\n",
      "Processing `How much is training speeded up?`...\n",
      "Processing `What experiments do they perform?`...\n",
      "Processing `What is mean field theory?`...\n",
      "Processing `Which datasets do they evaluate on?`...\n",
      "Processing `Do they compare against a system that does not use streaming text, but has the entire text at disposal?`...\n",
      "Processing `Does larger granularity lead to better translation quality?`...\n",
      "Processing `Do they report results only on English data?`...\n",
      "Processing `What is the baseline method?`...\n",
      "Processing `What aspects are used to judge question quality?`...\n",
      "Processing `What did the human annotations consist of?`...\n",
      "Processing `What characterizes the 303 domains? e.g. is this different subject tags?`...\n",
      "Processing `How long is their dataset?`...\n",
      "Processing `What metrics are used?`...\n",
      "Processing `What is the best performing system?`...\n",
      "Processing `What tokenization methods are used?`...\n",
      "Processing `What baselines do they propose?`...\n",
      "Processing `What is the size of the dataset?`...\n",
      "Processing `What models are trained?`...\n",
      "Processing `Does the baseline use any contextual information?`...\n",
      "Processing `What is the strong rivaling system?`...\n",
      "Processing `Where are the debates from?`...\n",
      "Processing `What is the state-of-the-art model in this task?`...\n",
      "Processing `How does this result compare to other methods KB QA in CCKS2019?`...\n",
      "Processing `Do they have an elementary unit of text?`...\n",
      "Processing `By how much do they outpeform existing text denoising models?`...\n",
      "Processing `In their nonsymbolic representation can they represent two same string differently depending on the context?`...\n",
      "Processing `On which datasets do they evaluate their models?`...\n",
      "Processing `How do they determine demographics on an image?`...\n",
      "Processing `Do they assume binary gender?`...\n",
      "Processing `What is the most underrepresented person group in ILSVRC?`...\n",
      "Processing `How long did the training take?`...\n",
      "Processing `Is the proposed model smaller or bigger than the conventional NMT system?`...\n",
      "Processing `Do they compare to state-of-the-art models?`...\n",
      "Processing `how many sentences did they annotate?`...\n",
      "Processing `what dataset was used in their experiment?`...\n",
      "Processing `what are the existing annotation tools?`...\n",
      "Processing `what ontologies did they use?`...\n",
      "Processing `How much improvement is given on RACE by their introduced approach?`...\n",
      "Processing `what pruning did they perform?`...\n",
      "Processing `Do they evaluate binary paragraph vectors on a downstream task?`...\n",
      "Processing `How do they show that binary paragraph vectors capture semantics?`...\n",
      "Processing `Which training dataset do they use?`...\n",
      "Processing `Do they analyze the produced binary codes?`...\n",
      "Processing `How long is the dataset?`...\n",
      "Processing `Do they use machine learning?`...\n",
      "Processing `What are the ICD-10 codes?`...\n",
      "Processing `Do they release their code?`...\n",
      "Processing `What media sources do they use?`...\n",
      "Processing `What evidence is presented that humans perceive the sentiment classes as ordered?`...\n",
      "Processing `What size of dataset is sufficiently large for the model performance to approach the inter-annotator agreement?`...\n",
      "Processing `Which measures of inter-annotator agreement are used?`...\n",
      "Processing `What statistical test(s) is used to compare the top classification models?`...\n",
      "Processing `What is the baseline?`...\n",
      "Processing `How is their NER model trained?`...\n",
      "Processing `Do they use pretrained word embeddings such as BERT?`...\n",
      "Processing `How well does the system perform?`...\n",
      "Processing `Where does their information come from?`...\n",
      "Processing `What intents do they have?`...\n",
      "Processing `Is proposed approach compared to some baselines?`...\n",
      "Processing `What datasets are used for this tasks?`...\n",
      "Processing `How big are improvements on these tasks?`...\n",
      "Processing `Which downstream tasks are considered?`...\n",
      "Processing `How long are the two unlabelled corpora?`...\n",
      "Processing `Do the authors report only on English data?`...\n",
      "Processing `How is the impact of ParityBOT analyzed?`...\n",
      "Processing `What public online harassment datasets was the system validated on?`...\n",
      "Processing `Where do the supportive tweets about women come from? Are they automatically or manually generated?`...\n",
      "Processing `How are the hateful tweets aimed at women detected/classified?`...\n",
      "Processing `How many GPUs do they use for this task?`...\n",
      "Processing `Do they use all the hidden layer representations?`...\n",
      "Processing `What languages are used for the experiments?`...\n",
      "Processing `What is the caching mechanism?`...\n",
      "Processing `What language model architectures are examined?`...\n",
      "Processing `What directions are suggested to improve language models?`...\n",
      "Processing `What logic rules can be learned using ELMo?`...\n",
      "Processing `Does Elmo learn all possible logic rules?`...\n",
      "Processing `Which metrics are used for evaluating the quality?`...\n",
      "Processing `Are face tracking, identification, localization etc multimodal inputs in some ML model or system is programmed by hand?`...\n",
      "Processing `What are baselines used?`...\n",
      "Processing `What are the baselines for this paper?`...\n",
      "Processing `What VQA datasets are used for evaluating this task? `...\n",
      "Processing `How do they model external knowledge? `...\n",
      "Processing `What type of external knowledge has been used for this paper? `...\n",
      "Processing `What is the proposed algorithm or model architecture?`...\n",
      "Processing `Do they attain state-of-the-art performance?`...\n",
      "Processing `What fusion methods are applied?`...\n",
      "Processing `What graph-based features are considered?`...\n",
      "Processing `How does Overton handles contradictory or incomplete supervision data?`...\n",
      "Processing `What are high level declarative abstractions Overton provides?`...\n",
      "Processing `How are applications presented in Overton?`...\n",
      "Processing `Does Overton support customizing deep learning models without writing any code?`...\n",
      "Processing `what metrics are used to evaluate the models?`...\n",
      "Processing `what are the baselines?`...\n",
      "Processing `what is the size of the dataset?`...\n",
      "Processing `what dataset did they use?`...\n",
      "Processing `How is the quality of the translation evaluated?`...\n",
      "Processing `What are the post-processing approaches applied to the output?`...\n",
      "Processing `Is the MUSE alignment independently evaluated?`...\n",
      "Processing `How does byte-pair encoding work?`...\n",
      "Processing `How many general qualitative statements are in dataset?`...\n",
      "Processing `What are state-of-the-art models on this dataset?`...\n",
      "Processing `How are properties being compared annotated?`...\n",
      "Processing `What state-of-the-art tagging model did they use?`...\n",
      "Processing `By how much do they improve upon supervised traning methods?`...\n",
      "Processing `Do they jointly optimize both agents?`...\n",
      "Processing `Which neural network architecture do they use for the dialog agent and user simulator?`...\n",
      "Processing `Do they create the basic dialog agent and basic user simulator separately?`...\n",
      "Processing `Is this done in form of unsupervised (clustering) or suppervised learning?`...\n",
      "Processing `Does this study perform experiments to prove their claim that indeed personalized profiles will have inclination towards particular cuisines?`...\n",
      "Processing `What baselines do they compare to?`...\n",
      "Processing `What training set sizes do they use?`...\n",
      "Processing `What languages do they experiment with?`...\n",
      "Processing `What language model is trained?`...\n",
      "Processing `What machine learning models are considered?`...\n",
      "Processing `What is the agreement of the dataset?`...\n",
      "Processing `Do the authors offer any potential reasons why cross-validation variants tend to overestimate the performance, while the sequential methods tend to underestimate it?`...\n",
      "Processing `Which three variants of sequential validation are examined?`...\n",
      "Processing `Which three variants of cross-validation are examined?`...\n",
      "Processing `Which European languages are targeted?`...\n",
      "Processing `In what way are sentiment classes ordered?`...\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "test_questions: Dict[str, Dict] = utils.load_json(Path(\"qasper/test_questions.json\"))\n",
    "for idx, (question_id, question_data) in enumerate(test_questions.items()):\n",
    "        print(f\"Processing `{question_data['question']}`...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import jsonlines\n",
    "import torch\n",
    "import re\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "os.environ['HF_HOME'] = '/workspace/P76125041/.cache/'\n",
    "\n",
    "# CUDA_DEVICE = 1\n",
    "\n",
    "RETRIEVER = \"sbert\"\n",
    "READER = \"unified\"\n",
    "MODE = \"full\"\n",
    "TOPK = 3\n",
    "\n",
    "retriever_map: Dict[str, str] = {\n",
    "    \"sbert\": \"sentence-transformers/multi-qa-mpnet-base-cos-v1\",\n",
    "    \"stella\": \"/workspace/P76125041/models/stella_en_400M_v5\"\n",
    "}\n",
    "\n",
    "reader_map: Dict[str, str] = {\n",
    "    \"unified\": \"allenai/unifiedqa-v2-t5-3b-1363200\",\n",
    "}\n",
    "\n",
    "def format_paragraphs(raw_paras: List[dict], mode: str) -> List[str]:\n",
    "    if mode == \"full\":\n",
    "        return [\n",
    "            f\"Section Title: {para.get('section_name', '[UNNAMED SECTION]')}\\n\\nText: {para.get('text', '')}\"\n",
    "            for para in raw_paras\n",
    "        ]\n",
    "    elif mode == \"toptitle\":\n",
    "        return [\n",
    "            f\"Section Title: {re.sub(r':::.*', '', para.get('section_name', '[UNNAMED SECTION]')).strip()}\\n\\nText: {para.get('text', '')}\"\n",
    "            for para in raw_paras\n",
    "        ]\n",
    "    else:\n",
    "        return [para.get('text', '') for para in raw_paras]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all paper contents from test set\n",
    "data_file: Path = Path(\"qasper/test_papers.json\")\n",
    "with open(data_file, \"r\") as file:\n",
    "    test_papers: Dict[str, Dict] = json.load(file)\n",
    "\n",
    "# load all questions from test set\n",
    "questions_file: Path = Path(\"qasper/test_questions.json\")\n",
    "with open(questions_file, \"r\") as file:\n",
    "    test_questions: Dict[str, Dict] = json.load(file)\n",
    "\n",
    "# initialize retriever\n",
    "### SBERT\n",
    "embedding_model = SentenceTransformer(retriever_map.get(RETRIEVER))\n",
    "\n",
    "# prepare paragraph embeddings for each paper [approx time: 5min]\n",
    "all_paper_ids: List[str] = [q[\"from_paper\"] for q in test_questions.values()]\n",
    "paper_para_embeddings: Dict[str, List[List[float]]] = {}\n",
    "for paper_id in all_paper_ids:\n",
    "    paragraphs: List[str] = format_paragraphs(test_papers[paper_id].values(), MODE)\n",
    "    para_embeddings: List[List[float]] = embedding_model.encode(paragraphs)\n",
    "    paper_para_embeddings[paper_id] = para_embeddings\n",
    "\n",
    "# Save paragraph embeddings to JSON\n",
    "embedding_file = f\"qasper/test_paper_embeddings_{MODE}.json\"\n",
    "with open(embedding_file, \"w+\", encoding=\"utf-8\") as f:\n",
    "    json.dump(paper_para_embeddings, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n",
      "1451\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "from typing import List, Dict, Set\n",
    "\n",
    "# load all questions from test set\n",
    "questions_file: Path = Path(\"qasper/test_questions.json\")\n",
    "with open(questions_file, \"r\") as file:\n",
    "    test_questions: Dict[str, Dict] = json.load(file)\n",
    "\n",
    "all_paper_ids: Set[str] = set()\n",
    "paper_ids: List[str] = []\n",
    "for question in test_questions.values():\n",
    "    all_paper_ids.add(question[\"from_paper\"])\n",
    "    paper_ids.append(question[\"from_paper\"])\n",
    "\n",
    "print(len(all_paper_ids))\n",
    "print(len(paper_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paper_ids = list(test_questions.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "# load all questions from test set\n",
    "questions_file: Path = Path(\"qasper/test_questions.json\")\n",
    "\n",
    "with open(questions_file, \"r\") as file:\n",
    "    test_questions: Dict[str, Dict] = json.load(file)\n",
    "\n",
    "def find_duplicates(lst: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    Finds and returns duplicate elements in the list.\n",
    "    \"\"\"\n",
    "    return [item for item, count in Counter(lst).items() if count > 1]\n",
    "\n",
    "# Example usage:\n",
    "lst = [1, 2, 3, 4, 5, 2, 3, 6]\n",
    "duplicates = find_duplicates(test_questions.keys())\n",
    "print(duplicates)  # Output: [2, 3]\n",
    "len(test_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"1911.10742\"\n",
    "MODE = \"full\"\n",
    "embedding_file = f\"qasper/test_paper_embeddings_{MODE}.json\"\n",
    "\n",
    "# load embeddings\n",
    "with open(embedding_file, \"r\") as file:\n",
    "    paper_para_embeddings: Dict[str, List[List[float]]] = json.load(file)\n",
    "\n",
    "print(len(paper_para_embeddings[paper_id]))\n",
    "print(paper_para_embeddings[paper_id][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "full_title_error_file: Path = Path(\"results/full_titles_top3/error_cases.json\")\n",
    "no_title_error_file: Path = Path(\"results/no_titles/error_cases.json\")\n",
    "\n",
    "# get all paper contents from test set\n",
    "with open(full_title_error_file, \"r\") as file:\n",
    "    full_title_errors: Dict[str, Dict] = json.load(file)\n",
    "    \n",
    "# get all questions from test set\n",
    "with open(no_title_error_file, \"r\") as file:\n",
    "    no_title_errors: Dict[str, Dict] = json.load(file)\n",
    "\n",
    "print(full_title_errors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('CUDA_HOME'))\n",
    "total time (sec): 725.7992417812347"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "hi = [\"1\", \"2\", \"3\"]\n",
    "print(str(hi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/P76125041/miniconda3/envs/exp-b4/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/workspace/P76125041/miniconda3/envs/exp-b4/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/workspace/P76125041/miniconda3/envs/exp-b4/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_path = \"3\"\n",
    "\n",
    "model = SentenceTransformer(\n",
    "        \"dunzhang/stella_en_400M_v5\", trust_remote_code=True, device=\"cuda:1\"\n",
    "    ).cuda(1)\n",
    "\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_text = \"Which three variants of cross-validation are examined?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "query_prompt_name = \"s2p_query\"\n",
    "model = SentenceTransformer(\n",
    "\"dunzhang/stella_en_400M_v5\", trust_remote_code=True, device=\"cuda\"\n",
    ").cuda()\n",
    "\n",
    "query_embedding = model.encode([question_text], prompt_name=query_prompt_name)\n",
    "para_embeddings = model.encode(paragraphs)\n",
    "# print(query_embedding.shape, para_embeddings.shape)\n",
    "# (2, 1024) (2, 1024)\n",
    "\n",
    "similarities = model.similarity(query_embedding, para_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-b4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
